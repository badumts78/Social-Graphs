issue
Algorithms that should possibly be added to documentation#TITLE_END#I have made an attempt to find functions in `networkx/algorithms` that appear in the `__all__` variable of a `.py` file but are not referenced in the documentation: ``` all_simple_edge_paths all_triads all_triplets betweenness_centrality_source descendants_at_distance edge_betweenness find_cliques_recursive find_cores find_threshold_graph generate_edgelist greedy_coloring_with_interchange is_threshold_graph is_triad maximum_matching _naive_greedy_modularity_communities node_attribute_xy node_degree_xy parse_edgelist project random_triad read_edgelist recursive_simple_cycles transitive_closure_dag triads_by_type triad_type trophic_differences trophic_incoherence_parameter trophic_levels write_edgelist ```  I haven't gone through each of these to check if it should be added to to documentation. I did look at a couple of examples: - `triads_by_type` was mentioned in the recent issue #4104, and it does look as though it should be added. - `algorithms.bipartite.matching.maximum_matching` is just an alias for another function and is mentioned in the text of the documentation, do perhaps it does not need to be added.  The following shows where the functions are defined. (This was created using a simple script. Hopefully it is correct!)  ``` all_simple_edge_paths   networkx/algorithms/simple_paths.py all_triads   networkx/algorithms/triads.py all_triplets   networkx/algorithms/triads.py betweenness_centrality_source   networkx/algorithms/centrality/betweenness_subset.py descendants_at_distance   networkx/algorithms/traversal/breadth_first_search.py edge_betweenness   networkx/algorithms/centrality/betweenness_subset.py   networkx/algorithms/centrality/betweenness.py find_cliques_recursive   networkx/algorithms/clique.py find_cores find_threshold_graph   networkx/algorithms/threshold.py generate_edgelist   networkx/algorithms/bipartite/edgelist.py greedy_coloring_with_interchange   networkx/algorithms/coloring/greedy_coloring_with_interchange.py is_threshold_graph   networkx/algorithms/threshold.py is_triad   networkx/algorithms/triads.py maximum_matching _naive_greedy_modularity_communities   networkx/algorithms/community/modularity_max.py node_attribute_xy   networkx/algorithms/assortativity/pairs.py node_degree_xy   networkx/algorithms/assortativity/pairs.py parse_edgelist   networkx/algorithms/bipartite/edgelist.py project   networkx/algorithms/bipartite/projection.py random_triad   networkx/algorithms/triads.py read_edgelist   networkx/algorithms/bipartite/edgelist.py recursive_simple_cycles   networkx/algorithms/cycles.py transitive_closure_dag   networkx/algorithms/dag.py triads_by_type   networkx/algorithms/triads.py triad_type   networkx/algorithms/triads.py trophic_differences   networkx/algorithms/centrality/trophic.py trophic_incoherence_parameter   networkx/algorithms/centrality/trophic.py trophic_levels   networkx/algorithms/centrality/trophic.py write_edgelist   networkx/algorithms/bipartite/edgelist.py ```  I used the following script to find the functions that appear to be missing from the documentation.  ``` find networkx/algorithms -name "*.py" | xargs cat | tr -d ' \n' | grep -o '__all__=\[["a-z0-9_,]*\]' | grep -o '"[a-z0-9_]*"' | tr -d '"' | sort | uniq > tmp_funcs.txt cat doc/reference/algorithms/*.rst | tr -d ' ' | sort | uniq > tmp_doc.txt comm -2 -3 tmp_funcs.txt tmp_doc.txt  comm -2 -3 tmp_funcs.txt tmp_doc.txt > functions_possibly_missing_from_doc.txt for f in $(cat functions_possibly_missing_from_doc.txt); do echo $f; grep -l -r "def ${f}" networkx/algorithms | grep -v ".pyc" | sed 's/^/  /'; done ```  Would someone be happy to take a look at the list of functions, and possibly to check whether any functions from other modules such as graph generators are missing from the documentation? 
issue
Fix lowest_common_ancestors (issue #4942)#TITLE_END#This fixes the bug in `lowest_common_ancestors` using the [change](https://github.com/networkx/networkx/issues/4942#issuecomment-919675945) proposed by @dschult.  The discussion in issue #4942 includes an extra (naive) algorithm. Should we add this as an additional option, or perhaps just use it for testing?  We should also add a test case to make sure the bug doesn't reappear.
issue
Fix fast_gnp_random_graph for directed graphs (issue #3389)#TITLE_END#This fixes #3389 with the change suggested by @proto-n in that issue.  I have generated hundreds of thousands of graphs to confirm that the edge counts are as expected.
issue
Add max_weight_clique to doc#TITLE_END#I forgot to add the max_weight_clique function (#4016) to the documentation, so I have added it in this pull request. I have made a few minor changes to the function's docstring, such as correcting the syntax for italics and references. I have also also added my name to the list of contributors.  I'd also like to say a quick "thank you" to @dschult and @jarrodmillman for reviewing and merging my code in #4016. This was my first contribution to a large piece of software, and it has been an enjoyable experience.
issue
Add maximum weight clique algorithm#TITLE_END#I have implemented an algorithm for maximum weight clique that is often faster than enumerating all maximal cliques, which I understand to be the current best way to solve the problem in NetworkX. The new algorithm can also be useful for solving the _unweighted_ max clique problem by calling max_weight_clique(G, None).  It would only require a few more lines of code to also cover the maximum (weight) independent set problem, so perhaps that would be worth considering.  This is my first attempt to commit to a large open source project, so hopefully I've done it roughly right!
comment
@harristeague I agree about what you've said about the cause of the problem, and I'm also unsure about what the best solution would be.  My two cents is that the non-determinism seems to be due to the `arbitrary_element()` call on [this line](https://github.com/networkx/networkx/blob/c47964d7ab849d143cd57607a2a6dbbb53bbaf30/networkx/algorithms/flow/preflowpush.py#L232). This just calls `next(iter(...))` on a set, so it's unsurprising (and probably not worrying) that the answer changes from run to run.
comment
#4592 might be related.
comment
I wonder if the max clique function in this PR is still useful compared to `nx.max_weight_clique(G, weight=None)` and `nx.graph_clique_number()`. I did timings for a few random graphs:  ``` def time():     print("maximum_clique_set    ", timeit.timeit("maximum_clique_set(G)", number=1, globals=globals()))     print("nx.max_weight_clique  ", timeit.timeit("nx.max_weight_clique(G, weight=None)", number=1, globals=globals()))     print("nx.graph_clique_number", timeit.timeit("nx.graph_clique_number(G)", number=1, globals=globals()))     print()   G = nx.gnp_random_graph(250, .1) time() G = nx.gnp_random_graph(50, .5) time() G = nx.gnp_random_graph(30, .9) time() ```  with  these results:  ``` maximum_clique_set     0.39374078999389894 nx.max_weight_clique   0.019198676993255503 nx.graph_clique_number 0.014624504998209886  maximum_clique_set     0.24361919799412135 nx.max_weight_clique   0.0033190690010087565 nx.graph_clique_number 0.006590124990907498  maximum_clique_set     0.9121223280089907 nx.max_weight_clique   0.004532831997494213 nx.graph_clique_number 0.006706512998789549 ```  (This uses the version of the function by @jtorrents).  For these three graphs at least, the functions that are in NetworkX already are much faster.  I can still see the usefulness of having separate functions for max independent set and minimum vertex cover. It's also very possible that there are some graphs for which the `maximum_clique_set` function is fast.
comment
An update on the algorithm in the previous comment (from my other GitHub account - sorry for being confusing!):  If you replace the function `get_ancestor_set` in the big code listing above with the following version, the naive algorithm's speed seems to be competitive with the existing algorithm.  ```python def get_ancestor_set(G, v):     """Return a set containing v and all of its ancestors in G"""     ancestor_set = set([v])     to_visit = [v]     while to_visit:         w = to_visit.pop()         for u in G.predecessors(w):             if u not in ancestor_set:                 ancestor_set.add(u)                 to_visit.append(u)     return ancestor_set ```
comment
One last thought on this: the naive algorithm could be further sped up by caching the ancestor list of each vertex. Perhaps this caching feature could be made optional so the user can choose a slower method that requires less memory.
comment
@dschult I've now implemented a memoised version of the `get_ancestor_set()` function. Here are the timings for one run with n=300 and p=0.1:  ``` Current algorithm: 11.491 seconds, 44481 LCAs Naive algorithm with cache: 0.465 seconds, 44674 LCAs Naive algorithm: 5.465 seconds, 44674 LCAs ```  So the cache gives an order of magnitude improvement in speed, at least for this sample of size 1 :-)  As you say, it's unfortunately O(n^2) in memory.  I'm also curious to test your proposed fix for the existing algorithm from [this comment](https://github.com/networkx/networkx/issues/4942#issuecomment-873127745). You mentioned changing line 276 to `my_ancestors = nx.dag.ancestors(dag, v)`. I think this may refer to line 272 https://github.com/networkx/networkx/blob/bcc7a7e42b6e152ac57658e884f5184b33463b0b/networkx/algorithms/lowest_common_ancestors.py#L272 in the current version, but I couldn't figure out what needed to be changed?
comment
I'm afraid I haven't got my head round the Bender algorithm yet, but the good news is that when I changed line 272 to the version you suggest, it always agrees with the naive algorithm.  The naive approach seems to scale fine - staying about an order of magnitude faster than Bender - although I haven't tried it on anything that takes more than about 10 seconds to solve so I should probably do a larger experiment!
comment
Sorry to be putting so many code listings in this thread! Below is the latest version of my naive algorithm implementation, for the record. I think there might even be a citation on Wikipedia to a paper that describes this kind of algorithm.  I'd like to do a proper pull request with a tidied-up version of this algorithm (perhaps as an additional option for a lowest common ancestor algorithm), but I don't have time at the moment. So it would be fantastic if someone else would like to do the PR. Otherwise, I might have time next summer when my PhD is finally submitted!  ```python import random import time import networkx as nx  def find_successor(G, v, S):     """Find a successor of v that is in the set of nodes S"""     for w in G.successors(v):         if w in S:             return w     return None  def get_a_lowest_common_ancestor(G, common_ancestors):     """Return a lowest common ancestor from the given set of common ancestors"""     # Start with an arbitrary node v from the set of common ancestors.  Follow     # arbitrary outgoing edges, remaining in the set of common ancestors, until     # reaching a node with no outgoing edge to another of the common ancestors.     v = next(iter(common_ancestors))     while True:         successor = find_successor(G, v, common_ancestors)         if successor is None:             return v         v = successor  def get_ancestor_set(G, v, ancestor_cache):     """Return a set containing v and all of its ancestors in G"""     if ancestor_cache is not None:         if v in ancestor_cache:             return ancestor_cache[v]     ancestor_set = set([v])     to_visit = [v]     while to_visit:         w = to_visit.pop()         for u in G.predecessors(w):             if u not in ancestor_set:                 ancestor_set.add(u)                 to_visit.append(u)     if ancestor_cache is not None:         ancestor_cache[v] = ancestor_set     return ancestor_set  def all_pairs_lowest_common_ancestor(G, pairs=None, use_cache=False):     """Return the lowest common ancestor of all pairs or the provided pairs"""     result = []     ancestor_cache = {} if use_cache else None     if pairs is None:         for i, v in enumerate(G.nodes()):             v_ancestor_set = get_ancestor_set(G, v, ancestor_cache)             for j, w in enumerate(G.nodes()):                 if j > i:                     break                 w_ancestor_set = get_ancestor_set(G, w, ancestor_cache)                 common_ancestors = v_ancestor_set & w_ancestor_set                 if common_ancestors:                     result.append(((v, w), get_a_lowest_common_ancestor(G, common_ancestors)))     else:         for v, w in pairs:             v_ancestor_set = get_ancestor_set(G, v, ancestor_cache)             w_ancestor_set = get_ancestor_set(G, w, ancestor_cache)             common_ancestors = v_ancestor_set & w_ancestor_set             if common_ancestors:                 result.append(((v, w), get_a_lowest_common_ancestor(G, common_ancestors)))     return result ```
comment
I'm not an expert on this part of NetworkX, but I've had a quick look at this. The two graphs you give are isomorphic, and each has three connected components. Using the `master` branch of NetworkX from GitHub, I tried `nx.is_isomorphic(source_graph, target_graph)` and using `GraphMatcher` the way you did. Each finished in less than a second. So perhaps the functions have been improved since the version of NetworkX you are using?
comment
No probs! But the graphs don't seem to have changed. Can you post the updated ones as a new comment?
comment
Oh I see, the problem is in these lines  ``` target_graph.add_nodes_from(source_graph_nodes) target_graph.add_edges_from(source_graph_edges) ```  :-)
comment
They do seem to be isomorphic?  ![source](https://user-images.githubusercontent.com/4913181/85704534-48df3400-b6d8-11ea-9f69-ac57890c5c4a.png) ![target](https://user-images.githubusercontent.com/4913181/85704556-51376f00-b6d8-11ea-824b-5641b179ae03.png)
comment
Or not. Oops.
comment
It looks like VF2 doesn't handle connected components well, but I don't know enough about the algorithm to say why. You can definitely speed it up by splitting the graphs into connected components. The code below is hopefully correct and runs quickly.  I wonder if something like this should be added to NetworkX?  ``` def find_index_of_isomorphic_component(cc1, G2_cc):     # cc1 is a graph, and G2_cc is a list of graphs     # Returns the index of a graph in G2_cc to which cc1 is isomorphic, or     # None if no such graph exists in G2_cc     for i, cc2 in enumerate(G2_cc):         if (nx.is_isomorphic(cc1, cc2)):             return i     return None  def is_isomorphic_using_components(G1, G2):     # G1_cc and G2_cc hold components of source and target graphs     G1_cc = [G1.subgraph(c).copy() for c in nx.connected_components(G1)]     G2_cc = [G2.subgraph(c).copy() for c in nx.connected_components(G2)]     if len(G1_cc) != len(G2_cc):         return False     for cc1 in G1_cc:         cc2_index = find_index_of_isomorphic_component(cc1, G2_cc)         if cc2_index is None:             # No remaining component of G2 is isomorphic to cc1             return False         del G2_cc[cc2_index]     return True  print(is_isomorphic_using_components(source_graph, target_graph)) ```
