issue
Integration of Ben Edwards' GSoC project 2011 on community detection#TITLE_END### Incomplete Pull Request  This pull request is incomplete and it's meant to discuss pulling @bjedwards' GSoC project on community finding and other related code into the main networkx repository. ## General Remark  There have been several discussions on the mailing list showing interest in replacing the basic Graph data structure with custom ones, e.g., using a database instead of the dict of dict model. Should we rely on particular class interfaces, for example, Graph.adjacency_iter or DiGraph.predecessors_iter, instead of accessing the dictionary structures directly in order to facilitate other data models?  Direct access of the dictionary structure is extremely widespread throughout networkx; one could argue that any customized data structure would have to expose a dictionary-like interface anyway because a rewrite of networkx is not worth the time and effort. ## Observations - spectrum.py and laplacian_spectral.py are disorganized and seem to serve   similar purposes. Can we reorganize it? - similarly quality.py and community_quality.py isn't all the code community   related? ## Related Issues  The GSoC project issue #549 (used to be Trac 557), other related issues are an algorithm based on Newman(2006) PNAS #160 (Trac 158), the Louvain method #239 (same on Trac), and a fast detection algorithm #245 (same on Trac). 
issue
add setup for networkx-matrix code#TITLE_END#Branch to write and discuss matrix graph implementations plus specialized algorithms. 
issue
matrix graphs#TITLE_END#Dear all,  As a side note, when I talk about a graph here I mean `Graph`, `DiGraph`, `MultiGraph` and `MultiDiGraph`.  I want to start a discussion here about the use for dense and sparse matrix based graph classes. The reasoning for that is recent work by @FedericoV on directed clustering and my own need for a sparse matrix representation. So far the linear algebra based functions each convert a graph instance into a desired matrix. Chances are high, though, that if I want to work with one of those functions, I will use others as well and it'd be more convenient/consistent to have one matrix based graph to pass to each of them.  We've talked about designing different specialized graph classes in #980 and I would like design dense and sparse matrix based graph families. Using `numpy` and `scipy` seems obvious here since we can leverage, for example, the [csgraph](http://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html) and we can also have a look at [APGL](http://pythonhosted.org/apgl/index.html) (although I think not all of its code is correct). My goal would be to have classes that _feel_ almost like the normal classes and matrix based functions that follow the same naming convention and, hopefully, parameters, as the in the standard `networkx` package.  My main question is whether these classes should live inside `networkx`, be an off-shoot like the scikits-... or an entirely separate package.  Cheers 
comment
Is this generally the case in networkx? Cause in the literature it's often an edge from j to i. On Jan 17, 2015 11:36 PM, "Dan Schult" notifications@github.com wrote:  > Merged #1335 https://github.com/networkx/networkx/pull/1335. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1335#event-220567056. 
comment
I think what's needed to merge this and other similar projects in future, is a definitive decision on what the structure returned by such algorithms should look like. If I remember correctly the louvain code returns a dictionary, where keys are module indices and values are lists with the respective nodes. A list of sets with nodes belonging to one cluster also seems a sensible structure. I don't know if there any preferences out there but I strongly believe that a unified return structure should be implemented in order to make the detection algorithms interchangeable. 
comment
I recently could make good use of the Special\* classes. Thank you for the implementation @dschult! When using the _thin_ graphs I noticed that the example with the lambda makes the graphs unpicklable, so I wrote a tiny class instead:  ``` python class AttrFactory(object):     def __init__(self, attr, **kw_args):         super(AttrFactory, self).__init__(**kw_args)         self._attr = attr      def __call__(self):         return self._attr ```  Which can then be used, for example, in the following way:  ``` python simple = SpecialDiGraph(edge_attr_dict_factory=AttrFactory(dict())) ``` 
comment
> Does this class have advantages over a named function? Perhaps it saves a lookup?  The only other way I see to do this with a function is with a global variable (or module level global), which seems less intuitive to me, or with a variable at definition time of the function via a default argument, which is too inflexible. If there's an easier and better way I'm all ears. 
comment
What I prefer about the class is it's pickling behavior. The edge data is pickled with the class instance and if the class definition, i.e., the module, is available at unpickling time then this works _out-of-the-box_.  In order to use a function, in your first example, it (1) also needs to be defined in an importable module and (2) I need to define the global variable before unpickling. This seems problematic to me if I want to unpickle some network and I first need to define a global variable of a specific name with a specific value.  Your second example should probably read:  ``` python def shared_edge_factory(attr_dict):     def factory():          return attr_dict     return factory ```  and cannot be pickled at all:  ``` PicklingError: Can't pickle <type 'function'>: attribute lookup __builtin__.function failed ``` 
comment
Ah yes, `dill` the far superior choice. 
comment
Just a note, I went back to using the class since stdlib modules like `multiprocessing` use pickle to transmit data. 
comment
> Do we have some reasonable graph and a rigorous algorithm to test the speed of different designs? I typically think of single_source_shortest_path() on a random graph with n=1000 nodes and p=0.2 prob of edges. There is also the benchmark.py file in the repo but it is mostly speed-testing specific important methods in isolation. Other suggestions?  I don't have suggestions for benchmark situations or algorithms right now but I strongly believe that `DiGraph`s, `MultiGraph`s and some type of broad degree distribution need to be included in order to avoid surprises.  P.S.: You can easily surround those code blocks with  <pre> ```python # code ``` </pre>  for better legibility. 
comment
Personally, I'm very interested in this as I work in bioinformatics myself. I'd be happy to review the code, tests and documentation. I can't make the call on whether and where this should go into networkx, however. 
comment
That's gonna be one hell of a review... Things I noticed so far: Can you configure the behavior of autopep8? I really dislike the opening parenthesis and the next line to be on the same level, it potentially wastes so much horizontal space, for example (the indentation here is a bit wonky),  ``` python variable = call_to_a_really_long_function_name_as_often_in_networkx(                                                                     a=1, b=2) ```  I much prefer one or two levels of indentation.  Networkx uses the one letter variable name `G` all over the codebase. This violates two principles of PEP8: variables should be lowercase, variables should be descriptive. So maybe something like `graph` or `net` would be a better fit. As far as I can tell autopep8 did not touch this issue at all. 
comment
I'm with @jfinkels on this one. There's a [nice blog post](http://blog.ionelmc.ro/2014/05/25/python-packaging/) that has a lot of the details of the points that have come up here. The author also [created a repository](https://github.com/ionelmc/cookiecutter-pylibrary) that can be used with `cookiecutter` in order to set up a package structure. It's a lot to take in at first with the integration of travis, coverage, bumpversion and appveyor but networkx has most of these set up and running already, so it wouldn't be too deep of a dive.  Anyways, have a read of the blog post, it convinced me, at least.  @chebee7i Learn to love [tmux](http://tmux.sourceforge.net/) :wink:  
comment
Kinda late to the table but I was wondering whether the same structure/shallow/deep logic should be applied to `subgraph`? 
comment
While it's a nice implementation of the operation, I think implementing [edge](http://mathworld.wolfram.com/EdgeContraction.html) and [vertex contraction](http://mathworld.wolfram.com/VertexContraction.html) is not that hard. What I'd like to see along side it is an algorithm that identifies all such edges or vertices that can safely be contracted. If you can dig up some references for that, I'm happy to implement it. 
comment
Did some digging around and GraphChi ([C++](https://github.com/GraphChi/graphchi-cpp) and [Java](https://github.com/GraphChi/graphchi-java) implementations) seem to have some promising algorithms. I'm not sure what the licenses are for those projects but I'm sure we could contact the authors about it. 
comment
Just to chime in quickly: This Friday I will submit my thesis, yay! So I'm definitely planning to take up work again on #1076 or could potentially mentor someone else who wants to take a stab at it. I'm not sure I qualify as a mentor, though, I've never been part of GSoC.  Another project idea: I've recently started porting some of my own projects from Python 2 to being 2&3 compatible. Packages like `six` and `future` make this quite comfortable and imho a single code base is the best way forward. I think this would go hand-in-hand with the 2.0 API and maybe a general PEP8 clean-up of `networkx`. What do you think? 
comment
I have some time on my hands but have not been following the details. How exactly can I help? 
comment
@MridulS thank you for your work on the wiki page, looks great!  Just wanted to mention that for Boost.Graph there is the excellent [graph-tool](http://graph-tool.skewed.de/).   Although there are definitely synergies with the 2.0 API, I wouldn't put using adjacency matrices strictly in the backend corner. The main appeal to me is the potential to then use powerful linear algebra on those matrices. That means that beyond a different storage format, these matrix based graphs will re-implement a number of functions that can be replaced by matrix/vector operations.  Still, I think a GSoC project for it would be great. I'd classify it as hard. I'd definitely mentor for it and I would also mentor a single source base Python 2&3 compatible implementation of the new API. 
comment
> isn't NetworkX already a single source base for 2/3?  True but there have been complaints about the inefficiency of `range` in Python 2, for example. Also, `dict.items` and similar are implemented as iterators for Python 2 by `six` and `future`. Nothing big but a simple `from builtins import range` at the top of most modules might already be all that's needed. 
comment
@ysitu done 
comment
@MridulS I agree that there are definitely parallels between the different project proposals and that designing new graph classes will depend on the new API. However, I'd be cautious about taking on too much work at the same time. The new API not only includes changes to the classes but also requires adjusting the entire code base. Not always the most challenging work but definitely a lot of it. So I would wait for what applications we get and if noone applies for the matrix classes and you happen to be done early with your project then we can definitely take it on. 
comment
>    Add algorithms for directed triangles. Clustering #1331, Triadic census #191, ... >  >    Add community finding algorithms. A previous GSoC project made some progress on this #764. There have been other contributions since #951, #1092 (Louvain) and #1265 (Girvan-Newman), #617 (label propagation)  I remember starting work on the integration but then waiting for the spectral code to be integrated. I can take a look again, modularity is certainly an important topic somewhat missing from networkx  >    Rework the matplotlib drawing package as a separate package from networkx #1325, with a more sensible interface, proper arrows #1248, #1357, #418. >  >    Implement multilevel graph layout algorithms (force-directed, eigenvector) >  >    Create IPython notebook based tutorials and examples  I'd love to have/create more of those. I haven't checked recent progress on https://github.com/jdfreder/ipython-d3networkx maybe @jdfreder can comment. It'd be amazing to have those visualizations available.  >    Incorporate or interface to http://www.graphclasses.org/smallgraphs.html >  >    Implement more shortest path algorithms (or add features) #1120. k-shortest paths #793, restricted paths #480, etc >  >    Add network metrics for weighted networks #730 >  >    Add more algorithms and support for bipartite graphs #1294, #1322 >  >    Better XML format readers with SAX #448  Although I love lxml, it's a nasty dependency to add. So I agree that SAX is the way forward.  >    Create new web site with design and logo. Improve documentation web site #1221  Not my forté but important nonetheless. 
comment
Although I agree that it'd be really exciting to meet everyone in person and have a sprint or just a chat, I will not be able to attend. I was able to go to EuroScipy once but would have a hard time convincing my current employer that I should go to Austin.  Definitely a separate issue/mailing list discussion: The IPython team has regular hangouts which especially with GSoC coming up could be a nice idea and would ameliorate somewhat that we all live in different countries.  
comment
My preferred usage (where I have control over the code) would be the explicit import of an add-on. I really dislike overwriting existing functions (hi R). As you mention it is difficult to ensure that you shadow all networkx versions of a function and it is the opposite of explicit (imho).  What I sometimes do is an alternative import, for example,  ``` python try:     import simplejson as json except ImportError:     import json ```  where those modules provide the exact same functionality. Of course, here the problem is that networkx has everything in the top level module, otherwise this could be as simple as:  ``` python try:     import networkx.addon as util except ImportError:     import networkx.util as util  result = util.some_function ```  Just my $ 0.02. 
comment
I will be gone until Sunday evening (GMT +1) so I'll probably miss most of the discussion. I think 2 slots is the safe number. Three might be possible if we do a "floating" mentoring where each one of us feels responsible for all projects. On the upside I expect many of the community to comment on code anyway, on the downside it's less clear for the students and ourselves who the immediate contact partner is and who is responsible in the end.  
comment
I signed up both on melange and for the form and was accepted as mentor. I'm a little unclear about how we'll approach the selection process. Any recommendations? 
comment
@ysitu that's the way I thought about the projects. I can definitely confirm that I'd like to mentor projects related to backends and linear algebra. 
comment
Hey everyone, what's our preferred way of reviewing proposals going to be? Comments and ratings on Melange, private gitter chatrooms per proposal or public issues per proposal? Something else entirely?  We have until April 13 to decide on the minimum and maximum number of slots and about a week after that we have to make final calls on the applications and decide on allocating mentors to students. 
comment
As mentioned in #1446 I will be away for the next few days. So yes, I forgot about the -48hrs for Terri :smile: I might be able to comment quickly on Friday but I think 2 slots is probably the most realistic. 
comment
You can always pull in the tutorial repository using git subtree or sub-module if you're worried it will be missed otherwise. I think it's an excellent idea because it also makes contributing much less daunting than working with the main repository. 
comment
Hi, I just made a pull request to your pull request :smile: here: https://github.com/rnelsonchem/networkx/pull/1  It goes a little further with this idea. First of all, it removes the `mask_zero` and `mask_nan` arguments. I think, this is better done outside of the function. Just pass an appropriately modified `pandas.DataFrame` to the function, e.g.:  ``` python df = pd.DataFrame(...) nx.from_pandas_dataframe(df.loc[df["junk_column"].notnull() & df["important_column"] != 0]) ```  Second, you can now specify the columns of the source and target node (for a directed graph). Additionally, you can give the column name(s) that are to be used as edge attributes. At the moment the edge attributes are the same name as the column names.  This is a concept only. I did not fully test this, nor adjust the tests accordingly. What do you think? 
comment
OK, my bad.  It's fairly easy.  ``` python from operator import itemgetter  src_i = df.columns.index(source) tar_i = df.columns.index(target) indeces = [df.columns.index(attr) for attr in edge_attr] get_attr = itemgetter(indeces)  for row in df.itertuples(index=False):     g.add_edge(row[src_i], row[tar_i], **{name: attr for (name, attr) in zip(edge_attr, get_attr(row))}) ```  This method would also allow you to rename the edge attributes if you pass another list with names.  I'll test which method is faster. It might be worth keeping the overhead of iterating over row `Series`. 
comment
> I disagree that we ultimately want to have this function change the column names. I think it is easy enough to change the DataFrame names before passing it here. You made the point earlier that we should expect the users to check for zeros and NaNs.  I completely agree with you. The data frame should be prepared beforehand.  Looks good from my point of view. 
comment
@FedericoV: I would prefer returning the whole list of coefficients for the random networks. Then you can use whatever statistical insight you would like. Also, using an ER-like graph as a null model for any type of graph passed in seems... bad, for lack of a better word right now. 
comment
That is probably the best option. That way someone could even employ switch randomization if they wanted to. 
comment
Somehow this problem screams metaclass to me but since you actually want different classes to avoid confusion, this seems like an elegant solution. 
comment
I think it comes down to whether you want (a) different back ends to be clearly differentiated by their respective classes or (b) rather to instantiate the `Graph` class with different back ends. A metaclass is a really good solution for (b) imho. Either from a keyword argument during instantiation or as you mentioned from a global option.  I would rather do something as outlined by @dschult in [his comment](https://github.com/networkx/networkx/pull/980#issuecomment-32137249), though, as it seems easy to define new classes (it has to be made clear which class variables to replace) and it is explicit as they inherit from the `Graph` class rather than obscurely replacing some variables. Also, I think it's important that this can be done interactively rather than defining the back end in a configuration file. 
comment
What I will suggest is probably a ton of work in the code base but I just want to put the thought out there: If the algorithms were changed to rely only on certain iterators, and it would require **careful** debate which iterators are the minimum requirement for each graph class (`Graph`, `DiGraph`, `MultiGraph`, `MultiDiGraph`), each new class could implement these iterators in the most suitable way possible for the underlying data structure.  So, for example, from the tutorial:  ``` python >>> FG=nx.Graph() >>> FG.add_weighted_edges_from([(1,2,0.125), (1,3,0.75), (2,4,1.2), (3,4,0.375)]) >>> for n,nbrs in FG.adjacency_iter(): ...    for nbr,eattr in nbrs.items(): ...        data=eattr['weight'] ...        if data<0.5: print('(%d, %d, %.3f)' % (n,nbr,data)) ```  this code clearly relies on `nbrs` being a `dict` (or at least providing the `items` method) but could easily be changed to use `edges_iter(data=True)`.  I don't know the code base well enough to be able to say what other examples of `dict` uses are out there but there are probably a lot of them. Even though it is a lot more work (and possibly slightly slower) using only a given interface of the graph classes would make it much cleaner and easier to drop-in new classes. 
comment
Silly question: You have seen [this drawing function](http://networkx.github.io/documentation/networkx-1.9.1/reference/generated/networkx.drawing.nx_pylab.draw_networkx_nodes.html#networkx.drawing.nx_pylab.draw_networkx_nodes)? 
comment
I'm sorry if you asked for this functionality before, I must have missed it. Both @bjedwards and myself have implemented the modularity maximization (also for the directed case) before. You can find his [GSoC project](https://bitbucket.org/bedwards/networkx-community) and [my own code](https://github.com/Midnighter/Everyday-Utilities/blob/master/meb/utils/network/algorithms.py).  Both are dated and not fully integrated so it's great that you're issuing this pull request. I don't have the time right now to compare the code but maybe it'll be helpful. Cheers. 
comment
What @hagberg means is that you can create a degree sequence from a power-law with given exponent and then use the function [expected_degree_graph](https://networkx.github.io/documentation/latest/reference/generated/networkx.generators.degree_seq.expected_degree_graph.html#networkx.generators.degree_seq.expected_degree_graph) to generate a random graph from that sequence.  @AgostinoSturaro The documentation for the [configuration model](https://networkx.github.io/documentation/latest/reference/generated/networkx.generators.degree_seq.configuration_model.html#networkx.generators.degree_seq.configuration_model) basically has the answer that you're looking for. It uses the utility function [powerlaw_sequence](https://networkx.github.io/documentation/latest/reference/generated/networkx.utils.random_sequence.powerlaw_sequence.html#networkx.utils.random_sequence.powerlaw_sequence) to construct a degree sequence and then creates a graph using the configuration model. Basically, you can provide an arbitrary distribution (from the module random, numpy, scipy or whatever) to [create_degree_sequence](https://networkx.github.io/documentation/latest/reference/generated/networkx.utils.random_sequence.create_degree_sequence.html#networkx.utils.random_sequence.create_degree_sequence) and create a graph from that sequence. Note, however, that the configuration model potentially introduces self-loops and parallel edges. 
comment
If you read the examples for the [configuration model](https://networkx.github.io/documentation/latest/reference/generated/networkx.generators.degree_seq.configuration_model.html#networkx.generators.degree_seq.configuration_model) that I gave you, you will see that it shows you exactly how to do what you want. 
comment
Please also take a look at my [notebook](http://nbviewer.ipython.org/gist/Midnighter/248f1a5d8c21b39525ae). 
comment
There's already a large collection of examples that could easily be converted to notebooks. Definitely an easy project for GSoC. 
comment
It's for small graphs only but there is the [graph atlas](http://networkx.github.io/documentation/networkx-1.9.1/reference/generated/networkx.generators.atlas.graph_atlas_g.html#networkx.generators.atlas.graph_atlas_g). It needs to be imported separately:  ``` python from networkx.generators.atlas import graph_atlas_g ``` 
comment
Which versions are you using? I'm on graphviz version 2.38.0 and pygraphviz 1.3rc2. I can easily implement the example from that post:  ``` python In [1]: import pygraphviz as pgv  In [2]: g = pgv.AGraph()  In [3]: g.add_edge(1, 2)  In [4]: g.add_edge(2, 3)  In [5]: h = g.subgraph([2, 3], name="funny")  In [6]: print(g.string()) strict graph {         subgraph funny {                 2 -- 3;         }         1 -- 2; } ```  Does this work for you or can you explain your problem in more detail? 
comment
(Almost) Verbatim copy from my mailing list post. In general open questions like this are better discussed there.  > That depends a lot on the file format you would like to store them as. You can easily pickle a list of many graphs, for example. Although it's a little bit of a cheat you could also generate a zip or tar archive that contains multiple other files. >  > So what's your preferred format? 
comment
Since you decided to open the wiki, it'd be a good place to list the labels in use and describe their intended meaning. 
comment
``` python import matplotlib.pyplot as plt fig = plt.figure() ax = fig.add_subplot(2,1,1) import numpy as np t = np.arange(0.0, 1.0, 0.01) s = np.sin(2*np.pi*t) line, = ax.plot(t, s, color='blue', lw=2) ax.lines[0] ```  What if you add `plt.show()` at the end? Does that work? 
comment
If I interpret the discussion in #980 correctly, then replacing the backend without creating a new class is actually undesired. 
comment
You may have to adjust the `setup.py` script to also install data files and not just Python scripts. 
comment
You could just iterate over the nodes:  ``` python for node in your_digraph:     in_deg = your_digraph.in_degree(node)     out_deg = your_digraph.out_degree(node)     # do something ```  or using comprehensions:  ``` python for (in_deg, out_deg) in ((your_graph.in_degree(node), your_graph.out_degree(node) for node in your_graph):     # do something ``` 
comment
Look at the source code for the functions they are similar to:  ``` python for node in your_digraph:     in_deg = len(your_digraph.pred[node])     out_deg = len(your_digraph.succ[node]) ```  just a little more fancy to yield generators. 
comment
I was thinking about that, too, but I wasn't sure the order of the two iterators is guaranteed, so you might end up with the in-/out-degrees of different nodes, no? 
