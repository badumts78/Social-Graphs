comment
I'm in favour of using an algorithm from the literature - either a corrected version of the Bender algorithm or an algorithm from another paper. But I was thinking that it might be a good idea to have a naive implementation too, for testing the main algorithm and perhaps also as a stop-gap until we have a correct, fast implementation.  This is the implementation I came up with (with `all_pairs_lowest_common_ancestor()` being the entry point). I based this on the brute-force algorithm [described in Wikipedia](https://en.wikipedia.org/wiki/Lowest_common_ancestor#Extension_to_directed_acyclic_graphs).  ```python import random import time import itertools import networkx as nx  def find_successor(G, v, S):     """Find a successor of v that is in the set of nodes S"""     for w in G.successors(v):         if w in S:             return w     return None  def get_a_lowest_common_ancestor(G, common_ancestors):     """Return a lowest common ancestor from the given set of common ancestors"""     # Start with an arbitrary node v from the set of common ancestors.  Follow     # arbitrary outgoing edges, remaining in the set of common ancestors, until     # reaching a node with no outgoing edge to another of the common ancestors.     v = next(iter(common_ancestors))     while True:         successor = find_successor(G, v, common_ancestors)         if successor is None:             return v         v = successor  def get_ancestor_set(G, v):     """Return a set containing v and all of its ancestors in G"""     ancestor_set = set([v])     for _, w in nx.bfs_edges(G, v, reverse=True):         ancestor_set.add(w)     return ancestor_set  def all_pairs_lowest_common_ancestor(G, pairs=None):     """Return the lowest common ancestor of all pairs or the provided pairs"""     result = []     if pairs is None:         for i, v in enumerate(G.nodes()):             v_ancestor_set = get_ancestor_set(G, v)             for j, w in enumerate(G.nodes()):                 if j > i:                     break                 w_ancestor_set = get_ancestor_set(G, w)                 common_ancestors = v_ancestor_set & w_ancestor_set                 if common_ancestors:                     result.append(((v, w), get_a_lowest_common_ancestor(G, common_ancestors)))     else:         for v, w in pairs:             v_ancestor_set = get_ancestor_set(G, v)             w_ancestor_set = get_ancestor_set(G, w)             common_ancestors = v_ancestor_set & w_ancestor_set             if common_ancestors:                 result.append(((v, w), get_a_lowest_common_ancestor(G, common_ancestors)))     return result   # DAG generator based on https://stackoverflow.com/a/13546785/3347737 n = 200 p = 0.2 random.seed(1) G=nx.gnp_random_graph(n,p,directed=True) G = nx.DiGraph([(u,v) for (u,v) in G.edges() if u<v])  start_time = time.time() lca_count = len([item for item in nx.all_pairs_lowest_common_ancestor(G)]) end_time = time.time() print("Current algorithm: {:.3f} seconds, {} LCAs".format(end_time - start_time, lca_count))  start_time = time.time() lca_count = len([item for item in all_pairs_lowest_common_ancestor(G)]) end_time = time.time() print("Naive algorithm: {:.3f} seconds, {} LCAs".format(end_time - start_time, lca_count)) ```  Here are results from one run: ``` Current algorithm: 2.462 seconds, 20011 LCAs Naive algorithm: 3.409 seconds, 20040 LCAs ```  So, the algorithm I've pasted above is just a little slower and finds some extra lowest common ancestors, at least on this one graph.  Let me know if you'd like me to put this in a pull request in some form.  But to reiterate, I do think a published algorithm would be better. (And I'd love to know what the correct version of the Bender algorithm is.)  
comment
I've only had a very quick look at the paper you linked to @mamonu, but one question that springs to mind is whether the [-1,1] interval is just something the authors observed for graphs they looked at rather than something that is true for all graphs. Certainly the wording "Moreover, values of [omega] are restricted to the interval -1 to 1 regardless of network size." makes it sound like a general thing, but the definition of omega makes me wonder. Is there a proof in the paper that I've missed?
comment
You could also see if you get close to some of the paper's values for the karate graph (`nx.karate()`) from the first row of Table 1.
comment
... I got 0.35 for the karate graph, whereas the paper says 0.08. But it might just be down to random variation.
comment
I've had a little look at `algorithms/smallworld.py`, and noticed the following things that may be worth checking. (Unfortunately I don't have time to check them properly, and will have to stop looking any further at this issue.)  - C is calculated using transitivity in NetworkX but using the clustering coefficient in the paper. These can be very different (http://pages.stat.wisc.edu/~karlrohe/netsci/MeasuringTrianglesInGraphs.pdf) - The `niter` and `nrand` parameters perhaps don't match the paper? I haven't checked this properly. - I also noticed that the NetworkX implementation of *sigma* uses the `random_reference` function. It might be worth checking whether this unusual random graph generator is the standard way to calculate sigma.
comment
@fabriz-io From a quick read, this looks like really great work! Unfortunately I don't have time time at the moment to look carefully through what you've done; I'll need to leave that for others.
