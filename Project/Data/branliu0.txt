issue
Barabasi-Albert Graph not showing power law degree distribution#TITLE_END#It seems that the Barabasi-Albert generator isn't producing graphs with a power law degree distribution:  ![image](https://f.cloud.github.com/assets/237647/2493978/eac4ce6c-b2af-11e3-85f9-17ccd64c3ddd.png)  Am I interpreting something incorrectly?  The networkx implementation of BA starts with m _unconnected_ nodes. It sounds like maybe the first _m_ nodes are supposed to be connected in some manner?  Sources: 1. http://en.wikipedia.org/wiki/Barabasi-Albert_model -- see first sentence under section Algorithm. It says "the network begins with an initial connected network of m0 nodes." 2. http://barabasilab.neu.edu/networksciencebook/download/network_science_december_ch5_2013.pdf -- page 8, it says "We start with m0 nodes, the links between which are chosen arbitrarily, as long as each node has at least one link" 3. http://stackoverflow.com/questions/10622401/implementing-barabasi-albert-method-for-creating-scale-free-networks -- the answer says that the first m0 nodes are completely connected. But of course, this is just an SO answer with no reference.  My brief attempt to redo the BA graph model with m0 being completely connected doesn't seem to produce a power law degree distribution either:  ![image](https://f.cloud.github.com/assets/237647/2493995/b4f34510-b2b0-11e3-928e-cde2fab4b14c.png)  Any ideas here? 
issue
Bug in personalized PageRank#TITLE_END#I believe that personalized PageRank is not working properly in graphs that have dangling nodes, i.e., nodes that do not have any outgoing edges.  One set of the culprit lines are in pagerank_alg.py, Lines 198-200 (https://github.com/networkx/networkx/blob/master/networkx/algorithms/link_analysis/pagerank_alg.py#L198). The lines are reproduced here:  ``` py dangling=np.where(M.sum(axis=1)==0) for d in dangling[0]:     M[d]=1.0/n ```  It appears to add a constant-weighted edge from the dangling node to all other nodes. This makes sense in the default case when the personalization vector is just 1/alpha for all nodes, but it does not make sense when we have a specified personalization vector.  For example, consider the following graph:  ``` py g = nx.DiGraph() g.add_edge(1, 2) ```  If we use a personalization vector of (0, 1), then all the pagerank weight should be on the second node, because there is no "source of rank" (quoting from Brin et al.) for the first node, and thus the second node acts like a "rank sink."  However, using networkx returns:  ``` nx.pagerank_numpy(g, personalization={1: 0, 2: 1}) Out[17]: {1: 0.29824561403508776, 2: 0.7017543859649122} ```  with the exact same results for `nx.pagerank` and `nx.pagerank_scipy`.  The solution to this should be to add on the personalization vector _first_, which automatically takes care of all dangling nodes. Then we can normalize the nodes afterward. In particular, it should go after Line 210: https://github.com/networkx/networkx/blob/master/networkx/algorithms/link_analysis/pagerank_alg.py#L210  So why wasn't this caught by tests? It's quite interesting to note that the only test we have for personalization (https://github.com/networkx/networkx/blob/master/networkx/algorithms/link_analysis/tests/test_pagerank.py#L93) uses a _complete graph_, rather than the graph used for all other test cases, which includes a dangling node.  If others agree that this is a problem, I'd be more than happy to submit a pull request for fixing this. 
issue
Specify a distribution for dangling node out-edges in PageRank#TITLE_END#The primary motivation is that in the previous code, if you wanted to specify a personalization distribution, a uniform distribution would still be assigned to dangling nodes (weighted with probability 1 - alpha). People may want to use the same personalization distribution to patch up dangling nodes, or they may even want to do something entirely separate (who knows...). This pull request allows you to specify that distribution for dangling edges.  This addresses #978 and #960.  (1) First fixed files to conform to PEP8 standards (these are in separate commits) (2) Patched `pagerank`, `google_matrix`, `pagerank_numpy`, and `pagerank_scipy` to allow for the dangling node distribution as a parameter. Right now it defaults to a uniform distribution if not specified. (3) Also added a few new tests to cover the new code.  **Question**: We may actually want the default to be the personalization vector, rather than the uniform distribution. (Note that when the personalization vector is not specified, it will still be uniform). What do you guys think? I think this might be a preferred behavior. 
comment
I'm not entirely certain, but I'm pretty sure this data is incorrect.  For one thing, if you look at my analysis in issue #978, you'll see that in the `google_matrix` function, instead of adding `ev^T` to as is described in your quoted text, we are adding `e/n` to dangling nodes, and then re-adding `ev^T` to all nodes again afterward.  In particular, for your graph (which is the same as the one I used in #978, so there's no confusion), I'm pretty sure that we should have  ``` py In: nx.google_matrix(g, personalization={1: 0, 2: 1}) Out: Matrix([[ 0. , 1. ],              [ 0. , 1. ]]) ```  I could be wrong, but I'm interpreting the output of `google_matrix` as a matrix `M` where `M_ij` is the probability of moving from node i to node j during PageRank's random walk. Given our only edge (1, 2) and personalization vector (0, 1), we should always move from node 1 to node 2, and once we are at node 2, we should stay there.  Based on this reasoning, I also think that the PageRank score with the personalization vector (0, 1) should be (0, 1), where node 1 gets a score of 0 because a random walk will not spend any time at node 1, and node 2 gets a score of 1 because a random walk ends up spending all of its time there.  Does that makes sense? 
comment
Fair enough -- I just a took a closer look at Langville & Meyer pg. 12, and I think the key is that technically the personalization vector `v` should have all strictly positive entries (even in the part you produced inline, it says `v^T > 0`), such that adding `ev^T` will always result in an irreducible matrix. However, right now for rows with all zero entries, we first add `e^T/n` and then we add `(1 - a)v^T` to that row again later. I'm thinking we should just add `v^T` to those dangling rows once. That way when the random walk hits a dangling node, it will actually go to the "restart distribution"/"personalization distribution" on the next step -- in the way we have it now, it goes to any node at uniform with probability alpha, and goes to the personalization distribution with probability only (1 - alpha). What do you think?  Part of what brought this to my attention, though, is that I wanted to add a personalization vector that will have entries equal to zero, but that I know will not result in a reducible matrix. I would've liked to use the pagerank function, but I had to end up writing my own version of the pagerank function. Do you think there's a way to allow for this as well? 
comment
Bump. Thoughts? If something I said isn't clear, I'm happy to try and clarify. 
comment
I'm willing to send a pull request for this. When you say add a warning, do you mean to put it in the docstring of the function? 
comment
Will do -- might be a few days since I have some other stuff on my plate for now. One other note -- the offending file right now doesn't follow PEP8 style guidelines. Is there any particular reason, or would you mind if I did some reformatting? Mainly things like placing spaces around operators and some other non-Pythonic things. e.g., `if x is None` should be changed to `if not x`. 
comment
Agreed! Thanks Aric. 
comment
This can be closed now. 
