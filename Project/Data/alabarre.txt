issue
Faster `intersection_array` computation for checking distance-regularity#TITLE_END#<!-- Please use pre-commit to lint your code. For more details check out step 1 and 4 of https://networkx.org/documentation/latest/developer/contribute.html -->  I made a number of improvements to the `intersection_array` function:  1. calling `nx.is_regular` instead of manually checking whether the input graph is regular; 1. the input graph is very unlikely to be distance-regular: the number b(n) of distance-regular graphs (https://oeis.org/A241814) is a tiny fraction of the number of a(n) of connected simple graphs (https://oeis.org/A001349). In light of this, computing shortest path lengths as we go instead of precomputing them all seems more reasonable. 1. checking connectedness first allows us to avoid catching exceptions in the main loop. 1. we don't need to call `nx.diameter` at the end of the function, since we can update its value as we go with simple calls to `max`. 1. moreover, keeping track of the diameter as we go also allows us to stop early: distance-regular graphs on n vertices are known to have a diameter of at most (8 * log_2 n) / 3 (see https://doi.org/10.1016/j.ejc.2004.07.004, Theorem 1.5 page 81), so if at any point the current diameter's value exceeds that then we can give up early. 1. no need to build lists for computing `b` and `c`, since we are only interested in cardinality. 1. I thought bidirectional searches for shortest (u, v)-paths would (asymptotically) amount to more work than launching single-source shortest paths algorithms, since the former uses two BFS anyway. After comparing both approaches in practice, I turned out to be right: the bidirectional searches often take much longer than the original version of `intersection_array`.  # Results on distance-regular graphs  I compared the running times of the current version of `intersection_array` with mine on the ten largest distance-regular graphs I could get (data comes from https://www.distanceregular.org/). `is_distance_regular` is NetworkX's current implementation, `my_is_distance_regular` uses the proposed improvement, and `my_intersection_array_2` computes (u, v)-paths rather than the single source paths to all vertices.  ``` Comparing running times for distance regularity checking  The following instances are known to be distance-regular now dealing with instance  ./tests/test_data/distance-regular/coset-externarygolay.s6             729 vertices, 8748 edges     now running is_distance_regular     done in 2.1808555030147545 seconds, answer is True     now running my_is_distance_regular  done in 1.7023403490020428 seconds, answer is True     now running my_intersection_array_2 done in 4.411169429979054 seconds, answer is ([24, 22, 20], [1, 2, 12])  now dealing with instance  ./tests/test_data/distance-regular/shortened-extended-ternary-golay.s6 729 vertices, 8019 edges     now running is_distance_regular     done in 3.3089937710028607 seconds, answer is True     now running my_is_distance_regular  done in 2.775795002002269 seconds, answer is True     now running my_intersection_array_2 done in 4.316444240976125 seconds, answer is ([22, 20, 18, 2, 1], [1, 2, 9, 20, 22])  now dealing with instance  ./tests/test_data/distance-regular/games.s6                            729 vertices, 40824 edges     now running is_distance_regular     done in 7.66312413700507 seconds, answer is True     now running my_is_distance_regular  done in 6.080848899000557 seconds, answer is True     now running my_intersection_array_2 done in 8.80728219798766 seconds, answer is ([112, 110], [1, 20])  now dealing with instance  ./tests/test_data/distance-regular/witt.s6                             759 vertices, 11385 edges     now running is_distance_regular     done in 2.950715120008681 seconds, answer is True     now running my_is_distance_regular  done in 2.4445975279959384 seconds, answer is True     now running my_intersection_array_2 done in 5.15949073000229 seconds, answer is ([30, 28, 24], [1, 3, 15])  now dealing with instance  ./tests/test_data/distance-regular/iif.s6                              990 vertices, 3465 edges     now running is_distance_regular     done in 2.6043268000066746 seconds, answer is True     now running my_is_distance_regular  done in 2.290155616006814 seconds, answer is True     now running my_intersection_array_2 done in 11.459608876990387 seconds, answer is ([7, 6, 4, 4, 4, 1, 1, 1], [1, 1, 1, 2, 4, 4, 6, 7])  now dealing with instance  ./tests/test_data/distance-regular/sksgraph.s6                         1024 vertices, 16896 edges     now running is_distance_regular     done in 5.97368586299126 seconds, answer is True     now running my_is_distance_regular  done in 5.078227958001662 seconds, answer is True     now running my_intersection_array_2 done in 9.856664725986775 seconds, answer is ([33, 30, 15], [1, 2, 15])  now dealing with instance  ./tests/test_data/distance-regular/cube10.s6                           1024 vertices, 5120 edges     now running is_distance_regular     done in 3.033381476998329 seconds, answer is True     now running my_is_distance_regular  done in 2.5856517939828336 seconds, answer is True     now running my_intersection_array_2 done in 30.335086243983824 seconds, answer is ([10, 9, 8, 7, 6, 5, 4, 3, 2, 1], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  now dealing with instance  ./tests/test_data/distance-regular/hamming4.6.s6                       1296 vertices, 12960 edges     now running is_distance_regular     done in 6.618309289013268 seconds, answer is True     now running my_is_distance_regular  done in 5.388119548995746 seconds, answer is True     now running my_intersection_array_2 done in 28.552194002026226 seconds, answer is ([20, 15, 10, 5], [1, 2, 3, 4])  now dealing with instance  ./tests/test_data/distance-regular/grassmann263.s6                     1395 vertices, 68355 edges     now running is_distance_regular     done in 32.80391989799682 seconds, answer is True     now running my_is_distance_regular  done in 24.02292016299907 seconds, answer is True     now running my_intersection_array_2 done in 39.16650592099177 seconds, answer is ([98, 72, 32], [1, 9, 49])  now dealing with instance  ./tests/test_data/distance-regular/odd7.s6                             1716 vertices, 6006 edges     now running is_distance_regular     done in 7.135661044012522 seconds, answer is True     now running my_is_distance_regular  done in 6.108615503995679 seconds, answer is True     now running my_intersection_array_2 done in 54.69806831300957 seconds, answer is ([7, 6, 6, 5, 5, 4], [1, 1, 2, 2, 3, 3])  Comparing totals:     is_distance_regular    : 74.27297290205024     my_is_distance_regular : 58.477272361982614     my_intersection_array_2: 196.76251468193368 ```      # Results on random regular graphs  Same as above, but with random regular graphs.      ``` The following 10 instances random 20-regular graphs now dealing with instance 0 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.2792291679943446 seconds, answer is False     now running my_is_distance_regular  done in 0.0033934070088434964 seconds, answer is False     now running my_intersection_array_2 done in 0.002207571000326425 seconds, answer is False  now dealing with instance 1 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.315355560014723 seconds, answer is False     now running my_is_distance_regular  done in 0.003575303009711206 seconds, answer is False     now running my_intersection_array_2 done in 0.0019738929986488074 seconds, answer is False  now dealing with instance 2 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.2876819689990953 seconds, answer is False     now running my_is_distance_regular  done in 0.003264581988332793 seconds, answer is False     now running my_intersection_array_2 done in 0.001720152999041602 seconds, answer is False  now dealing with instance 3 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.2935480609885417 seconds, answer is False     now running my_is_distance_regular  done in 0.003167785005643964 seconds, answer is False     now running my_intersection_array_2 done in 0.002119503973517567 seconds, answer is False  now dealing with instance 4 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.2664987099997234 seconds, answer is False     now running my_is_distance_regular  done in 0.0032383989891968668 seconds, answer is False     now running my_intersection_array_2 done in 0.00220975698903203 seconds, answer is False  now dealing with instance 5 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.3034619920072146 seconds, answer is False     now running my_is_distance_regular  done in 0.0033776959753595293 seconds, answer is False     now running my_intersection_array_2 done in 0.0020072120241820812 seconds, answer is False  now dealing with instance 6 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.36016765399836 seconds, answer is False     now running my_is_distance_regular  done in 0.0032529000018257648 seconds, answer is False     now running my_intersection_array_2 done in 0.002387827989878133 seconds, answer is False  now dealing with instance 7 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.5374458819860592 seconds, answer is False     now running my_is_distance_regular  done in 0.00350969098508358 seconds, answer is False     now running my_intersection_array_2 done in 0.002192362997448072 seconds, answer is False  now dealing with instance 8 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.6173524199984968 seconds, answer is False     now running my_is_distance_regular  done in 0.004194754990749061 seconds, answer is False     now running my_intersection_array_2 done in 0.0025923799839802086 seconds, answer is False  now dealing with instance 9 -- 2048 vertices, 20480 edges     now running is_distance_regular     done in 2.5984127949923277 seconds, answer is False     now running my_is_distance_regular  done in 0.0035534100024960935 seconds, answer is False     now running my_intersection_array_2 done in 0.002110676985466853 seconds, answer is False  Comparing totals:     is_distance_regular    : 23.859154210978886     my_is_distance_regular : 0.034527927957242355     my_intersection_array_2: 0.02152133794152178  ```
issue
`find_asteroidal_triple` improvement#TITLE_END#The current version of `find_asteroidal_triple` builds the complement of the graph and stores all its edges, which causes performance and memory usage issues for large graphs. This is easily fixed by iterating over non-edges in the graph.
issue
Added girth computation function#TITLE_END#<!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> I implemented the algorithm mentioned on Wikipedia to compute the girth of a graph. This request seems to have been dropped a while ago (https://github.com/networkx/networkx/issues/1030), hopefully it is still of interest to some of us.
issue
is_chordal crashes on empty graphs#TITLE_END#<!-- If you have a general question about NetworkX, please use the discussions tab to create a new discussion -->  <!--- Provide a general summary of the issue in the Title above -->  ### Current Behavior  <!--- Tell us what happens instead of the expected behavior -->  Running `nx.is_chordal` on an empty graph raises `StopIteration`.  ### Expected Behavior  <!--- Tell us what should happen -->  The function should simply return `True`, since the definition simply requires that all cycles of length >= 4 contain a chord. Paths or trees are chordal since they have no cycles, and the function does not crash on these, nor on edgeless graphs.  ### Steps to Reproduce  <!--- Provide a minimal example that reproduces the bug --> Just create an empty graph and run the function:  ```python >>> import networkx as nx >>> graph = nx.Graph() >>> nx.is_chordal(graph) Traceback (most recent call last):   File "<stdin>", line 1, in <module>   File "/usr/lib/python3/dist-packages/networkx/algorithms/chordal.py", line 89, in is_chordal     if len(_find_chordality_breaker(G)) == 0:   File "/usr/lib/python3/dist-packages/networkx/algorithms/chordal.py", line 320, in _find_chordality_breaker     s = arbitrary_element(G)   File "/usr/lib/python3/dist-packages/networkx/utils/misc.py", line 233, in arbitrary_element     return next(iter(iterable)) StopIteration ``` ### Environment  <!--- Please provide details about your local environment -->  Python version: 3.9.2 NetworkX version: 2.5  ### Additional context  <!--- Add any other context about the problem here, screenshots, etc. --> 
issue
Memory issue with read_graph6#TITLE_END#<!-- If you have a general question about NetworkX, please use the discussions tab to create a new discussion -->  <!--- Provide a general summary of the issue in the Title above -->  I'm having memory issues when loading some moderately large graphs stored in the graph6 format (they were produced by `networkx` by the way, and neither generating them nor writing them to file using `write_graph6` raised any issue).   ### Current Behavior  <!--- Tell us what happens instead of the expected behavior -->  When loading the file using `read_graph6`, all memory is quickly used up and the system will become unresponsive if I fail to kill `python`.  ### Expected Behavior  <!--- Tell us what should happen -->  The file should be read and loaded into an `nx.Graph` object without any issue.  ### Steps to Reproduce  <!--- Provide a minimal example that reproduces the bug --> Decompress [the attached file](https://github.com/networkx/networkx/files/10989656/problematic-instance-for-read_graph6.g6.zip) (warning: decompressed file size is about 130M), open `top` in another terminal to "see" the problem, and then simply try to initialise the graph:      >>> import networkx as nx     >>> graph = nx.read_graph6("problematic-instance-for-read_graph6.g6")     # be ready to kill the interpreter  ### Environment  <!--- Please provide details about your local environment -->  Python version: 3.9.2 NetworkX version: 2.5 OS: Debian 11.6  ### Additional context  The graph contains "only" 40.320 nodes and 564.480 edges, and `nauty-showg` has no problem displaying its contents in under a second. Smaller graphs of the same family yield no issue, but they're an order of magnitude smaller (the largest similar graph that poses no problem has 5.040 nodes and 52.920 edges).  If you have `nauty-showg`, you can load the graph without any issue as follows:      >>> import subprocess     >>> import networkx as nx     >>> G = nx.Graph()     >>> output = subprocess.check_output(("nauty-showg", "-l0", "problematic-instance-for-read_graph6.g6"))     >>> for row in output.decode().split('\n')[2:]:     ...    line = row.strip()[:-1].split()     ...    G.add_edges_from((int(line[0]), int(x)) for x in line[2:])   <!--- Add any other context about the problem here, screenshots, etc. --> 
issue
DiGraph.__init__ does not allow sets#TITLE_END#I find it very convenient to be able to initialise graphs using: ```python >>> import networkx as nx  # version 2.2 >>> G = nx.DiGraph([(1, 2), (3, 4)]) >>> G = nx.DiGraph(((1, 2), (3, 4))) ``` However sets do not work and I don't understand why they have been disallowed: ```python >>> import networkx as nx >>> G = nx.DiGraph({(1, 2), (3, 4)})  # fails with frozenset too Traceback (most recent call last):   File "<stdin>", line 1, in <module>   File "/usr/lib/python3/dist-packages/networkx/classes/digraph.py", line 309, in __init__     convert.to_networkx_graph(incoming_graph_data, create_using=self)   File "/usr/lib/python3/dist-packages/networkx/convert.py", line 161, in to_networkx_graph     "Input is not a known data type for conversion.") networkx.exception.NetworkXError: Input is not a known data type for conversion. ``` Shouldn't DiGraph (and Graph, etc.) accept *any* iterable of tuples?
issue
Is DiGraph.predecessors() slow?#TITLE_END#While working on [an improved algorithm for transitive closure for DAGs](https://github.com/networkx/networkx/pull/3445) I noted that what was dragging my algorithm down seemed to be the calls to `DiGraph.predecessors`. The two functions return exactly the same result as `transitive_closure`:  ~~~python3 def transitive_closure_nx_better_dag(G):     TC = G.copy()      for vertex in list(nx.algorithms.dag.topological_sort(G))[-1::-1]:         TC.add_edges_from((vertex, u) for u in descendants_at_distance(TC, vertex, 2))    # defined below      return TC  def transitive_closure_nx_better_dag_pred(G):     TC = G.copy()          for vertex in list(nx.algorithms.dag.topological_sort(G))[-1::-1]:         TC.add_edges_from(product(TC.predecessors(vertex), TC.successors(vertex)))          return TC ~~~  But the timings are very different as you can see below, which is one of the reasons why I added the function:  ~~~python3 def descendants_at_distance(G, source, distance):     """Returns descendants at a given distance from a source.      Similar to bfs_successors except we don't keep the vertices closer than distance.         """     current_distance = 0     queue = deque([source])     visited = {source}     while queue:         if current_distance == distance:             return set(queue)         current_distance += 1                  newqueue = deque()         for vertex in queue:             for child in G.successors(vertex):                 if child not in visited:                     visited.add(child)                     newqueue.appendleft(child)                              queue = newqueue              return set() ~~~  Here are my results for a single run on a random DAG (checks are basically asserts comparing the edge and vertex sets):      $ python3 better_transitive_closure_dag.py      Testing transitive closure on a random DAG with 800 nodes and 239513 edges     Running time for NetworkX's TC+:                         16.93851137161255     Running time for transitive_closure_nx_better_dag:        3.103268623352051     Running time for transitive_closure_nx_better_dag_pred:  64.09326243400574     Checking that transitive_closure and transitive_closure_better agree ...     ok     Checking that transitive_closure and transitive_closure_better_pred agree ...     ok 
issue
Misplaced dominating_set algorithm in networkx/algorithms/dominating.py#TITLE_END#This is probably a broader issue, but if there's an interest in using `networkx` to solve NP-hard/complete problems we should probably distinguish between exact and approximation algorithms.   The `approximation` subdirectory was likely created to that end, and in my opinion the (misleadingly named) `dominating_set` function that can be found in `networkx/algorithms/dominating.py` should move to `networkx/algorithms/approximation/dominating_set.py` -- preferrably in a way that at least temporarily does not break existing code (issue deprecation warnings?).  This principle might apply to other functions as well (I haven't checked), and perhaps at some point we will need to rename those functions and follow the same patterns as in, say, `mst.py`, to select preferred algorithms, or ask for the best solution out of all heuristics in a transparent way, and so on.
issue
Faster transitive closure computation for DAGs#TITLE_END#This is an improved algorithm for computing the transitive closure of a DAG using topological_sort (so an exception will be raised if the input graph is not a DAG): traverse vertices following a reverse topological order, and connect each vertex to its descendants at distance 2 as we go. The topological order guarantees that each successor v of a node u is connected to all descendants of u in the transitive closure, and therefore it is enough to connect u to nodes at distance 2 from u rather than attempting to explore all descendants.  The algorithm is probably folklore but I didn't find a reference in my books, feel free to add one if you can.  I also added a function to compute the descendants of a node at a fixed distance. `bfs_successors` worked just as well in my tests but resulted in code that was harder to read. Moreover, the new function can be of general interest and is less memory-hungry since each iteration only stores nodes at a fixed distance, whereas `bfs_successors` will keep previous nodes as well.  A taste of the kind of results one can expect using the time function (I used assert to check that both computed closures have the same vertex and edge sets):       $ python3 better_transitive_closure_dag.py      Testing transitive closure on a random DAG with 800 nodes and 239532 edges     Running time for NetworkX's TC:             17.484230041503906     Running time for TC dag-optimized:           3.222315549850464  I'd gladly add my benchmarks but I'm at loss as to where to include them. I shamelessly copied the tests for `transitive_closure` in the meantime.
issue
Providing a topological order to DAG-related algorithms#TITLE_END#A few functions in `dag.py` rely on `topological_sort`. Chances are people who know they are working on a DAG in an application may need to compute a topological order anyway, and it would make sense to allow them to feed that order to functions that need it instead of having each function recompute such an order for their own needs.   Would you agree to transform `some_dag_function(G, ...)` as follows?  ```python     def some_dag_function(G, ... , order=None):         if order is None:             order = topological_sort(G)         # rest of the function ```
issue
Additional topo_order parameter for functions that rely on topological_sort#TITLE_END#As previously discussed [here](https://github.com/networkx/networkx/issues/3446), I added an optional `topo_order` parameter to functions that rely on `topological_sort`so that users can provide their own topological order if they have already computed one.  I left the following two functions alone:  1. `has_cycle` in `dag.py`: if you call `has_cycle`, then either you haven't computed a topological order yet; or you already have, and in that case you already know the result that `has_cycle` would return. 1. `_relabel_inplace` from `relabel.py`: it did not seem like a sensible choice to me. I'm having trouble picturing a user wanting to provide a topological order there, but maybe it's just because I've never had a need for the function that calls it.
issue
Faster transitive_closure using BFS on TC#TITLE_END#This is a simple modification of the naive approach currently implemented in networkx: for each vertex in the graph, add all edges from that vertex to its descendants. My two changes are pretty minor and linked:      1) I use BFS instead of DFS (through the descendants function);     2) I traverse the transitive closure instead of the original graph.  The rationale is that as we keep adding shortcuts to the transitive closure, we'll eventually reach all accessible vertices quicker with a BFS than with a DFS.  Surprisingly, change 2) does not seem to have any impact. Change 1) on the other hand resulted in more than a 10x-speedup in my tests (random digraphs on >= 400 vertices generated by fast_gnp_random_graph (p >= .5)).
