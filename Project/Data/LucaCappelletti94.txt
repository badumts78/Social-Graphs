issue
Implemented conversion from networkx graph to latex graph.#TITLE_END#Implemented conversion from networkx graph to latex updating my old implementation of [pyadigraph](https://github.com/LucaCappelletti94/pyadigraph), converting arbitrary graphs into [Adigraph pure tikz graphs](https://ctan.org/pkg/adigraph).  Example: ![Adigraph](https://github.com/LucaCappelletti94/pyadigraph/raw/master/example.png?raw=true)
issue
Maximum disjointed paths#TITLE_END#Implemented Maximum edge disjointed paths algorithm within a <img src="https://latex.codecogs.com/gif.latex?\left(2\sqrt{m}+1\right)" />-approximation of the optimal maximum disjoint paths, where <img src="https://latex.codecogs.com/gif.latex?m" /> is the number of edges in the graph.  
issue
Center selection#TITLE_END#Implemented 2-approximation of [Center selection](https://en.wikipedia.org/wiki/Metric_k-center) and an appropriate test suite.
issue
Karger#TITLE_END#Implemented [Karger](https://en.wikipedia.org/wiki/Karger%27s_algorithm) algorithm and created its test suite, both in the iterative version and parallel version using multiprocessing.
issue
Implemented maximal matching of minimal weight and created test suite.#TITLE_END#As of title, I've implemented `min_weight_matching` based on `max_weight_matching` and created a complete test suite.  I'll be using this implementation in Christofides.
issue
Parallel harmonic centrality#TITLE_END## Parallel harmonic centrality Implemented parallel version of harmonic centrality, using [Pool from multiprocessing] to(https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool).  ## Extra features Additionally, the version includes both compressed caching using [compress_json](https://github.com/LucaCappelletti94/compress_json) and parallelization across [SLURM](https://slurm.schedmd.com/documentation.html) clusters computing nodes (such as [Galileo](https://www.cineca.it/it/content/galileo)) with virtual shared disk. The SLURM parallelization is achieved simply by touching a temporary file, so to avoid having to deal with inter-node communication systems such as [OpenMPI](https://www.open-mpi.org/) at this scale. The implementation also uses [auto_tqdm](https://github.com/LucaCappelletti94/auto_tqdm) to show an optional loading bar as the various tasks are completed.  ## Requirements The implementation requires a number of packages to be added to the requirements:  - [touch](https://github.com/andrewp-as-is/touch.py), used for a touching an empty file. - [compress_json](https://github.com/LucaCappelletti94/compress_json), for reading and writing temporary compressed JSON files. - [auto_tqdm](https://github.com/LucaCappelletti94/auto_tqdm), for showing an optional loading bar automatically adapted to its context (either [jupyter](https://jupyter.org/) or console) - [dict_hash](https://github.com/LucaCappelletti94/dict_hash), for deterministically hashing dictionaries. Used for creating the cache path names.  ## Comparison with single-thread version I have run a comparison on a 12 thread machine and the parallel implementation, as can be expected, vastly outperforms the single-thread one:  ![Comparison](https://user-images.githubusercontent.com/7738570/64864878-555a3a80-d638-11e9-8d71-a2423e43dc85.png)  ## Test on SLURM cluster Tests have been run on the Cineca's Galileo SLURM cluster to verify that the proposed method (touched temporary files) works properly and so far no collision have been identified, even though they should be possible even though unlikely since rarely SLURM jobs get to start all together. Better synchronization, if needed, can be surely achieved by wrapping the proposed implementation with OpenMPI.  _I hope this work can be useful._  Have a nice day, Luca 
issue
Christofides#TITLE_END#Implemented [Christofides](https://en.wikipedia.org/wiki/Christofides_algorithm) and its test suite. I have no idea how to test if the approximation of the TSP is correct, but for certain the cycles returned are Hamiltonian.
comment
Well the package uses old latex APIs, basically no dependencies, so I don't think it will cease to work. As per additional features, I wouldn't know, for all intents and purposes for me it is a finished job. Still, I am currently working on a rust graph library for graph machine learning things, so it is possible I'll find the need to add some new stuff there from time to time.
comment
I must admit I don't recall anymore, I wrote the thing initially to drow augmenting directed graphs, so it may have to do something about that. I'll re-read how I process them in the source code and get back to you.
comment
Ah yes, basically the thing was that the whole library was initially for drawing augmenting paths on graphs, and as such, it needed the edge weights for the path when the augmenting paths feature was used, even when the weight was not displayed.
comment
Hello @dschult, I'll go take a look at the code and let you know. As of now I don't recall.
comment
Ok so, both `style` and `label position` can be anything that tikz accepts.
comment
Yes, I would also go for linking the tikz documentation, and saying it can be just about anything supported by tikz plus adding some examples.
comment
It isn't uncommon to split the job in a number of tasks bigger than the processors, it clearly depends on the expected size of the graph. Therefore, it may be more proper to adjust the number of blocks on the size of the graph: I may be looking into this soon enough, if it is decided to proceed for implementing a number of functions using multiprocessing.  As to why not using exactly `n=size of pool`, it generally has to do with the expected size of the resulting object or just to receive partial results every n blocks. For instance, it is common to store the partial results in case something goes wrong down the line (memory errors, for example). In the code you pointed out it doesn't seem to be the case, if I'll get the time I'll fix up the code.
