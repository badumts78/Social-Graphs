issue
ENH: add normalize option for `subgraph_centrality` and its `_exp` version#TITLE_END#Add normalize keyword-only arg for subgraph_centrality and _exp version see #1243 This has been around a LONG time. Not sure if anyone needs subgraph_centrality to be normalized or maybe they are doing it on their own. But it unifies the API between centralities a little. Default value is `False` for backward compatibility.  Tests are added in the doc_strings. Nothing fancy here so I didn't add any others to the test suite.  Fixes #1243 
issue
Figure out how to best structure imports within the package#TITLE_END#This discussion crosses over a couple of PRs and shows we probably need to figure out how the imports are currently working and how we should best arrange them to work better.  I think they are better after #4401 #4403 and #4296.  But the discussion in #4277 isn't conclusive and relative imports discussed in #4424 and  dschult#13 show this a bigger issue than we probably originally thought.   Let's figure out the import order for the networkx namespace, propose a couple good ways to import networkx features from within the package and decide on one of them.
issue
CI: doc example build failures#TITLE_END#It looks like we are still getting CI doc build failures even though the CI credits don't seem to be the issue. One example is [this log from PR](https://app.circleci.com/pipelines/github/networkx/networkx/12253/workflows/3639481f-bab1-484a-90f6-311547dfd5ac/jobs/21368)   I think I recall that the other failures also occurred in `plot_parallel_betweenness.py`. If that's true might this be because gthe example tries to run on hardware that doesn't provide parallel capabilities?  The error message is only: `Killed` so its hard to tell what's happening and we can't seem to recreate it locally.  Thoughts?  
issue
Switch to the NumFOCUS Code of Conduct#TITLE_END#NetworkX met with some of the NumFOCUS Code of Conduct Onboarding Team to discuss our switch to using the NumFOCUS Code of Conduct (instead of our own Code of Conduct based roughly on the SciPy CoC about 5 years ago).  The main difference is that now violations can be reported to the NumFOCUS Code of Conduct Working Group. They provide more expertise and more separation from the community. They review the situation, gather information and make recommendations to the NetworkX community which we then decide whether to follow. We could have made our policy to have the Working Group make decisions which we would then promise to implement.   This PR includes new language connecting our documentation to the NumFOCUS information and to the Working Group reporting form.
issue
Code of conduct changes#TITLE_END#NumFOCUS has released a new Code of Conduct and has asked projects to consider whether they would like to adopt the NumFOCUS Code of Conduct. Our current CoC was based on the SciPy CoC in force at that time. We didn't put a huge amount of time developing it, partially because we have so little experience with what a CoC should include, etc.   Most of the discussion in our community meeting was positive toward leaning on the work the CoC committee has done, and the relative expertise of those folks. We also like the idea that an event could be reported to a group other than the NetworkX Steering Council, which could likely be personally involved.  The main concern with adoption of the CoC is that it might impose a complex process on the project -- or include a procedure for removal of a person from the project -- without project approval.  We should continue to collect opinions from folks not at the meeting. And we should start the process of adoption of the NumFOCUS CoC, at least enough to find out what is involved. Dan will coordinate this activity and others should prepare for a vote at the next community meeting 4/25).  Please add comments here about what we discussed and what else we should consider. Thanks!
issue
Setup vf2pp files for expansion to subgraph problems#TITLE_END#This PR sets up the VF2++ functions to allow easier addition of the subgraph versions of the problem. The main change are to add the degree dicts to the namedtuple data structure and  make the `return False` statements in the main generator function more useful.    Notes: - Moved the degree dict creation to `_initialize_parameters`. Moved this call to before where the degrees are used. - added G1_degree and G2_degree degree dicts to the graph parameters namedtuple.  - Reordered the intro of the main function vf2pp_all_isomorphisms.   - Replaced `return False` (which doesn't do much inside a generator) with `return "helpful string"`, which is the same as `raise StopIteration("helpful string")`. The string is only seen if you manually use `next` on the generator function. But that can be helpful for debugging why there is e.g. no isomorphism.   - Inline the function `_precheck_label_properties` and make more readable. - Removed the degree dict input parameter from the `_find_candidates` helper function. - changed the name of utils.labels_different to utils.labels_many. - adjusted the tests accordingly. - Reworded, unified language and corrected the main module doc_strings.  If this is too much to review I can split it up. Most of the changes are due to the change in the namedtuple.
issue
Revert "ignore autoflake and pyupgrade changes"#TITLE_END#Reverts networkx/networkx#8333  My mistake -- the PR #7870 didn't need to be ignored. It was ignoring a different PR (#7861). 
issue
ENH: adds ISMAGS support for directed and multigraph with tests and refactor#TITLE_END#With this PR, ISMAGS supports directed and multigraph inputs. If inputs do not have the same directed status an exception is raised. But multigraph and graph can be compared. So no restriction there.  Refactored methods `_refine_node_partition`, `_map_nodes`, and `_process_ordered_pair_partitions` to replace the recursive with a (DFS) queue approach.  In addition, the ISMAGS class no longer requires that nodes be sortable.   Corrected error while handling `candidate_sets` which is a dict-of-set-of-frozensets. For each node, we want to find the minimum length of the frozensets associated with that node. So we should use `min(len(x) for x in candidate_sets[n])` rather than `min(candidate_sets[n], key=len)` which returns the smallest length frozenset. This ensures that the nodes with the fewest candidate mapping-image nodes are considered first, which in theory should return the first isomorphism faster.  (Not a correctness issue -- just a performance hit.)  In switching away from recursive methods, we somehow no longer need the bug-fix for ISMAGS. The bug arose for strange cases where symmetry was getting top/bottom partitions with different numbers of elements. I haven't chased down how that error occurred in the original code, but I suspect we were not respecting the node order or the ordering of partition list by the size of the parts. It doesn't affect the answer, but reduces the number of partitions to consider (the strange cases were aborted once detected).  This PR updates and adds tests especially DiGraph tests from VF, VF2, VF2pp.  Many comment changes, variable name changes and removal or renaming of helper functions. I also removed the manually provided cached properties in favor of just computing those attributes in the `__init__` method (they would never change unless the graphs change which would violate other assumptions).  Names: a "partition" is a list of "parts". Each "part" is a set of nodes. We should only use the plural "partitions" when we have more than one partition.  I did not change the term "color" to "label" in this PR. I also did not move functions in or out of the class structure (to make a function oriented setup instead of methods). I did not refactor the methods like `is_isomorphic` to be functions. We can discuss those changes for the whole isomorphism subpackage separately.  I thought about keeping the old code alongside the new, but given the needed correction and the bug-fix shortcut, I don't think we should retain that version. The Sandia folks mentioned that they saw some suspicious parts of the ismags code.  I did not use the Sandia python version of ISMAGS because it was clearly a translation of the original java code, and didn't feel right for NetworkX. It is also a command line tool used for graphs stored in a specific ascii file structure e.g. "XX00YX" to indicate which edges exist between, e.g., 3 nodes and their edge type/color/label.  Are there things I can do to make this easier to review? It's often hard to review the switch from recursive to non-recursive. And we've got 3 such conversions here. Should I split that from the handling of DiGraph changes? It might be worth reading the new ismags.py as its own new file rather than the diff, but I'm not sure.  I will wait a week or two and then review it with fresh eyes if that helps.
issue
MAINT: Ignore graph hashing warnings in tests#TITLE_END#The graph hashing warnings were showing up while testing.  This sets `conftest.py` to ignore the UserWarnings about graph hashes. I also added a missing space to the warning message.
issue
Test failures regarding pytest-mpl and `mpl_image_compare`#TITLE_END#Looks like we are getting test failures due to CI not understanding `mpl_image_compare`. I don't see any install of pytest-mpl during the installing packages phase of the test.  But I may not be looking in the right place.  It's not due to a new release of pytest-mpl (no recent release).  This started happening this afternoon (June 2).
issue
Fix round-trip to and from pygraphviz.AGraph setting spurious graph attributes#TITLE_END#It looks like, for historical reasons, pygraphviz is adding a default node attribute named 'label' and assigning it the value '\\N'.  The NetworkX code has been accommodating that, and tests for the **second** round-trip resulting in graph attributes the same as the results for the first round-trip.  The test of this strange behavior recently started failing because the default node attribute for label is being reported as of GraphViz v13.0. See #8119. So we need to fix something. And it'd be nice to have it work across graphviz and pygraphviz versions.  This PR checks if pygraphviz reports either 1) no node attributes or 2) node attributes that are named 'label' and have the value '\\N'.  If either happens, we don't set any graph attributes in the graph created by `from_agraph`. Similarly we also check whether no graph attributes or no edge attributes are are reported and in that case we also don't update the newly created graph attribute dict.   The result is that a round-trip with no default attribute values set creates a networkx graph without any graph attributes set during the round-trip. The avoids the original (very old) issue with the round-trip not being the same as the original graph. And it avoids the difference between graphviz versions.   I will also make a PR to pygraphviz to remove the addition of node label default values set to `'\\N'` as it hopefully is no longer needed. But this workaround should continue to work across any version changes of graphviz and pygraphviz.  Fingers crossed.  But for now -- does this do what we want it to do?
issue
Update deploy-docs yml to use Python 3.12 when deploying the docs#TITLE_END#I'm not sure what the big-scale implications are of changing this. So please check if it makes sense.  nx-parallel is used when deploying the docs in a yml file that runs with Python 3.11. We just shifted the python version requirements of nx-parallel to Python 3.12+ so we could use `itertools.batched` (which was added in 3.12).  We could of course write our own version of that tools and include it in nx-parallel and then remove it again after the Python 3.14 release.  But we'd prefer to just up  the requirements-- it is unlikely we will have a release of nx-parallel before October anyway.  Is there a reason the docs are deployed using Python 3.11? We are using the latest github `main` branch for nx-parallel and nx-cugraph when building our docs. Is it OK to make the docs with a newer Python than our minimal requirement?  This PR tries to update the deploy-docs  so they use Python 3.12 when deploying.
issue
Add paragraph about university classes to mentored projects#TITLE_END#We've talked about adding to the mentored projects page a paragraph that makes suggestions to people who are running university courses with NetworkX projects.   Here's an attempt at such a paragraph. I want it to tread the fine line of being welcoming while also making suggestions for how they do it. The big ideas are: - consider improving what is already there rather than creating new. - work with the class on a separate fork and the PR to that fork is ready, then make the PR to NetworkX.  Is this too discouraging? too encouraging? :)
issue
Tweaks and notes from a dive into backends.py#TITLE_END#I took a dive into some of `backends.py` and came up with these suggestions for notes and code tweaks to ease readability of some parts. Push back on any of this, and/or push up corrections, reversions, additions.   None of these changes should be changing behavior of the code **except** that `_fallback_to_nx` seems to be due for deprecation (v3.5) so I removed it and the test of the deprecation warning. If there are other steps needed for deprecation of backend features, I can revert these or add those (or you can). 
issue
DOC: docstring changes to `to_dict_of_dicts` and `attr_matrix` and input name change in `min_fill_in_heuristic`#TITLE_END#This finishes up Issue #6804 to follow-up on warts found during the `_dispatch` code audit.  Fixes #6804  It includes: - changing the input name `graph` to `graph_dict` in `min_fill_in_heuristic` as it needs to be a dict-of-neighbor-sets rather than a networkx graph. - updated the doc-strings of `to_dict_of_dicts()` to explain that input `edge_data` is a scalar value, that it should not be a container, and that it's default is the full edgedata dict from the graph (which is not a scalar value).  - updated the doc-string of `attr_matrix()` and `attr_sparse_matrix` -- but I left the code requiring that every node/edge must have the requested attribute (no default for missing values).   I decided not to change some of the items -- they will still be warts (and are still unchecked in that issue's list, though with a note that it is by choice not to fix it).  
issue
DOC: Clean up mentored projects page: move visualization project to completed section#TITLE_END#This moves the visualization project to the completed section. It seems to be orthogonal to the new projects PR (no conflicts with it).
issue
MAINT: replace the SHAs for blame and move the changes within pre-commit.#TITLE_END#Moving the changes within pre-commit is a hack to get them to register in blame as having been changed here. I added a blank line to the pre-commit file as well.   Changing the SHAs will also show up in the commit. And I removed the two added in the linting PR and added the one for the commit that was the merged PR.
issue
Change CRLF format of two files#TITLE_END#I ran into `test_network_simplex.py` as having CRLF line endings and so searched using `find . -exec file "{}" ";" |grep CRLF` (from [stackoverflow answer](https://stackoverflow.com/a/73969) and found one other such file. This just changes to ending with newline. 
issue
Make test file names unique to be threadsafe#TITLE_END#When running tests in parallel, the test filenames should be unique or they overwrite each other.  Fixes #7995  hopefully... :)  But for sure this could be a problem anyway.  There was talk about making these all tempfile examples, but the point of the examples is to show the various ways you can save to files -- so we would lose the point of the examples. There still might be a solution there, but I'm putting this out there as a way to avoid the intermittent failures.
issue
new try at will_call_mutate_inputs#TITLE_END#This followup PR for backends.py tweaks focuses on  `_will_call_mutate_inputs()`. I'm hoping it is as performant as the original code while being more readable.  Comments add info like:  - no nx function currently use a mutates_input dict with more than one item.  - When `mutates_input` is a dict, it can be "not copy" or an arg_name of the function's arg that holds a node or edge attribute name to be mutated with default `None` meaning no mutation. So the code can just check if the value `is not None`. - if new nx functions mutate graphs is other ways this code will need to change.  I think all info from previous comments is still there. I also pulled out `len(args)` partially in the spirit of iterating over the dict, but also because it helps allow the arg-value lookups to easily visibly match code between the "not copy" and "arg_name" cases.
issue
Fix pydot get_strict error on CI#TITLE_END#Recent change to pydot removed an optional arg to pydot's `get_strict` method. We were only passing in None. So I removed it and we'll see how the CI fares.
issue
CI "extra" workflow is failing doctest in readwrite/multiline_adjlist.py#TITLE_END#PR #7978 went from passing to failing without changing anything related to this part of the codebase.  Probably it is due to a new version of one of our "extra" packages.  [Link to details](https://github.com/networkx/networkx/actions/runs/14625303630/job/41035260524?pr=7978)  ``` 343 >>> G = nx.path_graph(4) 344 >>> nx.write_multiline_adjlist(G, "test.adjlist") 345 >>> G = nx.read_multiline_adjlist("test.adjlist") 346  347 The path can be a file or a string with the name of the file. If a 348 file s provided, it has to be opened in 'rb' mode. 349  350 >>> fh = open("test.adjlist", "rb") 351 >>> G = nx.read_multiline_adjlist(fh) UNEXPECTED EXCEPTION: TypeError('networkx.classes.graph.Graph.add_edge() argument after ** must be a mapping, not int') Traceback (most recent call last):   File "/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/doctest.py", line 1395, in __run     exec(compile(example.source, filename, "single",     ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                  compileflags, True), test.globs)                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "<doctest networkx.readwrite.multiline_adjlist.read_multiline_adjlist[4]>", line 1, in <module>   File "<class 'networkx.utils.decorators.argmap'> compilation 1846", line 5, in argmap_read_multiline_adjlist_1841     import itertools              ^^^^^^^   File "/home/runner/work/networkx/networkx/networkx/utils/backends.py", line 535, in _call_if_no_backends_installed     return self.orig_func(*args, **kwargs)            ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^   File "/home/runner/work/networkx/networkx/networkx/readwrite/multiline_adjlist.py", line 386, in read_multiline_adjlist     return parse_multiline_adjlist(         lines,     ...<4 lines>...         edgetype=edgetype,     )   File "<class 'networkx.utils.decorators.argmap'> compilation 1835", line 3, in argmap_parse_multiline_adjlist_1832     import gzip             ^^^   File "/home/runner/work/networkx/networkx/networkx/utils/backends.py", line 535, in _call_if_no_backends_installed     return self.orig_func(*args, **kwargs)            ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^   File "/home/runner/work/networkx/networkx/networkx/readwrite/multiline_adjlist.py", line 298, in parse_multiline_adjlist     G.add_edge(u, v, **edgedata)     ~~~~~~~~~~^^^^^^^^^^^^^^^^^^ TypeError: networkx.classes.graph.Graph.add_edge() argument after ** must be a mapping, not int /home/runner/work/networkx/networkx/networkx/readwrite/multiline_adjlist.py:351: UnexpectedException ```
issue
CI dispatch test is raising for nx_loopback tests#TITLE_END#Two active PRs are having troubles with CI failures for the nx_loopback interface for testing. I think it is probably involved with checking whether the graph has been mutated. But I can't tell.  - PR #7902 (with [error log](https://github.com/networkx/networkx/actions/runs/13742147584/job/38432619799?pr=7902)) about node contraction.  - PR #7589 (with [error log](https://github.com/networkx/networkx/actions/runs/13726601438/job/38394421439?pr=7589)) about the new draw interface.  
issue
Clean up after `random_k_out_graph` speed-up#TITLE_END#There are a few things to handle/clean-up after the recent speed-up of `random_k_out_graph` in #7702.  - [x] We should run the python version if numpy is not available for the numpy version (note that the python version now includes a ~20x speed up over the old python version. And there is a further ~3x speedup from the numpy version. So the python version is much improved even for folks without numpy.) [addressed in #7718] - [x] need to figure out how to handle the `_dispatchable` decorator in cases like this with two implementations that have different decorators. - [x] decide on and implement a deprecation process/warnings, etc. if any.  Comments about any of these are appreciated. Do we need a deprecation cycle for any of this?
issue
Followup ideas from the `_dispatch` code audit#TITLE_END#PR #6688 required going through a lot of our code base and finding out which input arguments were treated how. In that PR, many comments were created and attached to various parts of our codebase. This Issue attempts to collect the comments that didn't lead to an immediate solution or answer.  The idea is to check them off when we either resolve them or decide not to resolve them. If I've missed something please add it in the comments and I'll try to put it on the list via editing.  ## Timing impact - [x] timing of functions when not dispatched     Full test suite for networkx: (very crude test using pytest time reporting)       - without dispatch decorators (before PR #6688): 48.14s          - with dispatch decorators: 48.83s       - in nx-loopback test mode without dispatch decorators (should take longer due to converting graphs): 56.37s       - in nx-loopback test mode without dispatch decorators (should take longer due to converting graphs): 154.24s     Time a function call that takes very little time: `G=nx.path_graph(5); %timeit nx.shortest_path(0,3)`       - without dispatch decorators: 2.67 µs       - with dispatch decorators: 3.3 µs     Increase number of nodes:  `G=nx.path_graph(50);%timeit nx.shortest_path(G, 0, 47)`       - without dispatch decorators: 21.9 µs       - with dispatch decorators: 22.3 µs - [x] timing of import of networkx:        Change to new git branch, then `python -X importtime -c "import networkx"`       - without dispatch decorators: ~70 +/-2 ms    (200+/-4 ms for first invocation after switching branches)       - with dispatch decorators: ~72 +/-2 ms    (204+/-4 ms for first invocation after switching branches)  ## API warts: - [ ] algorithms/tree/operations.py::join  accepts a list of 2-tuples with (graph, root) pairs [**left unchanged:** leave until rooted trees have unified interface] - [ ] algorithms/tree/branchings.py::maximum_branching: handles "preserve_attrs" input specially for True/False. If True it preserves all the attributes -- not just the one used in the algorithm.  [**left unchanged:**] - [x] convert.py::to_dict_of_dicts has argument `edge_data` which is a value [changed docs but not behavior] - [x] algorithms/approximation/treewidth.py::min_fill_in_heuristic has input `graph` which is expected to be a dict. - [x] algorithms/shortest_paths/astar.py::astar_path The trick here is that when heuristic arg is callable, all node attributes need to be preserved.  We won't change this. - [x] allowing `graph_name` to be specified in `LoopbackDispatcher.convert_from_nx` helped a lot with debugging for this PR - [x] Also note that `LoopbackDispatcher.convert_from_nx` has a list of function names for which converting the graph doesn't happen.  We can add to that list if needed (escape hatch for debugging). - [x] _dispatch handles inputs to `to_numpy_array` specially for dtype argument. [this special treatment is reasonable. So we won't try to change this.] - [ ] we should make some functions in `flow.py` private; like: `preflow_push_impl` [**left unchanged:** there is more refactoring to do that just making private] - [ ] we don't convert the inner graphs when they are used as edge keys or attributes (see `test_christofides_hamiltonian`.)  [**left unchanged:** not required for backend to treat inner graphs the same way python version does] - [x] the current _dispatch can't specify a string as the default value of an edge_attr argument. If this becomes as issue we could use the string provided so long as it doesn't equal an argument of the function. [We will do that if/when it is needed.] - [ ] attr_matrix and attr_sparse_matrix require the edge attribute must be present for all edges.  [**left unchanged:** but added to doc-strings] - [x] Some cases use "weight" internally for edge attribute but don't allow that name to change. Already a separate Issue: #6753 Find them with `git grep -A 2 '_dispatch.*preserve_edge_attrs={"G":'`   - second_order_centrality   - quotient_graph   - panther_similarity   - generate_random_paths   - number_of_walks   ## Misc notes: - it is okay for backends to raise if not all edges have "weight" attribute when @_dispatch gets `edge_attrs={"weight": None}`  ## Questions: Q: how to handle create_using inputs when dispatching?  ## For the future, perhaps we should: - have a dispatch class instead of a closure - create a decorator to indicate whether multigraph edges can be merged with `min`, `sum`, etc. - see whether dispatch should be included in the `networkx/drawing/layout.py` algorithms.
issue
Intermittent CI failure: "speed" subpackage of osmnx not found.#TITLE_END#Every once in a while CI fails with an error like the following. It looks like an issue depending on the version of osmnx that is installed because they changed their api to no longer have a subpackage `speed`.  We should figure out the version depednece and fix the CI so it always gets a version that works.  ``` Extension error: Here is a summary of the problems encountered when running the examples:  Unexpected failing examples (1):      ../examples/geospatial/plot_osmnx.py failed leaving traceback:      Traceback (most recent call last):       File "/home/circleci/project/examples/geospatial/plot_osmnx.py", line 30, in <module>         G = ox.speed.add_edge_speeds(G)             ^^^^^^^^     AttributeError: module 'osmnx' has no attribute 'speed'  ------------------------------------------------------------------------------- make: *** [Makefile:48: html] Error 2 make: Leaving directory '/home/circleci/project/doc'  Exited with code exit status 2 ```
issue
Handling numpy scalars in contributor guide#TITLE_END#Let's add a note to contributor guide about ensuring return values are not numpy scalars.
issue
Allow edge weights for `harmonic_diameter` #TITLE_END#The harmonic diameter (which is the harmonic mean of all distances) from #5251 uses path lengths for unweighted networks. It would be good to allow weights path lengths via a keyword argument like `weight=None`. This kwarg can have the same name doc_string entry and optional function interface as for the shortest_path functions.   
issue
CI failure in the geospatial example from using osmnx #TITLE_END#The error message is: ``` Extension error: Here is a summary of the problems encountered when running the examples:  Unexpected failing examples (1):      ../examples/geospatial/plot_osmnx.py failed leaving traceback:      Traceback (most recent call last):       File "/home/circleci/project/examples/geospatial/plot_osmnx.py", line 30, in <module>         G = ox.speed.add_edge_speeds(G)             ^^^^^^^^     AttributeError: module 'osmnx' has no attribute 'speed' ```  The new 2.0 release of osmnx refactored many of the subpackages and within them the modules. It [looks like](https://github.com/gboeing/osmnx/issues/1123) their v1.9.4 is constructed to give FutureWarnings for anything that will break in 2.0. If this takes too long to track down we can pin to v1.9.4 in our examples/CI.
issue
MAINT: delay loading of backend_info to after imports#TITLE_END#Fixes #7671   Delays updating backend_info and setting up configs until the end of the import process so that backends which import networkx inside their backend_info function don't run into a circular import issue.  We should test this locally with nx-cugraph installed just to make sure nothing strange occurs.
issue
Docs for Stable 3.4 Tutorial first result is a RunTimeWarning#TITLE_END#The [3.4 doc Tutorial](https://networkx.org/documentation/stable/tutorial.html) has a RunTimeWarning on its very first cell. The RunTimeError occurs on importing networkx and it looks like a circular import error. The Warning states:  ``` /opt/hostedtoolcache/Python/3.11.10/x64/lib/python3.11/site-packages/networkx/utils/backends.py:541:  RuntimeWarning: Error encountered when loading info for backend parallel:  cannot import name '_registered_algorithms' from partially initialized module 'networkx.utils.backends'  (most likely due to a circular import)  (/opt/hostedtoolcache/Python/3.11.10/x64/lib/python3.11/site-packages/networkx/utils/backends.py)   backend_info = _get_backends("networkx.backend_info", load_and_call=True) ```  Looking back at the commit log (along with artifacts for the CI doc build) shows that this first started occurring with [PR 7585](https://github.com/networkx/networkx/pull/7585).  So we must have missed a circular import during review (we would have had to look at the tutorial of the docs artifact).  Can we find a circular import in [PR 7585](https://github.com/networkx/networkx/pull/7585)?  Thanks @rlratzel for noticing this! 
issue
Doc: visually hidden edge on gallery example Max Ind Set#TITLE_END#An email bug report from edwin.van.der.klaauw@woodwing.com:  > I’m enjoying your documentation of NetworkX a lot. > > I just noticed something confusing on [this](https://urldefense.com/v3/__https:/networkx.org/documentation/stable/auto_examples/algorithms/plot_maximum_independent_set.html__;!!Bt8fGhp8LhKGRg!DfsARZwCPV2af182hhRJ-v7kPQYG3D84WVmxpm79hxm8lPQuMoFO4Mn6ielbnVqWln6VE5CYwM4r6zaT4SRsNMecL1LKpw$) page. That page gives an example plot of a graph that does not show the (6,1) edge. > > The reason is that the edge is visually exactly overlapped by edges (1,7) and (7,6) combined. As a result, looking at the plot alone, I can’t see that vertexes 1 and 6 are adjacent. The output is written as “Maximum independent set of G: {8, 1, 10, 3}”, which is correct. But then I started wondering why vertex 6 is not listed in that set. So, I started digging a bit. > > It turned out that the seed parameter value 39299899 causes it to plot in the confusing way. By picking a different value, the graph got plotted differently and the (6,1) edge became visible. >  > The graph currently plotted on the page for seed 39299899 shows 13 edges only (one is hidden). By picking seed 333222113 it shows 14 edges, now including the (6,1) edge.  I think we should choose a simple seed value like 42, but maybe I haven't looked at what that would produce.
issue
Correct the members of steering council#TITLE_END#The docs don't show all the members of the steering council. This adds @boothby to the list of steering council members which matches the permissions and teams as shown by github permissions. Note that this does not require changes in any github permissions -- just the docs.
issue
retain adjacency order in nx-loopback copy of networkx graph#TITLE_END#This rewrites the copy functionality within nx-loopback to ensure that the neighbor order is the same in the copy. Instead of copying edges, this copies adjacencies (careful to ensure opposite representations of an edge point to the same datadict).  The result is that the incoming PR for fixing colliders and v-structures passes its tests. Also, dfs_labeled_edges test passes. In fact all the topological sort functions that were excluded pass the tests as well, though I'm not sure whether we should still exclude them from the nx-loopback tests because I don't know what the issue was regarding the function changing the original graph. E.g. maybe we shouldn't be copying the graph at all since it needs to be changed.  ~~This also seems to fix the long time for the nx-loopback tests. It is still about 8 secs (~10-15%) longer to run, but not multiple minutes.  Almost all of that is due to using `G._adj` directly instead of `add_edges_from` and `G.edges`.~~ [Edit: error on my part caused mistaken speed results. With the change it is the same speed as it was.]  @eriknw can you look at my comments near the top of the diff: 1) should any of these functions marked as being skipped be included? (~~I think the topo sort maybe~~) [A: Only the dfs_labeled_edges is fixed by the order of edges in loopback. The rest stay in place.] 2) the conversion of an AntiGraph to a normal graph class is just `G = nx.complement(A)`. But I'm not sure how that helps the code near the comment about AntiGraph.  [A: Removing comment about AntiGraph]
issue
Understanding nx-loopback graph conversion#TITLE_END#I'm having trouble figuring out how the `nx-loopback` backend converts the graph during testing.  It appears that the order of the nodes and edges change between running a doctest with vs without the nx-loopback backend. So I went looking for where/how the `nx-loopback` backend converts the graph during testing. I could not find any `dispatcher` object for the `nx-loopback` backend. I also looked for any code that tested for loopback to see if there is a hardcoded conversion/copying of the graph. And while I found a place where `copy(graph)` is used, it doesn't seem to be relevant to this test -- and I looked at `copy(graph)` manually and it doesn't cause the change in order I see when using nx-loopback.    The case I have been working with/testing is in a doctest, in case that matters.  I put the following in an example in a docstring (inside dag.py in my case): ``` >>> G = nx.DiGraph([(1, 2), (0, 4), (3, 1), (2, 4), (0, 5), (4, 5), (1, 5)]) >>> for node in G.nodes: >>>     print(f"{node}: {G.pred[node]}") ``` Running `pytest --doctest-modules --pyargs networkx/algorithms/dag.py` produces: ```     1: {3: {}}     2: {1: {}}     0: {}     4: {0: {}, 2: {}}     3: {}     5: {0: {}, 4: {}, 1: {}} ``` Running ` NETWORKX_TEST_BACKEND=nx-loopback pytest --doctest-modules --pyargs networkx/algorithms/dag.py` produces: ```     1: {3: {}}     2: {1: {}}     0: {}     4: {2: {}, 0: {}}     3: {}     5: {1: {}, 0: {}, 4: {}} ``` The order of the nodes in the G.pred dict change.  So, what is actually happening to `G` when the nx-loopback backend is used? Thanks!
issue
Remove import warnings during to_networkx_graph conversion#TITLE_END#Fixes #7401  This replaces the ImportWarnings with `pass` in the case when the environment doesn't have pandas or numpy or scipy installed. The idea is that this removes unwanted warnings without causing any surprises (the user could not be using an input that is a pandas/numpy/scipy object type because they library isn't installed).  Further improvements involve changing the order of consideration in `to_networkx_graph` so that common cases are caught first. That is a separate PR #7424 
issue
Update convert to use attribute based conversion#TITLE_END#In discussion about #7401 and #7402  This change removes all time spent issuing warnings and failing on imports during class construction. Instead of importing in order to check the type, it checks an attribute that identifies the type of the class.  Based on profiling, this removes the ~0.6s spent issuing 6000 warnings when creating 2000 graphs. The timing is the same  as removing the code that checks for pandas, numpy and scipy.   But I'm still not sure the timing is the trouble.  IMO It is really all the warnings that is the trouble.  If we decide this is a good way forward, we should expand the error message to say that the user should ensure all packages needed to read the data object are installed.  Thoughts?
issue
Move the backend docs and connect the config docs. Both in a single sidebar entry.#TITLE_END#Here's one attempt at separating the backend docs from other utilities.  It also adds the config docs to the html documentation on the same page as the backend description. When other config options (non backend) get introduced we should separate the config and backend pages.  So, now in the Reference pages there is a sidebar entry called "Backends and Configs". It describes the backend system, the _dispatchable decorator, and the nx.config object.  It'd be nice to have an overview of the `nx.config` object and its purpose and how to use it. But this PR only moves what we've already got in the doc_strings into the html docs.
issue
Add Python features that started with v3.10 now that v3.9 support is dropped#TITLE_END#I looked at the changes in Python 3.10 to see what we could now use and noticed: - [ ] the `match` statement which is a super-powerful switch/case system. - [x] the `|` operator acts with classes now (as this PR implements). The result is like a tuple combining classes in e.g. `isinstance(obj, class1 | class2)`   * Note: automatically handled by `ruff` `UP` rules in #7028 - [x] `itertools.pairwise` is now available (probably making our utility `pairwise` obsolete, but I haven't checked the details yet.  This issue should be considered done when someone has looked at these issues, not when all possible changes are considered. Otherwise this would stay around forever. Feel free to add other features of interest that I missed.  See #7028 [Edited to add this link]
issue
Dispatch related test failure on main branch when testing locally. #TITLE_END#I'm running tests locally on the main branch and get a test failure in `networkx/algorithms/bipartite/operators/tests/test_binary.py::test_intersection`. According to the comments there, that test should only be run "if not performing auto convert testing of backend implementations".  I suspect I have a misconfigured environment. Or at least an environment different from CI. Or is this unrelated to the dispatchable mechanism?  ```         ##################         # Tests for @nx._dispatchable mechanism with multiple graph arguments         # nx.intersection is called as if it were a re-implementation         # from another package.         ###################         G2 = dispatch_interface.convert(G)         H2 = dispatch_interface.convert(H)         I2 = nx.intersection(G2, H2)         assert set(I2.nodes()) == {1, 2, 3, 4}         assert sorted(I2.edges()) == [(2, 3)]         # Only test if not performing auto convert testing of backend implementations         if not nx.utils.backends._dispatchable._automatic_backends: >           with pytest.raises(TypeError): E           Failed: DID NOT RAISE <class 'TypeError'>  operators/tests/test_binary.py:57: Failed ========================================== short test summary info =========================================== FAILED operators/tests/test_binary.py::test_intersection - Failed: DID NOT RAISE <class 'TypeError'> ```
issue
make doc_string examples order-independent by removing np.set_printoptions#TITLE_END#Fix the pytest-randomly tests in networkx/linalg which started occasionally failing due to a new doc_string test that changed the print setting for numpy arrays. The printoptions were set to print 4 digits for array values. Luckily the example still fits in the code width using the standard 8 digits.  So this fix just removes the line that sets the numpy print options and expands the example to show 8 digits.  This should go in before v3.3
issue
DOC: add doc suggestions for arbitrarily large random integers tools#TITLE_END#Follow up to #6869 based on suggestions at the end of that discussion. main changes: - make `seed()` method raise `NotImplementedError` instead of quietly mask the random interface to `seed` - attempt to make the paragraph describing when to use which wrapper class more clear.
issue
Allow seed of np.random instance to exactly produce arbitrarily large integers#TITLE_END#Fixes #6848   - Introduce PythonRandomViaNumpyBits to replace PythonRandomInterface.    Follows Robert Kern @rkern comment suggestion in numpy/numpy#24458 - rewrite old interface to not raise (and use new interface to get a value) when high limit is > max int64 value - set up `create_py_random_state` to use old interface for possible Legacy users     (RandomState that isn't the default RandomState; All others users dont have claim to maintain the stream) - set up `create_py_random_state` so all other input provides the new interface - add smoke test of requesting very large int even with old interface - update tests of create_py_random_state to reflect changes  Technical debt: - Update decorator doc_strings to reflect numpy.random.Generators - shift to `random()` from `random_sample()` - correct the broken tests in test_decorators (assert a,b => assert a==b - add decorator tests for numpy Generator class
issue
Heads up.... numpy random integer generators cant handle large integers#TITLE_END#It looks like numpy random number generators cant handle large integers (large being > `2**64`, i.e. not held in `int64`). They raise an error saying that the values can't be held in `int64` (which of course is correct).  And if you try to specify that they use python integers (which don't have that limit)  by using `dtype=np.object_`, it raises stating that np.object_ is not of integer type.  We probably use large integers more than most scientific libraries (20**20 is the number of possible edges with 21 nodes). So, we may run into this more often than some.   Workaround: instead of `seed = np.random.default_rng(); seed.integers(0, 20**20)`, we could convert it to a floating point computation and discretize later: ```python seed = np.random.default.rng() def randint(a, b, seed):     return a + int(seed.random() * (b - a))   randint(0, 20**20, seed) ``` This could/should be added to the `utils.misc.py` class `PythonRandomInterface` method `randint`.  ### Numpy fix Since numpy is creating the value, it seems like they don't need to raise an error for `dtype=np.object_` in `np.random.Generator.integers`. It could be made an integer Python object and be perfectly good output. We could/should look into making a PR to numpy to allow `integers` to return Python integers when `dtype=object_`  I think this is the first time we've noticed a feature of `random` that isn't provided by `numpy.random`, but there may be others.
issue
Update imports to use lazy imports#TITLE_END#This PR changes the idiom `import numpy as np` within each function by moving it to the top of the module and using `np = nx.lazy_import("numpy")`, with similar treatment for the other libraries.  Another PR will change the NetworkX loading process to ensure lazy loading of all internal packages (subpackages).
issue
Initial setup of lazy_import functions.#TITLE_END#Lazy load of potentially uninstalled libraries.  Also, a function to set up the internal sub-packages in a lazy-load fashion.  This first version will fail tests in environments without numpy because pytest.importorskip is not lazy yet.   Still needs: - way to handle pytest.importorskip - Loader class instead of monkey patch on SourceFileLoader - Way to identify a lazily-loaded-not-yet-used module 
issue
Stabilize test of approximation.connected_components#TITLE_END#There are Sporadic test failures when testing connected components of `AntiGraph` in `test_kcomponents.py`. The tests checks whether ccs of G and AntiGraph(G) are the same. But that is not true for graphs with isolated nodes or with nodes that connect to every other node. One of the graphs being tested is generated using `gnp_random_graph` without a seed. So occasionally it will construct a graph with a node connected to every other node.  Fix: Assign a seed to make sure the random graph doesn't produce such a node  
issue
Avoid directed_laplacian_matrix causing nans in some cases.#TITLE_END#Fixes #6865  Uses `np.abs` before `np.sqrt` to force round-off that switches sign to stay positive. That avoids creating nans via sqrt.  I'd like to add tests for this -- but the example from the OP is 407 lines long. So I'm still looking for a test.
issue
fix doc build errors/warnings#TITLE_END#Minor doc build errors warnings
issue
Separate dispatching test from other actions#TITLE_END#I got confused between the dispatching/loopback tests and the "normal" networkx tests while reviewing #6929 I think it would help me (and maybe new people?) if the tests for dispatching were split out from the rest of the matrix. This PR is an attempt to do that. I made a new matrix called `backends` that only include Ubuntu with python 3.11. Suggestions welcome!
issue
Fix triangles to avoid using `is` to compare nodes#TITLE_END#Fixes #7038   Using `n is not node` compares whether n and node are the same object, not whether they are equal. Thanks to a bug report in #7038 we discovered that the string "46" can sometimes be created twice and not [interned](http://guilload.com/python-string-interning/)so that `n is not node` returns True even though both are equal to "46".  The error occurs in `triangles` when ensuring that a node in not included in its list of neighbors (ruling out self-loops). `n` is the neighbor in question and `node` is the node being checked.  This only goes to prove that "46" is not always the same object as "46". They are the same object if they are interned.  Unfortunately figuring out when interning occurs is quite complicated and it may change. So I could not think of a good test to write for this error.  So, this PR just replaces the idiom `n is not node` with `n != node` which is really how node comparisons should be written always.  I don't think we usually do backports but if we release a 3.2.1 for any reason, we should include this fix. It does not occur in v3.1.
issue
Improve error messages for misconfigured backend treatment#TITLE_END#Fixes #7047   Allow pytest to run even when `nx-loopback` is not available resulting in 5 failed nx-loopback tests, but other tests run OK.  I put a warning message into this version for the PR, but we could instead have no message (no one should ever be in this situation except maybe our future selves). And we could raise an exception instead of a warning.  These error s create a ModuleNotFoundError which is handled by our lazy_loading code. The error message in that code tries to provide information about the `context` of the error, but in this case the context's value is `None`. So I added a hack to handle the None allowing the error message to be created in cases where the context is None. 
issue
Add label workflow pull_request type synchronize and echo message#TITLE_END#If `edited` works fine this will not be needed. But I think `synchronize` might be what we want. edited doesn't seem to get triggered upon a push to the branch.  <!-- Please use pre-commit to lint your code. For more details check out step 1 and 4 of https://networkx.org/documentation/latest/developer/contribute.html --> 
issue
Refix minimum_cycle_basis and scipy.sparse conversions and add tests#TITLE_END#Follow-up testing from #6788 showed a number of interesting bugs and deficiencies. - My error using `any` instead of `sum` followed by `% 2` in one line. This led to the returned "basis" being not independent in some cases. - The tests don't check non-uniformly weighted graphs sufficiently to catch the above error. So add tests. - One reported example from @mbr085 raised a ValueError which was caused because an edge was added as `np.int32` instead of python `int`. I chased this down to `convert_matrix.py` and the handling of conversion from sparse matrices. More below if you are interested.  This PR makes 3 commits to fix these issues. The first adds tests to show the errors reported in #6788. The second adds the sum/modulo term to enable passing those tests. The third corrects the sparse matrix treatment to match that of conversion from numpy arrays.    TL;DR: We have to be careful not to mix nodes that are `np.int32` and nodes that are python `int` in the same graph because they equate to each other (and thus are the same node) but they compare to other nodes differently. For example `int(5)` and `np.int32(5)` are equal so you can work with both on a graph as if they are the same node. But the `np.int` value raises a ValueError when compared to, say, a tuple `(5, 1)` because it treats the tuple as an array and the sizes don't match. The python in returns `False` when you check equality of an integer to a tuple. Either storage of nodes is fine, but if you use `np.int` and then **also** add nodes to your graph that are tuples, you can get ValueErrors raised when comparing the tuple-node with the np.int-node.  NetworkX handles numpy.array conversion by making nodes python int.  It handles scipy.sparse conversion by making nodes int -- but crucially, failing to convert the np.int32 indices to python in when adding edges. So inside `G.adj` the keys are python ints and but the keys to the inner nbrdict `G.adj[5]` are `np.int32` with the same value. Amazingly, this works fine almost all the time. But if you add another node that is a tuple, you can run into a raised ValueError when our algorithms check whether two nodes are equal.  I fixed the conversion functions to convert `indices` values to python int so thy match the nodes in the graph.
issue
Add other correct test results to the test checks for divisive#TITLE_END#Some tests had more than opne possible answer, but only checked for the one given by this implementation. Testing on nx-parallel as mentioned in networkx/nx-parallel#36 .  This PR adds the other possible correct test results to these tests. 
issue
add seed to graph creation#TITLE_END#Fixes #7236   One of the test graphs is currently random graphs and that sometimes affects the `max_tries` count needed for the `directed_edge_swap` to be successful.  I added seed to the graph creation call so the tests become deterministic. [Edited -- only one graph creation routine needed seed added: `random_tree`]
issue
add seed to tests of fast_label_propatation_communities#TITLE_END#Add seed to tests of `fast_label_propatation_communities` These tests were sometimes failing. Fixes #7240  
issue
Provide non-normalized directed laplacian matrix calculation#TITLE_END#When we created the directed versions of the laplacian_matrix function, we included the two normalized matrices, but we didn't include the non-normalized laplacian matrix. See #3297, #2404, #741 At the time of #741, the `laplacian_matrix` worked for directed and undirected graphs and returned the non-normalized version of the matrix. (A single stream of code can be used for either Graph or DiGraph in the non-normalized case.)  The restriction to only undirected came later #833 and without discussion about the distinction between normalized and non-normalized. The recent activity about `total_spanning_tree_weight` in #7100 and number_of_spanning_trees in #7065 shows that the original non-normalized version of the laplacian matrix works for directed and is needed for calculations like the number of spanning trees.  I propose that we use the existing code for `laplacian_matrix` to return the matrix for either undirected or directed graphs. The doc_string should include "See Also" pointers to the `directed_laplacian_matrix`, `directed_combinatorial_laplacian_matrix`, and `normalized_laplacian_matrix`. In the doc_strings for those functions (or maybe in the module level doc_string, or maybe both:  We should describe how all these relate.  (The non-normalized version is produced by the main function, and normalized versions for undirected and the two normalizations for the directed case are provided by the other functions.) We should also point out that another possible non-normalized directed version would have the in-degree of each node on the main diagonal instead of the out-degree.  That can be obtained by applying the function to `G.reverse(copy=False)` instead of `G`. Perhaps an example would be useful to include showing this.   This issue is proposing to do one change for code, add some tests, update docs:  - remove the `not_implemented_for("directed")` decorator on `laplacian_matrix` - add some tests for `laplacian_matrix` as applied to directed graphs - change the docs s described above  This is perhaps more involved than some "good first issue" items, but only in that it has more steps. I think each item is "straight-forward" (which in many math classes means "possibly difficult but using known methods"). So I've put the good-first-issue label on here.
issue
New PR for Fixes minimal d-separator function failing to handle cases where no d-separators exist#TITLE_END#I couldn't push a rebase to PR #6438 (permission denied even though the branch allows maintainers to submit. perhaps because fork was originally not from networkx/networkx but I'm not sure why). So....  I have made a new PR with a rebased version of #6438.  @jaron-lee if you would like to pull this branch into your branch that might work. Otherwise we can use this new PR to continue the discussion from the old one. All the commits should still give attribution to @jaron-lee.  Fixes #6430   For completeness, the original post was:  >Fixes https://github.com/networkx/networkx/issues/6430.  >Implement a check to verify the candidate minimal d-separator set is in fact a valid d-separating set Added test to verify the function now returns None if there are no valid d-separating sets Updated release_dev.rst to indicate changed API behavior Updated documentation as required
issue
Check `not_implemented_for` usage pattern for mistakes#TITLE_END#I have noticed a few places where the `not_implemented_for` decorator is called such that it excludes MultiDiGraph, but not MultiGraph nor DiGraph. That might be correct in some of these cases, but I suspect some/many of these functions are not supposed to be used with directed graphs nor with multigraphs.    A quick `git grep 'not_implemented_for("directed", "multigraph")'` shows: ``` networkx/algorithms/approximation/maxcut.py:@not_implemented_for("directed", "multigraph") networkx/algorithms/approximation/maxcut.py:@not_implemented_for("directed", "multigraph") networkx/algorithms/community/asyn_fluid.py:@not_implemented_for("directed", "multigraph") networkx/algorithms/distance_regular.py:@not_implemented_for("directed", "multigraph") networkx/algorithms/distance_regular.py:@not_implemented_for("directed", "multigraph") networkx/algorithms/isomorphism/tree_isomorphism.py:@not_implemented_for("directed", "multigraph") ``` We should check them.
issue
Document walk_type in directed_laplacian and friends#TITLE_END#The current documentation of the "walk_type" argument to the many directed laplacian matrix approaches is opaque. And the logic is hidden within the actual code because it occurs in a helper function presumably so the logic is the same for all these functions. The logic should be fairly straightforward to explain. For the directed graph case it is given in `linalg.laplacianmatrix._transition_matrix` as ```python     if walk_type is None:         if nx.is_strongly_connected(G):             if nx.is_aperiodic(G):                 walk_type = "random"             else:                 walk_type = "lazy"         else:             walk_type = "pagerank" ``` We should put a word description of this into the `walk_type` parameter description.. 
issue
Remove or improve create_using argument#TITLE_END#In #1393 we started to formulate an API that would remove the ```create_using``` keyword in favor of generating (or passing around) ```(edges, nodes)``` pairs.   A first step was #3028 which added a base class method to add nodes and edges to a graph. This allow ```G.update(generate_path(9))``` so long as ```generate_path``` creates any of:  - ```edgelist``` - ```(edges, nodes)```  tuple -  a graph object  With the update method present, we can remove the ```create_using``` argument and document that people should replace the idiom:      G = nx.path_graph(9, create_using=nx.MultiGraph)  with:       G = nx.MultiGraph().update(nx.path_graph(9))     # we might be able to change the constructor to allow nx.MultiGraph(nx.path_graph(9))  We might also consider changing the generators to yield edges or return (edges, nodes).  This could become a v3.0 feature, but for now I'm putting v2.3 as the milestone.  Reread the discussion in #1393 and then discuss here what the design should be.  :)
issue
label check on push and change check name#TITLE_END#Make label check activate on each push (currently red x disappears after first run because not rerun on push)  I also renamed the label check so it hopefully makes it clear that a reviewer gives the label. Other name suggestions welcome. I tried to keep it under 20 chars.
issue
Add label check when pull request is edited instead of push#TITLE_END#the push trigger occurs even if it is not a pull_request. So we need to use a pull request action rather than push. This PR tries to add the pull_request type `edited` instead of push.
issue
Deprecate shortest_path functions to have consistent return values in v3.3#TITLE_END#See #6527 for a discussion of the changes needed for making the return values consistent.  This deprecates the function to be changed. `single_target_shortest_path_length` and `shortest_path`  in the case that `source is None and target is None`
issue
3.2.1 release#TITLE_END#Closes #7064  I think we have about 5 actual bug fixes merged since the release.  One affects being able to import networkx on some configurations. If we're thinking of a patch release, which PRs should be included?  Here's my take. But others should push back and Jarrod should give the final word. We have merged 16 PRs since the release:   Only one should **not** be included in a patch release (the one dropping python 3.9). All the rest can be included. OR we could just add the 5 that fix broken things.  Fix broken stuff: #7034 - syntax error upon import #7041 - is should be == #6825 - fix new from_numpy feature of no edge attr #7042- add find_extendability to the docs (broken link) #7030- release numbers stopped working with glob. go back to manual list  Safe to backport but not anything broken: #7043  - favicon #7048  - avoid docs warnings #7053 - remove code from 6925 that is after `return` statement (not usable) #7056 - doc typo #7057 - don't allow negative sizes in complete_multipartitite_graph  #7018 - harmless doc improvement #7055 - fix small graph name attribute #7062 - better error handling that shouldn't ever happen except to us. #6999 - add tadpole #7029 - update release process  Don't backport: #7028 - drop support for Python v3.9  ```
issue
backend nx-loopback not found when trying to run pytest#TITLE_END#In some configurations (that I haven't been able to track down) I can't use pytest with networkx.  I've traced the problem to [line 55 in conftest.py](https://github.com/networkx/networkx/blob/46d67fecfec615215cfa8b26e5024ae743e5973a/networkx/conftest.py#L55) where the `nx-loopback` entry_point is obtained and stored in the backends dict.  The problem arises when the `entry_points()` call returns an empty list.  The next line then tries to iterate to find the first element of the list, but gets a StopIteration instead.  I'm thinking that we need to add a check there for finding no entry point and handle it in some way. But I don't know enough about entry points to figure out what to do. And I'm guessing a new developer trying to test their added function to networkx wouldn't know what to do either.    Do I have to do something with my environment to turn on the networkx entry_points? Do I have to turn on entry_points to run pytest on my local repo?  <details>  ``` INTERNALERROR> Traceback (most recent call last): INTERNALERROR>   File "/Users/dschult/mambaforge/lib/python3.10/site-packages/_pytest/main.py", line 265, in wrap_session INTERNALERROR>     config._do_configure() INTERNALERROR>   File "/Users/dschult/mambaforge/lib/python3.10/site-packages/_pytest/config/__init__.py", line 1046, in _do_configure INTERNALERROR>     self.hook.pytest_configure.call_historic(kwargs=dict(config=self)) INTERNALERROR>   File "/Users/dschult/mambaforge/lib/python3.10/site-packages/pluggy/_hooks.py", line 514, in call_historic INTERNALERROR>     res = self._hookexec(self.name, self._hookimpls, kwargs, False) INTERNALERROR>   File "/Users/dschult/mambaforge/lib/python3.10/site-packages/pluggy/_manager.py", line 115, in _hookexec INTERNALERROR>     return self._inner_hookexec(hook_name, methods, kwargs, firstresult) INTERNALERROR>   File "/Users/dschult/mambaforge/lib/python3.10/site-packages/pluggy/_callers.py", line 113, in _multicall INTERNALERROR>     raise exception.with_traceback(exception.__traceback__) INTERNALERROR>   File "/Users/dschult/mambaforge/lib/python3.10/site-packages/pluggy/_callers.py", line 77, in _multicall INTERNALERROR>     res = hook_impl.function(*args) INTERNALERROR>   File "/Users/dschult/NX/NXmyshort/networkx/conftest.py", line 56, in pytest_configure INTERNALERROR>     networkx.utils.backends.backends["nx-loopback"] = next(iter(backends)) INTERNALERROR> StopIteration ``` </details> 
issue
fix extendability function name in bipartite.rst#TITLE_END#fix extendability function name in bipartite.rst  The name of the function maximal_extendability in the bipartite docs was not updated from `find_extendability` during PR review. So, the function doesn't show in current docs and creates quiet failures for documentation builds when looking for `find_extendbability`.  This might help e.g. #7040 successfully build the docs, but I am not sure. It should be fixed in any case.
issue
Rename function `join` as `join_trees`#TITLE_END#Closes #6906   Rename `algorithms.tree.operations.join` to `algorithms.tree.operations.join_trees`. Deprecate old function name.  I kept the old function name in the docs with a message stating the deprecation and pointing to the new name.  Added note to deprecations.rst and updated tests.
issue
change default for new function `join_trees` #TITLE_END#As mentioned in #6910 we could make the default behavior for the `label_attribute` be that the old node labels are not stored on the joined trees.  > In other words, rather than adding "_old" attributes to every node in the joined tree by default, would it make more sense to only create these attributes when label_attribute is explicitly provided? This would have the advantage of not creating a bunch of extra node attributes by default.  So, let's remove the part of the code where the "_old" attribute is added by default. That will also require a check below where we store the old labels. If `label_attribute is None` then we just return the graph without storing the old labels.  Let's also add a keyword argument `first_label` matching that argument of `convert_node_labels_to_integers`. Instead of the root node being `0`, it would be `first_label` which is determined near line 106 when we `chain([0]...` and the `accumulate` function will also need a new `initial` argument.  We should add tests of both no label attribute and first_label other than 0.
issue
Some examples in tournament.py don't use tournament graphs#TITLE_END#Most `tournament` functions assume the input graph is a tournament. And the docs say the user must check whether it is a tournament. But some of the example don't use tournament graphs.  The docs say that the function's output is undefined when the graph is not a tournament graph. So, while the examples do produce the output they claim to produce, it is a meaningless result.  For example: `tournament.is_strongly_connected(G)` returns False even though the graph in the example is strongly connected. ```python >>> G = nx.DiGraph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3), (3, 0)]) >>> nx.tournament.is_strongly_connected(G) True >>> G.remove_edge(1, 3) >>> nx.tournament.is_strongly_connected(G)  # this graph is strongly connected. False ``` The function `is_strongly_connected` returns `False` even though the graph G is strongly connected.  This is because both examples used are not tournament graphs (exactly one directed edge between each pair of nodes).  I think every example in this module should add a line checking if the graph is a tournament before running the function. There may be others that are not valid example uses of the function. And those examples will reinforce the good practice of checking each time a graph is changed.
issue
add missing `join` deprecation stuff to release_dev and conftest#TITLE_END#I forgot to update `conftest.py` and `release_dev.rst` for the renaming `join` deprecation in #6908  This adds them.
issue
Review G.adj as property instead of attribute#TITLE_END#Currently G.adj is a property that creates an AdjacencyView of G._adj. Is this better than creating an AdjacencyView during Graph.__init__?  As a property, it speeds up graph creation by one AdjacencyView instantiation. But it slows very slightly each use of the form G.adj  Are there other reasons besides speed that a property is better?  (like, switching G._pred and G._succ to reverse directed edges)  Let's look into it...
issue
Add test about zero weight cycles and fix goldberg-radzik#TITLE_END#Fixes #6874   The NetworkX shortest path algorithm Goldberg-Radzik handles negative edge weights. It finds negative weight cycles and raises in this case. But it incorrectly identifies zero-weight cycles as negative cycles.  But zero weight cycles do not make the shortest path unbounded. The path found is not unique, but it exists and has a finite length. (Bellman-Ford in NX correctly reports these paths.)   This PR corrects Goldberg-Radzik to report shortest paths in the presence of zero-weight cycles. It adds a test of g-r and b-f algorithms. And it adds a short note in the doc_string of goldberg-radzik that the behavior has changed as of NX v3.2.  I don't think we need to deprecate since it was an incorrect result. But I thought a note in the doc_string would help people who find that their code now behaves differently.  <!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> 
issue
add docs for source input of dfs_predecessor and dfs_successor#TITLE_END#Add to the doc_strings for `source` input argument of `dfs_predecessors` and `dfs_successors`. Fixes #6795
issue
number_of_walks might use a weighted edge attribute#TITLE_END#number_of_walks might use a weighted edge attribute if that graph has any edge attributes not equal to 1. That will make it compute the number of walks incorrectly since it uses a power of the adjacency matrix.  This fix simply changes the call to adjacency_matrix to have `weight=None` explicitly set. Test added too.
issue
generic_bfs_edges has apparently duplicated input arguments#TITLE_END#The `generic_bfs_edges()` function has input arguments `neighbors` and `sort_neighbors` and they seems to do the same thing -- or more precisely, their functionality duplicates.   `neighbors` is "A function that takes a newly visited node of the graph as input and returns an *iterator* (not just a list) of nodes that are neighbors of that node"  while `sort_neighbors` is "A function that takes the list of neighbors of given node as input, and returns an *iterator* over these neighbors but with custom ordering."  In the code, the neighbors of a node are found using: ```python lambda node: iter(sort_neighbors(neighbors(node))) ``` But that just means that `neighbors` should have been set by the user to this composition of functions.  We need to at least make the doc_string more clear, but it seems that we should eliminate the `sort_neighbors` input and have users just provide `neighbors`. And the doc_strings should correctly reflect whether these function actually need to return iterators or not.
issue
Fix minimum_cycle_basis and change to return cycle instead of set#TITLE_END#Fixes #6787  Fixes #6783  This does change the output of this function from a set to a list.  But given that it was returning wrong results and sets that were not very useful, I'd argue we should just fix this instead of deprecating.  @mbr085 I think this will fix both of your problems. You can install from the branch for this PR, but it might be easier to download the changed `cycles.py` file and then install it in your `site-packages/networkx/algorithms/cycles.py`. Sorry for the broken code. I hope this fixes it.   I think the code is easier to read this way too, but I wish I could avoid the 2-pass process to remove extra edges based on multiplicity. I managed to convince myself that you had to do 2 passes. But I wish I didn't. :) 
issue
keep out scipy 1.11.1#TITLE_END#Scipy released 1.11.1 without waiting for the fix we need to be able to use superLU.  
issue
Improve handling of create_using to allow Mixins of type Protocol#TITLE_END# switch create_using handling to `isinstance(_, type)` instead of `type(_) is type`  Fixes: #6243 
issue
add OrderedGraph removal as an API change in release_3.0.rst#TITLE_END#<!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> 
issue
Rewrite NXEP 3#TITLE_END#1) keep create_using 2) allow edgelist generation via [~~nx.path_graph.edges_plus(9)~~] `nx.path_graph_generator(9)`  
issue
try adding circleci artifact secret#TITLE_END#This PR will (maybe) work once the secret is added to the github secrets setting
issue
tweak _dispatch to allow G keyword. Add test.#TITLE_END#The `_dispatch` decorator assumes that the first argument is a positional argument. This PR allows `G` to be provided as a keyword. Note that `G` is still expected to be the first positional argument. This only affects how people can call it:  `nx.pagerank(G=G)` and `nx.pagerank(G)` should be the same.  Fixes #6458
issue
Update docs to reflect lazy imports#TITLE_END#Now that we can e.g. use `networkx.commuity` functions without explicit imports, we should update our documentation to reflect that feature. For example, with the community subpackage we can use `nx.community.<funcname>`. It should be similar for the others I believe. What are the others? Is there an easy way to search for documentation imports of subpackages?  :}
issue
Add project desc for visualization and ISMAGs#TITLE_END#I've added two possible project topics.  I could also add isomorphism VF3 or RI if we want to continue to focus on cleaning up the isomorphism subpackage.  I removed my name from the Notebooks project mentoring because I haven't been doing much review there. Please add your name as an interested mentor if you are interested. (or leave a comment and I will add it).  Suggestions?
issue
Fix len operation of UnionAtlas#TITLE_END#The UnionAtlas class was computing `__len__` assuming that the two dicts being unioned have distinct keys. That is not true in general, and certainly not in the case of directed graphs with bidirectional edges.  At the UnionAdjacency level the keys must be the same (one for each node). But inside the UnionAdjacency is a UnionAtlas and for directed graphs it is a union of `_pred[node]` and `_succ[node]` which can have different keys, but it doesn't have to have different keys.  This fix corrects the computation of `__len__` to use the union of the two sets of keys, rather than the sum of the lengths.   Fixes #6336   
issue
Update code in similarity.py #TITLE_END#The code in `similarity.py` uses some out of date Python constructions and would be IMO clearer and easier to maintain if it was updated.    Looking through the code for `match_edge` while reviewing #5516  I saw some rewrites that could take advantage of current Python features. For example, there are many `list` functions calls which should be made into list comprehensions. Similarly the `zip(range(m), g_ind)` and similar structures should either use `enumerate` or avoid indexing completely. e.g. `for g in g_ind` instead of `for i, g in zip(range(m), g_ind)`. Furthermore, the `match_edge` function (and likely others) would be easier to read IMO if we handled the case of an empty `matched_uv` up front.  That case falls through the logic, executing various `else` clauses when we know what will happen to it.  And there should be no default value on the keyword argument `matched_uv`.
issue
Create a Tikz latex drawing feature for networkx#TITLE_END#This PR provides a drawing feature (module and tests) from NetworkX to the TikZ library of TeX/LaTeX. The basic interface is `nx.to_latex(G, pos, **options)` to construct a string of latex code or  `nx.write_latex(G, filename, as_document=True, **options)` to write the string to a file.  The `as_document` option puts the LaTeX figure inside a document environment with preamble that loads TikZ. When False, the code for the figure stands alone in the file ready to be `\input` into your LaTeX file.  Alternatives to #5639 and #5702   This makes them obsolete if merged.
issue
NetworkX graphs to LaTeX#TITLE_END#This code was built using the adigraph latex package available on CTAN. It got old enough that the original repository was deleted and so was ported over to this branch. Original pr and discussion is #3591 
issue
Refactor vf2pp modules and test files#TITLE_END#This PR puts all the vf2pp modules into a single module, removing the subpackage vf2pp_helpers. For the tests, it forms two test files:   - tests/test_vf2pp.py      tests the public functions for the overall algorithm   - tests/test_vf2pp_helpers.py    tests the private functions used by the public functions  The test files seemed too big to combine any more than this, but I suspect we can/will reduce their size over time via simplification and more focused unit testing.  Other items I changed for better white-space handling: - switched from `itertools.cycle` to `it.cycle` by changing the import - changed the name `labels_different` to `labels_many`  I will adjust #6230 based on this new refactoring. I consider that as a separate issue and PR -- certainly harder to review. Thoughts?
issue
fix warnings for make doctest#TITLE_END#Fix the warnings that arise from `make doctest` 
issue
Allow MultiDiGraphs for LCA#TITLE_END#Fixes #5209 by checking that LCA and tree_LCA work for MultiDiGraphs. Removes NotImplementedFor exception in these cases. Adds tests.
issue
Reconfigure Adigraph interface from NetworkX to Latex#TITLE_END#This builds on PR #5639 and proposes a new interface focusing on  - to_latex(G, caption, latex_label, as_document=False) - write_latex(G, "just_a_figure.tex", caption, latex_label, as_document=False)  I have put it in a separate PR because I want feedback on the interface. I like the functions here, but handling the options for node_color/edge_width/etc feels kludgy. With two PRs we can more easily compare the two interfaces.  Suggestions?  The Adigraph interface is split into two classes:  - Adigraph handles a single graph and it's options.     * ADIG.to_latex_raw()     * ADIG.to_latex_figure()     * ADIG.to_latex_document() - AdigraphCollection handles multiple graphs to be put into subfigures within a figure.      * ADIGcoll.to_latex_figure()     * ADIGcoll.to_latex_document() 
issue
Add dfs_labeled_edges reporting of reverse edges due to depth_limit.#TITLE_END#Fixes #6239  The new reporting of edges labels them as "reverse-depth_limit" rather than simply "reverse" to make clear that they exist due to cutting off the exploration due to reaching the depth_limit. 
issue
Warn users about duplicate nodes in generator function input#TITLE_END#As we discussed last summer, some surprising results can occur if you input duplicate nodes in functions that use the nodes_or_number decorator. We decided to leave them in the code (some people might rely on them and they don't seem to be a problem for users).  For example:  nx.complete_graph([1, 1]) returns a graph with a self-loop.  nx.complete_graph([1, 2, 2, 3, 3, 3]) is the usual complete_graph on 3 nodes with selfloops added for node 2 and 3.   This PR adds a warning to the doc_strings for these functions just so the users have a hint that duplicate nodes in the list of nodes can cause unusual results.  
issue
Self ancestor is not found using lowest_common_ancestor#TITLE_END#This PR adds tests to show that `lowest_common_ancestor` and friends do not always work correctly when the 2 provided nodes are the same.  We should track down whether this is a corner case that can be fixed by special treatment, or a deeper bug in the implementation.  See #4458  
issue
Add heuristics for Traveling Salesman Problem #TITLE_END#From @Overriders work on #1508 and #1554 
issue
Update matching functions for error validation and speed#TITLE_END#Expand tests Change API to raise NetworkXError when matching involves nodes not in G Update is_*_matching to 100+ times faster.  Still need to:  - [x] consider moving away from representing matchings as sets of frozensets. - [ ] look to speed up max_weight_matching - [x] improve tests - [x] correct and improve documentation
issue
Update imports to use lazy loading#TITLE_END#This PR should only be merged **after** #4909   This PR changes the `import numpy as np` by moving it to the top of the module and using `np = nx.lazy_imports.load("numpy")`, with similar treatment for the other libraries.  Another PR will change the NetworkX loading process to ensure lazy loading of all internal packages (subpackages).
issue
fix greedy_modularity when multiple components exist.#TITLE_END#Fixes #5530   After the community merging iteration stops, we have to check whether the number of communities is still too many for the `best_n` criteria.  If so, merge the biggest 2 components repeatedly to keep modularity as big as we can.   
issue
Cache edges, degree, adj properties of Graph classes#TITLE_END#Use cached_property for caching the view properties of graph classes (not `G.name` which can be changed and thus should be a regular non-cached property).  The changed properties include: G.edges, G.degree, G.adj  (and G.in_edges, G.out_edges, G.in_degree, G.out_degree, G.pred, G.succ).  Added tests that multiple calls return the same object (caching is successful).   Changes to Graph, DiGraph, MultiGraph, MultiDiGraph and AntiGraph.  Also reportviews.py had to change one `__setstate__` function. 
issue
Attempt to reverse slowdown from hasattr  needed for cached_property#TITLE_END#As discussed in #5796 there was a slowdown in e.g. `copy` when we changed the base classes to use `cached_property` instead of `property`.  This was due to the need to check whether to reset the cached property during `__init__`.  But it actually doesn't have to happen during `__init__`. So I have moved that check to the places in our codebase where we are replacing or changing `G._adj`/`G._succ`/`G._pred`. The danger is that we change `G._adj` and don't get an updated cached_property.  Within our codebase this is only needed inside `graphviews.py`.  It is hard to know how many users would be affected. If they swap out `G._adj` without resetting the cache property on `G.adj` then they might see some strange results where `G.adj` reports one thing while `G._adj` reports another.  We could avoid all this trouble by computing `G.adj` as an attribute instead of a property.  It would cause creation of the `AdjacencyView` during `__init__` rather than on the first lookup of `G.adj`. So some benchmarks might show a slowdown. Indeed, creating a graph would be slower by the amount of time needed to create the AdjView. Presumably that is more than the time to do a `hasattr` which caused the slowdown we are currently fixing. The tradeoff is between potential for users playing with private attributes to mix things up vs extra work creating each graph object.   Note: I only really looked at the regression for `copy` assuming it would fix the others. We should check that the others don't have that slowdown after this change.
issue
update tests in base class and simple rename in convert.py#TITLE_END#This splits out the minor refactoring test improvements from #5836 to ease review.  - One test function was not a test method so not being tested in the multidigraph test subclass -- moved it into a class. - Tests of subclassing MultiDiGraph were copied and adapted to MultiGraph - Changed the names of the exception variables in convert.py where nested try/except clauses are named the same. This should ease debugging when it is not clear which exception is being raised. 
issue
Move factory attributes to the class instead of instance.#TITLE_END#We originally had copied the class factory attributes to the instance for an attempt as speed-up. It now looks like it is a small slowdown.  But perhaps more importantly given the minor impact of these speedups is that it should be easier to read the `__init__` function without getting hung up in all the factories. And trying to figure out why we would be copying them from the class to the instance anyway.   BTW my measure of the speed up is about 15-20% of the time to create a Graph object... but that is so small to start with that it probably isn't important.  Still, if this way is slightly faster and easier to read, maybe we should do it.  This has been separated from #5836 to ease review.
issue
make lazy_import private and remove its internal use#TITLE_END#A bug in lazy_import (see #5838) or in Python's LazyLoader causes trouble for direct import of subpackages after the lazy load but before the full load. We believe this stems from a difference in how LazyLoader imports to subpackages compared to the standard import process.  We are not sure whether this will need Python LazyLoader changes or if there is a workaround locally.    This PR removes the use of this function in NetworkX (one place) and makes the function itself private (hopefully easy for people who still want to use it to convert for the new version of NX, but discouraging others from using it until we figure out a fix).  
issue
Add cache reset for when G._node is changed#TITLE_END#Fixes #5893  Add a data descriptor which resets the cached G.nodes whenever G._node is set to a new object.
issue
Fix Louvain_partitions by yielding a copy of the sets in the partition gh-5901#TITLE_END#Fixes #5901 by yielding a list of copies-of-sets in the partition instead of the partition-list-of-sets that will be used for computing the next partition.
issue
Allow classes to relabel nodes -- casting#TITLE_END#Fixes #5896   Currently classes like str/int and friends don't work in `nx.relabel_nodes` even though they are callable and functions do work. When we check for the attribute `__getitem__` to see if we can use a lookup to the mapping, we should also check that the `__getitem__` attribute is a bound method. If it is not, we can't use it as a mapping so we try calling `mapping` like a function.  Here's a test that ensures classes can be used to "cast" nodes to a new type. It currently fails and this PR enables it. ```python     def test_relabel_nodes_classes(self):         G = nx.empty_graph()         G.add_edges_from([(0, 1), (0, 2), (1, 2), (2, 3)])         H = nx.relabel_nodes(G, str)         assert nodes_equal(H.nodes, ["0", "1", "2", "3"]) ```
issue
Add function bfs_layers to docs#TITLE_END#The bfs_layers function seems to have been left out of the docs.  Also fixed two doc_string formatting typos. <!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> 
issue
Add Tidelift security vulnerability link#TITLE_END#<!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> 
issue
update secutiry link to tidelift#TITLE_END#(not sure if it matters but trying to clear tidelift "tasks")  <!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> 
issue
add missing `seed` to function called by `connected_double_edge_swap`#TITLE_END#Inside `connected_double_edge_swap`, one call to `nx.utils.discrete_sequence` does not give the `seed` as input which will mess up trying to make it deterministic by providing a seed.    This PR adds `seed` to that function call.   I haven't been able to make this into a test -- and I spotted it while going through the code... not due to a behavior I didn't expect.
issue
VF2pp tests are not passing during the regularly scheduled randomly ordered tests #TITLE_END#I think we decided that this is due to some tests changing the graph stored in a test class attribute. @kpetridis24 has some ideas for how that would get solved. It'd be good to get this merged soon to avoid these failures, so we should put it in a separate PR.  
issue
Add ThinGraph example to Multi*Graph doc_strings#TITLE_END#Based on discussion in #5813 we should add an example of a subclass of the graph classes to the MultiGraph and MultiDiGraph classes. The `ThinGraph` example from the Graph and DiGraph doc_strings was suggested in that discussion and would be easy to copy over.
issue
check that expanders tests are all being run#TITLE_END#Based on a comment in #6014 it would be good to check that the tests in `generators/tests/test_expanders.py` are all being run. If they are, the tests should have raised an error upon usage of `.A` (which was fixed in #6014).  At least that's how I understand the discussion ... 
issue
Add bfs_layers function to the bfs_* module#TITLE_END#The PR #5788 by @kpetridis24 on VF2++ has a nice function called `BFS_levels` which in slightly generalized form would be a good addition to the [breadth_first_search.py](https://github.com/networkx/networkx/blob/main/networkx/algorithms/traversal/breadth_first_search.py) module. It is very similar to the [topological_generations](https://github.com/networkx/networkx/blob/155513702c6562882821f61be80a71ab4d3599b1/networkx/algorithms/dag.py#L161) function but not restricted to DAGs. The function in #5788 processes each level, while I think the generalized function should just yield the levels as sets of nodes like the `topological_generations` function does. I suppose it could be lists instead of sets since there is a graph ordering of the nodes.   What's the right name? The right signature? The right generated output? One idea:  `def bfs_layers(G, source, reverse=False):` yielding sets of nodes starting with `{source}` But maybe `bfs_levels`, or `bfs_generations`?  Should it be allowed to have multiple source nodes? (topological generations treats all nodes with no incoming edges as being in the initial generation) The other BFS functions require a single source node.
issue
Add to about_us.rst#TITLE_END#Fixes #5917  <!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> 
issue
Allow unsortable nodes in approximation.treewidth functions#TITLE_END#Fixes #5681   <!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> 
issue
Distance_measures.py functions should work for weighted graphs too.#TITLE_END#Many of the functions in `distance_measures.py` work for unweighted graphs, but have straightforward definitions for weighted graphs too.  We should go through this module and upgrade each function to work for weighted graphs.  It should be a straightforward fix with a keyword "weight" input which defaults to the current unweighted treatment.  The `shortest_path` functions provide a nice interface for that to use as an example.
issue
Update isomorphism VF2 algorithm#TITLE_END#@chebee7i says (from discussion in #3136):       I just noticed that someone put out a paper on VF2++      (https://www.sciencedirect.com/science/article/pii/S0166218X18300829). Looks very nice. Would      be interesting to offload the isomorphism checks to that LEMON library.  So I suggest either implement the new algorithm in python or find a way to offload to the LEMON library.
issue
Fix typo in bipartite closeness_centrality and thought-o in tests#TITLE_END#Fixes #5620  with typo in bipartite closeness_centrality and thought-o in tests
issue
Adjust the usage of nodes_or_number decorator#TITLE_END#Tweak the usage of nodes_or_number in a few functions. See #5582 That issue revealed that a few functions test the result against `int` instead of `numbers.Integral` when processing the output of the `nodes_or_numbers` decorator.  That leads to an exception when e.g. numpy integers are used to indicate how many nodes to generate in a new graph. While investigating that issue, a few other corner cases were found -- listed in #5582. And it was found that `star_graph` and `wheel_graph` don't need the decorator because they immediately call `empty_graph` on the results.  The functions changed are:  - star_graph - wheel_graph - lollipop_graph - complete_bipartite_graph  Other items that might be considered (not included in this first set of changes) are:   - ~~Consider not using `nodes_or_numbers` in `lollipop_graph` and `complete_bipartite_graph` to simplify code.~~ (does it?) <- no it doesn't simplify the code. - Changing variable names of the returned values from `nodes_or_number` to `_` when they are unused (this is the vast majority of cases).   - Consider making an optional argument to the decorator to signal whether to return two objects (the original input and the list of nodes) or just the list of nodes (default). 
issue
Fix triadic census#TITLE_END#The goal of this PR is to fix the nodelist parameter for `triadic_census`. Closes #5557   This first pair of commits add tests to show that the older code does not produce the correct results for some simple cases, and then add some changes to the algorithm to allow it to work with single node entries in the nodelist parameter.  What was needed is to revise the triads that have calculated counts instead of brute-force counts.  These are: '012', '102' and '003'.  It looks like the '003' special case code was fine.  But the '012' and '102' formulas calculated only the triads that involved an edge for a node in `nodelist`.  That means it misses any triads that have an edge between 2 nodes NOT in `nodelist` and an isolated node in `nodelist`. The case with no `nodelist` specified still works because no such missing triads exist... There aren't any edges between nodes not in nodelist because every node is in nodelist.  So, the default case for `nodelist` is fine, as suspected.  To correct for this, we need to add the missing triads using a formula for the number of edges not incident to neighbors of the node currently being counted. To wrap my head around this, I worked through the case with nodelist being a single node. An additional task is to count both the number of these edges and also whether they are bidirectional edges or single direction (the difference between '012' and '102' triads).   I couldn't stop myself from some efficiency changes such as moving the computation of `G.pred[u] | G.succ[u]` outside the loop to avoid recomputing it many times. And I renamed some variables like `n` to `N` and `m` to `ordering`.  I've run it on all graphs with 7 or fewer nodes checking against the output of `triads_by_type()`, and I've run it on numerous random graphs with 10 nodes and a couple of random graphs with 100 and 1000 nodes. These tests are in the test file though some are marked as slow.  @r1ght0us  Are you able to check whether this version does what you wanted it to do? 
issue
Remove full path from functions in the docs sidebars#TITLE_END#In the documentation, [the sidebar lists functions within a section](https://networkx.org/documentation/latest/reference/generated/networkx.generators.sudoku.sudoku_graph.html#networkx.generators.sudoku.sudoku_graph) and the names of the functions include the path to the function.  It doesn't fit in the sidebar.   Can we find a sphinx switch so that the sidebar names include only the function names and not its path?
issue
Fix min_edge_cover in special cases (#5538)  and correct documentation#TITLE_END#Fixes the defect described in #5538 while maintaining the current discrepancy between `matching` APIs for bipartite and non-bipartite matching functions. bipartite returns a matching dict of mates, while non-bipartite returns the matching as a set of edges.    That means that the corrected `min_edge_covering` still returns two types of sets -- if a bipartite matching algorithm is used, it returns a set with both 2-tuples for each edge; while if a non-bipartite matching algorithm is used, it returns a set with one 2-tuple for each edge.  It seems like this should be changed too. And I could add that to this PR or to a separate PR.  This PR also updates the documentation which e.g. currently says that this function only works for bipartite graphs.
issue
Update documentation for planar embedding#TITLE_END#Let's update the documentation to make it clear that the `check_planarity` function is the primary interface for the planar embedding tools.  Also, the class `PlanarEmbedding` is tricky to make sure it maintains the planar data structure. People not familiar with those ideas should definitely start with the `check_planarity` function.  See discussion in #5079
issue
Fix min_weight_matching to convert edge weights without reciprocal#TITLE_END#Fixes #5388   Add test and then fix code and docs.  Discussion point:  I think we need to remove the `maxcardinality` parameter because it doesn't make sense in a minimum weight matching to allow edges to not be included. You would return a matching with no edges. That would certainly minimize the edge weights.  Inside `min_weight_matching` we should always call `max_weight_matching` with `maxcardinality=True`.  I think this keyword should just be removed. But I deprecated it -- only instead of a warning it raises a NetworkXError telling the user that the concept doesn't make sense and that the parameter would be removed in v3.  But I still think it should just be removed now -- with a note in the release notes.  Thoughts?
issue
Update random number generators to use numpy.random.Generator  #TITLE_END#It looks like the latest numpy interface is now `Generator`. So we might need to update our random number generator treatment to handle that.  I believe the old syntax/interface of `RandomState` still works or we would have run into problems. But it'd be good to align ourselves with the latest/greatest.  I think the only place we should have to change is networkx/utils/misc.py  with functions `create_random_state`, `PythonRandomInterface` and `create_py_random_state`.  (Thanks @stefanv for bringing our attention to this.)
issue
Line graph raises an error when nodes are not ordered.#TITLE_END#When nodes are not ordered, the line_graph function raises a TypeError. ```python G = nx.path_graph([0, 1, 2, "a", "b", "c"]) nx.line_graph(G) ```  This happens because we want the 2-tuple identity of an edge to become a node in the line_graph. So to ensure a unique representation, we rewrite all edge tuples in the canonical order (u, v) where u<v.  There are other ways to achieve this outcome that don't rely on ordered nodes.  Let's use one of them. :}  For example, we can map the nodes to integers:  `node_index = {n: i for i, n in enumerate(G)}` and then order the nodes by comparing the `node_index` values. 
issue
line.py needs to be updated to use NX2.0 features#TITLE_END#The line.py module in generators/  has a function that returns a function that handles multigraph edges and graph edges. This is no longer needed because `G.edges` does exactly what this function returns.   I suspect there are other aspects of the module that should be given a face-lift.
issue
Adjust functions appearing in variables `__all__` but not in docs#TITLE_END#Aligns with #4111 but we should leave that Issue open and shift it to the next version -- a perennial reminder to check for orphan files.  The biggest change is that `greedy_coloring_with_interchange` was in a module that was not included in networkx. This PR renames that module, includes it in networkx, and adds it to the documentation. The doc_string is not up to our standards.  ``` edge_betweenness -> already deprecated find_cliques_recursive   -> revealed in documentation with statement that it is for pedagogical value. find_cores -> deprecated is_triad -> added to docs _naive_greedy_modularity_communities  -> already deprecated project  -> deprecated recursive_simple_cycles  -> revealed in documentation with statement that it is for pedagogical value. greedy_coloring_with_interchange  -> this is a misnamed private function not accessible from networkx namespace. So, I merged it's module to be included in `greedy_coloring.py` ```  The script from #4111 now reports only deprecated functions that were not already in the docs. ``` _naive_greedy_modularity_communities edge_betweenness find_cores project ```
issue
Some PRs closed when default branch changed#TITLE_END#A couple of PRs were closed when the default branch was changed to `main`. The "changes" page is still available. So we could open new PRs to include this code. Or this might be the nudge we need to determine whether they should be included or not.  - [x] #2191 maximal independent sets (see also `algorithms/coloring/greedy_coloring.py`) - [x] #1090 Community detection nodal roles that rely on Louvain detection being available.
issue
fix spelling in docstring of conftest.py#TITLE_END# 
issue
Changes to rst files to make doctests pass#TITLE_END#- skip running the entire test quite as an example in old_release_log.rst - capture output from subplot and rename DiGraph as DG in introduction.rst - Remove one line from 1->2 migration rst file     the line shows code that works for 1.x *and* 2.x.     But it no longer works for all v2.x code.  Readers should just use the     next line of code, so no reason to keep this in the file. - add nx. prefix to doctest function calls - rst double colons for example code 
issue
Fix neighbor degree for directed graphs#TITLE_END#Make `average_neighbor_degree` work for directed graphs, add tests and inline one function to simplify. 
issue
Style changes#TITLE_END#I'm pulling these style changes out of PR #4740 to ease review of both PRs.  Thanks to @mjschwenne   As far as I can tell, these are all changes which do not affect the results of `black` but do shorten the code and make it easier to read.  Most simply merge lines multiline expressions into a single line.  
issue
Allow greedy_modularity_communities to use floating point weights or resolution#TITLE_END#This builds on discussion in #4992 and #5000.  - adds a test for that breaks with a KeyError caused by round-off error when resolution is 0.99 - updates MappedQueue to split priority from the element in the heap. Add appropriate tests               This uses @tristanic's idea for a Class to do this. - update max_modularity.py module to work with the new MappedQueue class  Closes #4992  Closes #5000
issue
Change all Exception variable names from `e` to `err`#TITLE_END#This is to avoid the single letter variable name `e` which should be saved for an edge. In some places, a function used `e` to be an exception in one place and an edge in another.  This could be merged as a PR, or cherry-picked for another PR like #5127 
issue
Add a function to find the negative cycle using bellman_ford#TITLE_END#add code to find negative cycle if bellman_ford detects one  This just builds the code into the bellman_ford function. We should also provide it as a public function.
issue
minor tweaks in assortativity docs and code#TITLE_END#I think these are all minor doc formatting changes except these slightly larger ones: - restatement of Notes describing what `numeric_assortativity_coefficient()` computes. - "adjacent" -> "incident" in `node_attribute_xy()` - two code changes for readability in `node_degree_xy()`  (I collected these in June during discussion of `assortativity` but apparently never pushed them.)
issue
fix trouble with init_cycle argument to two TSP functions#TITLE_END#Fixed #4846 
issue
Fixes #1998 edge_load function needs documentation.#TITLE_END#There does not seem to be a reference for the definition of edge load. Our function seems like a reasonable way to count number of shortest_paths across each edge. But I don't see any articles that use this definition.  Also, I fixed some sphinx errors that have crept into the code base over time. 
issue
Pypi badge for Windows testing still flags a broken Appveyor test #TITLE_END#On the pypi networkx page, one badge is "failed" -- it's the badge for appveyor.  Also, I think the copyright year needs to get bumped to 2021.  I guess that all this will get fixed for v2.6 release.
issue
fix typo for PR number of deprecation#TITLE_END# 
issue
Deprecate k_nearest_neighbors#TITLE_END#The function name `k_nearest_neighbors` isn't used n any of the cited papers, is the same name as used for commonly used machine learning algorithms (which are not related to this degree_correlation work.  Perhaps it came from "nearest neighbor degree" which **is** used in the papers, and does depend on "k", but importantly, is not called "k nearest neighbors".  Since this function name is simply an alias we should just redirect users to `average_degree_connectivity` which is the name used for the actual function definition.  That name is not great either given that there is a function `average_neighbor_degree`, but that naming discussion can be delayed to another PR. (see issue #4936)  This PR also fixes handling of degree/in_degree/out_degree for average_neighbor_degree and adds some very simple tests for it.   More testing is needed in this assortativity package. 
issue
doctest inside doc rst files#TITLE_END#Maybe I'm missing something, but doctests inside introduction.rst and tutorial.rst are failing. We should fix them for sure, but is there a way to run them through CI, or maybe they already are...
issue
Remove unused `normalized` parameter from communicability_betweenness_centrality#TITLE_END#Fixes #4171
issue
move partition checking outside private _quotient_graph function#TITLE_END#Making the private function free from argument checking may allow Louvain method code to call the private version to avoid argument checking because we know the inputs are ok. 
issue
Allow selection of an attribute to be the Edge ID when writing GraphML#TITLE_END#See #4347 and also #3960   It looks like we should make it possible to select an edge attribute to be the "edge id" when writing GraphML.  
issue
Raise ValueError if None is added as a node.#TITLE_END#Removed some tests that checked that errors raised when None was a node.  Fixes #4788 
issue
test python-igraph directly from github#TITLE_END#Trying the python-igraph master branch where they have hopefully merged a fix for igraph/python-igraph#415 
issue
Looks like igraph example is failing in the CI#TITLE_END#The most recent PRs have a build error for the examples with the following traceback. Perhaps the API has changed due to a recent release? ``` Sphinx-Gallery successfully executed 60 out of 61 files subselected by:      gallery_conf["filename_pattern"] = '/plot'     gallery_conf["ignore_pattern"]   = '__init__\\.py'  after excluding 0 files that had previously been run (based on MD5).   Extension error: Here is a summary of the problems encountered when running the examples  Unexpected failing examples: /home/circleci/repo/examples/external/plot_igraph.py failed leaving traceback: Traceback (most recent call last):   File "/home/circleci/repo/examples/external/plot_igraph.py", line 39, in <module>     ig.plot(h, layout=layout, target=ax1)   File "/home/circleci/repo/venv/lib/python3.8/site-packages/igraph/drawing/__init__.py", line 480, in plot     result.draw(obj, *args, **kwds)   File "/home/circleci/repo/venv/lib/python3.8/site-packages/igraph/drawing/graph.py", line 1114, in draw     for idx, color in kwds["mark_groups"].items(): AttributeError: 'bool' object has no attribute 'items' ``` 
issue
Deprecate generate_unique_node#TITLE_END#Deprecate `generate_unique_node` Also add to contributing doc steps for deprecation  Closes #4454  Question:  prefix_tree returns `tree, root` but the root is now always `0` (not a UUID).  So I think we should change the output of the function to `tree` instead of the 2-tuple `tree, root`.  My question is how should this kind of deprecation be handled?  Is it a deprecation or "just" an API change?  I'm guessing it is an API change and so should be listed in the `release_dev.rst` file as such. But that we don't add warnings.. Right? 
issue
Check nodelist input to floyd_warshall#TITLE_END#Add tests for NetworkXError when nodelist is not a full order.  Fixes #4587  
issue
More for projects page: TSP and Graph Isomorphism#TITLE_END#Add 2 more projects to the list of projects (and minor edits to others) Traveling salesman problem: Asadpour (2017) provides the best approximate **directed** version. Graph Isomorphism: VF2++ for graph/subgraph/induced subgraph problems for directed/undirected. 
issue
Correct and update Atlas example#TITLE_END#An email from Philip Boalch points out that the previous code didn't include atlas graph number 208 and used a `could_be_isomorphic` fast routine which returned incorrect results.  The correct number of connected graphs  with 6 or fewer nodes is 142, not 137. Using the `GraphMatcher` method `subgraph_is_isomorphic` provides the correct result. 
issue
Add approximation algorithms for traveling salesman problem#TITLE_END#Combines #4083 and #3585 and brings up to master to avoid conflicts.  
issue
add special processing of `multigraph_input` upon graph init#TITLE_END#Fixes: #4720  Adding a keyword argument `multigraph_input=True` to a graph construction call should treat any incoming input data for the graph as a dict-of-dict-of-dict multigraph data structure.  This matches the behavior of `nx.to_networkx_graph` which is called by the `__init__` dunder.  Previously, the multigraph_input argument would be added to the graph attribute dict and ignored when processing the input data. 
issue
Remove function utils.preserve_random_state#TITLE_END#Remove and appropriately deprecate utils.preserve_random_state which as mentioned in #4732 looks like it was moved to utils in the same PR that its use was removed from networkx.  Needs:  (anything else?) - [ ] Deprecate warning in the function itself.  - [ ] Note in the list of deprecations to handle for v3.0.
issue
switch alias direction of spring_layout and fruchterman_reingold_layout#TITLE_END#This allows the `source` link to appear on the docs for this function. I believe it shouldn't affect performance/behavior in any way.  Fixes:  #4819 
issue
docs for spring_layout do not include `source` link#TITLE_END#I believe there is no `source` link in the documentation for `spring_layout` because of our alias: ```python spring_layout = fruchterman_reingold_layout ``` But ironically, there is no documentation page for `fruchterman_reingold_layout`.   Could we just swap which name is attached to the code and which to the alias? I think that doesn't change anything functionally, but allows sphinx to find the source code for this important function.
issue
Tighten partite_layout if/else code.#TITLE_END#Tighten up the bipartite_layout and multipartite_layout  if/else as describes in #3815 
issue
Add a test for kernighan-lin sweep#TITLE_END#There was a bug in this code fixed by #4398   But there are still no tests for it.  Let's add one that makes sure this is doing something reasonable.
issue
Replace generate_unique_node internally where not needed#TITLE_END#Relates to #4224 Relates to #4454  All internal uses of generate_unique_node are better served using more specific methods to choose a node. Two uses end up finding an unused node by counting down from -1 until the integer is not a node. I'm not sure that justifies a utility function to do that so I haven't implemented one, but we could make one (or a graph method) with api: `generated_unused_node(G)` or maybe `G.find_unused_node()`.  I like the idiom: ```python # find a node that is not in G node = -1 while node in G:     node -= 1 ```
issue
Allow relabel_nodes mapping to have non-node keys that get ignored#TITLE_END#Motivated by #4465 this PR allows the mapping used in `nx.relabel_nodes` to contain keys that are not nodes even when `copy=False`.  Previous behavior is that `copy=True` would ignore those keys and `copy=False` would raise a KeyError.  I'm not sure whether this needs to be Deprecated rather than changed because it would affect any user that relies on the KeyError "feature" to indicate whether the mapping has spurious keys. They can of course check for that with e.g. `if set(mapping)-set(G): raise KeyError`.  Still, maybe a note in the release_nodes is sufficient rather than a deprecation. 
issue
Update docs for clustering Fixes #4348#TITLE_END#Tries to make more clear that number of # of triangles double counts in the undirected functions. Doesn't change that behavior (which also saves dividing by 2 and then multiplying by 2 later). Just tries to make it more clear.  Fixes #4348
issue
NXEP 3: Allow generators to yield from edgelists#TITLE_END#This NXEP presents ideas from #3036 as a reformulation of how graph generators produce output. They can still return graphs as they currently do, but they can also yield from edgelists.  The generators are replaced with Graph Builders that allow create_using and edgelist options to be specified by the user.  The current edgelist reading tools like `G.update` will need to handle 1-tuples as potentially isolated nodes and also handle attribute dicts for graph, node, edge and multiedge objects as part of the edgelist stream.  The term edgelist may need to change to node_and_edgelist or something better.  We should create edgelist writing methods -- perhaps as part of `G.edges()` or perhaps a new method name. They need to basically be:  `chain( [(G.graph,)], map(tuple, G.nodes.data()), map(tuple, G.edges.data()))` (or with edge keys for multigraphs).  
issue
Updates to docs and imports for classic.py#TITLE_END#Minimal document updates... it snowballed from what I intended. I don't think it conflicts with #4401 
issue
Correct networkxsimplex docstring re: multigraph#TITLE_END#Fixes #4432  
issue
import style best practice?#TITLE_END#We have some code that takes great pains to import code one function at a time. Other code imports lots of functions from the same module, but lists each function that is used in the module. Other code just imports the package name and uses it when needed to call a function from that package.  ```python from numpy import min from numpy import max from numpy import abs  x = min(abs(max(y))) ``` VS ```python from numpy import min, max, abs  x = min(abs(max(y))) ``` VS ```python import numpy as np  x = np.min(np.abs(np.max(y))) ```  Recently, I was trying to figure out where a package (numpy) was being used in a module. It would have been quite easy if the 3rd style was used.... I just search for "np." in my editor.  With either of the first two styles I have to search for min and max and abs and those words appear much more frequently (in comments and in docs as well as code) and I have to check whether the function has been defined as a local variable inside the function before being called (unlikely with this example min/max/abs, but i worried about it anyway).  Between the first two styles, I prefer the second because the code is more compact. And we used to always do it that way, but about 6 years ago people started using the more verbose style and even changing imports to that style.  Is there a Style guide preference for imports that anyone knows about? Or even a blog post by people who've thought about it some?  I think we should work toward making ours more uniform. 
issue
fix typo in NXEP template#TITLE_END#Numpy -> NetworkX in 2 places. 
issue
Move a few imports inside functions to improve import speed of the library#TITLE_END#Some of the imports are at the module level still -- instead of inside a function. This negates our efforts at lazy importing, thus increasing import time (by a very small amount but users are still complaining).  Fixes #2900   I will take another look at this in light of #4277 before it should be merged.  Feedback? @pb-cDunn
issue
to_pandas_edgelist needs new signature and multigraph handling#TITLE_END#The ```to_pandas_edgelist``` function signature currently has unused parameters ```dtype``` and ```order```.  This is similar to ```to_numpy_matrix``` so maybe it was copied from there.  We could use ```dtype``` when we create the DataFrame, but ```order``` should not be there as it doesn't apply to pandas DataFrames.   Furthermore, this function doesn't handle MultiGraphs well. In particular there should be way to specify a column name for the edge key information.  I propose a parameter ```edge_key``` which is similar to the parameter used for ```from_pandas_edgelist```
issue
Update docstring for to_pandas_edgelist and add edgekey parameter#TITLE_END#Builds on  #4106 which added docs for `edge_key` to `from_pandas_edgelist`. This adds an edge_key parameter to `to_pandas_edgeslist`.  Also deprecates the unused `order` parameter in `to_pandas_edgelist` add line to deprecations.rst to remind to remove. 
issue
Skip memory leak test for PyPy#TITLE_END#Fixes #4382 
issue
Memory Leak is reported in our CI pypy3 tests#TITLE_END#Reported [here](https://github.com/networkx/networkx/pull/4379#issuecomment-732443978) by @jarrodmillman   @jarrodmillman and @dschult have restarted an occasional failed CI text on MacOS pypy3 due to failure of the garbage collection test when copying a graph.  Manual interactive testing on pypy3 gives: ``` >>>> gc.collect() >>>> [id(obj) for obj in gc.get_objects() if isinstance(obj,nx.Graph)] [4517884656] >>>> [id(obj) for obj in gc.get_objects() if isinstance(obj,nx.Graph)] [4517884656] >>>> G.copy() <networkx.classes.graph.Graph object at 0x000000010c9fb0c0> >>>> [id(obj) for obj in gc.get_objects() if isinstance(obj,nx.Graph)] [4506759360, 4517884656] >>>> [id(obj) for obj in gc.get_objects() if isinstance(obj,nx.Graph)] [4517884656] ```  This is consistent, but the two nx.Graph type objects exist for longer if you don't use `gc.get_objects()`. For example, I can print, set other variables, etc between `G.copy()` and  checking `gc.get_objects()` and there are 2 such objects. But as soon as I use `gc.get_objects()` the second nx.Graph object is collected.  The [pypy docs for garbage collection](https://doc.pypy.org/en/latest/gc_info.html) talk about how collection only occurs when `nursery` memory is depleted. So, even if a variable loses all it's references, it still might hang around for a while. I believe that is what we are seeing.  Any ideas for how to either change our test or find a way to fix it?
issue
Improve test coverage for coreviews.py#TITLE_END#Add tests to get test coverage up to 100% for coreviews.py 
issue
Add to tutorial info about edge order#TITLE_END#With Python 3.6 or later the behavior of Graph/DiGraph is ordered because dict's became ordered with that version.  And... The order of adjacency reporting... g.adj, g.successors, g.predecessors is now the order of edge adding. But the order of g.edges is not always the order of edge adding. The order of g.edges is the order of the adjacencies which includes both the order of the nodes and each node's adjacencies.  ```python g = nx.DiGraph() g.add_edge(2, 1)   # adds the nodes in order 2, 1 g.add_edge(1, 3) g.add_edge(2, 4) g.add_edge(1, 2) assert list(g.successors(2)) == [1, 4] assert list(g.edges) == [(2, 1), (2, 4), (1, 3), (1, 2)] ```  We should add this kind of info to the tutorial.  (thought of because of question #4248 )
issue
to_numpy/scipy array functions should not allow non-nodes in nodelist#TITLE_END#Currently, adding non-nodes to nodelist makes the created matrices have empty rows/columns showing an isolated node. But this (silently) means typos in nodelist create isolated nodes instead of raising an exception.  I don't think users should create isolated nodes when converting a graph to matrix form. It seems better to raise an exception to "avoid surprises" at the cost of making people add the isolated nodes to G. I'm proposing changing this to raise an error if nodelist contains nodes not in G.   Strictly speaking this is not backward compatible because someone might have written code that depends on the matrix conversion adding the extra nodes. But if we raise an exception with a good message, it should be obvious what the trouble is and easy to fix.  
issue
Fix bug where changing a yielded component set breaks the component finding routine.#TITLE_END#fix order of yield and seen.update in all connected components routines Fixes #4331  See also  #3859 & #3823   A similar bug was fixed in March 2020 for connected components -- this fixes the directed versions of that bug. We update the `seen` set with the component's nodes before yielding it so no longer has the problem reported in #4331 of changing the component set yielded makes the function report the same component again.
issue
Update plot_antigraph.py example to remove `_iter` in method name.#TITLE_END#`def adjacency_iter(self)` should be `def adjacency(self)` There may be other places (especially in the examples) where we've missed an ```_iter``` update.
issue
Add more useful ```__str__``` function for Graph objects#TITLE_END#Currently the ```__str___``` function prints the ```G.name``` property which is ```G.graph["name"]```. That is pretty rarely useful in my experience (often a blank string).  There is useful information in nx.info(G) and much less of it than there once was. Perhaps it would be reasonable to look at making something like ```nx.info(G)``` show for ```__str__```.  Are there reasons to avoid doing this? What implications does it have for ```__repr__```?  Issue #3956 requested a useful ```__repr__``` and so is related to this.
issue
remove code in add_nodes_from that supports ironpython <2.7.5#TITLE_END#rewrite add_nodes_from to relax code meant to allow ironpython pre-2.7.5
issue
Docs update#TITLE_END#Fixes #3955  helps with steiner_tree helps with #4111 Adds many functions that are missing from the docs
issue
new function rescale_layout_dict to rescale a dict-based layout#TITLE_END#merge #3146 which came from unknown repository @frhyme and @leeseunghoon originally proposed this.
issue
Get steiner_tree to work with MultiGraphs by postprocessing#TITLE_END#Fixes #3155
issue
improve Search Engine results for documentation#TITLE_END#It seems like every time I do a web search for a networkx function (or even networkx itself) the top results are documentation from v1.9 or v1.10.  This was true when we used ReadTheDocs too, but it is more true now.  Perhaps there is some way to redirect some of the pagerank from our older documentation to the newer stuff.  Something like rel=canonical might be helpful, though I'm not sure it is right because it is supposed to be for pages that are identical.
issue
add modularity to the docs#TITLE_END#Fixes #4095  modularity function was never added to docs when added to quality.py
issue
Allow G.remove_edges_from(nx.selfloops_edges(G))#TITLE_END#Fixes #4068 
issue
avoid duplicate tests due to imports#TITLE_END#Changing the name of ```TestGraph``` when importing it avoids having the tests run in the importing module.  This should speed up CI testing  I rename TestGraph as ttttGraph.  There must be a better name...?
issue
Update docs to be more accurate about speed of G.neighbors#TITLE_END#Fixes #3687
issue
Fix scaling of single node shells in shall_layout#TITLE_END#Also add a feature: rotation of initial angle for each shell                                 controlled by new argument "rotate"  Add a test to check this fix  Update for PEP8  Fixes #3753 
issue
Help ensure **kwargs doesn't clobber by using specific argument names#TITLE_END#ekey is the new name for the edge key. That is the change with the biggest backward compatibility impact.  Code that says ```G.add_edge(0, 1, key=3)``` will not do what it used to do for a multigraph.  Just think of multiedges as 3-tuples (u, v, ekey) and don't use the keyword for the 3rd argument.  Other changes like changing ```n``` to ```node_to_add``` will have minimal impact because not many people used those names anyway.  Fixes #1583  Addresses #1582 but doesn't solve it completely without changes on pygraphviz side (which were turned down when proposed almost 2 years ago). Perhaps this change should be turned down too. :)  Feedback welcome.
issue
Update docstring for from_pandas_edgelist#TITLE_END#Fixes #4106
issue
MacOS Graphviz MultiGraph tests failing on Travis#TITLE_END#The Travis Graphviz version for Mac OS is 2.40 while it is 2.3? for linux. But I'm not sure that is the issue. 
issue
Feature Big Picture URLs in documentation #TITLE_END#We should include more links to the documentation landing page and the github repository in our documentation.  Links to the landing page would allow people to switch between docs for different versions and between docs and code.  Links to the github repository would allow people to find the code/issues/PRs from the docs while staying in a browser (not installing source code locally via git).
issue
Deprecate context_manager reversed in favor of reversed_view#TITLE_END#The context manager reverses a DiGraph in-place which doesn't work well in multithread environments. (see #3936 ) The ```reversed_view``` a.k.a. ```G.reverse(copy=False)``` feature does the same as reversed, but you have to check if G is undirected. This set of changes replaces code that uses the context_manager with the view. It also marks the context manager as deprecated and removes it from the documentation. Scheduled for removal in v2.7  Fixes #3936
issue
Improve output object for asyn_lpa_communities#TITLE_END#Current returns an iterator of a dict-value view. But since the dict exists we might as well return the dict. Iteration doesn't save anything.
issue
Check antichains for speedups for node_cuts#TITLE_END#This comes as follow-up to #3039 and #3025  it seems the slow parts are now in antichains.
issue
Deprecation of a part of matplotlib in scatter#TITLE_END#Error message is something like:  ```python =================================== FAILURES =================================== __________________ TestPylab.test_multigraph_edgelist_tuples ___________________ self = <networkx.drawing.tests.test_pylab.TestPylab object at 0x7fe5ea4b9ba8>     def test_multigraph_edgelist_tuples(self):         # See Issue #3295         G = nx.path_graph(3, create_using=nx.MultiDiGraph)         nx.draw_networkx(G, edgelist=[(0, 1, 0)]) >       nx.draw_networkx(G, edgelist=[(0, 1, 0)], node_size=[10, 20]) drawing/tests/test_pylab.py:175:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  drawing/nx_pylab.py:273: in draw_networkx     node_collection = draw_networkx_nodes(G, pos, **kwds) drawing/nx_pylab.py:416: in draw_networkx_nodes     label=label) ../matplotlib/__init__.py:1538: in inner     return func(ax, *map(sanitize_sequence, args), **kwargs) ../matplotlib/cbook/deprecation.py:356: in wrapper     return func(*args, **kwargs) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self = <matplotlib.axes._subplots.AxesSubplot object at 0x7fe5ea480da0> x = masked_array(data=[-0.3801248 , -0.23547484,  0.61559965],              mask=False,        fill_value=1e+20) y = masked_array(data=[-1.        ,  0.06022964,  0.93977036],              mask=False,        fill_value=1e+20) s = masked_array(data=[10, 20],              mask=False,        fill_value=999999) c = '#1f78b4', marker = 'o', cmap = None, norm = None, vmin = None, vmax = None alpha = None, linewidths = None, verts = None, edgecolors = None plotnonfinite = False, kwargs = {'label': None}         @_preprocess_data(replace_names=["x", "y", "s", "linewidths",                                          "edgecolors", "c", "facecolor",                                          "facecolors", "color"],                           label_namer="y")         @cbook._delete_parameter("3.2", "verts")         def scatter(self, x, y, s=None, c=None, marker=None, cmap=None, norm=None,                     vmin=None, vmax=None, alpha=None, linewidths=None,                     verts=None, edgecolors=None, *, plotnonfinite=False,                     **kwargs): ```
issue
Transitive_closure now finds self-loops when cycles present#TITLE_END#Fixes #3187  TC=transitive_closure(G, reflexive=False)  --> cycles in G create self-loops in TC reflexive=True  --> each node gets a self-loop (for the path v-v). Called reflexive transitive closure.  reflexive=None --> no self-loops.  Note: the default has changed from what was effectively None to False.
issue
Fix many documentation based Issues#TITLE_END#Fixes #3106    moving test.py from tests directory to testing directory  Fixes #3189    update docs to correct formula for assortativity Fixes #3211     allow generators for community detection and update docs and tests Fixes #3324   expand documentation of bipartite generators Fixes #3481   expand documentation of betweeness_centrality and betweenness_centrality_subset 
issue
Resolve many documentation issues#TITLE_END#Fixes #3473 migration guide in_degree wording Fixes #3463 add warning to closeness centrality about in_degree Fixes #3498 cross document BSF functions  Fixes #3469 link to miles datafile in example Fixes #3489 allow scale=None in spring_layout 
issue
Add last 7 lines of Gomory-hu algorithm Fixes #3293#TITLE_END#Fixes #3293   Gomory-hu found the right weights, but not always the right tree.  The last 7 lines of the algorithm as published were not included in the function. This adds them.
issue
Allow jit_graph to read json string or json object#TITLE_END#Fixes #3197
issue
Keep shells with 1 node away from origin in shell_layout#TITLE_END#Fixes #3188 also fix some pycodestyle
issue
Handle edgeattr in from_pandas_edgelist when no columns match request#TITLE_END#Fixes #3562
issue
Change is_list_of_ints to make_list_of_ints#TITLE_END#New function ```make_list_of_ints``` allows any element i for which: int(i) == i Sequence can be any type of container or even iterator though it is exhausted in the process. Result is a new list unless the input is a list in which case elements that are not ```int``` are replaced in-place.  I believe all uses of ```is_list_of_ints``` have been changed to using ```make_list_of_ints```  Replaces #3335  Fixes #3322  
issue
Make draft of release notes for v2.4#TITLE_END#Also added printing commits to ```contribs.py```
issue
add check of attr type before converting inf/nan in GEXF#TITLE_END#Fixes #3612
issue
Fix sphinx errors And add links to single_source_dijkstra in docs for dijkstra_path/length#TITLE_END#Fixes #3618
issue
Add doc warning about self-loops for adamic_adar_index#TITLE_END#fixes: #3169
issue
Test names not found by nose for some tests doubled for others#TITLE_END#Some of our tests are not being found and others are being found more than once. This is especially noticeable in ```classes/tests/test_special.py``` Time for an overhaul... maybe time to move to ```pytest``` too sinc nose isn't supported anymore? 
issue
Fix operation order in partial_duplication_graph#TITLE_END#Fixes #3258
issue
Clarify migration guide in_degree suggestions#TITLE_END#This may be related to #2686   ============from networkx-discuss post by Matt Ball The migration guide (https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html) provides the following guidance under the "Writing code that works for both versions" section:  Change any method with _iter in its name to the version without _iter. In v1 this replaces an iterator by a list, but the code will still work. In v2 this creates a view (which acts like an iterator).  If I have existing code that works with networkx 1.x and use this guidance, I run into issues.  For example:  for node, in_degree in graph.in_degree_iter(): print(node, in_degree)     When removing the _iter suffix of in_degree_iter, it works correctly in version 2.x, but fails to work in version 1.x, due to not being able to unpack the node and in_degree:  for node, in_degree in graph.in_degree(): print(node, in_degree)   However, if I use this pattern, then it works in both 1.x and 2.x:  for node, in_degree in dict(graph.in_degree()).items():  Should the migration guide be updated to include this pattern, or did I miss some other trick that's available?  Thanks! -Matt
issue
Global efficiency attempt to speed up#TITLE_END#Fixes #3510   This makes the algorithm theoretically faster because we use ```all_pairs_shortest_path_length``` instead of finding paths between each pair of nodes separately. In practice it doesn't seem to make much difference for networks less than a few thousand nodes.  I also added a seed to a troublesome test of the random degree sequence generator to make it deterministic.
issue
Fix error message for single_target_shortest_path#TITLE_END#Fixes #3338
issue
Set seed in docstring test for random_degree_sequence_graph#TITLE_END#reported in #3437 
issue
Fix spring_layout bug with fixed nodes#TITLE_END#When fixed nodes are used in spring_layout() but positions are not assigned, the domain size variable ```dom_size``` is not given a value so Python raises an exception.   This PR sets ```dom_size``` in all cases, and also changes the code to check that all fixed nodes are assigned positions in the input ```pos```.
issue
Make is_graphical raise error for invalid input#TITLE_END#Allow is_graphcial to work with numpy arrays and lists of floats that have integer values  Fixes #3322
issue
Allow multidigraph edgelist in draw_networkx#TITLE_END#Fixes #3295
issue
Add reference for preferential_attachment_graph#TITLE_END#Fixes #3342
issue
Fix GEXF edge_ids and numpy types#TITLE_END#Allow write numpy type attribute values in gexf          Fixes #2204     Fixes #2969  Make sure edge_ids are unique          Fixes #1296
issue
Correct closeness_centrality for weighted directed G#TITLE_END#Fixes #3341
issue
Make tests deterministic in connectivity/#TITLE_END#This is an attempt to stop the tests from occasionally failing when random degree sequences take too many iterations to find a valid degree sequence. This PR sets a seed on the RNG for those tests making them deterministic.
issue
Pandas accepts unhashables#TITLE_END#Pandas accepts unhashables as DataFrame containers of keys as of 0.24.0 so we cant test that unhashables cause an exception.  Fixes #3298 
issue
Update docs for pydot_layout and pygraphviz_layout#TITLE_END#Fixes #3348
issue
Strange test failure with python3.7 --pre version for converting from pandas#TITLE_END#It looks like a Travis failure is related to a recent change in the libraries on Travis, but I'm not sure. The failure arises when the DataFrame ```df``` attempts to look up with an unhashable column name an is: ```df[{}]```.  See [Travis build #4884 for NetworkX](https://travis-ci.org/networkx/networkx/builds/478680555?utm_source=github_status&utm_medium=notification)
issue
Add node when singleton input to add_star/cycle#TITLE_END#This is basically a continuation of issue #2726. (The add_star/path/cycle functions need to add a node when the input is a singleton.)  Need to expand #2759 (for add_path) to add_star and add_cycle.
issue
Switch doc build to Python 3.x#TITLE_END#We currently build the docs with python 2.7. We stop support for python 2.7 after NetworkX v2.2. So we need to switch the doc building over to some version of Python 3. (probably 3.7?) I haven't started looking at this seriously yet, but I think most of it will be straightforward changes.
issue
Fix adjlist factory bug in DiGraph#TITLE_END#Fixes #3099 
issue
Update docstrings in gml to warn about underscores.#TITLE_END#Fixes #3100
issue
Add docstring to module for similarity#TITLE_END#Fixes #2997
issue
make bellman_ford use same pred dict sentinal value as dijkstra#TITLE_END#This brings APIs closer to the same for interchangability. Now an empty list is the pred value for any source nodes. (was [None] for bellman_ford)  Fixes #3094 
issue
Update docs with all functions left out of rst files#TITLE_END#Also add script to find these functions
issue
Stop pydot expansion of graph attributes upon repeated to_from#TITLE_END#Fixes #3079
issue
Add test for random number generators leaving global RNGs untouched #TITLE_END#This crude test simple runs simple versions of every function in NX that uses random numbers. It then tests that the default RNGs for both ```random``` and ```np.random``` are untouched by this process when seed=14 for all functions and when seed= the same rng for all functions. 
issue
Simplify the Graphview and SubGraphView system#TITLE_END#Over the past year since the introduction of ```GraphView```s in NetworkX 2.0 there have been a number of issues with manipulating and/or expanding the class structure of Views relative to the graph classes themselves.  This PR is an attempt to simplify the "graph views" by converting them from classes to functions which create graph objects with the right data structure for views. This means ```G.__class__``` can again be used to instantiate an empty graph of the same (Di/Multi) type.  Perhaps more importantly, subclasses which extend the interface don't lose those extensions when using the ```G.copy``` or ```G.subgraph``` methods.  An additional benefit is that the interface is much smaller: e.g. one ```subgraph_view``` function replaces 4 classes.  Original PR does the following: - add tests to show that extensions of base graph classes (only add new functions)   should be able to use the Graph.subgraph and Graph.copy methods easily - Remove ReadOnlyGraph class in favor of existing freeze() function - Switch all GraphView classes to generic_graph_view function - Switch all SubGraph classes to subgraph_view function - Introduce deprecated functions that act like the deprecated classes.  Still need to: - add docs - add tests - make sure backward compatible and marked as deprecated - remove GraphView and SubGraph construct from rest of codebase - update release docs  Fixes #2889 Fixes #2793 Fixes #2796 Fixes #2741  Comments welcome!
issue
allow open_file to handle pathlib objects#TITLE_END#Fixes #2942
issue
add binary mode to open_file for view_pygraphviz.#TITLE_END#This works on some OS as is, but adding the b-flag in mode makes it work for all.  Fixes #3060
issue
Unify random state treatment across package#TITLE_END#The goal is to allow a single RNG created with  ```numpy.random.RandomState()``` to produce all pseudorandom outcomes if desired. Default behavior is still to use the global RNGs supplied by ```numpy``` and Python's ```random``` package.  The easiest way to make those outcomes deterministic throughout the NetworkX package is to set the state of each of those two generators, and forget about setting anything else (use the default ```seed``` argument of None).  To make every random number created by a single RNG, create that numpy RNG and pass it into each ```seed``` argument for every function you call that has one. You cannot use a ```random``` style RNG for all functions.  Based on discussion in #1764   This PR involves the following:   - finding and update all code that uses random numbers but doesn't yet allow seed to be set.  - creating decorators and utility functions to allow easy use by developers of random numbers. Contributors can use either the ```random``` package or ```numpy.random```.  Decorators take care of processing the input ```seed``` to turn it into an appropriate RNG.  - update all code using random numbers to use these tools so they work together.  This PR does not update all code to use the ```numpy.random``` interface by default. Indeed, most code uses the ```random``` interface. That conversion is more than I'm up for before this release.
issue
Speedup transitive_reduction#TITLE_END#Speed up the ```transitive_reduction``` function which was repeatedly finding dfs of the same nodes. This should improve its speed and scaling with number of nodes and edges.  Relates to #3032
issue
update docs for transitive_reduction to show example using line_graph#TITLE_END#relates to discussion in #3043
issue
Pull request to set up python3.7 testing on travis#TITLE_END#Set up travis to run python3.7-dev but allow failures on travis.  Also applied the patch from #3046 to avoid StopIteration error.  @jarrodmillman can you see if this will be useful for getting 3.7 going? Merge if you think its helpful.
issue
Expand read/write_Graphml numeric type to support numpy numeric types#TITLE_END#Fixes #1556 
issue
 Fix up edge_dfs and add edge_bfs#TITLE_END#- add docs to explain that dfs_edges is not edge_dfs Fixes #2840  - clean up edge_dfs return values to unify length of yielded tuple Fixes #3037   - add edge_bfs to match edge_dfs Fixes #2792 
issue
Add make doctest back into the doc build pipeline #TITLE_END#Errors with numpy legacy in doc_strings were related to nose only needing it once per module while sphinx generated code either switched the order, or needed it for each function.  I added the legacy=1.13 workaround to each doc_string that needed it and that seems to work.  Fixes #2806
issue
Correct documentation wording for min_weighted_dominating_set#TITLE_END#Fixes #2927
issue
stop infinite loop in min_cost_flow#TITLE_END#Fixes #2906
issue
Add base class method `update`#TITLE_END#Second part of #1393 : adding an update() method to the base classes. Input parameters can be a graph or containers of edges and/or nodes.  As written this new method returns None like the `dict.update`.  Perhaps it should return the graph? That would allow `G.update(G1).update(G2).edges()` which some might think is/isn't a good thing.  I decided to exclude adjacency inputs for the update method because there are so many variations. I include examples in the docstring for adjacency inputs.
issue
Travis nbconvert giving error#TITLE_END#We are building the docs on Travis and converting the examples to Jupyter notebooks for the gallery. (This is using python2.7).  It started showing [a new error this weekend](https://travis-ci.org/networkx/networkx/jobs/396510217)  It looks like libpng is not behaving nicely. I'm not sure where to look for answers to this one.
issue
Update docs for G.copy and set_*_attributes.#TITLE_END#Fixes #2899 and #2859
issue
Add docs for graphviz_layout for complex nodes#TITLE_END#Show how to convert to integers, use GraphViz to get the layout and then translate back to pos dict with original nodes. Fixes #1568
issue
Add warnings about subgraphs to OrderedGraph#TITLE_END#See issues #2911 and #2912 
issue
Enable create_using to be a constructor#TITLE_END#Related to #1393
issue
speedup of filters for induced subgraph#TITLE_END#Check if induced node set is big or small to determine lookup method. Also add subgraph docs to warn that building the subgraph from scratch may be better and give alternate code to use for this.  Fixes #2887  @gboeing :  This should help speed up the case when most nodes are in the subgraph.
issue
Add docs to orderedgraph module about subgraph#TITLE_END#also add words to make more clear the expectations for these graph classes. Fixes #2911
issue
Remove cyclic reference in G.root_graph#TITLE_END#Related to #3011 and #2885 and maybe #2793
issue
Attempt to fix random test fail in bipartite.generators#TITLE_END#Fixing by setting the seed of the random number generator
issue
Remove circular references between graphs and views.#TITLE_END#Graphs no longer have references to views. Views still have references to the graph. This should ease subclassing the base graph classes because you don't have to worry about creating memory leaks.  Fixes #2915
issue
Remove cyclic references in base class views#TITLE_END#There have been a few troubles created because of the cyclic nature of storing edge/degree/node views as attributes in the base classes. These views then store the graph as an attribute. It might be simpler/better to have the base class create a view each time e.g. ```G.edges``` is called rather than storing a single view instance as an attribute.    Possible downsides?      - Will creating views on each method call hurt performance?     - Others?
issue
Add check to metric_closure for is_connected#TITLE_END#Fixes #2979
issue
Add colorbar example for DiGraph edge colors#TITLE_END#Fixes #2893 
issue
Make strongly_connected_components linear time O(m+n)#TITLE_END#Fixes #2831 with good help from @ghost for profiling and good help from @ArseniyAntonov and PR #2841 Here we also avoid mutating the graph so no copy needed.
issue
Order the output of columns in doctests for pandas#TITLE_END#Fixes #2977 
issue
Update docs for draw_networkx_labels#TITLE_END#Addresses #2919
issue
Update docs for draw_networkx_labels#TITLE_END#Addresses #2919
issue
Update docs for set_node-edge_attributes#TITLE_END#Fixes #2916 
issue
Allow line_graph to apply to multigraph#TITLE_END#The code is written for multigraphs and graphs, but recently put an errant restriction on multigraphs. Line 24 of line.py  See #2814   Short term fix is to call ```nx.generators.line._lg_undirected```
issue
Combine generator modules and tweak docs#TITLE_END#tree.py appended to trees.py inverse_line.py appended to line.py  fixed algorithms.rst section for edge_augmentation   
issue
Pull back graphviz version on osx_install to v2.38#TITLE_END#temporarily fixes #2852  There's a problem with graphviz 2.40 that is fixed in the development version 2.41 Hopefully that will be released soon so that the latest version no longer gives the error. This change just makes our tests use the old version. It doesn't make NetworkX work with v2.40.
issue
Add a large clique size heuristic function#TITLE_END#merges and fixes #2262
issue
Remove automatic processing of G.name attribute#TITLE_END#It's too hard to keep G.name consistently updated throughout the codebase.  Let users do it. This stops from deprecating G.name altogether, but it is a first step toward that if we decide to go that route.  Fixes #2651
issue
In operators.py, G.name is updated without effect#TITLE_END#The G.graph dict is updated after the name is changed so the name reset is overwritten. Many tests in other modules depend on this "no effect" outcome.  Perhaps we should remove the code that treats G.name as a special attribute?
issue
Add rooted product function#TITLE_END#Fixes #2062   (actually just merges that PR after fixing conflicts)
issue
Label Propagation Community Detection#TITLE_END#Contributed by @aitoralmeida in #1259  This resolves conflicts and updates docs and pep8
issue
change variable names to avoid kwargs clobber#TITLE_END#I didn't change functions defined in tests. I also left drawing routines as is at least for now.  Addresses #1582 Fixes #1583
issue
Minimum cycle basis#TITLE_END#This was originally #1649  by @debsankha Resolved conflicts to allow merge
issue
Switch to xcode 7.3 for osx_image in .travis.yml#TITLE_END#Looks like the latest Travis image change on OSX broke our testing system there. This makes it work again by using the older xcode (7.3) instead of the new default (8.3) but even the version 9 code gives the same error so we will have to deal with this more seriously at some point. 
issue
Fix bug in len(edges) for self-loops#TITLE_END#Fixes #2734 
issue
Adjust docs for graph class edge attrib assignment#TITLE_END#As suggested by @mddddd in #2728
issue
Deprecate component_subgraphs functions#TITLE_END#Resolves #1431   Also added tests for null graphs to component functions and  avoided creating lists of components when counting is sufficient.
issue
Fix Pydot tests so works with new version 1.2.4#TITLE_END#I removed the check for "True" on P.write_raw because now it returns None (nothing). We'll rely on exceptions being raised if the write doesn't work. We actually test the written contents later by reading the tmp file so should be OK.   
issue
Add Mycielski Operator#TITLE_END#Conflicts resolved from #2168  merges #2168 
issue
Adds prefix_tree, dag_to_branching, and example.#TITLE_END#This is #2060 with conflicts removed...  This commit adds two new functions and an example application using those functions.  - The `prefix_tree` function (in the new module   `networkx/generators/trees.py`) generates a prefix tree (aka a trie)   from a given list of strings (or integers, etc.). - The `dag_to_branching` function in `networkx/algorithms/dag.py`   creates the branching that results from interpreting the list of all   paths from root nodes to leaf nodes in the DAG as the root-to-leaf   paths in a prefix tree. - The example application of the `dag_to_branching` function, in the   `examples/applications/circuits.py` module, demonstrates how to   convert a Boolean circuit into an equivalent Boolean formula.
issue
Add inverse_line_graph generator from #2241#TITLE_END#merges #2241 
issue
Add docs for steiner_tree and metric_closure#TITLE_END#see code in #2252
issue
Fix bug in expected_degree_graph generator#TITLE_END#Fixes #2767 Also adds some tests that the number of nodes is correct (which would have flagged the bug) Not sure how to do a better set of tests for the edges given the randomness.
issue
Change how sparse6 tests filenames#TITLE_END#Use tempfile only to get a valid filename. The contextmanager closes that file, making it openable even in Windows. keep Appveyor from giving error.  Fixes #2777
issue
Appveyor is not giving permission for write_sparse6 tests#TITLE_END#I think the trouble is when we open a file and then pass the name of that file into ```write_sparse6```. The decorator tries to open the file again. That works on *nix systems, but not Windows. I'm not sure what changed last week to turn this on. But I'm hoping that passing in the file object instead of its name will do the trick. 
issue
MST doc note: isolated nodes with self-loops case#TITLE_END#isolated nodes with self-loops included in tree as isolated nodes. Fixes #2667
issue
Fix typo in write_gml and add test#TITLE_END#Fixes #2740
issue
Fix bug and add checks for non-convergent fiedler_vector#TITLE_END#Addresses #2381   There was a bug (default method didn't set method to 'pcg') that made the algorithm not converge consistently for any network which required more than one iteration to converge.  Even with the bug fixed, the algorithm doesn't always converge. The initial conditions are random, so restarting could work. I put in a check for non-convergence that simply raises a NetworkXError. The user would have to check this to restart with new random conditions.  It would be nice if we could make one of the other methods the default, but they have optional dependencies. Perhaps we would check if those optional dependencies are present and if so switch the default.  To be clear: the non-convergence is not a fault of the algorithm. That is apparently guaranteed to converge. But up to roundoff error the matrix W becomes singular -- specifically the second column becomes entirely zeros up to round-off.  Suggestions?
issue
Speedups for subgraph and copy methods#TITLE_END#Fixes #2743 Addresses #2716 Relates to #2687
issue
Update migration_guide_from_1.x_to_2.0.rst#TITLE_END#degree.values info and pickle info
issue
Dictionary comprehensions from #1700 merged conflicts#TITLE_END#This PR is #1700 with conflicts resolved.
issue
Predecessor Ideas?#TITLE_END#Issues #1696 and #1715 seem to suggest that there is demand for an API to access the predecessors dict in a similar way that G[u] accesses the successors dict. I'm opening an issue to collect the discussion on this question.  G.pred[u] has always been the way I do this, but that refers to a data structure element directly which makes it hard to generalize to new base classes.      - G.predecessors() and G.in_edges() are what we currently offer.      - #1696 suggests G.has_predecessor() though that could be written `0 < len(G.in_edges(u))`      - #1715 toyed with switching G.succ and G.pred to reverse the graph. (very shallow copy?)  Should we add has_predecessor? Should we add an analogue to G[u]? Maybe G.in_edges_dict(u)? Thoughts? 
issue
Node and Edge Views#TITLE_END#Now that we have iterators for `G.nodes` and `G.edges` we can reflect a little on the API change before we release it. I like the change so far but miss a few features of the old system and wonder if new users will miss them too.  **feature 1**: `__str__` for edges and nodes If I'm just playing around with a small Graph to see what works I will often print the nodes and edges to make sure I know what it is. When I `print(G.edges())` I don't get the edges. I have to `print(list(G.edges()))`. I can handle that and I will adapt... but new users might have trouble "looking" at their graph.   **feature 2**: re-useable result from G.edges When I try `es=G.edges()`, sometimes I'd like to iterate through them more than once and I can't use `es` for that unless I use `es=list(G.edges())`. I can of course use `G.edges()` again, but...?@!   **Views** Running up against this second trouble reminded me of Python3 dictviews. I also remembered @hagberg lobbying for subgraph-views long ago.  Views are low cost objects that allow multiple iterators to be generated from them, update contents when the graph updates, allow nice printing, allow `__contains__`, and many set-like operations.   I have created a [gist](https://gist.github.com/dschult/37c28fa27b613be64d4f)  with one attempt at `NodesViews` and `EdgesViews`.  **What do people think about using NodesViews and EdgesViews to enrich `G.nodes()` and `G.edges()`?**  Example:  ``` >>> G=nx.path_graph(9) >>> nv=G.nodes() >>> ev=G.edges() >>> print("nodes:", nv) nodes: [0, 1, 2, 3, 4, 5, 6, 7, 8] >>> print("edges:", ev) edges: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8)] >>> G.remove_edge(7,8) >>> print("edges:", ev) edges: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7)] >>> for e in G.edges(data=True): ...     print(e) (0, 1, {}) (1, 2, {}) (2, 3, {}) (3, 4, {}) (4, 5, {}) (5, 6, {}) (6, 7, {}) ```  I think @MridulS has already done the hard work of making these methods generators and finding how that affects our codebase. This suggestion should be relatively easy compared to that because the change will only involve the class methods themselves. @MridulS already found all the places where iterators cause trouble. Changing the iterators to views only adds features so he coding impact should be smaller.   If there is interest I will try to put together a PR. 
issue
Correct order of edge key and data in GEXFwriter.add_edges#TITLE_END#Fixes #2664
issue
Add tests for pickle of each view and fix where needed#TITLE_END#Fixes #2652   The EdgeViews have attributes that refer (without context) to graph methods like ```G.nbunch_iter``` and ```G._adj.items```.  Even worse, the EdgeDataViews store lambda functions on the ```_report``` attribute. In any case, these objects are thus not picklable. The graph would probably still be pickleable if it didn't use lazy properties to store the ```edges``` object. But the edge views would not be.  I decided it would be better to make them all pickleable rather than just fix the graph classes and then run into the same trouble later with the views.  I'm sure there are many methods to fix the pickles. The one I use here is to store enough context to recreate the troublesome attribute methods/iterators on Unpickling. That adds a couple of slots to the views which is not a problem. The graph classes also have to remove the lazy properties from the pickle (in ```__getstate__```). They get lazily recreated after unpickling.
issue
Replace __class__ with fresh_copy in G.reverse and elsewhere#TITLE_END#Fixes #2648
issue
Add deprecated methods for moved base class methods.#TITLE_END#This eases creating code that works for both v1 and v2. It doesn't address the concerns of #2634 but it eases the transition to v2.  What do people think about adding back the moved methods?
issue
Unravel subgraph chains#TITLE_END#Chains of views are view-of-view-of-view constructs. For subgraphs, if they get long (about 15 views) simple node/edge reporting slows to a crawl.  There could be times when we want the flexibility provided by short chains of subgraphs, but many common cases used in recursive algorithms should avoid chains.   I have changes the base class method ```subgraph``` to short-cut chains. That is, when the subgraph method of a subgraph view is called, it constructs a subgraph view of the original graph rather than of itself.   For people who want chains, the function ```nx.induced_subgraph(G, nodes)``` does not short-cut. I also considered ```edge_subgraph``` and ```restricted_view```, but short-cuts are harder to figure out from introspection. In the end I added documentation to describe options. Let the user choose which way they want to go. 
issue
Add miniconda osx test environment#TITLE_END#Since Graphviz 2.40.1 from homebrew seems to have trouble with multiedges and it is not obvious how to get an older version of graphviz via homebrew, I put together this alternative where miniconda is used for the optional deps version of osx tests.  related to #2614
issue
This seems to fix the shadow import for version#TITLE_END#It works when I make this change to release.py in already installed v1.11 site-packages. It makes sense that it would fix it for all future releases too.  Don't add the current directory to sys.path and then removing it again in release.py. Just import version.  We can still play with sys.path in setup.py.  That gets run  from the base directory. But release.py runs in the same directory as version.py So this is the only change needed.  I'm writing all this out because I know very little about this code and I'm hoping to either justify it to myself, or give someone else enough info to tell me I'm wrong (or right).  Fixes #2546
issue
shortest_path return types as discussed for #2510#TITLE_END#This is a partial solution to #2510.  This commit only handles the following:  Changed to return a dict: - multi_source_dijkstra_path_length - single_source_bellman_ford_path_length  Changed to yield 2-tuples: - all_pairs_shortest_path  changed to return dicts: - single_source_shortest_path_length - single_target_shortest_path_length  The following are in #2510: Still need these to yield 2-tuples: - all_pairs_dijkstra_path - all_pairs_bellman_ford_path  Need to add this (and yield 2-tuple): - all_pairs_dijkstra
issue
Make a nice printout with __str__ for Views#TITLE_END#I think this is sufficient for #2611  I haven't added tests to verify, but those tests won't be very interesting.   What do you think?
issue
Update docs of base class and function modules.#TITLE_END#Fixes #1208
issue
Simplify/remove base class methods#TITLE_END#We've often talked about removing methods from the base classes. V2.0 is one opportunity to do that. Which of these options should we do?  1. **Move self-loop methods to functions** We have already moved ```add_path``` and friends to functions.  What about the methods reporting self-loops?:  ```selfloop_edges```, ```number_of_selfloops```, and ```nodes_with_selfloops```.  2. **Combine ```G.node``` with ```G.nodes``` and ```G.edge``` with ```G.edges```** since they do essentially the same thing now. The plural versions allow both set operations and dict-like lookup with ```.data``` added to ```keys/values/items```. The singular versions don't provide set operations or the ```data``` method.  3. **Remove ```neighbors/predecessors/succesors``` methods in favor of set-dict objects ```adj/succ/pred```** This might be more user friendly if we change ```G.adj``` to ```G.nbrs```. Then we can also programmatically tell the difference between v1.x and v2.x.  4. **Move ```subgraph/reverse/to_directed/to_undirected``` to functions**
issue
Finalize view tests for v2.0#TITLE_END#Also changed testnames and filenames to make more sense. Fixes #2605 
issue
Simplify base classes.#TITLE_END#Fixes #2548   - Convert selfloop methods to networkx functions. - Remove G.node replacing with existing G.nodes - Remove G.edge replacing with existing G.edges - Make sure views using __slots__ can be pickled.
issue
One attempt to meld graphviews with base classes#TITLE_END#There are many choices for depth of copies with copy, to_(un)directed, subgraph, reverse.   Here is one suggestion that still needs rebasing and two tests fail from approxmate kcomponents (see #2581). But it is meant as a start of a conversation.  **Some choices made**  G.copy() - as_view=False     No longer deepcopy...like dict.copy() G.copy(as_view=True) - a View of the graph G.to_directed() - as_view=False    Still a deepcopy of data attributes G.to_directed(as_view=True)    Directed View of the graph G.to_undirected() - as_view=False    Still a deepcopy of data attributes G.to_undirected(as_view=True)    Directed View of the graph G.subgraph(nodes)  Return subgraph view. For new graph use G.subgraph(nodes).copy() G.edge_subgraph(edges)  Return a subgraph view. G.reverse()   Return a reversed view. For new graph use G.reverse().copy()  Graphviews are subclasses of graphs, so use of __class__() no longer works when G is a view.  New method: G.fresh_copy()   Return a new null graph with class of (possibly underlying) graph
issue
Lowest common ancestor#TITLE_END#Finally pulls #869  I updated code for py3 and nx2.0 and updated docs.  Someday these could be put in dag.py but that's already really long. Someday the tree version could go in tree/ but it only says it works  for di-trees which are DAGs. 
issue
Next attempt to meld graphviews with base classes#TITLE_END#A cleaner version of #2585. See conversation there.  Fixes #2547  Fixes #2581  Fixes #2580  Fixes #2048
issue
approximation.kcomponents seems to depend on order of nodes#TITLE_END#I'm finding that the test for the approximation k_components routine called ```test_karate_1``` no longer works with the subgraph views replacing the current subgraph implementation. At first I suspected an error in the view code. But it looks like it could be just a dependence of the approximation method to the order of the nodes reported.  Replacing the subgraph calls with subgraph view calls makes the ```k_number``` for nodes 24 and 25 show as 2 instead of 3. All other nodes are correct. In tracing it down, it looks like nodes 32 and 33 are placed before many of the other nodes when the views report nodes. The nodes reported appear to be correct, just in a different order. This may be affecting the approx k_number calculation.  Is this possible? reasonable?      If this is the problem, should the test check for two possible answers, one of which is wrong but almost right?  Are there better ways to handle this test?
issue
graphviews make H=G.__class__() fail#TITLE_END#Now that we have multiple subclasses of the graph classes which require arguments to their creation (like ```G.subgraph(nodes)```), the idiom ```H=G.__class__()``` no longer works.  I think something like ```H=G.fresh_copy()``` would fix the trouble.  As for the name, suggestions welcome.  I like ```fresh_copy``` better than anything with ```empty```, or ```new``` in the name.
issue
Convert subgraph/reverse/to_directed/to_undirected to readonly views#TITLE_END#Before I do this, I would like some feedback.  We have tried to make ```subgraph/reverse/to_directed/to_undirected``` fast and light-weight by doing a shallow copy of the graph structure and not copying attribute dicts.  We have run into problems with expectations for how shallow the copies are.  I lump these methods together because they all create a new graph from the original. I don't include ```copy``` because that already either does a ```deepcopy``` or a ```G.subgraph``` and I think it should do whatever subgraph does.  The returned objects are currently graphs in their own right -- somewhat independent of the original.  **I propose to make these return read-only views of the graph.** That implies that they are fast to create and lightweight, but that they are not independent of the original graph. All graph mutations would be reflected in the view and the view itself would not allow mutations.  Users can create an independent graph from the view using ```.copy()``` or ```nx.Graph(subgraph)```. These are not a shallow copy -- all attribute dicts become new dicts requiring memory.  Of course a ThinGraph could be used as a shallow copy (not including attribute dicts -- see docstring for Graph -- and maybe we would want to include this along with ```ordered.py``` as useful subclasses of the base classes).  With ThinGraph they would need to look up any attributes in the original graph.   What do people think?   - readonly views that update with the graph - easy to make into a mutable graph with ```Graph(subgraph)``` - some reduction in availability of shallow copy versions until we can figure out the best way to do that.  
issue
Add tests, fix my bugs for NotImplemented code in lowest_common_ancestor#TITLE_END#I introduced a bug into lowest common ancestor about using undirected or multigraph.  This PR adds tests for that bug and fixes it.
issue
Add tests for assert_edges_equal and fix#TITLE_END#Fixes #2601 
issue
multiedge edge data broken in assert_edges_equal#TITLE_END#The testing helper function ```assert_edges_equal``` only checks the last edge's edge data when there are multiple edges between the same two nodes. This comes from setting the datadict instead of adding to the datadict.      data = e[2:]     d1[u][v] = data     d1[v][u] = data  Instead, we should check if ```v in d1[u]``` and if so, extend the existing data with the new data. The order of this extension might be different though if the order of edges are different, so checking equality requires care here.
issue
contracted nodes multiedges#TITLE_END#clean up docs, add tests, clarify multigraph treatment.  Found bug in ```assert_edges_equal``` for multiedge data comparison. It doesn't seem to hav caused any errors elsewhere -- guess we don't tests multiedges this way often.
issue
Add fluid community to docs#TITLE_END#See #2420 
issue
Add reversed view and to_(un)directed view#TITLE_END#Part of #2547, this PR implements views for the reverse view of a directed graph. It also provides an undirected view of a directed graph in to_undirected and similar for to_directed.  Actually to_directed is the hard case as it requires the view to combine G.succ and G.pred without actually combining the data structures.
issue
Some changes to reduce the really long parts of tests#TITLE_END#The most impact is to only check gl1 and wlm3 using 1 of the flow_func algorithms. They are the slow graphs for testing. The other graphs still use all flow_func algorithms.  We could also cut down on check every pair of nodes to ensure a correct answer, but I didn't get into that.  This seems to cut about 30-40 secs off the time to run nosetests on my machine.  
issue
Filter nan edges in minimum_spanning_#TITLE_END#Fixes #2164
issue
GraphView with hidden_nodes and hidden_edges#TITLE_END#This implements a restricted view on graphs where ```hidden_nodes``` and ```hidden_edges``` do not appear in the resulting graph.  This implementation is read-only though you could change the original graph and the restricted view would update accordingly. At this point you can't update the hidden_nodes and hidden_edges without creating a new view. I might add that feature if I can come up with a good way to do it.   Related tickets include:    #762 #1011  #793 #335 and there are probably many other places where this feature has been discussed.  
issue
Update mst docs and decide not to add default argument.#TITLE_END#The default argument could apply to either weight or data so its confusing when there are two references. Looked at two new default arguments and decided it wasn't an improvement.  I did include changes suggested in the comment referred to in #2068   Resolves #2068  
issue
SubGraph filter on nodes/edges#TITLE_END#Here SubGraph means a view restricting (or enhancing) a graph.  Two filters are possible: ```node_filter``` and ```edge_filter``` ```node_filter``` takes a ```node``` argument and returns True to keep the node. ```edge_filter``` takes two nodes as arguments (or for multigraph, an additional key argument) and returns True to keep the edge.  This seems cleaner than the ```GraphView``` of #2512 and can hide nodes easily by building a filter function: ```lambda n: n not in hidden_nodes```, but can be much more involved.      def fish_nodes(n):         return 'fish' in G.nodes[n]     def red_edges(u,v):         return 'red' == G.edges[u, v]['color']     SG = nx.SubGraph(G, node_filter=fish_nodes, edge_filter=red_edges)  Related tickets include: #2512  #762 #1011 #793 #335 and there are probably many other places where this feature has been discussed.
issue
Fix rescale_layout, add tests and tweaks docs#TITLE_END#rescale_layout didn't work well for negative values. Fixes #2537
issue
Remove automatic assignment of G.name from many generators#TITLE_END#I left the names for atlas and small graphs as it seemed helpful in determining which graph was which. The other cases were sometimes incorrect or with incorrect parameter values (basically G.name doesn't always get updated when the graph gets updated).  So we'll put the user in charge of keeping it up-to-date.  Addresses #1906 
issue
G.name incorrect in some graph generators#TITLE_END#I have found that a few graphs have an incorrect `G.name` property. e.g. (I didn't look hard--there may be others)  ``` >>> B=nx.balanced_tree(3,3) >>> print(B.name)   # empty_graph is used to start the construction empty_graph(40) >>> G=nx.caveman_graph(3,3) >>> print(G.name)   # parameter arguments are not correct cavemen_graph(9, 3) ```  It seems that G.name is trying to play the role of `__repr__` in some cases. Perhaps that is too hard to keep correct over the years.  Also if we start allowing `path_graph(nodes)` the name could become cumbersome for long lists `nodes`.  Do people use the G.name property? Would the name without parameters be enough? What is the best way forward for maintaining the name property? 
issue
Isolate edge key generation in multigraphs#TITLE_END#Also add_edge/add_edges_from returns edge keys Fixes #2119 Fixes #1376 Fixes #1654 Relevant for #278 
issue
Handle graph name attribute in relabel_nodes#TITLE_END#Fixes #2134 simplifies read_gml handling of "name" as well. 
issue
Finishing touches on #2107#TITLE_END#The bulk of this PR is #2107. I removed `attr_dict=` phrases in two files and added some text to release_2.0.rst 
issue
Exceptions for missing source in shortest_path#TITLE_END#Fix conflicts for #1445 and use NodeNotFound exception. Fixes #1433  
issue
Adjust layout.py function signatures, docs, exposure#TITLE_END#- most function arguments now end with `scale, center, dim)` - document that random_layout does not use a `scale` parameter - remove general `flatten` routine -- restrict pos argument to dict of   list/tuple of numbers as previously documented. - expose `_rescale_layout()` as `rescale_layout()` and document - rename `process_params` as `_process_params` to show its helper nature  Fixes #1892 and Fixes #1749  Adjusted headers and PEP8 spacing. To see without pep8 changes, look at first commit only. 
issue
doc sphinx error removal#TITLE_END#- `efficiency` toc entry - link to definition of metric had to move out of parameters section. (in generators/geometric.py) 
issue
Allow community asyn_lpa test to have two answers#TITLE_END#Alternate form of #2329 which should stop random test errors while leaving the test that came from the references.
issue
Make clear to NOT assign to G.edge[u] or G.edge[u][v]#TITLE_END#Fixes #2271  Could be made unnecessary if we move G.edge to G._edge and create read-only G.edge (see #2469)
issue
Update docs in traversal. Add see also, change produce to iterate over#TITLE_END#Fixes #1195
issue
Fix docs for bipartite projection and pep8, decorators etc#TITLE_END#Fixes #2444
issue
Move data structure to private names and replace with readonly structures#TITLE_END#Fixes #2469  The inner datadict is read-write for node/edge data attributes.  Note that G.edge now takes 2-tuples (3-tuples for multigraph). I've left G[u] as returning read-write dicts for now. Easy to switch but thought I should think more before doing it.
issue
Add method .data() on Nodeview and Edgeview#TITLE_END#```G.nodes``` and ```G.edges``` objects now have methods ```keys/values/items/data``` where ```data``` allows you to extract a particular data attribute rather than the entire data dict.  I'm not sure that we need all of these methods. Even for dicts, the ```keys``` method seems superfluous. But ```keys/values/items``` come with the Mapping ABC class, and could theoretically be useful to someone.  Thoughts?
issue
Depth limited search#TITLE_END#rebase of #2258 and thus also #1928 to implement depth limited search using keyword ```depth_limit```  Fixes #2258  Fixes #1928
issue
Make graph attributes work both to/from with agraph #TITLE_END#Fixes #2450  and adds some tests of same issue  Before, from_agraph put all graph attributes in G.graph['graph']. Now graph attributes are put there and also updated into G.graph
issue
Change single_source_dijkstra output when target is specified#TITLE_END#Fixes #2248      When ```target``` is specified a 2-tuple (dist, path) is returned instead of two dicts. This is for single_source_djikstra and multi_source_dijkstra.  Other functions call them and have been updated accordingly.
issue
Add GML handling for bool and tuple types#TITLE_END#Fixes: #1973 #2118 #1985
issue
Add local_bridges to bridges.py#TITLE_END#Resolves #2273  Includes ideas about local_bridges from #2273 and adds tests. Bridges were also discussed in #903 @erotemic Perhaps parts of bridges.py could be used in (or improved by) #2459
issue
Switch parameters alpha and beta in waxman_graph#TITLE_END#Reversed calling order too, so non-keyword usage is the same, reducing backward compatibility problems. Minor tweaks with docs and tests.  Added a Note to the doctsring to explain this switching of order. Resolves #2310
issue
Add exception for DiGraph to max indep set#TITLE_END#plus tests  Fixes #2109
issue
Add multigraph example of contracted_nodes#TITLE_END#in docstring and tests Fixes #1540
issue
find_cycle.  Fixes #2323 and #2324 by fixing #2439#TITLE_END#Fixes #2439 as well as #2323 and #2324.  The problem was when a node is discovered that had already been used as a source to no avail. Old version added it as a potential head to be explored. It should just ignore that edge.
issue
Add node and edge views.#TITLE_END#This passes all tests but lots of work still in docstrings and /doc. But we can play with it and see what parts we like.  Addresses #1896  
issue
OrderedGraph and friends proposed structure#TITLE_END#OrderedGraph has been discussed quite a bit in #980 and this PR is based on that discussion. The topic of Graphs which track node order has also been discussed in #1181, #1244, #1267, #1267   This implementation includes examples (in docs and in tests) for 1) ordered nodes, 2) ordered nodes and edges and 3) thin (low RAM) graphs that don't need edge attributes [not related to ordered nodes].  This code structure is likely NOT to be the structure implemented in NetworkX 2.0 for alternate data structures if that version includes alternate data structures.  The reason is that this structure requires the data structure to have a 3-layer structure (dictlike-of-dictlike-of-dictlike) which is not desirable for some other data structures.   It may be useful for people who wish OrderedGraph or ThinGraph and don't want to wait for 2.0 
issue
Do we keep G.neighbors(n) in 2.0? predecesors(n)? successors(n)?#TITLE_END#There are some comments about removing it from long ago in #573  There are many other ways to get the neighbors of a node... but none named in this obvious way.  ```list(G.adj[n])``` or even ```list(G[n])``` 
issue
Revisit center and scaling choices for layout.py in v2.0#TITLE_END#We made some choices for v1.11 that were supposed to be temporary fixes to be improved in v2.0. Let's make sre that got straightened out.
issue
Update graphml to care for a number of issues.#TITLE_END#Fixes #2197
issue
Allow more flexible multigraph add_edges_from options#TITLE_END#Can now do (u, v, d) or (u, v, k) with networkx deciding for k only when dict.update(d) raises an exception.  Basically if d is a dict or iterator of 2-tuples assume it is edge data. Otherwise assume it is a key.
issue
Fix docs for maximal_matching and tensor_product#TITLE_END#Fixes #2157 and Fixes #2156 
issue
Add triangular and hexagonal lattice generators#TITLE_END#Fixes #1489 conflicts and adds pep8 and comments and smoke tests. Changed proposed name from triangle_lattice to triangular_lattice. 
issue
NX 2.0 iterator API#TITLE_END#As we move to the iterator reporting paradigm I think it is worth spelling out what the reporting API should be. My hope is that we can make it easier to build graph classes that don't rely on dict-of-dict-of-dict data structures. see also #980   I'll include a list of all methods further down. This first post only discusses reporting methods impacted by the iterator-change and/or d-o-d-o-d data structure.  See #1152 for similar treatement of .copy()   The big change I am proposing (in addition to returning iterators) is that the keyword "data" can be None, True or anything-else.  None means return no data(as before), True means to return the edge data as a dict (as before), anything-else means to return the edge-attribute-value associated with the edge-attribute-key stored in "data"; essentially G[u][v][data].  Notice that we would need to rule out edge attributes with key "True".  Proposed API for Reporting from Di/Multi/Graph - nodes() - iterator over nodes - neighbors(n, data=None) - iterator over neighbors of n.    -      If data is True: iterator over (nbr, eattr_dict)   -      if data not in (None,True): iterator over (nbr, eattr_dict[data])  - degree(nbunch=None, weight=None) - iterator over (n, deg(n))   -     if nbunch is not None: only iterate over nodes in nbunch… single node->single value   -     if weight is not None: deg(n) is sum of weights on edges instead of number of edges  - edges(nbunch=None, data=None, keys=None) - iterator of 2-tuples (undirected only once)   -     if nbunch is not None: iterator of edges from nodes in nbunch (undirected only once)   -     if data is True: iterator of edge info (n,nbr,eattr_dict)   -     if data not in (None,True): 3-tuple (n,nbr,eattr_dict[data]   - MultiGraph     -     if data/key is True/None: iterator of edge info (n,nbr,keydict)     -     if data/key is None/True: iterator of 3-tuple edge (n,nbr,key)     -     if data/key is True/True: iterator of 4-tuple edge (n,nbr,key,eattr_dict)     -     if data not in (True,None): 4-tuple (n,nbr,key,eattr[data])  - get_edge_data(u, v, data=None, key=None) eattr_dict for edge between u,v   -     if data is not None: eattr[data] for edge between u,v   -     For MultiGraphs if key is None, return eattr_keydict, otherwise look at edge u,v,key - adjacency_iter(data=None) - iterator of (n, {nbr:eattr, ...})  This is dict-of-dict... better way??    -     if data is not None: eattr replaced by eattr[data] 
issue
Rebase of Moves is_partition to community.utils#TITLE_END#fixes #1932  merge conflicts.  (I couldn't push to the original branch -- though I notice that is now the default for PRs -- at least for me) so maybe in the future it will be more commonly accessible. This is almost all written by @jfinkels....  Thanks!
issue
Should G.node and G.adj be read-only objects using G._node and G._adj#TITLE_END#Our philosophy has been to make the inner workings of NetworkX graphs available for people who want to mess with the guts of the data structures. But it sometimes leads to problematic code that changes the data structure without meaning to. We have a couple places in the code where we use       G.node[0] = {'foo: 'bar'}     # which should be either:     G.node[0]['foo'] = 'bar'   # or     G.node[0].update({'foo': 'bar'})  In this case little harm is done because while the datadict is replaced. it isn't pointed to by anything else. The ```G.adj``` object can be more of an issue: Don't use ```G.adj[0][1] = {'foo': 'bar'}``` because it replaces something that is also pointed to by ```G.adj[1][0]```.  Moving ```G.adj``` to ```G._adj``` and putting a read-only interface at ```G.adj``` would avoid these potential problems.  I've got some code to do it -- the question is whether to do it.
issue
Fix sphinx errors and class outlines#TITLE_END#Fixes #2223 as well as some errors that showed up in sphinx processing.
issue
dict.pop in json_graph.adjacency_graph#TITLE_END#Why are we copying the dict `d` and popping from it instead of just looking up the value?  (near line 150 of `networkx/readwrite/json_graph/adjacency.py`) 
issue
Add graph view classes for nodes/edge/degrees#TITLE_END#This is just a clean up before implementing views.
issue
Implement bounds method for distance_measure by Frank Takes#TITLE_END#Applied patch sent to networkx-discuss and added tests and style changes
issue
Appveyor error in test_asyn_lpa.test_two_communities#TITLE_END#About half the time one of the Appveyor configurations doesn't pass the test for two_communities. That registers a failure for the Appveyor tests on many PRs that don't touch the asyn_lpa code. No such errors are found when done on Travis.   The docstring for asyn_lpa_communities in algorithms/community/asyn_lpa.py says:      The algorithm is probabilistic and the found communities may vary on different executions.  When the error occurs, it is because one large community is found instead of two.  Does it make sense to allow that test to produce a single community OR the "correct" two communities? 
issue
Multigraph key simplification?#TITLE_END#It seems that the way we handle multigraph edge keys is sometimes hard to maintain (see #2107 but true elsewhere too).  Is there a way to handle it more simply?  Some thoughts include:  - defaulting to `key=None` for all add_edge actions. The first `G.add_edge(1,2)` gives `(1,2,None)`. The second does nothing! To get a multiedge one must specify a key. - default to using a UUID for the key to each multiedge.  (unique key within the MultiGraph and actually across all MultiGraphs).  Here `G.add_edge(1,2)` twice does add two separate edges.  Other thoughts? 
issue
Docs for compose now warn about MultiGraph edgekeys#TITLE_END#Fixes #1619 (doc changes only) while leaving #1654 to decide about whether edges should have unique identifiers (UUID). 
issue
Remove conflicts from #1894 (Update Exception Classes)#TITLE_END#Replaces #1894  related to #1705  
issue
Change add_path/star/cycle from methods to functions#TITLE_END#3 commits...   1) create the functions/tests,  2) change the usage,  3) remove the methods and clean up usage.  This touches a lot of code. Hope it doesn't create too many conflicts for existing PRs. Changes are mostly in docstrings so hopefully impact is minimal.     Fixes #1883  
issue
Namespace collision on nx.generators#TITLE_END#It looks like nx.generators is no longer the modules in the folder networkx/generators.  Instead it is the contents of networkx/algorithms/community/generators.py  Can we rename that file to avoid namespace collisions on the word generators? Maybe it should be moved to the generators directory and called `community_graphs.py`? 
issue
Change default role for sphinx to 'obj'#TITLE_END#Change double `to single ` for all function/method arguments. Remove double` around True, False, None. numpydoc/napoleon doesn't use any emphasis on these. Leave double `` when a literal python expression is intended. I found a couple of places where math mode was intended. Still need to look for more of those.  Fixes #1971 
issue
Doc_string Standards for arguments#TITLE_END#Currently the conf.py file for sphinx makes back-ticks use the `:math:` role.  That doesn't follow the numpydoc standard which uses `:obj:` to create a link to documentation for the listed python object. If not found, then emphasis is used instead.  With the standard, you might write `create_using` and you'd obtain a subscript u on the e of create.  See #1423   I'm not sure how to find all the places where single-backticks need to use math mode.  Changing conf.py is easy, but then we have to find the places that need math mode and add `:math:`.  Similarly we should find the places where double-backticks have been used for function arguments and use single-backticks instead.  What do people think?  Should we make a change to match numpydoc and gets easy links?  Or keep the easy in-line math mode but lose the easy links and emphasis? 
issue
assert_edges_equal assumes nodes are sortable#TITLE_END#The networkx/testing/utils.py functions `assert_edges_equal` and `assert_nodes_equal` sort the nodes thus assuming the nodes are sortable. Perhaps there is a way to avoid this using set equality or something similar.   
issue
 Fix sphinx autosummary doc generation errors.#TITLE_END#Some modules moved or were removed. A few indentation issues. Fixes #2025  
issue
Nose ignore docstrings#TITLE_END#This package makes nose report which test is being done in form: file.function Without it the docstring of the test function is used and is not as helpful to find the problem.  I also switched installation of scikits-sparse from conda to pip because it is not being supported on conda anymore.  We still have a travis-ci issue with gdal/osgeo/ogr not being installed, but I'll try to figure that out in another pr. 
issue
Add cycle argument to utils.misc.pairwise#TITLE_END#Add cycle keyword argument to pairwise Add tests for pairwise see #1883 
issue
Update generator docstrings for nodes_or_number#TITLE_END#also PEP8.   Files: classic.py and geometric.py 
issue
minor doc changes on weighted.py#TITLE_END#Fixed up some docs and whitespace noticed while reviewing #1690. One content change to get `cost` of uv-edge once in _dijkstra. 
issue
provides with_data options for shallow copies in G.copy#TITLE_END#providing one style of shallow copies with other styles described in the docstrings Addresses #1916 and supercedes #1917 also related to #1876  
issue
Fix k_core for directed graphs. Add tests#TITLE_END#Fixes #1959   Two fairly simple graph theory projects for someone interested in getting into the code: - Make k-core work for self-loops.  I think it might already work for multiedges with this fix. - Create D-core module (referred to in original issue) 
issue
Small changes leftover from #1847#TITLE_END#Should complete #1775 and #1847. 
issue
Fix docstring formatting in richclub and generic#TITLE_END#The richclub docstrings are a good candidate for pointing people to formatting choices in docstrings.  shortest_path/generic had some examples that didn't work, did pep8 on this one too--look at individual commits to avoid that if you need to.  added authors comment in generators/directed.  
issue
Adjusts imports in drawing layouts with graphviz#TITLE_END#No longer import nx_agraph and nx_pydot into the main namespace. They can be accessed within networkx as e.g. `nx.nx_agraph.write_dot` or imported as `from networkx.drawing.nx_agraph import write_dot`.  Changed examples and documentation appropriately. I've checked [readthedocs.org](http://dschult-networkx.readthedocs.org/en/v1.11) with this PR. 
issue
Adjust Imports of graphvviz pkgs for v2#TITLE_END#Cherry-picked from #1930.  nx_agraph and nx_pydot are now imported as modules to avoid name conflicts. Access them via: `nx.nx_agraph.write_dot` or `nx.nx_pydot.write_dot`, or import them using e.g. `import networkx.drawing.nx_agraph as nx_agraph` or `from networkx.drawing.nx_agraph import write_dot` 
issue
Rename degree_histogram example and add replacement#TITLE_END#Degree_histogram renamed degree_rank. New Degree_histogram example shows histogram. Discussed in #1774 
issue
Default version on readthedocs?#TITLE_END#Should the link http://networkx.readthedocs.org/ link to the "latest" docs (which build off of our master branch) or to the most recent release docs?  BTW readthedocs has a "stable" version that automatically finds the most recent version based on tag and branch names. Unfortunately our branch names like v1.11 appear more recent than our tag names networkx-1.10.  So it thinks v1.11 is the most recent release even though it is not a release yet. We could turn off the stable version on rtd, or we could change our branch names by adding a letter at the end: v1.10a (rtd will ignore those version numbers as a pre-release branch).  I've turned off the stable version on rtd for now. 
issue
Doc tweak on edges for v1.11#TITLE_END#Not needed for v2.0 
issue
Eigenvector_centrality power method shifted to A+I#TITLE_END#By computing eigenvectors with repeated multiplication by A+I the spectrum is shifted so that the positive real eigenvalue is dominant even for odd path graphs and others with multiple dominant eigenvalues. Fixes #1704  
issue
Fix layout.py and revert changes to default scales.#TITLE_END#The rescale function doesn't work as advertised due to scaling based on biggest positive value (ignoring most negative value). Other bugs include throwing out all edge information if the incoming graph is not an instance of nx.Graph.  Finally, the changes made to the API were made with NetworkX 2.0 in mind. I guess they should have been reverted when we made a v1.10 release to avoid surprises. See issue #1750 and #1759 
issue
Overload graph generators to allow nodes or numbers#TITLE_END#Create decorator `@nodes_or_number(which_args)` which replaces the arguments numbered by `which_args` with a 2-tuple `(n_name, nodes)` where `n_name` is the original argument value and `nodes` is the resulting list of nodes.  `which_args` can be a positive integer or a list of positive integers. The integers point to which argument includes the nodes. Keyword identity is not needed for any of the use_cases I have run into.  The reason for the 2-tuple is so that `G.name` can include the original call signature. Also, `star_graph(9)` has 10 nodes in it while `star_graph(range(9))` has 9 nodes. So that particular function needs to know how it was actually called.   Implemented for `empty_graph` in first commit and then many others in the next two commits. Generators which now allow integers or containers of nodes are:  ``` empty_graph complete_graph path_graph cycle_graph star_graph wheel_graph grid_2d_graph random_geometric_graph geographical_threshold_graph waxman_graph grid_graph lollipop_graph complete_bipartite_graph ```  These are the only generators that make sense to allow containers of nodes or integer number of nodes. So this issue should not expand in the future.   I have adjusted the doc_strings and hope it is clear. 
issue
Use pydotplus for v1.11#TITLE_END#Use pydotplus for all supported python versions - change to "import pydotplus"; remove flexible import cruft - remove encode("utf-8") in pydot_layout node names - in tests: sort edges for undirected 2-tuples - in travis.yml: pip install from tar.gz for python3.3 (pip couldn't find) - use assert_graphs_equal in tests - fix from_pydot to no longer create attributes 'name', 'graph', 'node' and 'edge' with value '' 
issue
Use pydotplus for all supported python versions#TITLE_END#- change to "import pydotplus as pydot"; remove flexible import cruft - remove encode("utf-8") in pydot_layout node names - in tests: sort edges for undirected 2-tuples - in travis.yml: pip install from tar.gz for python3.3 (pip couldn't find) 
issue
Fix layout for single node graphs and shells#TITLE_END#_rescale_layout was dividing by zero for single shells. Add tests. 
issue
Prep for v1.10.1 release#TITLE_END#brought in some commits from master that add appveyor, and update news.rst. Also change the release numbers in release.py 
issue
Make sure __all__ variables loaded in flow __init__.py#TITLE_END#The `__all__` variables are currently not loading correctly in the algorithms/flow/**init**.py file. This fixes that. 
issue
v1.11 Add utils functions to flow variable __all__#TITLE_END#Like #1767 but for v1.11 
issue
Fix some sphinx formatting errors#TITLE_END#``` make all doc_strings raw format r''' ''' correct underline lengths for headings fix latex issue in harmonic_centrality replace :ticket:, :url:, :doi:, :arxiv:  with links to webpages fix spacing with commas before colons so sphinx can parse well add "methods" heading in graph class rst files ```  Now if readthedocs updates their version of sphinx our docs should build without error. Partially fixes  #1667 #1724  
issue
merge iter_refactor branch into master#TITLE_END#This group of changes removes _iter methods and changes the associated methods from returning containers to returning iterators.  This backward incompatible set of changes is a large part of version 2.0  Related tickets that now may be closed are #1693 and #1655  Related tickets that can now be considered for merge are #1676 and #1662 
issue
New release 1.10.1?#TITLE_END#There are a number of bugs/problems with the v1.10 layout routines. (initially noted in #1750)  I propose that we fix them in a way that keeps the `_rescale_layout()` function as it was for v1.9. We then need to fix the new version of `_rescale_layout()` for v2.0. There are many other fixes needed for `spring_layout()` and maybe others too.  So, I am tempted to do the same for that routine: revert it to the v1.9.1 functionality in a release v1.10.1.   Then fix  the new functionality for v2.0.   Is this the right way forward?  Problems with layout.py in v1.10:   - rescale changes from `[0, scale]` box to `[-scale, scale]` box so users supporting both versions must check for version number to know what "scale" value to use. - the size of scale only uses the maximum position value instead of `max(abs( ))` so large negative positions are not in the box `[-scale, scale]` - the new version has an error (`dom_size used before it is assigned`) if `fixed is not None` and `pos is None`. - If the input is not an instance of `nx.Graph` the new code throws out all edges and spring_layout doesn't even make sense with no springs. - spectral layout also throws out all edge info in the input is not an instance of `nx.Graph`. 
issue
autosummary docs dont work on readthedocs#TITLE_END#Sphinx version 1.3.1 contains a bug in autosumary that doesn't allow it to work with our (and the commonly used) syntax.  The one-line code fix in https://github.com/sphinx-doc/sphinx/pull/1892 works like a charm. So we are Ok in the long run with readthedocs.org  But in the short run (and who knows how long that will be) we've got v1.10 and latest documentation that doesn't connect  the generated documentation pages to the toctree.  To see the effect compare:    http://networkx.readthedocs.org/en/latest/reference/algorithms.centrality.html   http://networkx.github.io/documentation/latest/reference/algorithms.centrality.html    The docs build locally just fine with the fix installed, but readthedocs indicates https://github.com/rtfd/readthedocs.org/issues/1290   that they will not be fixing the problem until Sphinx makes a release.  We could return to network.github.io until that time. We could continue as is (its been broken since late April). Other ideas? 
issue
Correct the documentation for simple_cycles#TITLE_END#Fixes #1808 
issue
Use sphinx stable branch via github on ReadTheDocs#TITLE_END#The stable branch of sphinx has the one-line fix for autosummary. So even though the release on pypi doesn't have it, we can get it from github. Fixes #1796 without creating our own fork of sphinx.  When sphinx 1.4 comes out we can switch back to using the pypi version. 
issue
Update examples for iterator edges and nodes methods#TITLE_END#This will help with readthedocs make_gallery #1667 These are solely due to the switch to our 2.0 API 
issue
Change comments about fringe tuples in dijkstra#TITLE_END#Fixes #1784  
issue
Update install.rst#TITLE_END#Update language for installing with pip. Change min python version to 2.7 or 3.3 (removing 3.2) 
issue
Rewrite harmonic_centrality to better use iterator#TITLE_END#Better use of iterator functionality of path functions, but still returns a dict.  So is not an iterator itself (yet -- should it be?). 
issue
Rewrite harmonic_centrality#TITLE_END#Build in iterator functionality but still returns a dict.  So is not an iterator itself (yet -- should it be?). 
issue
For v1.11 drop support for python3.2 and add 3.5#TITLE_END#same as #1778 for v1.11 
issue
Prepare for release of v1.11#TITLE_END#The changes from 1.10 include: - updating news.rst to include both 1.10 and 1.11 - adding appveyor - removing symbolic links in examples - adjusting layout with the center and scale arguments. 
issue
Remove all the symbolic links from the 'examples/' directory#TITLE_END#Apply #1707 to v1.10.1 
issue
to_networkx_graph needs .update() instead of new dict#TITLE_END#In convert.to_networkx_graph, we are overwriting instead of  updating the node dict.  We just need to update the existing dict. Fixes #1718  
issue
minor changes related to _iter removal#TITLE_END#Mostly doc string tweaks for graph classes, tutorial. Fixes of blockmodel and antigraph examples api_changes and release_2.0 updates. Addresses #1655  
issue
Turn off coverage for pypy travis tests#TITLE_END#Turning off coverage for the pypy tests reduces time from 50 minutes to about 5 minutes. 
issue
Add timing tests and baseline classes to compare to.#TITLE_END#This style of testing works with nose and compares to previous code (v1.9) so it gives long term baseline. Its a step toward v2.0 where base classes will change.  We can't overload this or tests will take too long. As currently config'd it adds about 5 seconds.  The code also runs on its own and produces a timing report.   python test_timing.py 
issue
Fix bug for iterator input for digraph.degree#TITLE_END#digraph.degree( iter([0]) ) doesn't work because the iterator is used twice.  Bug introduced in 2010 during conversion for python3. This PR also adds tests for this same bug. 
issue
Fixes issue #1539 Bug in relabel_nodes for isolated nodes#TITLE_END#Fixes #1593 relabel_nodes(copy=False) bug removed an isolated node when relabeled to itself.  Fix is that any node relabeled to itself is simply skipped in the relabeling. 
issue
SpecialGraph class for custom dict-like data structure.#TITLE_END#SpecialGraph and SpecialDiGraph allow ordered dicts or any other dict-like objects to take the place of dicts in the base classes. Examples include ordered nodes, ordered neighbors in adjlists. Another nice possibility is a low memory graph class for cases where edge attributes are not important.  Some basic tests are included, essentially making sure this class still acts like a graph.  More doc_string tests show the examples mentioned above.  Discussed in closed issue #439 (which came from trak 445) as well as discussions on the networkx-discuss list.  This might be a jumping off point for creating an SQL-based graph storage class too. 
issue
Remove randomness from test_petersen to avoid fails#TITLE_END#Instead of taking the min of 100 random samples of k nodes, we now take the min of all possible samples of k nodes. There's only 10 nodes so it is still very fast. 
issue
Expand data keyword in G.edges and add default keyword#TITLE_END#This is one feature for the 2.0 API as discussed in #1246  It change G.edges in a backward compatible way.   For G.edges(data=data) If data is True, the whole data dictionary is returned for each edge. If data is False, no data is returned. Otherwise data is used as the attribute name of the edge data to return.  The code     `[ (u,v,edata['weight']) for u,v,edata in G.edges(data=True) ]` can become (either works)     `G.edges(data='weight')`  The code  ``` for n,nbrs in G.adjacency_iter():     for nbr,ddict in nbrs.items():         print (n,nbr,ddict.get('weight',1)) ```  can become (either works)     `G.edges(data='weight', default=1)` 
issue
Comment out timing nosetests due to variation in travis test times.#TITLE_END#This comments out the nosetests for timing base classes. Due to variation in travis test times we are getting too many false fails.  Addresses #1323. Uncomment to use locally with nosetests. Or run the test script manually "python test_timing.py". 
issue
Address ironpython issues from #1327#TITLE_END#This puts add_nodes_from assignments inside the try/except clause so that ironpython works even pre-2.7.5. Fixes #1327  
issue
Python3 tests fixed for nx_shp.py#TITLE_END#Remove xrange and map to make nx_shp tests work with python3. Fixes #1227   All changes to test_shp.py are for python3.  All changes to nx_shp.py are to take advantage of "new" gdal/ogr iterator functionality for loops. 
issue
Allow overwriting of base class dict with dict-like: OrderedGraph, ThinGraph, LogGraph, etc#TITLE_END#I've melded the suggestions from #1268, #980, and others into the base classes. The small slowdown seen in #1268 went below 10% when special was no longer a subclass.  So the standard Graph and friends are just as fast as before. But now we can overwrite the various dict objects used in the base class.  Syntax is like this:  ``` python import networkx as nx from collections import OrderedDict class OrderedGraph(nx.Graph):     node_dict_factory = OrderedDict     adjlist_dict_factory = OrderedDict  G=MyGraph() ```  Or we can create a low memory graph class  ``` python import networkx as nx class ThinGraph(nx.Graph()):     all_edge_data = {'weight':1}     def single_edge_dict(self):         return self.all_edge_data     edge_attr_dict_factory = single_edge_dict G=ThinGraph() ```  For timing ratios, I used the code from #1313 to get the current class' time divided by the v1.9 base class time.  | Time Ratio to Baseline | Graph | MultiGraph | DiGraph | MultiDiGraph | | --- | --- | --- | --- | --- | | add_nodes | 1.079 | 1.015 | 1.016 | 1.024 | | add_edges | 0.991 | 1.005 | 0.977 | 0.996 | | add_and_remove_nodes | 1.017 | 1.014 | 1.015 | 1.015 | | add_and_remove_edges | 0.994 | 0.991 | 0.987 | 0.998 | | neighbors | 1.007 | 1.017 | 0.995 | 1.007 | | edges | 0.988 | 1.067 | 0.999 | 0.989 | | edge_data | 0.971 | 0.992 | 1.016 | 1.031 | | all_edges(adjacency_iter) | 0.997 | 0.997 | 1.004 | 1.012 | | degree | 1.011 | 1.026 | 1.003 | 1.064 | | copy | 1.045 | 0.983 | 1.009 | 1.017 | | dijkstra | 1.000 | 0.993 | 1.044 | 1.041 | | shortest_path | 0.975 | 1.008 | 0.948 | 1.003 | | subgraph | 1.029 | 1.028 | 0.926 | 1.081 | 
issue
Correct add_nodes_from use of adjlist_dict_factory#TITLE_END#Fixes #1394 
issue
Fix some tests and examples that sometimes failed#TITLE_END#Sometimes random graphs or dict order made these tests/examples fail with nosetests. 
issue
Shift travis to miniconda and pip mixture#TITLE_END#This expands travis coverage back to what wheel used to provide. See #1313 for brief discussion. It seems that a mixture of conda and pip gives the best coverage now. This attempt provides: - coverage/coveralls for 2.7 and 3.4 - dependent packages testing for 2.7, 3.2, 3.3, 3.4 - no dep testing for 2.7,3.2,3.3,3.4,pypy,pypy3,ipy 
issue
Allow Failures in pypy tests until we sort things out ;)#TITLE_END#This is a temporary measure to hide travis failures in pypy due to ordering. #1351 is discussing actual fixes. 
issue
Update link in credits to Guido essay on graphs#TITLE_END#Fixes issue #1369 
issue
Timing Tests Failing#TITLE_END#From #1322  @hagberg said: Those tests are not related to your code but instead in the timing tests. @dschult can you look at this? Do we need to run these tests regularly or are they primarily for evaluation performance when suggesting changes?  I am looking at them now....  I've tried the tests on a variety of machines and my numbers all come out to be with a ratio of 1.3 or lower. The travis code is occasionally giving results like 2.05 which is above the 2.0 cutoff.  I think maybe @chebee7i mentioned changing the cutoff to 3.0 to avoid this problem. But I don't understand what is making the travis ratios so much bigger on occasion.  To answer your question: the idea of running the timing tests on travis is to catch changes that slow down basic features when we don't expect them to slow it down. Perhaps @chebee7i change to cutoff=3 will work for now but when it has to be changed again to 4 and then to 5 we'll know something is going on.  It might not even be something in our code--but something about how python does lookups. But it shows the speed relative to our old (1.9.1) base classes.  I'm fine with taking them out of travis if that's preferred. But these are the reason it is in travis. 
issue
Fix issue #1041 in simple_cycles. Add test.#TITLE_END#Reset the closed status of nextnode so it doesn't get unblocked prematurely. 
issue
Non-recursive and faster simple_cycles#TITLE_END#Copied simple_cycles to recursive_simple_cycles and added a new simple_cycles that is the same algorithm but not recursive.  Two changes in the API for simple_cycle are that 1) it now returns a generator instead of returning a list. 2) Cycles no longer repeat the first node as the last.  The last node in the list is assumed to connect to the first node on the list.  Tests added to compare output of simple_cycles and recursive_simple_cycles  Thanks to erikasantos and friedsoap for instigating this and suggesting helpful changes (which I incorporated).  This addresses #874 #888 #889 
issue
Spring_Layout sometimes puts everything at origin#TITLE_END#One example where spring_layout puts all nodes at the origin is:  G=nx.path_graph(500) nx.draw(G)  See also the thread on the discussion list: https://groups.google.com/forum/#!topic/networkx-discuss/nzMzv5ntgc4 
issue
Make sure graph is connected in test_edge_cutset_random_graphs#TITLE_END#Still possible for test_articulation_points to fail due to 50 random graphs in a row that aren't biconnected.  But I'll wait to fix that until it happens. 
issue
Format cycles from simple_cycles w/o last node#TITLE_END#Cycles in networkx are represented by a list of nodes.  But simple_cycles() repeated the last node of the cycle.  This removes that repetition to bring simple_cycles in line with other cycle routines.  Related discussion in #889, #890.  Also corrected some doctests and updated tests for this change. 
issue
Allow larger k for small world graph creation.#TITLE_END#Fixes #894 See discussion there.  These changes move degree test inside inner loop to avoid check on common case of small k.  Also adds some tests. 
comment
I dont see any problem with adding a new function to the release that is not in the RC. Will that mess up anything? It will still appear in the release notes right?  It won't mix up any users because it is adding something --- not changing/removing.  So as long as this doesn't mess up maintenance or release processes I would say we are good to go.  
comment
Thanks for this Issue!  It forms an important part of our long-term discussion about how to provide a quick way to get information about a network without requiring lots of compute time.  This function has evolved over time toward a simple description -- mostly because the more complex summary data can be computationally intense, because different applications value different measures, and because it is easy for users to create their own `info` function with customized reporting useful for their application.  The number of nodes and edges is uniformly helpful. And we ported that to `print(G)` because `info` finally got quick enough to justify implementing a way to print a Graph.  We are still discussing best ways to provide this kind of interface.  The Pandas DataFrame provides a model that we are currently considering emulating.  `df.describe()` provides descriptive statistics for the data frame. And `df.head()` displays the first few lines of a DataFrame.  We have talked about both of these options: a collection of descriptive statistics, and a way to show a few edges.  The issue with descriptive statistics is still "which ones to compute?". And the function to show a few edges got bogged down in its name ("head" is potentially confusing since directed edges have a head and a tail.)  It would be great to hear more about your ideas for a "metadata- or data-provenance-focused prose version".  The examples you show above seem more table-like than prose-like. The current text provides information about (un)directed, and (multi)edges, how many nodes and edges (and thus average degree). Name is no longer included (it seems to be used very little -- but we don't have good data on that). And number of components seems potentially too computationally intensive -- though we could leave `print(G)` as it is and put more intense computations into `G.describe()'.  But what criteria should we use for what to compute and what to leave out? Or maybe we should just give a nice way to users to build their own `describe/info` method?
comment
Replaced by #8347  Thanks @jfinkels @Midnighter and @bjedwards 
comment
The GEXF standard uses "pid" and "parents" as special attribute names. We handle them according to the standard by placing them in the GEXF not as node data, but in the line defining the node. See e.g. `pid='1'` in the line that provides the node id for "node1", and similarly for "node2".  So the pid attributes do appear in the resulting file. They appear where they should show up for the GEXF standard -- and not where you are looking for them. If you want that information to show up in the data attributes section of the file, you will need to name it something other than "pid" or "parents".
comment
My quick read through the code suggests that we could implement a normalization factor for ```subgraph centrality``` fairly easily.  Also, we should probably always compute it. We've got the eigenvalues already so it's not computationally expensive... But I could be missing something too.      expw = np.exp(w - np.max(w))   # or maybe split into two lines...  [Edited to be max in stead of min]  Then changes to docstring including a warning about backward incompatibility for pre-post 2.0.  @Michael-E-Rose do you recall enough? Am I on the right track?
comment
Right -- max/min switcheroo...  thanks...  Every "raw" data is implicitly a normalized value. So the literature and usual treatment determines what is raw and what is normalized (in some sense).  I'm guess I'm worried about having the normalization agree with literature that uses the measure.
comment
I'm going to bump this to v2.1, as it is more subtle than I am able to delve into in this timeframe.
comment
What's your goal for having deterministic ```simple_cycles()```? Often when there is a problem that doesn't have a good solution,  there's another problem that could alleviate the need to solve the first. :) Ruling out using Python sets is a pretty restrictive criteria.
comment
Hmmm...    Another approach which is similar to sorting scc but doesn't involve a sort -- but does involve many ```__contains__``` operations:   ```scc = [n for n in G if n in scc]``` That way the order is as it is reported by G (which is deterministic). I guess in ```simple_cycles``` this would be:  ```for startnode in G: if startnode in scc: break```  Here's another idea:  if I understand correctly, hash values are nondeterministic for strings, but deterministic for integers. Would it be enough to [relabel your nodes](https://networkx.github.io/documentation/stable/reference/relabel.html) as integers? Does it make ```simple_cycles``` deterministic?
comment
As you explore this again, I'll just note that you shouldn't worry too much about relying on a "detail of CPython implementation" with regard to deterministic hashes for integers. dict order used to be nondeterministic and in Python 3.6 (not that long ago) it changed to be deterministic. That could easily happen to CPython sets someday. So, any system we come up with depends on Python implementation. The way they handle hashing of integers is less likely to change than the determinism of the set object itself (in my estimation).  :)
comment
The request here is to make the result deterministic (because their firm doesn't like nondeterministic functions due to difficulties with testing and reproducibility). The solution offered is to relabel the nodes to be integers. Their only remaining concern is that hashes are not guaranteed to stay the same in Python for all future versions.  I dont really want to start rewriting every function that uses sets to avoid using sets.  Better to lobby CPython `set` maintainers to adopt the hash handling techniques from dicts to the set objects so that order is deterministic.  We have avoided sets before because of this difficulty. It is a bigger problem than this one function. If we do care about it let;s open an issue directly about the bigger picture question of making set objects deterministic.
comment
I have updated this Generalized Petersen Graph pull request.  I added tests in the `test_small.py` module. I added cross-links in docs for the graphs that are isomorphic to a generalized petersen graph. I updated the function to allow the case when `k == n/2` because wikipedia says (and has citations) "some people allow `k == n/2`. " Made the function check for directed versions of `create_using` and raise if it is. The decorator in `small.py` only works with functions that have create_using as the only input parameter. And the `@not_implemented_for` decorator doesn't work because `G` is not an argument of the function.  I added a doc_string citation to the wikipedia page, and connected the doc_string to the docs.  :)  Should be ready for review.  
comment
Nice summary and statement of the problem and some possible solutions.  The current doc_strings were created as "have a single doc_string for all 4 graph cases and copy any changes to all 4 supposedly identical versions".  Of course, over time they have diverged -- but I think only slightly. The idea behind copying the identical doc_strings was so that someone reading the code had the doc_string right there with the code. And no special code was needed to attache the inherited doc_strings to the class methods.  When the views and example subclasses arrived, no global effort was completed to update/unify the doc_strings.  So, it is not just `degree` that has 4 copies of a docstring that is not customized for the subclasses. It is actually *most methods* have docstrings that are not customized for the subclass. We were a small shop and made an effort to keep them synchronized while keeping them in the code of each subclass so people reading the code could see the docstring there in the code. We could keep all the doc_strings separate from their code, but we felt that keeping them close had merit -- and sometimes even helped us recall stuff without having to look at a separate module to find the docstring.  I think we didn't go with option 2 because  - the users would have to look at all 4 docstrings to be able to tell what the differences are. The doc_strings mostly look the same, but where do they differ? (no surprises) - we worried about divergence being overlooked.  (improvements in one that didn't get transferred to others would be hard to identify -- and might even be hard to even see how to transfer to others.  Now that we have the wonderful CI-magic, we could construct a test to ensure that we know every time a doc_string from one class is changed when the others were not updated. This would enforce synchrony without removing the docs from the code.   For option 2, where the doc_strings differ, is there a CI-magic solution to ensure that they have similar style and explain the differences between subclass methods rather than just what one method does?  I like the idea of doing a case study with `degree`.  That's a good case too because there are sufficient differences to make it useful and not always straight-forward. :}
comment
It looks like PR #5529 didn't fix the problem for `InDegreeView` and `OutDegreeView` and their counterparts in the other classes.  I foresee this arising for other base class method doc_strings (e.g. edges, nodes, adj?, others?). Perhaps a broader in scope, yet more focused effort on getting all typing correct in the base class doc_strings is warranted. 
comment
I’m adding a pointer to #5699 because it relates strongly to this conversation. It started by updating the doc_string of `remove_edges_from` in `MultiGraph`. And those changes led to other improvements in related doc_strings. The doc_strings are now specific to the multigraph classes (and maybe they already were different from Graph/DiGraph, but maybe not).  The method `remove_edges_from` is identical in both MultiGraph and MultiDiGraph (because it is inherited—so the methods are identical). The `remove_edge method is not similarly inherited, so can potentially be different for all 4 classes.
comment
Good point! --     But it also makes sense to put all the dot stuff in one place.   Maybe the dot/pydot/pygraphviz should all be in read/write instead of drawing? At a fundamental level, drawing via pygraphviz **is** writing in a specific format.   OTOH... why would you write a dot file except to draw the network?  Hmmmm.....  I don't like splitting `write_dot` from the pygraphviz interfacing code, but someone looking for all our writable formats might not find it in `drawing`.  Could we keep the code together but have the docs for write_doc listed with read/write?
comment
Also relevant to this discussion is the recent PRs for test representations (drawing using unicode/ascii) and for conversion to LaTeX.  Which should be in read/write and which should be in drawing?   Maybe we should be combining readwrite and drawing directories??
comment
The ```union``` name will likely be confusing either way. If we take out the error, then people expecting the Graph Theory definition will silently get some nodes combined. The current setup means people expecting it to work when node sets are not disjoint at least get a noisy error instead of a silent confusing result.   Perhaps it is best to remove ```union``` in favor of the ```G.update``` method which simply adds the nodes and edges of one graph to another. That avoids the use of this problematic term. The name ```update``` comes from Python ```dict``` so is hopefully clear.   We could also point to ```G.update``` in both ```disjoint_union``` as well as the docs for operators.
comment
I think you mixed up ```R.graph.update(G.graph)``` which updates the R.graph "graph attributes" with ```R.update(G)``` which adds/updates the attributes, nodes and edges of R.  The ```compose``` processing isn't redundant.  It is true that the ```compose``` operation handles node and edge attribute collisions in favor of the right (second) argument. But this is well documented.  We do NOT treat nodes as equal only if their attributes are equal... To make that behavior work, you should construct your nodes to include all that info... that's one advantage of arbitrary hashable nodes. e.g. instead of node ```"a"``` having attributes ```{"color": "blue", "height": 5}``` you can make the node ```("a", "blue", 5)``` if you want treating nodes to be equal only if their attributes are equal.    In fact, it gets even messier than our discussion has touched... There are in fact two types of disjoint union (labeled and unlabeled nodes). That's why we have it set up this way.  The ```disjoint_union``` function treats the two graphs as unlabeled graphs (nodes have no identity). It doesn't matter if the nodes are equal or not. The resulting output is a graph that contains the two input graphs and no edges between them. The nodes are relabeled to make the structure work. The ```union``` function treats the two graphs as labeled graphs (nodes have identity). The resulting graph contains the two input graphs and no edges between them AND nodes have the same identity as they do in the original graph.  I suppose we should put much of this discussion in the docs in some coherent way...    But the main point of this issue is that people expect ```union()``` to be what we call ```compose()```.  The first step should be to add text to the error message whenever people input 2 graphs that share nodes. The error message should probably refer to the compose function as well as the disjoint_union function as well as the G.update method.  That may be toomuch for one error message. The doc_strings of these should also probably be updated to refer to each other and explain the difference.    Changing the names of these functions is the next possible action to take, but I'm -1 without a more complete plan for how to proceed.
comment
I'm not convinced there is anything we need to do here other than add better documentation to ```union``` and probably other operators.    The current situation makes people who try ```union``` when they want ```compose``` run into an exception...  which is a good thing to have happen. Especially if we had documentation, or a better exception message.  The word ```union``` is clearly defined in Graph Theory and we follow standard definitions wherever reasonable. The word ```compose``` sometimes refers to what we call ```lexicographic_product```, but other than Harary's text, I haven't seen the use of ```compose``` in that sense. Certainly users will notice from the docs and the output of ```compose``` that they don't have a ```lexicographic_product``` if they use ```compose```.  The idea that ```union``` and ```disjoint_union``` are the same ignores the distinction between unlabeled graphs where node identifiers are not important and labeled graphs where the node labels are important. It is a subtle distinction and would only be hidden if we combined these two functions.   So, I think we just need to add some documentation; especially updating it to account for the ```G.update``` method we now have available.  I'm -1 on removing ```union``` or combining it with ```disjoint_union```.
comment
I actually don't think any of the code needs to be changed. The compose code is very readable and clear and the ```G.update``` code has more cruft -- harder to read and now that I look at it, might not retain edge keys for multigraphs. It seems like some rooting around would be needed to make sure it's all good... could be that new tests would show what is needed.  All these changes are good things really... but I'd rather spend time on new code than replace working code.  How about we get a ```relational_composition()``` function instead of changing ```compose```?
comment
It would be great if you can put together a function for relational composition. I'm not sure it fits with the other operators... so if you can't find a good match put it in its own module --  and that could be outside of the ```operators``` folder if that works better.   Thanks!
comment
Thanks @NeilGirdhar !  This really helps give a feel for the impact of this change.  Given the feedback I've heard from multiple sources, this or something like this will likely be implemented at some point. But I have to say that I think it makes the Python code just about unreadable unless you have bought into annotations and types and have been taught how Python handles them. I can't imagine my undergraduates just out of a second course in Python reading through this code and understanding what is going on. Even with the import statements, there is no way to distinguish whether ```Graph``` is a class or a type, and whether ```Node``` is a class or a type. They look the same, but they are very different beasts.   Secondly, why have you changed the names of the factory functions? Adding "default_" to the start makes it sound like they can be set or overridden via an input parameter.  They are not like defaults in any way. They ARE the factories. If you want to change them, you make a new class. Am I missing something?  Thirdly, my quick read suggests that you removed ```__init__``` from DiGraph. Can you explain what changes you have or have not made? (not just in DiGraph, but everywhere...)  I know you added the annotations, but what else did you do?  Thanks again for this -- I am trying to re-teach myself and learn how this is helpful. I want to protect the readability and pedagogical nature of the code as well. I'm hoping we can end up providing both annotations and readable code. :}
comment
The reason we use the awkward looking code (and others):  ``` self.graph_attr_dict_factory = self.graph_attr_dict_factory ```  is for speed (it becomes an instance attribute instead of a class attribute) but I'm not sure what the impact is especially with so many new versions of Python since this was tested.
comment
stub files are the way to go. These could be hard to maintain... Are you up for that?
comment
I'll just insert a short comment here.   For this case, a PR was merged 2 days ago that removes the lines which set instance variables that have the same name as the class variables. So, if you can pull in those latest changes from the upstream main branch it will get rid of all those errors.  
comment
We have made some progress in moving toward types. #5127 put in place CI-tools running mypy against the code base on each PR. Unfortunately storing stub files in a separate repo from the main repository makes it hard to include CI, etc. My understanding is that NetworkX is open to including stub files in the main repository so that they can be maintained properly and tested and flagged for updates when code and stubs don't match. This will hopefully make them robust and should eliminate any concern about drifting from the code base.  If you have stub files for typing the base graph classes, that would be a productive PR to create. It would be much more likely to be merged than an inline PR because it doesn't impact the readability of the code. Stubs give the advantages of automatic IDE type checking while minimizing impacts on code readability.  I think the tools are getting better and the standards and customs of the scientific python community are developing for typing. So my guess is that we are heading in that direction. But most of the scientific python packages are moving slowly on this -- and that is probably prudent. Even Python is releasing typing features that will take years to be able to take advantage of. But we are moving.   As far as reluctance for inline typing annotations, the biggest difficulty is readability. One of the nice things about Python is that is was designed for people learning to program. The code is simple and elegant -- hiding many of the details about types and memory allocation and pointers that other languages focus on. I think the notation and syntax are improving -- more importantly, people are learning how to make the annotations better.  But I think the reluctance is also deeper than that. For many years a selling point and design choice for python packages has been "duck-typing", where you don't check types. You try to make it quack and if it does, then you treat it like a duck. If Python code starts to look like C or Java code then we might as well be programming in C or Java -- which are often faster anyway because they are compiled. So there is a long history of phrases like "use hasattr, not isinstance". And the goal of writing code to accept any reasonable input is a reasonable goal.  The code I have seen with inline annotations requires a novice to learn a huge amount -- or else just try to ignore all the annotations at first. I'm hopeful that Python can find a way to ease the learning curve required to read code with annotations. But it also seems straight-forward, practical and much less dangerous than it used to be to keep the typing in stubs files. So the code is still accessible, and the tools can check the types. I expect in the not-to-distant future, tools will be able to merge stub files with the code. Also in the other direction, tools will be able to split annotated code to de-annotated code and stub files. I expect that it will become much easier to see the code in the form that you want to see it.  I'd love to see this kind of energy around writing more network analysis functions. But there seem to be many people who feel more comfortable contributing type annotations rather than writing new code.   @fmagin to address your implied question: where should we put our efforts to try to get this done?  I don't know the answer -- but I think: - it would be good to add a stub files version of typing as a PR. People can make suggestions and improvements similar to the work in this PR. We can try it out. - We have been including type information in our doc-strings for a long time. It would be good to have tools that can check the doc_string type information against the inline or stub annotations.  A good place for this code is likely to be mypy. But other avenues for contribution are available or can be created.  - it would be good to improve the tools like mypy and others which work with code and annotations to allow people to see the code they want with annotations as they want. This includes:    - merging stub files with code to get inline annotations.    - splitting inline annotations into stub files and code files.    - pulling type annotations out of numpy-style documentation and creating inline annotations (or stub files)    - taking inline type annotations and putting that information into doc_strings.  More tools like this could have a large impact on the python user base and the acceptance of type annotations.
comment
This is a very constructive set of comments -- thanks @fuglede. I agree that we don't want to duplicate what's already in csgraph. And we want to put wrappers around some csgraph algorithms (with adapter code). And we might want to make some sort of ```SparseGraph``` with a ```nx.Graph``` front end to a csgraph data structure. That seems like it might be tricky. But that could allow many NX algorithms to be used on a csgraph data structure. That would reduce moving data back and forth somewhat, but more importantly, might allow some of our pure-python algorithms to be sped up via cython. I'm afraid I don't know enough about what is needed to make cython work well...  I guess I should ask a question: Is it better to have an interface that allows NetworkX algorithms to run on csgraph objects, or to have an interface that allows the csgraph algorithms to work on a NetworkX graph?   Or am I thinking about this all wrong and its best just to make a fast converter from csgraph to nx Graph and back...  Probably all would be helpful, but which should be tried first?  
comment
My immediate thought of an algorithm with many adjacency lookups would be the shortest_path or shortest weighted path (dijkstra) routines. But there may be others.   The NetworkX Graph structures hold the adjacency in dict-of-dict structures that are also pretty fast. I would have thought the performance advantage of ```csr_matrix``` would be when you want to do linear algebra manipulations of the adjacency structure. For example, finding the largest eigenvalue/vector (pagerank).  But I see that csgraph doesn't include those algorithms so maybe I don't understand the advantages of csr_matrix very well.
comment
Can you be more specific?
comment
This sounds promising. 
comment
I think the boundary between graph learning algorithms and graph algorithms is pretty porous. I think there is a fair region of overlap where algorithms of one can be included in packages for the other.  We'd like to expand connections with the graph algorithms in sklearn. The package you point to also looks pretty good -- so I think we should support these other packages and also make room for tools at the boundary (which sometimes might extend over the boundary).   How's that for a wishy-washy answer.  A lot depends on interests and time of developers too of course.
comment
Thanks for the suggestion here.  This seems more complex than it first appears. For example, there is no Type Node.  Indeed, nodes can be any hashable object. Similarly, there is no type for an edge. We currently document types in docstrings for parameters and for return values. But those types are often not standard. For example:  string, callable, bool or None     Do we lose this kind of flexibility with the annotations?  I can imagine this ballooning into a huge maintenance headache. Can you help me to understand the costs of such a change? Are there automated ways (like Travis-CI) to check what is annotated and are there automated ways to manage the collection of types we use?
comment
Sorry for poking my head in here again when the discussion is really over in #4014 but I want to ask something related to a comment above.  Is there a way to specify that a Node should be "hashable and not None"?
comment
Lexicographic BFS can be created from our breadth-first-search tools by using a sorted set of neighbors.  But the algorithm for [LexBFS](https://en.wikipedia.org/wiki/Lexicographic_breadth-first_search) does this more efficiently.  It looks like there is no immediate support for implementing this feature -- so I'm going to close this. But you can still comment on it and it can be re-opened if there is interest.
comment
That message has been there for 6 years which makes me think there is a way around the difficulty you are talking about.  Though I could be wrong of course.   Is there an equivalent graph to the problem you are interested in that doesn't use multiedges?  For example, is the capacity between 2 nodes equal to the sum of the capacities of the multiedges between those 2 nodes? Any ideas on what an aggregate edge weight would be between two nodes?  Alternatively, is there a way to add nodes which make it no longer a multigraph?  I'm hoping there is a graph transformation that would turn your multidigraph into a digraph with equivalent features so you could solve the problem on the digraph.   If not, we'll put this request on the list of features to add...  But since it has been that long without anyone requesting it, I'm hoping there is a relatively simple workaround (and maybe we could add that to the codebase.)
comment
Be careful with H=nx.DiGraph(G) because that just makes the last edge iterated over in G between (u,v) become the edge in H.  So, it is ignoring the multiedge attributes -- not e.g. summing the capacities.  The bit of the output you show does have many edges with 0 flow. But it has an edge with 835 and that is close to the 838 range that you state would be reasonable.  Also be careful about travel_time as the weight. The weight is taken to be a cost per unit flow on the edge. So more flow means more cost and it doesn't sound like travel_time is cost per flow.  I would suggest you take a simple 3 or 4 edge example graph with your attributes and play with it and its attributes until you understand how it all fits together.  ```python G=nx.MultiDiGraph() G.add_edge(0, 1, capacity=53, travel_time=20) G.add_edge(0, 1, capacity=3, travel_time=10) G.add_edge(1, 2, capacity=23, travel_time=10) G.add_edge(2, 3, capacity=30, travel_time=15) G.add_edge(1, 3, capacity=53, travel_time=20) flow = nx.max_flow_min_cost(G, 0, 3, 'capacity', 'travel_time') ... and others ... ```   
comment
I resolved the conflicts with this PR and the main branch. Just wanted to warn you that this changed your branch on your fork. So you'll need to pull it down to local before doing further work.  I don't know of any blockers, but clearly we need more reviewer resources/time to address PRs like this and others.   One thing I notice is that this has no tests.  Can you add some tests in the `drawing/tests/test_layout.py` module? 
comment
Thanks @matthieugouel for this submission. I think all discussions have now been addressed. Reviews should follow fairly quickly.
comment
Thanks for this introduction (at least to me) of treelets.  The already existing `all_simple_paths` function has a `cutoff` parameter and `source` and `target` parameters. So maybe you don't need a new function for `all_bounded_simple_paths`.  This is also very similar to (maybe a generalization of?) triad census, which does a similar thing for 3 nodes. Is this a generalization of the triad census idea?  :)
comment
It is usually not necessary to merge or rebase the main branch into your branch. That is only needed if the lines of the file you are changing get changed on the main branch. That's called a "conflict" and you can rebase on or merge with the upstream main branch to "resolve the conflict". For this PR, most of your changes are in new files for the repository. So there can't be conflicts with the main branch there. It would only be if e.g. someone added their name to the contributor list at the same line that you did. :)
comment
Hmmm.... but the `nx.all_simple_paths` function uses a starting node with a single traversal.  The `nx.all_simple_paths` function ([docs here](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.simple_paths.all_simple_paths.html)) takes as a target either a node or an iterable of targets. So you can do:   `nx.all_simple_paths(G, source=node, target=G, cutoff=LENGTH)`  and then filter the results for an exact length if desired.   You would need to do this call for each starting node in `nodes`. So in your use case, you could use `itertools.chain` to chain those together (or use a nested while loop).  Something like:  ```python     iterable_of_path_iterator = ((p for p in nx.all_simple_paths(G, s, target=G, cutoff=LENGTH) for s in nodes)     for path in itertools.chain(iterable_of_path_iterators):         if len(p) != LENGTH:   # or put this check inside the iterator for nodes             continue ```
comment
The all_simple_paths(G, a, G, cutoff=3) function does a single traversal from a and yield the paths it finds to any node in the targets iterable (which it turns into a set). So it doesn't start over a traversal for each of d, c, and e. It does of course only have one source. But it checks each node and if that node is in tragets, it yields the path to that node and continues the traversal.
comment
Understand completely about the bandwidth problem and we all appreciate this PR.  Do you have a pointer to a dataset or a way to get a dataset of a chemical compound where finding treelets is of interest? It would be cool to have an nx-guide that works through the problem showing the sub-structures of interest. Similarly, is there a publication/review where finding motifs has been done that would be accessible to non-experts in chemotaxis?  Thanks again!
comment
In addition to maybe breaking the DE highlight of the link, the :wiki: syntax makes it harder to copy/paste the link url from a text view of the code (e.g. via github). In summary, not having the url in text form makes getting the url harder. The gain of the :wiki: syntax is (I think?) an abbreviated link that takes less space in a text view of the docs. Are there other +/- going on here? 
comment
The last comment in #6710 points out that not all the nodes are present in the networkx result for `immediate_dominators`. So having all nodes in a returned dict is not as relevant for this literature as for some other algorithms.   I'm good for excluding `start` from the returns dict. 
comment
OK... looks like I'll approve this and make a small PR to add the hash to the ignore file. :)
comment
I not an expert in this algorithm, so I will need some more explanation if possible. The very same example that you show is in our test suite with the answer tested to be 3 empty sets. So the discussion probably needs to be about the definitions.  A discussion of this example appears [in the PR comments](https://github.com/networkx/networkx/issues/2071#issuecomment-211410380). The response is essentially: DF('a') = {} because 'a' has no immediate dominators. And I admit that is not very convincing given [the definition we are working with](https://en.wikipedia.org/wiki/Dominator_(graph_theory)). Perhaps @xmagpie can explain a little more.  I can't find any worked examples published where the entry node is part of the loop. Do you have any references for that case? Or definitions which explicitly mention how the entry point node is to be handled?  The cited paper, "A Simple, Fast Dominance Algorithm" by Cooper et al in 2001 has code for the algorithm and the code only checks if the number of predecessors is >= 2.  But its not clear to me that they consider either self-loops or back-edges to the entry node. Any help you can provide to convince us that our tests are wrong (or right) would be helpful. :)  Thanks! 
comment
Thanks for the suggestion... How would you prefer the start node to be handled? As I see it, we could exclude it from the resulting dict -- or include it in the dict with an appropriate indication that it is the special node without any idom.  We chose the latter approach so that all the nodes are included in the dict (otherwise you wouldn't know what the starting node was without access to something out of the dict. Our way of identifying the start node is to have the dict provide the start node itself as the idom. I suppose one could use `None` or some other indicator.  There are a few ways to customize the results to get what you want: ```python idom = nx.immediate_dominators(G, node) idom  # our result indicating starting node by idom[n] == n node_idom = {u: v if v != u else None for u, v in idom.items()}      # change to indicate starting node with idom[n]==None del idom[node]   # if you don't want the node in the dict ``` 
comment
Given the above conversation, would you agree that it is troublesome to leave out the start node as a key from the returned dict? Would it be sufficient to have the starting node's value be `None` as discussed above?   We'd be supportive of a PR to change the starting node's value in the dict to `None`. Of course there might be issues with that that haven't been discovered in this discussion. But it'd probably be a good start -- and it would hopefully alleviate the concern from #6713, though I haven't looked at that recently. 
comment
Does changing the default `join` function to `operator.add` (or even `list.__add__`) speed this up?
comment
I think we should rely on published literature to determine the correct way to do this.  Ideally, there is a unified approach to weighted degree centrality.   There are too many options for how to do it to believe that the one we choose will be the right one.  If the literature hasn't settled down on a single useful definition for it, then we could provide a way for the user to choose -- or we make the user write their own. :} 
comment
To make the tests work in the examples, you should make the output match exactly -- including white space. So put all of line 181-182 onto a single line.  You can tests these kinds of changes locally by installing `pytest` and running e.g. `pytest --doctest-modules bfs_edges.py tests/test_bfs_edges.py`  Also install `black` and run that like `black bfs_edges.py` or black --diff bfs_edges.py` to help with style issues.
comment
Yah -- that is strange.  It has been "working" for 14 hours. And the image is there if I click on "Details". So I don't think it is anything related to your PR. :}
comment
It may be more effective to find bugs by running `pytest` locally on your machine rather than having to wait for the github testing machinery to finish.  To install, something like `pip install pytest`, or `conda install -c conda-forge pytest` should work. But it may already be installed depending on your system.  Then to run it use:  `pytest --doctest-modules laplacianmatrix.py tests/test_laplacianmatrix.py`.   You can check syntax by just running `python laplacianmatrix.py` locally. 
comment
Oops...  forgot to check the docs.  Can you connect this to the docs by adding a file `doc/reference/algorithms/perfect_graph.rst`? Maybe look at `doc/reference/algorithms/moral.rst` as a similarly simple case.
comment
Sorry -- I seem to have forgotten another step:  We need to add the line `perfect_graph` to the algorithms index.rst file: `doc/reference/algorithms/index.rst` (the lines are alphabetically ordered)  The CI documentation artifact webpages still don't show the is_perfect_graph docs. and the Ci documentation (under build docs section log) says that `perfect_graph.rst` is not referred to anywhere so it doesn't have any links to it.  Easy fix.
comment
Yes, this should be added to the "Raises" section of the doc_string. We do list `NodeNotFound` and `ValueError` for inputs `source` and `method`. But we should definitely add `NetworkXNoPath`.
comment
This is a consistent issue when people use floating point edge/node attributes in algorithms that look for min/max/equality/etc.  That is, any time we use floating point data for algorithms where floating point errors change the answers.  Our advice is to convert the data to integer type, e.g. int(weight * 1e12), run with that and convert any final result back to floating point.  The algorithms just don't work for floating point.  In your example the approximation using those tolerance levels with those data values does work. But those tolerances won't work with other data values. I don't know of a way that we can make this work for any user input. So, our current policy is to make the user decide how to convert to and from floating point, rather than doing the conversion in a way that we choose. We also considered raising an error for floating point values on attributes, but we decided not to take that approach.   It's also possible that its time to re-consider that discussion/decision. But I'm pretty sure we don't want to use approximate computation with tolerance levels that are chosen arbitrarily when they should be chosen based on the data being used. Thanks for this PR.   Perhaps it would be helpful if you explained what happened that led to this PR. ??
comment
In which place is the normalization description incorrectly distinguishing between directed and undirected graphs?  We are counting undirected paths in a directed way, so we get double counting and the raw counts get divided by 2 to report the unnormalized value.  When we normalize, we officially take this unnormalized value (already divided by 2) and divide it by `n(n-1)/2`.  In actuality we don't divide by 2 and then multiply by it again. But mathematically we are taking a shortcut by not computing the correct unnormalized values if the user wants the normalized values.  So... I think the docs should describe the unnormalized values and the normalization that is used to convert **them** to normalized values. Even if that isn't the way we actually compute it in the code. The normalization factor is different for directed and undirected graphs IIUC. 
comment
I like this change to `_plain_bfs` here. We are making it return the entire component as a set instead of yielding the elements of the component. This might look like its wasteful because we are returning the whole set, but this module doesn't use the individual elements of the set. And we have to create the set anyway within the BFS so we've already taken that memory. Let's use it. :)  I think changing `is_weakly_connected` to using `_plain_bfs` is slightly harder to reason about than using `next(weakly_connected_components(G))`. Is there a performance difference here? What's the reason for doing this?    As for bigger picture questions:  **Code Duplication:** Removing code duplication is a Good Thing so long as it doesn't hurt readability and performance. I would put both readability and performance ahead of reducing code duplication. The reason `_plain_bfs` appears in both of these modules is to 1) be simple/readable and fast and 2) to make it easy to find without a lot of searching (which editors are getting better and better at, so maybe isn't as much of an issue).    For some history, we have BFS rewritten throughout NetworkX, often with slightly variations (there are so many places to stick an extra operation inside BFS). At one point we included an impressive version of BFS that let you add hooks to do extra things at each stage of the traversal. But of course it required checking and running those hooks at each stage too. So it was slower and harder to read. We removed that code partly because BFS written in Python is actually quite easy to grok and tweak. The Python language's readability makes duplicating and tweaking the code better than creating a unified flexible BFS utility function somewhere. At least that was our impression at that time.  So, I'm open to removing duplicated code, but not if it sacrifices performance or readability. The 13-20 lines of duplicated code for BFS is often worth it.   **Walrus Operator** Also, I'd prefer to not have this style of walrus operator (where the variable can instead just be defined in the previous line). There are situations where the walrus operator is needed and helpful. In this case, the walrus is used to reduce the number of lines by one -- while obscuring both the definition of `n` and the check for null graph. I think the advantage of saving a newline char is not worth the disadvantage of the extra parens and the combining of two ideas: assigning to `n` and checking for the null graph.   From my perspective, the reason a walrus operator was **not added to Python** for so many years is because of this temptation to reduce the number of lines by adding parentheses and code that can't be read aloud as almost english. The reason the walrus was eventually added is because it can define variables in places where we can't just define the variable ahead of time.   I'm open to better reasoning for when to use the walrus, but I would like us to have a good reason for using it when we do.
comment
I think the additional information provided by `np.testing` is the reason SciPy gives for continuing to use the `np.testing` functions. But SciPy is fairly slow to adapt to new paradigms. What does NumPy suggest about using `assert np.allclose` vs `np.testing.assert_allclose`? The testing version's docstring says it is identical except for default tolerance levels. But obviously one raises and the other returns True/False.
comment
I agree -- let's treat the doctests as an official part of the test suite. :)   Thanks for looking into this @Peiffap 
comment
To ensure the same results you need to reset the `seed` between runs. Otherwise the state of the random number generator at the start of each run is the state that was left after generating random numbers in the previous run.   The convenient way to do this is by using the `seed` parameter of the function `kernighan_lin_bisection`.  Try something like: ```python for i in range(10):     A0 = [0, 1, 6, 7, 8, 10, 11, 14, 16, 21, 22, 23, 25, 26, 29, 31, 34, 35, 37, 38, 40, 41, 42, 43]     B0 = [2, 3, 4, 5, 9, 12, 13, 15, 17, 18, 19, 20, 24, 27, 28, 30, 32, 33, 36, 39, 44, 45, 46, 47]     part1, part2 = kernighan_lin_bisection(G, partition=(A0, B0), weight='weight', seed=0)     print(part1, part2) ``` (You should also be able to use `random.seed(seed)` within each loop if you prefer)
comment
Thanks very much for reporting this!  The small size of the example gives me hope we can track the behavior.   I get the same result for recursive and non-recursive. There are 6 cycles, two of them with 5 nodes. Can you verify that? That would be a pretty important error if our two functions disagreed!
comment
I'm not sure where you got the mtest code. Networkx has a recursive function `nx.recursive_simple_cycles` and the single-call function `nx.simple_cycles`. Those both give the correct result. Right?
comment
It's true that code which misses some cycles leaves complexity questions moot.  :)   Correctness before complexity. Publishing papers about complexity bounds is typically about getting a proof of the complexity bound. The implementation is rarely proven to be of that complexity bound. Why does a tight complexity bound matter to you?   And// Or      How can we help you in this exploration? 
comment
I don't think any of the examples you show are returning incorrect results by the networkx code, right? And complexity arguments don't guarantee anything about "a reasonable amount of time". They only say how the time increases as the size of the problem increases. The "arbitrary constant" in front of the complexity often determines whether the results come in a reasonable amount of time.   Do you have any tests for the code you wrote, or the code taken from a comment here on github?  It would be good to get tests  for those codes before you spend a lot of time investigating them.  Perhaps the NetworkX tests would be a good place to start?
comment
Is this worrying about optimization of a feature that doesn't take much time anyway?  How much difference does this make?
comment
Where does the cubic complexity come from, and how did we avoid it? 
comment
If we try to move path construction inside the bigger loop, we'd need more to track the sources, check if paths is None and target is None in the big loop. Not clear its simpler -- but if you think it might be go ahead. 
comment
What is the motivation for using the itertools version instead of the recipe? 
comment
Note that the `dispatch` CI error occurs because the numpy version returns G as a numpy scalar.  We didn't catch that before. It should probably use the `.item()` method to remove the resulting graph from its numpy wrapper. 
comment
Be careful -- the rescaling for `edge_betweenness` and `edge_betweenness_subset` are not supposed to be the same. That is, they are set up so `edge_betweenness(G)` agrees with `edge_betweenness_subset(G, G.nodes, G.nodes)` [line 173 in subset module](https://github.com/networkx/networkx/blob/ae93776cf89f2f67c0121995c50f3fffaa5b46b0/networkx/algorithms/centrality/betweenness_subset.py#L173)  The subset version also doesn't have a `k` so that's why the `_rescale_e` function doesn't do anything with `k`.  I suspect it has the same signature as the same named function in `betweenness.py` due either to copy/paste, or intentionally to align the two (in prep for a further extension of the subsets where we somehow use k?  Anyway, whatever the history, we don't need to keep the `k` for`edge_betweenness_subset`. But we need it for `edge_betweenness`.
comment
The keyword `endpoints` refers to whether the endpoints of the paths are counted as being "passed through" for the definition of betweenness. Right now the helper functions don't have default values for endpoints. That helps avoid phrases like `endpoints=endpoints, sampled_nodes=sampled_nodes` when calling. (we usually have the same variable names in the code and the helper).  But if you think it is more clear to have the default in `_rescale` apply to edge_betweenness values (where endpoints will always count) and use non-default for the node cases, that's fine with me.  The important thing is to make sure all the various `_rescale...` functions have the same behavior after your refactoring as they did before.   (Is this really worth it?)  :)
comment
I'd rather stick with `endpoints` than switch to `needs_adjustement` because `needs_adjustment` doesn't tell us what the adjustment is or why it is needed. `endpoints` does...  Even for `edge_betweenness` (because we always include the endpoint edges in our paths).
comment
> The issue is that edge_betweenness_centrality doesn't pass its value of k to _rescale_e. This is (I believe) a bug.  Yes, indeed -- not passing `k` to `_rescale_e` in `edge_betweenness_centrality` is a bug.  This is the point I was missing. I got confused about whether we were talking about the subset version or not.  (another reason to consolidate the rescaling function I guess! :)  > Right now the helper functions don't have default values for endpoints. That helps avoid phrases like endpoints=endpoints, sampled_nodes=sampled_nodes when calling.  What I meant here is that when you *do* want to pass in the non-default value the call looks like: `function(stuff, endpoints=endpoints, sampled_nodes=sampled_nodes)` (since we name the variables the same in the helper function as in the main function) rather than  `function(stuff, endpoints, sampled_nodes)` Anyway, this is in the weeds and not something to mess with -- just do what you think is best for readability.  So, I would agree with you that we should "connect up" the `k` in `_rescale_e` for `edge_betweenness_centrality`. Thanks!!  
comment
Setting k should not make V smaller. It should leave V the same but only sum over k starting nodes s from V.  Use all_pairs shortest path, so sample the starting node and compute to all targets for the sampled starting nodes. At least that is how it works in node_betweenness. 
comment
We worked out the node betweenness centrality scaling in the `_rescale` function. And the number of paths each node could be on depended on whether that node was one of the starting nodes (when endpoints were not counted). But we don't have that problem here. I don't think edges get affected by the starting node. They are just as likely to be counted by any starting node (the counting process doesn't track whether they are connected to a starting node). So, I think we just need to correct for directed/undirected by the 1 or 2. We normalize by the number of edges possible: `k*(n-1)` or (when undirected) `k*(n-1)/2`.  The unnormalized case scale up by `n/k` and then divide by 2 if undirected.  Said another way, I think the "pairs in only one direction" is equally likely for all edges so we don't end up adjusting for it. We scale edge betweenness independent of which nodes were sampled.  For the 3-cycle example with k=2, edge `0-1` could have 1 or 2 paths through it. And we have k*(n-1) = 2*(3-1) = 4 total possible paths counted out of the possible 3*(3-1)=6 paths in the graph. So we should scale up by n/k = 3/2 for the directed unnormalized counts, by n/(2k)=3/4 for the undirected unnormalized case, and divide by k*(n-1)=4 for the normalized case.  This is the kind of discussion we did in May with PR #7908 and we decided to wait until later to figure it out with edge betweenness. So, thanks for working through this with me. Am I missing something (its easy to do that).   My conclusion is that we had it right in the `_rescale_e` function. Let's make this agree with that.
comment
I agree with @Peiffap that option 2 is my preferred choice: a single boolean kwarg to mark both lists as directed (and if `False`, both are treated as undirected).    Part of the text for option 2 wonders how to compare directed edges with undirected edges. I think we should not provide this functionality. A set of directed edges is just not comparable to a set of undirected edges. IMO, make the user convert them as desired and then call the function with either directed or undirected options. 
comment
I think the suggestion was to find a single (could be randomly generated with a "seed" so it is the same every time the test is run) tournament graph long with a pair of nodes that are reachable and another pair of nodes that aren't reachable.  That's the hard work. Then the code just uses that graph and those two pairs of nodes to make sure the computed answer is correct.
comment
If you set the seed for the random graph it will always be the same graph. So find a seed which gives a tournament graph that has at least two unreachable nodes (and find out what nodes they are).  Do all this before creating the test.  Then create a test creates this graph (add a comment that the seed must be left the same or the nodes might not work) and use the two nodes to check for reachability.
comment
This approach seems overly complex to me -- but you haven't provided any code for the DAG method. The current code in this PR is also overly complex. We don't need to compute all the pairs that are in the same scc (or in different sccs). We only need one pair for each case.
comment
I took a look at the PR and I think it is changing the interface in a way I find confusing at first glance. Perhaps I am missing something -- or maybe we just need to make a new function instead of changing an existing function. Making a new function could also make this easier to review -- (it looks like almost every line has changed, so it's hard to tell what this does to t he existing function).  My questions:   - this seems to provide a heuristic method that (sometimes) does not return an equitable coloring. That seems confusing given the name of the function. - this seems to add a few new strategies for coloring -- but the diff makes it look like you are also changing/replacing an existing strategy.  Can you make the additions without removing any existing strategies? Even if the strategy is essentially the same as an existing one, it is probably better to add a new one and leave the old so people counting on it can continue to use it.  minor nit:  can you use `log(m)` instead of `lg m` for the documentation of complexity? push back if that's not ok.
comment
This is another case where we have to be careful with the definitions!  The screenshot from the paper does not tell us whether they are using distance to mean "number of hops" or "sum of weights".  It is pretty clear that the original code was only made to work for unweighted graphs. It uses `len(path..)` to be the distance, and it doesn't include a `weight` keyword when calling `minimum_spanning_tree`. The discussion from the original PR suggests that the `weight` parameter was an after-thought--but I didn't look hard at that. But a look at the papers shows that the algorithms were intended to be used with weighted edges.  So I think we should consider this as a bug in the weighted steiner tree problem. While the unweighted case is presumably OK, the case with weights needs to be considered more carefully.  We will need to make all the distance measurements and all the `minimum_spanning_tree` calls use the edge weights.    **Be careful here though.**   The graphs created (`G1` in mehlhorn and `M` in Kou) have named the edge attributes "weight" and "distance" respectively. So we should **not** simply call the minimum_spanning _tree function with `weight=weight`.  The value of the `weight` keyword has to match the distance measure stored in the graph being acted on.  
comment
+1 for the `_pp` suffix. That's what VF2++ will use: either `vf2pp` or `vf2_pp`
comment
I think we should leave `generate_random_paths` where it is until NXEP3 is figured out. Then there will be many more "path constructors" to put this with.  (The idea is that many of our graph generators are really just constructing edges and/or paths so maybe we should make those available as edge constructors rather than -- or in addition to -- graph constructors.)
comment
The problem with making it a private function is that people won't see it or know they can use it. This is potentially a useful function for sampling paths. We shouldn't hide it.  If we use an underscore and still put it in the `__all__` variable and have it in the docs and in the main namespace, then I'm fine with making it private. But I'd prefer not to hide it from the documentation.  If it was up to me, I would leave it where it is without an underscore. If we get NXEP3 in place for v3.0 then I feel like we can still move the function to a new location within the package for v3.0. The API for the function will be the same. The function will still be the same -- we're only deprecating the location within the package. `nx.generate_random_paths` will work exactly as it does now.  Only users who used the full path to the function will have to change, and the error they get will be clear and easy to fix.  I'm open to adding the `_` if that's what should be done -- I'm just wanting verification that changing the location of a function within the package is a big enough change that the cost of keeping this function private is worth the benefit.
comment
Interesting... Lots of Options...   Maybe @rossbar and I can explore the possibilities and come up with a rationale for when to do which. :}
comment
Welcome back! :) 
comment
Thanks for this question -- I hadn't opened up the file wider to see the similarity between panther and panther++.  MNost of that code is the same -- checking inputs and computing the panther path stats.  To help us avoid divergence in those code-paths does it make sense to have a helper function that computes as much as it can that is common?  Then if someone fixes an issue for e.g. panther, it will get fixed for panther++ too without having to remember to change it in both places. You can make that function "private" by starting the name with an underscore `_` and  dont put it in `__all__` at the top. Also don't worry about docs more than comments to help code-readers understand what is going on.   I think that code change will also help me easily see what is different between similarities and vector similarities. The panther path generation will be the same, and the processing of the paths will hold the key to knowing the difference.  And -- yes -- let's update the panther code to match the panther++.  I'll bring up your questions about backward compatibility at the community meeting. The change will not lead to a hidden (surprise!) change -- it will raise an exception with a hopefully helpful message so it will be clear how to fix their code. That is, of course a change -- but better than a potentially hidden change.
comment
Can you merge main into this branch -- there is a fix @rossbar put in to get rid of some `extra` errors. And merging that into this will tell us if there is anything else affected there. I doubt there is anything after the found error, but it's good to make sure. :)  Thanks!!
comment
What size networks is this method aimed for? The heap approach is not faster in my quick tests until we have about 1 million nodes. I run `panther_similarity` on graphs with 200 nodes and it takes more than 1.5s. 400 nodes gets me to 7.5s. 800 nodes gets me to 40s. So 100,000 nodes could be about 2000 days.    What do you think about removing the heapq stuff and going back to sorting and slice indexing?
comment
I suggest using numpy -- and it looks like it doesn't matter what we use at this part of the algorithm.
comment
Can we stop optimizing the sort -- at least until we make sure that is the slow part of this algorithm?  The code we've got (with the quicksort) will never run for tens of thousands of nodes, let alone a million.   Optimization should first identify where the bottlenecks are.
comment
I think the remaining item for this is the default value of `D` (the number of dimensions to choose).  The paper uses 50, but the SciPy code switches to the more inefficient handling for `D>= 10`. I see that the default here was originally at 50 and now has been set to 15. But that is still in the size where we don't get any benefit from similarity vectors (or am I understanding something incorrectly).
comment
The sklearn version of KDtree has a default dimension of 40.  And there is a pure python version of KDtree that claims to be about 60 lines of code. But it has a CC0 license and I'm not sure if that plays nicely with us, but it means it probably wouldn't be too hard to code it up.   I'm thinking we should probably merge this and figure out how to handle the KDtree issues in follow-up PRs. Thoughts?
comment
The `length` value there is used when scaling the shifts by dividing by  the length. So a too-small length will affect position shifts.  This code is over 17 years old so the discussions don't appear in github. But the [commit that made this change](https://github.com/networkx/networkx/commit/0723fcdd9193ebc9e7ddd61e5b37e4aee0448947) explicitly removed `np.max(length, 0.01)` and replaced it with `np.where(length<0.01,0.1, length)`. It appears in both functions this way. My (admittedly very vague) recollection is that this makes "too short" lengths equal 0.1 instead of "too short", but it also allows users to get slightly below that value if they really want to. Notice that the `distance` values are set to 0.01 when they become too small. So it is only the scaling part that is affected by this.  This leads to my question:  How does this code affect you? Are you wanting it to be set in a particular way? Does this impact a picture you are making?  My inclination is to leave the code as is unless it is making something difficult or suboptimal.  Thanks for bringing it up! I understand that it looks a little strange.
comment
From the [contracted_nodes docs](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.minors.contracted_nodes.html): ``` For non-multigraphs where u and v are adjacent to a third node w,  the edge (v, w) will be contracted into the edge (u, w) with its attributes  stored into a “contraction” attribute. ``` My understanding is that it is not uniformly agreed upon in the literature what to do with edge attributes when contracting nodes/edges. So we have set up the code to provide all the edge attributes in a dict store under the attribute `contracted`. We leave it up to the user to determine the new weight values using the new edge's attributes and the attributes stored in the `contracted` attribute.    Thus, when contracting nodes `u`, `v` with a common neighbor `w`, if you want the resulting edge weights to be the sum of the weights in the original graph of (u, w) and (v, w), then code like the following is needed: ```python H = nx.contracted_nodes(G, 3, 4) for u, v, cdict in H.edges.data("contraction"):     if cdict is not None:         H[u][v]["weight"] += sum(attr['weight'] for attr in cdict.values())  ```  That said, reading the docs makes it quite clear to me that we don't describe what we are doing with the edge attributes very well. The above quoted sentence seems to be all we provide. We also give no examples with edge attributes. And the `contracted_edge` function does not describe attributes at all.  Better docs might help alleviate this issue. 
comment
Would this be solved by a doc_string comment that it really returns a forest rather than a tree? Does it always return a tree when the seq values are positive? Or is it sometimes a forest then too?
comment
Yes -- the bug with the wrong number of nodes is really pretty bad. And the resulting degree sequence must not be correct either -- though I haven't tried it. So, yes, I agree. We should be checking validity. I was thinking that providing a forest that matches the provided degree sequence would be sufficient. But it looks like a better solution is to enforce the validity of the degree sequence.  Thanks @Peiffap!
comment
This looks like the tree module should house this code. We can/should add "see also" links to it from the center-related code docstrings. But let's give it a new section heading in `doc/reference/algorithms/tree.rst` with a name related to distance measures or centers or centroids -- whatever you think fits the best.  The failed tests are not related to your PR. This afternoon they started failing due to a change in the drawing test software. So look to see if any of your stuff is causing an error -- but right now they are failures in the drawing tests. Just in time for your PR. :)  Thanks!
comment
Centrality creates measures of how central each node is. Distance measures are typically trying to find single number measures of networks -- like diameter or center. Centrality tries to find the key/important nodes. Center is trying to find the geographic middle (of a non-positioned object :)
comment
The [oeis (sequence encyclopedia) website](https://oeis.org/) has under sequence A000055 (number of trees with n unlabeled nodes) a collection of pdfs by Peter Steinbach, entitled the [Field Guide to Simple Graphs, Volume 3](https://oeis.org/A000055/a000055.txt).  This collection includes [chapter 2 on Centers and Centroids](https://oeis.org/A000055/a000055_7.pdf) which states that the Centroid is also called the barycenter or center of mass (third paragraph).  So I'm convinced that it is true. A tree centroid is the barycenter of that tree.
comment
No need to merge the conflicts.  I'm on it. (and I'm handling the recent comments too) I knew this would happen when I merged the "tree.center" PR so I'll fix it.
comment
How does this compare to #6896? And what about to #7965?  Also -- after a quick look: - would this be better placed inside the `astar.py` module?  - Can you (after looking at the adastar and isastar PRs) describe how this PR fits with the others? Are we entering a time when the astar variants will grow? Can this one be combined with astar, or is the algorithm sufficiently different to make that difficult? Minor things that could help review: - can you help ensure that comment lines and docstring lines are wrapped to ~80 chars?  - you will need to connect these functions to the documentation. They will need an entry similar to astar in `doc/reference/algorithms/shortest_paths.rst`.
comment
How does this compare to #6896? And what about to #7965?  Also -- after a quick look:  - would this be better placed inside the astar.py module? - Can you (after looking at the adastar and isastar PRs) describe how this PR fits with the others? Are we entering a time when the astar variants will grow? Can this one be combined with astar, or is the algorithm sufficiently different to make that difficult? - The benchmarks can/should go into the asv benchmark files in `benchmarks/benchmarks/algorithms.py` - You will need to connect these functions to the documentation. They will need an entry similar to astar in doc/reference/algorithms/shortest_paths.rst.  
comment
Thanks for the contribution -- we are waiting for some sort of thoughts on an organization or at least a way to handle the large number of variants of the A* algorithm. Perhaps your professor has thought about this. 
comment
Thanks for this contribution!  We have a need for iterated deepening depth first search as well. But I've not run into using it as an approach to the astar shortest path problems. In the doc_strings please include references -- though I expect that the wikipedia article is good -- they usually are.  Thanks for the tests -- I haven't looked through in detail yet, but they are key to having this be reviewed easily.  Some big-picture questions:  - The recursive nature of this is potentially a problem given Python's recursion limits and historical heavy memory use in recursive functions.  I do think I read they were working on that, so it is possible the memory use is not as bad as it used to be -- but I'm sure it is worse than using a queue to store what is needed. And the recursion limit makes it "difficult" to use for large problems. Have you checked into these issues? It sounds like you've done some testing of speed.  And one comment suggests you've thought about memory for other parts of the function. Any ideas about memory usage for the recursion? (Don't do a lot of testing or checking, I'm just wondering what you've already done.)  - It looks like this could fit nicely in the `astar.py` module. That would make connecting it with the docs slightly easier too. And of course, you will need to add docstrings to these functions. The doc_strings for the astar algorithm should make a nice template for those.  Any thoughts about putting it in that module? 
comment
I believe the default maximum recursion depth is 1000. The user can reset that maximum (see [sys.getrecursionlimit](https://docs.python.org/library/sys.html#sys.getrecursionlimit)). That might give an idea of how big a graph you would need to run into the default limit. So maybe `G=nx.path_graph(1050)` would lead to an error. But I may have missed a shortcut or read the code incorrectly.  I wonder why the recursive one took half the time? I am used to the non-recursive one taking less time. It doesn't have to create the new function object each iteration. But I vaguely recall that Python was going to work on reducing that bottleneck.  So my info may be out of date -- or maybe its a matter of improving a bottleneck in the queue-based code. Hard to tell.  As for other astar variants -- we should indeed take a look at what variants are out there. And we should should at least find a nice unified interface for astar adastar and idastar.  I don't think that has to happen for this PR. But it's the right path forward.
comment
Thank you for reporting about the class project. I figured that must be what is going on.   Could you ask your professor to consider combining these approaches into a unified approach to astar variants? Or maybe have them open an Issue to coordinate the discussion of astar algorithms in NetworkX. Thanks to the professor and to you!
comment
IIUC the edit distance counts removal of a node as distance 1. Removal of the node gets edge removal for free. Said another way, the node does not need to be an isolated node in order to be removed.  So removing a node and its self-loops is a single step and thus distance 1. 
comment
There is definitely something strange going on here. I prefaced my previous comment with IIUC. It seems I don't understand correctly. :/   It looks like even when nodes are deleted, the connected edges need to be deleted too. That is, the edit distance includes both.   I found a comment in the match_edges helper function inside `similarity.py` that says: ```         # only attempt to match edges after one node match has been made         # this will stop self-edges on the first node being automatically deleted         # even when a substitution is the better option ``` But that doesn't say when self-loops **are** deleted relative to when nodes are deleted.   Maybe @sheldonkhall or @aparamon have a useful perspective. I also notice that there are packages specifically for graph edit distance named GED4py and GMatch4py that might be helpful in tracking this down.  Thanks @marcosbodio for the report of this issue!
comment
I agree that `swap_d` can/should be removed without deprecation. It is not a public function, and is not discussed in relation to any other threshold graph manipulation functions.  Thanks @rossbar !
comment
You should use the pre-commit hooks stored in the repo (as described in the [contributor guide](https://networkx.org/documentation/stable/developer/contribute.html#development-workflow)).  The single-tick vs double-tick is subtle in that code should use double-ticks while symbols from code that are identifiable as parameters or functions should use single-tick to get the link in the documentation to that parameter or function. That's hard for the linters to distinguish. As for sections, I don't think linting can tell  you to change a "related methods" section to a "See Also" section...  But perhaps I'm just not aware of the latest linting???
comment
Yes -- Thanks for the example of it not working reliably. This function should treat directed graphs in the same way that the `find_cliques` function treats them. I think simply adding the decorator and a line in the doc_string is sufficient. Maybe a test? (for both if there isn't one for find_cliques).
comment
See also:  #7972  
comment
One reasonably simple gallery example to work from would be [examples/algorithms/plot_davis_club.py](https://github.com/networkx/networkx/blob/main/examples/algorithms/plot_davis_club.py)
comment
The [article referenced by the minimum_node_cut docs](https://www.cse.msu.edu/~cse835/Papers/Graph_connectivity_revised.pdf) defines a minimum node cut as a node set of least cardinality such that removing those nodes leaves the graph disconnected or trivial (top of page 9).  The [wikipedia page on minimum cuts](https://en.wikipedia.org/wiki/Minimum_cut) discusses edge cuts, and defines a cut as a partition of the nodes into two disjoint subsets, and a minimum cut as such a partition that is minimal "in some metric".  I was not able to come to a definition of a minimum node cut from this page.  Following the definition in the article referenced, a minimum node cut for a complete graph would be all the nodes of the graph. That seems to be a reasonable approach. Raising on a complete graph (removing the "or trivial" from the definition in the paper) is another approach, but I like being consistent with the definition in the paper referred to in the docs. Thoughts?
comment
> I agree, however, that paragraph also states that the node cut is a set S, such that S ⊂ V, so the set of all nodes would not be a valid node cut.   Be careful with the subset symbol. The phrase S ⊂ V means that S is a subset of V not necessarily a proper subset of V. S could equal V and still have S ⊂ V.  So the set of all nodes would be a valid node cut (usually not minimal).   @Peiffap  Thank you! > I'm still in favor of not raising for complete graphs, but if we want to go with the definition Dan is referring to, we should be mindful that the trivial graph is not the null graph but the graph with one node and no edges... Which matches the current behavior of the function if I understand correctly. It does leave the question of how to handle self-loops on the remaining node, e.g. what should we return on a graph where each node has edges to all other nodes and a self-loop.  The trivial graph has one node in it. So for a complete graph, any set holding all but one node is a node cut!  I was incorrect to say that the set of all nodes would be the node cut.   As for self-loops, the current behavior seems fine to me. It does have a corner-case where the complete graph that also has self-loops for every node will lead to returning the set of all nodes. I'd be in favor of just leaving it as is. I'm not sure we even need to document it, but we could.    
comment
I think the `dominator` connection is a good one. The additional step that the dominator literature tackes with the DAG is that it adds an "entry node" which connects to all the nodes with zero in-degree. That is a nice way to finesse the tricky business of having multiple sources in your DAG.   So u dominates v if all paths to v require passing through u.  In the words of your nice watershed/dam example, there is no other way for "water" to get to node v once u is blocked.  I think this might mean that the nodes that `u` dominates are what you are looking for. Can you check that this is correct?
comment
I get the option of picking a label when I create a PR, and I can add them later also. But I suspect I have a permission setting that most people don't have. Anyway, I added the documentation label and it seems to be passing that test now.  I checked -- and yes, you have to have "triage" permissions to add/change labels on a PR.
comment
The image looks qualitatively similar with a dataset including the first 5000 lines of the current dataset.  You can try it out by changing the graph creation line `from_pandas_edgelist` to have `facebook[:5000]` as it's input.  If you think that is an OK figure, then we could shrink the filesize from ~218K to ~12K.
comment
The layout is only computed once. So that's not the time bottleneck. The animation is almost certainly what is taking the time... but I suppose we could profile it (compare to a static version for example, and to a 2 frame animation?) to make sure.
comment
I think it would be better to switch to static images for the docs. We can have animations for a specific example that shows how to do this for any others that would look nice with animation. But it does seem like the examples are not the right place for animations. Both build time and build storage requirements are pretty large.  Thanks for reporting your findings. We have talked about the long term implications of animations in examples during our community meetings. And I hope I am representing that discussion here. (If anyone has more to add please join in.) Does this seem reasonable to you @Florian2Richter ?
comment
I think the right place would be the `nx-guides` collection of notebooks that use larger datasets and are encouraged to explore topics in more depth. That allows us to have large-scale docs while keeping the shipped NetworkX relatively thin.  Take a look as let us know what do you think?
comment
I believe the bidirectional case is `DG = nx.star_graph(5).todirected()`. Outward could be done using `DG = nx.DiGraph(nx.star_graph(5).edges)` Inward could be `DG = nx.DiGraph(nx.star_graph(5).edges).reverse()` or `DG = nx.DiGraph([(v, u) for u, v in nx.star_graph(5).edges])`    This type of thing is a motivating example for the NXEP we explored a while back involving making versions of the generators that return the edges and nodes rather than the full graph. But docstring examples might do the trick here. I don't think there is much difference in performance either but I haven't checked.
comment
No -- finding **all targets** in a list can/should be done manually by the user. At worst you can find them al one taregt at a time. Or you could use single_source_djikstra to find all nodes from one node and then eliminate the targets found doing that, wash and repeat until list of targets is small enough and then do them one at a time.  You can see why its a tricky interface -- lots of different ways to do it depending on the particular situation.
comment
Thanks for this!  It's not in a format ready for networkx yet, but there is some good code there. Let's talk about the big picture ideas first:  Did you look at the flow computations already in NetworkX?  Why implement another one? do any of them use a similar method that this is an improvement on?  At least [some people -- SO post](https://stackoverflow.com/questions/56042395/what-is-the-difference-between-simplex-method-and-network-simplex) feel that a recent good simplex method often performs better for the network problem than a network simplex method (because simplex methods have improved so much). Do you have thoughts on this?  Does this method use a simplex method tool? I see `gurobi.log` as one of the files (which shouldn't be merged into the library) and I know they have a simplex method, but basically nothing else about that tool.   Does this PR use any libraries that networkx doesn't already use?  Thanks!
comment
I don't see changes on the scale of nanoseconds. Am I looking in the wrong place (the shortest path benchmarking PR  #8059 ) It seems like some cases are ms and 30% slower.  I might be able to wrap my filter around that and swallow for the impressive gains elsewhere, but I want to make sure I am understanding the information we've already got in these two PRs. 
comment
Is there any chance that your environment has a networkx backend installed? Might it be a copilot environment or something that would add nx-cugraph? Any backend package might have an incorrect name for the nx_loopback feature.  Still -- I don't know how that would affect the import of NetworkX.  But I thought I would ask.  I have also found some discussion of changes from nx-loopback to nx_loopback within networkx.  https://github.com/networkx/networkx/pull/7494#issuecomment-2160769168 But those changes were all made before NX 3.4.   
comment
To skip them, would we use something like "any chars not in 'a-zA-Z_'"? And if there are other chars, what do we do? Ignore them? Or raise with a message telling info about the trouble-causing backend?
comment
The discussion in #6873 [suggested starting from](https://github.com/networkx/networkx/issues/6873#issuecomment-1880386482) `complete_to_chordal_graph` where this function could just check `is_chordal(G)` and if so, return the result of the `complete_to_chordal_graph` function.  There are 3 other functions there that all stem from a single function's output.  Is there a reason for this approach over that approach? Can we extend this to get to the other functionality, or is this a different approach?
comment
Looks like this needs: - using "import or skip" to skip tests when numpy is not available:  `np = pytest.importorskip("numpy")` inside each test function/class/module that you want to skip. I think here it might be each test function. - add the module to the "needs_numpy" list in `networkx/conftest.py`.  And we should figure out how to handle backward compatibility for people who don't have numpy. It looks like this affects both `is_reachable` and `is_strongly_connected`.  I don't think that has to be in this PR, but it might be easy/nice to have it here if we decide we need it.  @rossbar Do you think we need a fallback option in the code to allow it to work for folks that don't have numpy available? I'm +0 for providing that. I don't think `is_reachable` is used much. But I suspect `is_strongly_connected` could be.
comment
To speed up the `__getitem__` parts you should replace `G.pred[u]` with `G._pred[u]` and `G[u]` with `G._adj[u]`. That bypasses the read-only `Adjacency` class `G.pred` which is slower than the read-write dict class `G._pred`. :)    These are definitely into the weeds of optimization -- but that is what profiling is all about.
comment
The default `pos` value is provided by `spring_layout`. It is highly unlikely that the layout you are seeing is created by `spring_layout`.  Are you sure you don't have `pos` created in some other way (maybe for something else?) that is messing up the drawing code you are using?  Also, both `nx.draw` and `nx.draw_networkx` treat `pos` the same. In fact `nx.draw` calls `nx.draw_networkx`.
comment
No -- `nx.draw` just calls `pos = nx.spring_layout(G)`. But there is a random part of that function -- you should get a different layout each time you call `spring_layout` unless you add a `seed` keyword arg. So, by using the default `pos` values in `nx.draw` you should also get a different layout each time you run it.  Can you include a small example code that recreates your problem so I can see what might be going on?
comment
Are you sure we need to find the gcd between two SSCs? That implies the gcd between periods of two SCCs is the period of the combined graph. But I think that is incorrect. I argue here that an aperiodic graph must have all nodes being aperiodic. Said another way, a graph has period 1 (is aperiodic) when no recurrent nodes have period greater than 1.  I believe the original algorithm was correct. We have to have g==1 for every SCC to have an aperiodic graph. **TL;DR**  I am having a hard time finding a reference that considers aperiodicity with multiple SCCs. The period of individual states/nodes seems to agree across sources. But most sources only consider a graph with a single SCC. And it is quite rare for the sources to define the period of a graph when there are multiple SCCs.    The most clearly stated definition I have seen [comes from lectures at Brown University](https://cs.brown.edu/courses/cs145/lectures_temp/lec18_markovStationary.pdf) and describes it for markov chains as: **A Markov Chain in which no recurrent states are periodic is called aperiodic.**  A state is periodic when its period is > 1. A state is aperiodic when its period is 1. The whole chain is aperiodic only when all of its states have period 1.   Note that recurrent states translate to graphs as nodes that lie on cycles -- that is the nodes are in an SCC. Translating the markov chain term **aperiodic** to transition graphs, we get **a Graph is aperiodic when the period is 1 for each node that lies on a cycle**. Let me stress that **all nodes** must be aperiodic (have period 1) for a graph to be aperiodic. If any nodes have period > 1 then the graph is not aperiodic.  Viewing the term "aperiodic" from an ergodic theory perspective, we want aperiodic and irreducible to imply ergodic (well mixed -- single long term probability distribution, etc). The irreducible condition rules out more than one SCC. That is probably why almost nobody seems to care about aperiodic with multiple SCCs. But looking at it from ergodic theory perspective, aperiodic means we don't have structures that bind the probability into non-constant patterns over time. That needs to be true everywhere -- not anywhere. So one SCC that is aperiodic and another that is periodic will not make the graph aperiodic. There is still a place (the other SCC) where probability has patterns over time.   **Example to show why we need gcd between cycle-lengths within a SCC:** We need the gcd of the periods of two cycles within an SCC to find the period of the nodes in that SCC. The nodes in the SCC interact and add their cycle lengths, so the resulting times when a node can return to itself are multiples of the gcd. For example, a 8-cycle and a 10-cycle can allow a node to return to itself after 8, 10, 16, 18, 20, 24 and any even number above 24. So the even numbers includes all times at which the states can return to themselves. Thus those nodes have period 2. (See [the only reference from the periodic graphs wikipedia article that discusses multiple SCCs](http://cecas.clemson.edu/~shierd/Shier/markov.pdf) though it is very brief.) That is because the cycles are interacting.   But cycles in different SCCs don't interact. We can't travel along one cycle and then the other cycle to find the possible return times. The possible return times for nodes in the two SCCs are just different. If you want to try to define return times by looking at two particles starting at two nodes and see when they return (which is what ergodic theory would be interested in I think) then it seems that the period of a graph with two SCCs of relatively prime periods would be the product of the periods. That is how long it would take for the "system" (of two particles) to return to its original state.  So, a graph with two SCCs, one with period 2 and the other with period 3, have some states/nodes with period 2 and also other states/nodes with period 3. So the graph is not aperiodic!! It has some periodic recurrent nodes. Not all nodes are aperiodic.  To show a graph is aperiodic, we need g==1 for every SCC. To show a graph is not aperiodic we only need one SCC to have g>1.  `g` should be recalculated for each SCC independent of the other SCCs (because the cycles can't add to each other across SCCs). And we should test each for g==1. They all need to be g==1 to have an aperiodic graph.   Can you see any holes in this story? Can you justify another treatment of "aperiodic" when there are multiple SCCs? What am I missing?
comment
I'm looking at the test of multiple SCCs from above. I think the two `add_path` calls should be `add_cycle` to make the example work -- right? Then we have a 2-cycle and a 3-cycle with an edge allowing probability from the 3-cycle to leak over into the 2-cycle and never come back.  So the long term behavior of the Markov Chain is to end up on the 2-cycle with probability sloshing back and forth between nodes 1 and 2. That is dependent on initial conditions, and it not a stationary probability distribution... Thus not ergodic. Importantly, the existence of two nodes with period 2 is sufficient to show that the whole graph is not aperiodic. The random walk will have a time dependent pattern that in general depends on the initial conditions.  Hmmmm... this could get even messier! In some sense the period of the other SCC doesn't matter much in the long run.  So if the first SCC was aperiodic and the second was period 3, and the direction of flow is into the aperiodic SCC, then should we call the whole graph aperiodic because with probability 1 we end up in the aperiodic SCC? Now we've got aperiodic depending on flow other than cycles. And maybe the whole idea of aperiodic is not easily defined with multiple SCCs.  Perhaps we should be raising an exception for graphs that have multiple SCCs.   Thoughts?
comment
Once again, this is too long...   In summary:  I think we need to enforce (through checking or through documentation) that this function **only** be run on a strongly connected digraph. That is the only case we have a clear definition of `aperiodic graph`. And that is the only case where any of the definitions I have found have a performant algorithm.   **TL;DR**  **Definitions**: I have found three definitions of an "aperiodic graph" used in the literature. They agree on the important case that is relevant to theorems involving ergodicity of the Markov processes described by these graphs. So there doesn't seem to be one that is "better" than the other.  1) An aperiodic graph has the gcd of all cycle-lengths equal to 1. 2) An aperiodic graph has every node being an aperiodic node. A node is aperiodic when the gcd of all cycles-lengths through that node is 1. 3) A digraph is aperiodic if and only if a globally reachable node is aperiodic. A globally reachable node `n` has a directed path from each node to `n`. The node's period `d` is the gcd of all cycle lengths from `n` to `n`. The node `n` is d-periodic if `d>1` and aperiodic if `d=1`.   The definitions differ for the example described above with cycles (1, 2, 3, 4) (1, 2, 4) and (5, 6, 7, 8) and edge (4, 5). Nodes 5, 6, 7, & 8 have period 4, so are not aperiodic. Thus definitions 2) and 3) say the graph is not aperiodic, while definition 1) says the graph is aperiodic since `gcd(3, 4, 4)=1`. Definition 3) means disconnected graphs cannot be aperiodic because they have no globally reachable nodes. While 2) allows disconnected graphs so long as each connected part has every node aperiodic. There are many other examples where the definitions differ.  The Javis and Shier paper defines when a **state** is aperiodic. It does not talk about when a **graph** is aperiodic (other than a strongly connected digraph).   It does give (in Theorem 3) a statement that when a single strongly connected component with no outgoing edges has all nodes aperiodic then the usual nice properties for ergodic cases holds. It then immediately states that from that point on it will only consider the "important" case of a graph that is a strongly connected digraph -- and that such a graph is called *aperiodic* if the gcd of all cycles is 1.  The care with which the paper avoids defining "aperiodic graph" for anything other than an irreducible system is subtle but telling.  After much search for a "best" definition, I noticed that the definitions agree on every statement that requires both aperiodic and irreducible (or not decomposable, or strongly connected, or having a single communicating class). Until people start finding a use for looking at periodicity in a larger sense, I don't think there will be a definitive answer as to which definition is more useful. And, importantly, I should stop looking for a way to restrict the definitions to a single closed strongly connected component. The definition are all fine and agree in the case of a single closed strongly connected component.  **Algorithms**: A closer look at the whole of the paper by Jarvis and Shier ([linked here and above](http://cecas.clemson.edu/~shierd/Shier/markov.pdf) and cited both in our current docs as well as the wikipedia article) describes two algorithms. The first uses a depth-first-search in section 17.2.3 to order the nodes and (with other stored information from the DFS that is not described in this paper) can be used to identify the strongly connected components. This is essentially Tarjan's algorithm for finding strongly connected components (and we get them in their reverse topological sort order as a biproduct). The second uses a breadth-first-search and is intended to find the period of a strongly connected digraph. The paper states parenthetically that the graph must be strongly connected:  ```A breadth-first search of the (strongly connected) digraph G visits all nodes of G in order of non-decreasing level.```  The paper then proceeds to describe how to find `d` during this BFS process.  The breadth-first-search approach presented in 17.3.2 is implemented in our code, though our code adds a check at the end and recursively calls if a condition is met. The result is that our code provides answers that contradict each definition of `aperiodic graph` given above. It doesn't make sense -- and that is because it was written and presented as only working for strongly connected digraphs.  Our code should ensure that we are working with a strongly connected digraph G. It currently does not -- leading to surprising results and thus, this discussion.  Also note that the recursive part of our function is not part of the BFS algorithm from this article. (It **is** part of the description for the DFS algorithm though.) The recursive part will never be called with a strongly connected digraph. The authors must have added it as a way to extend the algorithm to more than just strongly connected digraphs. It was merged before we had pull requests and discussions, so there is no record. and there are no references. It seems to me to be incorrect.  Implementing algorithms for definitions 2 or 3 will be non-performant because they will require finding which cycles go through each node. We would need to find all cycles. And that approach will perform much worse on the important case of a strongly connected digraph than our current algorithm with the recursive part removed.   **Other sources**: The definition from the Brown Univ lecture notes (linked above) is more generous in that it defines an aperiodic graph as a graph where every state is an aperiodic state. That can apply to a much larger class of digraphs. And the notes build on this definition to get theorems about ergodic features such as unique steady state probability distributions, etc. But the definition is only used in tandem with a restriction of a graph with a single connected component (where the two definitions agree).   A [thesis from University of Toronto by Zhiyun Lin](https://www.researchgate.net/publication/267701724_Coupled_Dynamic_Systems_From_Structure_Towards_Stability_And_Stabilizability) defines a stochastic, indecomposable and aperiodic (SIA) square matrix to be one that has the nice ergodic properties. It then provides and proves its Theroem 2.8: ```A stochastic matrix E is SIA if and only if its graph G(E) has a globally reachable node which is aperiodic.```  Notice that it doesn't call the graph aperiodic. It defines aperiodic nodes and uses them in this theorem. (The thesis also defines aperiodic graphs in the same way our docs do, but does not use that definition to prove anything -- so it is not clear that they considered it very thoughtfully.)  The key wording that has changed here is that we need a **globally reachable node** to be aperiodic and we get SIA.  That along with our Brown lecture notes leads me to believe that the correct way to extend our current function beyond strongly connected digraphs is to use the definition:   But we don't have an algorithm to find whether such a condition holds!!  We could make one up like:  - do an all-pairs-shortest-path `sp` on the reverse of the graph. Then check  `sp[node]==len(G)` for each node and keep the nodes for which is it true. These are globally reachable nodes. - find the period for each of these nodes by performing BFS starting from that node. Any back edge coming back to the root node forms a cycle, and we track the gcd of all those cycle lengths.  If the gcd is 1 for any globally reachable node, the graph is aperiodic.  But that algorithm is not performant. Applied to a strongly connected digraph it would require n BFS traversals instead of the 1 BFS traversal used in the current algorithm.  And we have no reason to look for this definition of aperiodic 
comment
Niiice...  :) What is the source for your theorem? It makes a lot of sense!  I had forgotten we have an `attracting_components` function. That helps a lot because we can just start the current algorithm with an arbitrary node from the attracting component. No need to worry about leaving the component.  And I think your idea of approaching multiple attracting components could work too.  But definition 2) (Brown Univ lect notes) wants **every** node to be aperiodic, while definition 3) (Univ Toronto thesis) only needs any globally reachable node to be aperiodic.    To understand the difference, consider a graph consisting of two disconnected strongly connected components. One with period 1 and the other with period 2. So the Markov chain could end up being period 2. Or it could end up period 1 (aperiodic). Which of these happens depends on initial conditions. The ergodic results of a unique steady state probability distribution do not hold. (And they wouldn't even hold if both components were aperiodic (period 1)). The ergodic results must have a single attracting component. So what about the definition of aperiodic?  So, what should our homemade definition of an aperiodic graph give for this case? The graph as a whole has periodic nodes of period 2, so it should not be aperiodic, but it has some globally reachable nodes that are aperiodic so it should be aperiodic.  
comment
This sounds like a good approach to me -- and we could include an example in the doc-string of how to use attracting components for the simple two SCC/one attracting SCC example.  
comment
How can a DAG have periodicity? Acyclic is in the name. :)  Ahh -- I see...  you are saying we should return `False` if it is acyclic.   But be careful with using the definition in wikipedia and in our docs. They are both mistaken in the same way -- . They are coming from the same article and that article includes quite clearly that this is defined for strongly connected graphs.  So the definition doesn't apply to acyclic graphs even if both wikipedia and our docs give that as an example. We can fix both our docs and the wikipedia article.  Can you find anywhere else that aperiodic is discussed alongside acyclic graphs?
comment
The test failure for Label is fixed when a maintainer assigns a label to the PR (click on the "view details" link of the failing test to see a better message than fits in the title of the test). I've assigned one now so that's why it passes.  Using the phrase "fixes #PR_no" or "closes #PR_no" (and other options) instead of "This is for issue #PR_no" will [automatically connect the PR with the issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) that it is fixing. When the PR is merged, it triggers the closing of the issue.   Thanks for your first submission!  I think this PR is based on the mistaken assumption in #7786 that we can use `>=` instead of `==` in the matching function. I guess we should close that PR. When we switch to using `>=` for matching, it allows monomorphism instead of isomorphism. But that breaks using that matching function for the isomorphism problem. This is a problem with the API for isomorphism/monomorphism. We can't use the same GraphMatcher class with the same edge_match argument for those two problem types.  The tests here check that this edge_match function works for these examples in both the isomorphism and monomorphism problems. But I believe there must be other examples for which the isomorphism problem will break with this PR.  Using `>=` will allow unequal edges to match -- which we don't want to happen for isomorphisms.  The solution to #7758 is to create your own matching function (as you have done for the monomorphism case you are interested in, where the proposed work-around in #7758 fails). I hope to refactor the isomorphism API to make it more obvious that changing from isomorphism to monomorphism will require changing the matching functions.  Do you agree that this proposed matching function will not work for some cases of the isomorphism problem?
comment
If you try to store all the simple cycles in a e.g. list, you will run out of memory. Also, the code you posted above can't be the code you actually use because it only creates the generator -- and doesn't iterate over it -- so no work is actually done.  Are you using it with code that will store all the paths?
comment
Do you mean this function is still working?   :)  The issue is that the function never stopped. So I guessit is still working. :)  @antoscha, If you want us to work on this, then you will need to provide more information. The current OP code runs instantly because it only creates the iterator. It doesn't iterate over the iterator. And the comments suggest that  1) the number of cycles can grow very rapidly as some edges are added (and with other edges they grow very slowly).  2) this explosion in number of cycles could fill memory if the iterator output is stored -- that could explain using up memory.  What do you have that is new information about this fuction?
comment
What part of this issue are you observing?  If you want us to work on this, then you will need to provide more information. The current OP code runs instantly because it only creates the iterator. It doesn't iterate over the iterator. And the comments suggest that  - the number of cycles can grow very rapidly as some edges are added (and with other edges they grow very slowly). - this explosion in number of cycles could fill memory if the iterator output is stored -- that could explain using up memory.  The example above from @antoscha uses a function that has `return list(cycles)`.  That is likely to cause memory problems because it is trying to store all the cycles in memory.  If your problem is a memory issue, try running `sum(1 for _ in nx.simple_cycles(G)` to see if that finishes... Or you could write the results to a file as they are created -- anything to keep them all from being stored in memory.  What do you have that is new information about this function?
comment
This has had many eyes and a fair amount of time -- discussion has all been in agreement and 2 approvals (and the author makes a 2nd core approval). I'm merging it.  Thanks everyone!
comment
We need to test equality when we are doing isomorphism checks. The `<=` or subset check is the test needed for monomorphism checks. I think this is an API wart that needs to be figured out before we can fix it.
comment
I'm going to close this PR so it doesn't take more review cycles. The problem with this approach is that it changes the NX provided edge matching functions so they work for the monomorphism problem. But this breaks them for the isomorphism problem.   The isomorphism and monomorphism problems require different edge matching criteria/functions. I think the api itself needs improvement to make this more clear.
comment
Thanks for this report!  This is a blast from the past.  I suspect it is the change from `subgraph` constructing a graph to the "new" view based system. In v2.0+, subgraphs are views rather than newly constructed graphs. This saves memory, but is slower if you examine the edges of the subgraph more than once. In those cases, it is faster to make a copy of the subgraph.  Can you try changing the subgraph creation to `subgraph = graph.subgraph(component).copy()` and report back whether this fixes the slowdown? Thanks!
comment
I think the nice speed of the 1.11 subgraph method is partly due to avoiding the base class building tools. For example, instead of `G.add_edges_from`, it builds/updates the adjacency structure manually. That avoids all the arg unpacking and checks because we know this is coming from an existing graph class object. So we know what the data looks like.  Here is a version of the old 1.11 `subgraph` method -- updated to be used with the modern graph classes. ```python def subgraph(self, nbunch):     bunch =self.nbunch_iter(nbunch)     # create new graph and copy subgraph into it     H = self.__class__()     # copy node and attribute dictionaries     for n in bunch:         H._node[n]=self.node[n]     # namespace shortcuts for speed     H_adj=H._adj     self_adj=self._adj     # add nodes and edges (undirected method)     for n in H._node:         Hnbrs=H.adjlist_dict_factory()         H_adj[n]=Hnbrs         for nbr,d in self_adj[n].items():             if nbr in H_adj:                 # add both representations of edge: n-nbr and nbr-n                 Hnbrs[nbr]=d                 H_adj[nbr][n]=d     H.graph=self.graph     return H ```  I'm going to close this issue. But if anyone has suggestions or comments please post here and we'll reopen it.
comment
I think this is different than the other BC function.  First and most importantly, this doesn't have the `endpoints` kwarg. Next, it was already set up to handle the number of sample pairs `k`. And finally, there wasn't any dependence on kmax or epsilon before. So there shouldn't be any normalization dependence on k now.
comment
Tha label has to be added manually by a core dev. (the name says "Labels / reviewer label?" which is not clear partly cuz it has a length restriction. But I think/hope that clicking on "view details" give a better message. :)  Anyway, this just means one of us should look at your PR. :)
comment
I get: ``` print(nx.non_randomness(g))                                                                                                         (-1.847759065022571, -5.842436964513824) ```
comment
The article [referenced in the docstring](https://epubs.siam.org/doi/10.1137/1.9781611972795.61) discusses this quantity in Theorem 1 where they show that a quantity follows a normal distribution in the limit k << n and n large.  So clearly, this case doesn't follow that.  Perhaps we should include a maximum limit on p (not 1, or we divide by 0 and have the same problem).  Or, we could raise an exception when the value is bigger than 1.   What makes the most sense here?   What does it mean to have an infinite nonrandomness?  The different results for different orders of the same graph must be due to a community algorithm that depends on ordering of edges/nodes. I haven't looked at this in this case yet. But many of those methods do depend on ordering. So I'm not so surprised. 
comment
It looks like spurious node labels `\\N` are being added during the NX<->agraph roundtrip now.  The `pygraphviz` library adds those because they used to be needed for GraphViz. I suspect that library should update that bug-workaround -- and it might need to be graphviz version dependent.   I added print statements to the networkx test and it produces the following upon failure.   ``` G.nodes=NodeView(()) H.nodes=NodeView(()) HH.nodes=NodeView(()) G.edges=EdgeView([]) H.edges=EdgeView([]) HH.edges=EdgeView([]) G.graph={'graph': {}, 'node': {}, 'edge': {}} H.graph={'graph': {}, 'node': {'label': '\\N'}, 'edge': {}} HH.graph={'graph': {}, 'node': {'label': '\\N'}, 'edge': {}} ```   A workaround for us which should work for all graphviz and pygraphviz versions is to check for either an empty default dict, or a dict with `'node': {'label': '\\N'}`. If it is either, then don't update the graph dict we are creating. Otherwise, do update the graph dict we are creating.  Of course, if someone has intentionally set the default value of graph attribute 'node' to be that dict, they might have troubles when they are using a version of graphviz that doesn't assign those default values in the dict. But that is **very** unlikely.  I'll make a branch with the fix -- might be easier than understanding my explanation here. :) 
comment
Is this a problem with pydot or with network or with graphviz? Do the long names work with the pygraphviz interface functions?
comment
I agree with @amcandio. The problem is with how you are setting your seeds.  Both the test in the original post and the test you wrote for `PythonRandomInterface` set the seed only once at the beginning. So it **should** give different results on each iteration of the loop. To get the same results you have to reset the seed before the second call.  Just FYI, this code has the same behavior with the latest version of NetworkX. It gives different results (because the state of the random number generate has changed).  You need to add a line: ```python     random_state = np.random.RandomState(42)     init_group = partition_graph(graph, seed=random_state)      for it in range(100):         random_state = np.random.RandomState(42)  #######  <---- add this inside the loop to reset the rng state.          group = partition_graph(graph, seed=random_state)          if group != init_group:             print(f'Broken in iter: {it}')                          print('\nInit. group:')             print(init_group)                          print('\nGroup:')             print(group)             break ```
comment
Is there a reason you feel a "guarantee" is helpful?  If CPython changed the ordered nature of dicts, we would likely go back to needing OrderedGraph. And Pypy and other versions of Python don't have that ordered nature to them.  I am certainly not interested in promising to rewrite our code to ensure that the base classes will always retain this ordered property. We are just taking advantage of this trait of CPython. We aren't promising it will be that way forever.  Next -- the order of edges is NOT the order in which they are added. The order of edges comes from the adjacency structure, which loops through each node (in node order) reporting all edges for that node in the order that the node's edges were added.  
comment
Is there a reason to remove the existing license line in the "classifiers" section? If any tools currently look for that it would disappear and they wouldn't find it.   I don't know what tools do what with the license info, so this is probably a newbie question.
comment
Lots of approvals here --- @MridulS can you self-merge when its the way you want it? Thanks!
comment
If you have cycles to spend on reviewing old existing PRs that would be helpful. :)
comment
I'm going to close this PR, though it might be of interest to those who want to see how CI could work with uv. We do not plan to make a shift from the current pip system.
comment
Thanks @rossbar!!
comment
Yeah -- this is almost exactly what our `OrderedGraph` suite of subclasses did (using the builtin [collections.OrderedDict](https://docs.python.org/3/library/collections.html#collections.OrderedDict)).  We removed that after dicts became ordered.   I think you are envisioning that users could provide an ordering function. In my experience, subclassing `dict` has surprisingly large performance implications. Overriding the dictview properties like `keys` and `items` is also subtle because those views have many traits that the results of `sorted` don't have. For example, `keys` has the set operations. And lookups for keys are O(1) while the lists from `sorted` are O(n).  We got rid of OrderedGraph to simplify the interface. There will need to be a good use case to add it back as an option -- and to make it the default will need some exploration of performance impacts at the least.  But just from a simplicity perspective, `dict`s are widely understood and flexible. They help make NetworkX understandable.
comment
Another feature of benchmark time that might be controversial  idk... It might be more accurate timing to take the minimum time of many runs instead of the average time. To the extent that other system activity slow down a run, using the minimum could remove the effects of system activity on the benchmarks. :}/
comment
I agree with @rossbar  Let's not spend review brain cycles on fixing pydot from within networkx. Of course you could make a pr to fix pydot itself.   I'm going to close this -- but the conversation can continue either here or in the Issue.
comment
This sounds promising to me!  Thanks @Schefflera-Arboricola --   I think you are right that this module is so large we might find it easier to read and review if it was refactored. A new directory alongside `utils` makes sense to me. We could move the tests (which I think are still in `classes`) into that directory too.   Looking forward to input from @eriknw on this too.  What would be a good way to separate it into modules? 
comment
I have added the enhancement label -- The labels are assigned by the reviewer rather than the submitter.  Thanks for this! The choice of modules to add look good. I haven't looked at the content yet -- just wanted to get the label in place so you get the "green checkmarks"! :)
comment
That's a good question.  I think you indicated that the complexity changed since the publication because the complexity of dijkstra changed.  Is that correct?  If so, is NetworkX using the faster version of Dijkstra?  Then -- we have to make sure that we actually implemented the new Kou algorithm as the correct complexity. That is, the publication proved it could have a certain complexity. But did we do that?   Also, O(|S|(|E|+|V|log|V|)) seems to also be O(|S||V|^2)... At least when |E| ~= |V|^2. So what are the constraints on these asymptotic statements? Are we restricting to sparse networks?  OK... my rant is over... go ahead and put in the docstring what the complexity is. :)
comment
Be careful with Fibonacci heaps :) The original paper admits that they are often slower even though they are asymptotically faster. And that has been our experience as well. See a python implementation and [description of the results](https://github.com/danielborowski/fibonacci-heap-python?tab=readme-ov-file#results) Also see this stack overflow [discussion on whether anyone has actually implemented a performant fibonacci heap](https://stackoverflow.com/questions/504823/has-anyone-actually-implemented-a-fibonacci-heap-efficiently).  I'm not saying you shouldn't do this -- but you should not just dive into it without a plan for how to overcome the troubles involved. :)  Theoretical complexity measures are not always indicators of the best way to do something. They are valid in a limit. And we don't live in a limit.   To paraphrase a great economist (Keynes) who was countering works that considered only the long term impacts on the economy, "The long run is a misleading guide to current affairs. In the long run we are all dead". 
comment
Thank you for putting this together @Schwarf   These changes are made to the base class methods. But they should only apply for the PlanarEmbedding subclass. That is, someone who happens to create edge attributes named 'cw' will lose those attributes when they haven't ever used the planarity code. Can you move these changes to methods in the PlanarEmbedding class?
comment
The automated close occurs if you put special text into the PR posting that connects the PR to the issue, or if someone has connected this PR to the issue in the Development section on the right sidebar of this page.  Those are the two ways I know of.  To use text to connect the PR to the issue, put `Fixes #7645` in the original post. I have edited the initial post to include that now.  I think it might also work by putting it into the commit description. And there are other syntaxes like `Closes #7645` that work too.   [See details in the github docs](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue)
comment
How does Leiden know when to stop?  The resolution parameter doesn't have anything to do with stopping or iteration. It changes the function you are optimizing.  Isn't Leiden an iterative optimization solver? 
comment
Thanks for your contribution!
comment
That suggestion is to change one line on the file..... and that line is only one part of a fairly long multi-line assignment statement.  The full statement will now be: ```python faux_inf = (         3         * max(             chain(                 [                     sum(c for c in DEAF.edge_capacities if c < inf),                     sum(abs(w) for w in DEAF.edge_weights),                 ],                 sum(abs(d) for d in DEAF.node_demands),             )         )         or 1     ) ``` Notice the addition of 3 characters "sum" to the line handling demands.
comment
Hmmm....  It looks like you could even get rid of the `chain` now... and the list too.....  it's the max of 3 values now. It just keeps getting better. :)
comment
You can always make changes to your own PR. No need to ask for permission. Use the same branch. Make a new commit. Push it to the same branch on your github fork. That will automatically update the PR on the main repo.  
comment
Ack -- I think you replaced the entire list comprehension with `Gless` instead of replacing `Gless` in this line with `G` (the way it used to be). Then removing the creation of `Gless` and use `F` everywhere below that `Gless` is currently used.  Can you see how this might work?   - `F` as originally defined does not hold any selfloop nodes So it is like Gless. The line defining `F` was Ok before. - Then later on below when we now use `Gless` we could use `F` instead because it (like Gless) doesn't include those nodes. - That means we don't have to construct `Gless`.  Make sure you can see how this would work before making the changes. Ask if it isn't clear.
comment
We do not report single undirected edges as chordless cycles. We do report parallel undirected edges as chordless 2-cycles:  ```python G = nx.MultiGraph([(0, 1), (0, 1)]) list(nx.chordless_cycles(G))  # ->  [[1, 0]] ```  I think the "expected_cycles" should write the 2-cycle as `[3, 4]` instead of `(3,4)`. But I haven't worked through if this is causing the error you show above.
comment
Sorry for the delayed response...  I found a test that didn't set the random number generator seed -- so it was producing a different test each time we ran it. So i added a seed for that. And I added a test to hunt for these errors.  I eventually found the trouble... I had read the creation of F as ruling out nodes with self-loops. But it didn't. While `u` was ruled out of being put into F if any self-loops (u, u) exist, `v` was added to F without checking anything about self-loops. I also added a line in the loop about multigraphs that ruled out nodes with self-loops there too.   Hopefully the changes here will help it pass the tests.  You will need to "pull" this branch from your fork to your local machine to pick up these changes before you do anything more locally. 
comment
Wow -- this is quite a nice description of the impact of this PR. And you've taken care in exploring the many possible impacts too. It is really nice work!! :)  The change to the code is fairly isolated to one main section of code. I have not dived into that yet.  The changes to the docs look good.  Other code changes are mostly about setting up this function with a `method=` keyword argument instead of our automatic switching between NumPy (dense) and SciPy (sparse) treatments. Our current implementation switches for graphs larger/smaller than 500 nodes.  With the two options now specified as `FR` and `L-BFGS`, we have the method name `auto` which performs the current switching between methods.  This is a nice way to use the current setup by default and still allow users to specifically choose dense or sparse approaches no matter how many nodes there are.  I hope to be able to dive into the code changes soon. In the meantime, here are some questions: - Does it make any sense to have tests for these changes? I can't think of any way to do that. But maybe there is something. - What does `L-BFGS` stand for?  - Is there a friendlier way to abbreviate the two method names?  I believe most users of `spring_layout` will not know what `FR` and `L-BFGS` stand for.  Can we add a "Notes" section in the docs that in a few sentences describes the difference between the methods? I think perhaps wording can be taken from this PR. - We should make sure the code works with all supported versions of SciPy.  I think it does based on the tests.  Thanks very much for this code and for the beautiful presentation of results. I am trying to figure out if there is a way to include some of this -- or something like it in our docs. Maybe in a gallery section on layouts. But I'm not sure how to make that work with the sphinx gallery system. If nothing else, we could add it to the nx-guides repository.  Thanks!!
comment
Sorry for the delay. And I still haven't dug through the code in detail.  The main change needed for this PR is to somehow make it possible for old code to be able to recreate what used to happen (for backward compatibility -- and assuming this isn't too difficult to do).  Perhaps the easiest way to do this would be to make a "method" option for the old version of this code. The intent is to allow them to get the same results as before by adding this other method option keyword to their previous call to `spring_layout`.   My current understanding is that there are two fundamental changes made to the sparse algorithm by this PR. First the nodes in each components are given an additional force (based on the average position of nodes in each component) to reduce the separation between components (keep it finite). Second, we are minimizing energy rather than simulating a time-dependent (dynamic) newton's law force system to find its steady state. Theoretically the minimal energy state and the steady state in a dynamic setting should be the same (for a single component network).   **Two questions**: 1) Why didn't you choose to update the small graph option to use the optimizing approach? (I don't want you to do that -- I'm just wondering whether you investigated it at all.)   2) Would it be reasonable to scale the connected component force by a keyword arg parameter? That is, do you think it would be helpful to be able to increase or decrease the force between connected components?  **Tests**: I think we do not need to change the tests as you describe above. Yes, it could be done, but the existing tests are good enough. We might want to add a test checking the method names, but let's not do that now. We can do that in a separate PR if desired.  **Method naming**:  Let's please add another method value to get the old version of the sparse results. As for the various names:  - Is it possible that someone in the future would want to implement a different optimization routine than L-BFGS?  - Is it important to indicate whether the method is good for large or small networks in the name? - Is it important to indicate whether it is simulating dynamic forces to a steady state vs minimizing energy? - And I guess we want to keep the abbreviations short -- with a Notes section to explain in more detail.  Perhaps: "auto"/"dense-ss"/"sparse-ss"/"sparse-me" with "ss" for steady-state and "me" for minimal-energy.  But maybe "FR" and "L-BFGS" with good descriptions is better after all.
comment
Looks like ruff F401 checks for unused imports.  
comment
What does s i m refer to? :) 
comment
How much should we worry about these mixing up the blame? I know we can have `blame` ignore a PR/commit. But is it worth the trouble? And what kind of PRs are good candidates for ignoring?
comment
Ack!  that was me :(   Sorry.... 
comment
Am I reading this backwards? It looks like square clustering is slower after the changes...  Maybe the headings are switched?
comment
You can make that change in your branch locally, commit it and push it up to your fork on the same branch. Github will add that commit to this PR nicely -- and you won't have to make a new PR.
comment
I think he meant: just move the guts of the `generate_isomorphism` directly into the function where it is currently called. No need to enclose it in a function definition or call that function.   Also, this helper function is not part of the public API -- it is just old enough that it doesn't have a leading underscore. So we are not worried about users who might be using it directly.  Finally, there is an `assert` statement in that code that shouldn't be needed (especially if the code is moved inside the function where the children are created).  Could you remove that line?  Thanks for this!
comment
Should we test/have the docs describe that a newline should not follow the header?
comment
Ack -- I just merged another PR and now there are conflicts again... Sorry -- :}
comment
Let's remove the change from dict comprehensions to `dict.fromkeys`.   :) 
comment
Thanks for that feedback.  We have an issue #8005 which is considering making this function more prominent and perhaps more aligned with what folks that need it would like. Constructing random walks seems like something we should provide more easily. If you have ideas or suggestions we'd like to hear them :)
comment
The failed test is almost certainly an update to one of the packages our CI is testing (or just using) in the "extra" suite of tests. It works on my machine locally, so we'll chase down what updated package is causing troubles.  You don't need to worry about that for this PR. :)
comment
Just two (maybe small -- let me know if not) requests left: - based on my skimming of the paper, the choice of alpha and beta is not well understood and probably changes with problem type and dataset.  Can we say something about that in the parameter descriptions? - should we require personalization values to be positive? (raise? or ignore negative? Now we ignore zero, but are negative useful?)
comment
Depending on your needs, you could try `from_numpy_array`. It will create a bipartite graph if the input represents a bipartitie graph. The nodes sets are not indicated automatically, but you can find them with `top_nodes, bottom_nodes = nx.bipartitie.sets()` and set node attributes using `G.add_nodes_from(top_nodes, bipartite=0)`, etc.  I think to update `from_bipartitie_matrix` to handle numpy arrays you only need to update `convert_matrix._generate_weighted_edges` using some form of `((int(u), int(v), A[u,v]) for u,v in zip(*A.nonzero())`
comment
There is *some* computational cost to networkx counting the number of edges. It is about a third the time of using `bfs_edges` for 1000 nodes. And it should scale with number of nodes instead of number of edges.So I think this should be effective as a shortcut approach.   Are there cases when a user would probably not want these shortcuts computed? For the case of 1000 nodes, I estimate it would take 465us instead of 350us (~30% longer) for a non-shortcut result. And 115us for the shortcut case. But those estimates are not using this code.  If that's an issue we could add a kwarg to turn on (or off) the shortcut feature. Thoughts?
comment
`len` is typically fast -- so `len(G)` is too. But we have to compute the `len` of each nbrdict in the dict-of-dict data structure. I think of it as being quick O(n) -- but then I think of `is_connected` as being quick too O(n+m) = O(m) :)  In terms of caching, we thought about caching way back in the day -- and some factors led us not to choose it:  - caching updates do slow down building and changing the graph and many algorithms/codepaths never use it - the data structure could be changed directly by users without our knowing about it (not so true anymore -- views kind of discourage that kind of grow-your-own approach).  - there is a simplicity to the data structure without caching (again views detract from that argument - it seemed at the time, that the user could easily cache the result themselves and use it where needed.   Another data structure could have avoided this issue -- if we had the dict-of-dict point to the relevant edge info within a list of edge info dicts. Then the number of (unweighted) edges would be fast: `len(edge_info_list)`.  That's the history of it -- but the history leading to no caching doesn't mean we can't start caching now -- of course, we should take into account these factors in our decision.  I did a little more testing, and it looks like for `G=nx.binomial_graph(10_000, 0.0001)` that `%timeit G.number_of_edges()` gives 1.12ms while `%timeit nx.is_connected(G)` takes 21us.  So there must be features of the code that make these large graphs with few edges fast already. Have you done some tests with this PR? Which cases lead to improvements?
comment
Thanks for catching and reporting that typo!  I agree with @rossbar that changes to docs are easier/better than changes to keyword arg names in the function signature. And I can't say that the keyword name says much about what is actually happening here with either name.   If a better name for the keyword is proposed, we could change the kwarg name with a deprecation cycle. But that should be a separate PR. In this PR lets change the docs to match the actual argument used. Thanks!
comment
The examples are found automatically IIUC. Sphinx gallery takes care of that.
comment
Here's my twist on the "who came up with it first" debates: Many ideas in mathematics have been discovered and rediscovered and rediscovered. Who should get credit?  I'm not so interested in who did it first. I'm interested in who did it in a way that other people noticed so that it brought the collective state of human knowledge of mathematics to a point where it can use those ideas to expand our knowledge. Remember, none of mathematics was invented, it was discovered. We are explorers, looking for the patterns of the universe.  But more precisely for this discussion, the intent of the documentation is that the cited paper introduced the concept in the framework of centrality measures.  Almost certainly Landau in 1895 was not setting the idea of eigenvector centrality into the context of general measures of centrality. But I could be wrong -- perhaps there is a string of literature extending back that far that I'm not aware of. Still... the current literature traces back to the 1986 paper by Bonacich. It should be kept. But the other can be added as well. :} 
comment
Thanks very much for this!  I'm all for correcting and merging literature histories. And even more so for adding clauses that make it clear under what conditions the method/theory applies. I would welcome updates and corrections in this function and in other functions in the library!  Also I have no vested interest in the current reference per se, except that someone (maybe me) trusted the person who added that reference. You have earned both Mridul and my trust (and maybe others) through these interactions and are welcome to improve our docs and our code. :} Thanks!
comment
I think this is what should be computed.  If you don't want the cycles, your network should not have cycles. I mean, in order to get a "net flow of 100 outward from node 0", you will need to account for the in-flow to node 0 from nodes 24 and 27. So, your pump will have to cause 113 to flow out from node 0 in order to get node 0 to be a source of 100 net flow.  If you want to not allow those back-flows to node 0 you could remove the inward edges to node 0.  But in the flow problems I am familiar with, you **do** want those back-flows. Consider the water-pipe analogy. If we want to force 100 units of water into node 0 and have it flow through the system, we will need to pump 113 units of water out of node 0.
comment
That sounds quite reasonable.  Would you like to suggest a short paragraph for the docs?  I can put it in, or if you prefer you can make a PR.  But I think you understand the confusion better than I, so it might be helpful to have your wording in there.
comment
When you say "cost-efficient", what cost are you referring to? We have a capacity on the edges, but are you using a cost attribute too? We have a function [max_flow_min_cost](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.flow.max_flow_min_cost.html) which might be helpful in that case...
comment
How about: ``` Note that the resulting maximum flow may contain flow cycles, back-flow to the source, or some flow exiting the sink. These are possible if there are cycles in the network. ```
comment
Convergence criteria are tricky...  Here we are questioning what scale to use for determining convergence of the algorithm. There are many ways to define the norm of the convergence error and many ways to scale that norm to make it meaningful or useful.   We scale by the number of **nodes** in the graph (not number of edges as in your example, but in this case it doesn't matter much). That is, `tol` is the "per node error tolerance". We choose this style because then a reasonable default works well for any size graph.  As you point out, you can specify any tolerance you'd like so you can un-do this choice of tolerance if you prefer a different tolerance measure.  BUT, you can only do so if you **know how tol is actually used in the function**!  That probably means the best solution is to inform the users and choose a reasonable default.  I propose that we leave the default behavior as it is, but add to the description of the tolerance parameter for the `pagerank` function.  ```     tol : float, optional         Error tolerance used to check convergence in power method solver. ``` could be replaced by ```     tol : float, optional         Error tolerance used to check convergence in power method solver.         This is scaled by the number of nodes so that large graphs aren't held         to an unrealistic standard. That is, `abs(x - xlast).sum() < len(G) * tol`         is the criteria stopping criteria. If you prefer `tol` to be the vector tolerance         you should divide your tolerance level by the number of nodes in `G`. ```  Better wording can probably be used.
comment
Yes, both of us want reasonable defaults that work well for any size graph. The feeling is/was that `N*tol` would do that. You are saying it doesn't (and based on real world graphs and data). This is definitely worth considering/pursuing.   One difficulty is the phrase "the scores will have converged to one part per million".  If I understand correctly, the current system interprets this as "each score will have converged to..." while your suggestion is closer to "the scores as a whole will have converged to...".  This leads you to compute the error by using the norm instead of the norm divided by the number of elements in the vector.  I would have to re-look up the implications of this distinction. But is that difference clear?  I'm trying to call into question your assumption that the norm is the correct way to measure the error of this calculation.  Said another way, if the current calculation stops after one iteration then the average error of each of the scores is less than one in a million -- as requested. If we use `err<tol` then the average error over all the scores must become very small as the number of nodes increases.  Does this makes sense? 
comment
I added some of the text from the recent comment to the top post (the PR description). @HirokiHamaguchi can you check that the top comment/post on this PR summarizes the changes in this PR and why they are made -- and why they are justified? @amcandio is this the kind of thing you are aiming for?   
comment
I do not consider this a minimal fix.  Can we just change `and` to `or`?  It would be much more straight-foward to start a new branch from `main`. Then you aren't backing out changes you already made in the other PR.   And I'm fine with you adding the new tests along with the simple fix.  That will also help separate the discussion of correctness from the discussion of code refactoring. 
comment
Be careful to use the right class for your matcher: GraphMatcher is for undirected non-multiedge graphs  If you use `MultiDiGraphMatcher` (or even `DiGraphMatcher` since you don't have any multiedges) for the directed case, it returns False as expected.  I guess this shows that we could/should check if the input graphs are the right directed/multiedge form in the `__init__` method for each class. I think this would become mute once we switch completely to a function oriented interface: `nx.is_monomorphic` but that doesn't exist yet. 
comment
Ack -- how did we get all these approvals (and mine was first!) without it passing the tests!!   hee hee.... movin' too fast I guess.  You should be able to override both the `add_edges_from()` and `add_edge()` functions and be able to validate the edges as you add them: ```python import networkx as nx  class ManyAttrGraph(nx.MultiGraph):     def add_edge(self, u_for_edge, v_for_edge, key=None, **attr):         #####################                add validation here         if not ("food" in attr and "water" in attr):             raise ValueError(f"Each edge must have attributes food and water. Got {attr}")         #####################         super().add_edge(u_for_edge, v_for_edge, key=key, **attr)      def add_edges_from(self, ebunch_to_add, **attr):         keylist = []         for e in ebunch_to_add:             ne = len(e)             if ne == 4:                 u, v, key, dd = e             elif ne == 3:                 u, v, dd = e                 key = None             elif ne == 2:                 u, v = e                 dd = {}                 key = None             else:                 msg = f"Edge tuple {e} must be a 2-tuple, 3-tuple or 4-tuple."                 raise NetworkXError(msg)             ddd = {}             ddd.update(attr)             try:                 ddd.update(dd)             except (TypeError, ValueError):                 if ne != 3:                     raise                 key = dd  # ne == 3 with 3rd value not dict, must be a key             #####################                add validation here             if not ("food" in ddd and "water" in ddd):                 raise ValueError(f"Each edge must have attributes food and water. Got {ddd}")             ######################             key = super().add_edge(u, v, key)             self[u][v][key].update(ddd)             keylist.append(key)         return keylist   MAG=ManyAttrGraph() MAG.add_edge(0, 1, food=1, water=4) MAG.add_edges_from([(1, 2, {'food':3.4, 'water':5.4}), (2, 3, {'food':4.4, 'water':7.7})]) try:     MAG.add_edge(5, 6)     assert False, "ValueError should have been raised" except ValueError:     pass try:     MAG.add_edges_from([(5, 6, {3:4})])     assert False, "ValueError should have been raised" except ValueError:     pass try:     MAG.add_edges_from([(6, 7, {"food":4})])     assert False, "ValueError should have been raised" except ValueError:     pass  print(MAG.edges(keys=True, data=True)) ```
comment
Can you try running  ```python G = your_subclass() G.add_nodes_from(node_and_attr_list) G.update(nx.from_numpy_array(M, nodelist=list(G))) ``` 
comment
I would really rather not get into changes like this -- variable names with `queue` - same name with `stack` and `min([x,y]) -> min(x,y)`.  The changes take reviewer time and don't affect users and make the "blame" trace of history harder to follow. There are plenty of stalled contributions to look through and comment on.  And suggestions for better doc_strings and examples are always welcome!
comment
I also think we still want line lengths at 88-max. When we switched to ruff we assured ourselves that it would enforce the same linting that we had with black.   I didn't know we had a way to enforce it in doc_strings. That was missing in black at some point (but maybe was added later and I didn't notice?). It would be good to have a tool that can provide that kind of enforcement for both code and doc_strings.
comment
To use the terminology of [the ABC subpackage of the `collections` std library](https://docs.python.org/3/library/collections.abc.html) what we need for these functions is a Sized Iterable. That is, it needs a `len` method and an `iter` method. A `Collection` seems to be the smallest named collection that provides both Sized and Iterable. But it also provides `contains` and so is a container as well.  Usually these distinctions are not the focus of objects people use. Lists, sets, dicts and tuples are all this kind of thing.    In the base classes we introduce the notion of an `nbunch` ... short for "bunch of nodes" and we aren't usually too specific about which operations it supports.   I'm +0.5 for this standardization -- for example it makes it clear that lists, tuples, sets and dicts are OK. And I'm fine with leaving out the need for `len`. But others may feel differently. 
comment
Did you mean to close this PR?  If not, you can open it again.
comment
This is correct -- the example should use the supergraph to position the communities -- and then center each community around that point. But we need two other changes to make it look good -- first it complained about an extra space between `supergraph ,` when it should be `supergraph,`.  Second the picture didn't look very good because the spread of positions in each community is about 1 (from spring_layout), while we scaled the centers by a factor of 50. So the communities look like they are just a bunch of points.  Adjusting the `50` to be `2` makes it look quite good.  So @morteza-24, if you want to reopen the PR with the space after supergraph removed and change the 50 to a 2 I think we're good to go.  And if you don't we can make a PR if you prefer.  Either way:  Thanks!!
comment
Two comments from a quick read-through. Why  is it better to iterate over the two-hop nbrs (x) of v (to find the combinations u-v) than to iterate over the combinations u-v to find the number of x? Are there fewer two-hop nbrs than combinations of u-v? Or is each lookup faster for finding u-v combinations given x than finding how many x exist for a u-v combination?   :)  This algorithm might be one where we could try making a dict-of-sets from the graph at the beginning -- throwing out selfloops and converting to sets so we don't have to do that repeatedly. We are doing the conversion already, so it won't it down for sure. How much it helps depends on how often we are repeating the lookup/conversion of neighbors to sets. But, in any case, we've done it elsewhere with success. Naming like `G_adj` makes the conversion from one to the other fairly easy -- but you'll want to remove the checking for selfloops code and maybe change other pieces too.  This is a kind of "since we're in the rabbit hole already" point to make. So its fine to say "enough for this PR".
comment
Yikes!  Thanks for this!  I can't get it to fail when calling it on an unweighted graph. And the weighted feature was added well after the original function was written. Perhaps we are breaking some assumption in the usebounds alg by computing with weights?  For example, there are expressions like `ecc_upper[i] + 1 <= 2 * minlower` which I could believe don't work the same for weighted and unweighted graphs. But I haven't dived into the algorithm to check that.  Perhaps we should just jettison the usebounds functionality. For a long time, it has been untested and hidden (in the sense that the docs didn't describe the `usebounds` parameter at all) until this year... 
comment
Thanks @franktakes for this important insight! It helps to know the algorithm being used!  :) :)  I'm going to close this. Different approaches can use a different PR. (or we can reopen this is desired) Comments here will still be notices and read.
comment
I'm going to merge this PR about `could_be_isomorphic` soon unless someone else posts something. We have 3 core devs who have taken a look at it. And I think 2 of us have taken a fairly deep dive in. :)
comment
I think this is ready to be merged. All concerns have been responded to. (I wish I had looked at this before I merged the nx-guides notebook so merging this could have allowed clean up of some parts in the notebook where it says we should change things once this PR is merged...)  :)  We can do more in follow-up PRs.
comment
@eriknw can you summarize what this changes? The logic is rewritten but it seems like much remains the same. IIUC for the old system, the scaling factor should be 1/(n*(n-1)) but when endpoints is True n should be shifted to n-1 in that formula. when directed is False it should be multiplied by 0.5 and when `k != len(G)` it should be multiplied by n/k. And the discussion makes me think that something was off in the n/k factor. Is the difference that we now use (k-1)/n when endpoints is True?
comment
In terms of simplifying the code: we could take advantage of the various places we replace n with n-1. That reduces the number of checks for `endpoints`.  Something like: ```python def _rescale(betweenness, n, normalized, directed=False, k=None, endpoints=False):     N = n if endpoints else n - 1  # could put this in the call arg 'n' to _rescale and remove endpoints kwarg      if N <= 1:         return betweenness     if normalized:         scale = 1 / (N * (N - 1))     else:         scale = 1 if directed else 0.5      if k is not None and k < N:         scale *= N / k     if scale != 1:         for v in betweenness:             betweenness[v] *= scale     return betweenness ```  Also, we might consider `k==n` as a mistake and raise an exception indicating that this isn't a sample -- it is the whole population. Making the special case `k==n` raise an exception might(?) be better for reducing mistakes by users.  But that can be a separate PR for sure.  I'm shooting for some code that describes the following ideas rather than going through every combination of vars. The ideas feel orthogonal -- but that may be just me: - `endpoints` reduces the effective size of scaling from using n to n-1. - `normalized` divides by n*(n-1) -- i.e. the number of possible pairs. - unnormalized undirected counts need to be divided by 2 - scale up the sample to full BC using factor n/k  
comment
> Yeah, this is a great question. I need to think on it more carefully. The original code didn't do this either, so I usually try to minimize the changes done in a PR.  Agreed!  That's a bigger question and should be in another PR.  I'll raise a separate issue.  The code looks quite readable now (at least to me)!  Thank you!!  I believe the only question remaining is whether to allow `k>n` by setting `k = None` in that case, or to only set `k = None` for the `k==n` case and have `sample` raise for `k>n` case. The previous code raised for `k > n`. I have a slight preference for raising when the sample is larger than the population size. I'm fine going ahead and computing the population value when the `k=n`. That is what the previous code did.  Sorry for making this take so long -- I have learned a bunch, but slowed this down for sure...
comment
What do you mean when you say "GEXF supports liststring"? These words likely don't mean "a python list of strings". My recollection is that GEXF doesn't support containers, but maybe something has changed. 
comment
I think you are mistaken that this converts the input to a list.  The data attribute is a string (that happens to be a string of a list).  If you start with this file ```details <gexf xmlns="http://www.gexf.net/1.2draft" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.gexf.net/1.2draft http://www.gexf.net/1.2draft/gexf.xsd" version="1.2">   <meta lastmodifieddate="2024-07-11">     <creator>NetworkX 3.4rc0.dev0</creator>   </meta> <graph mode="static" defaultedgetype="directed">      <attributes class="edge">      <attribute id="0" title="connectedports" type="liststring">      </attribute>      </attributes>      <nodes>          <node id="0" label="APP-server1" />          <node id="1" label="DB-0-1" />      </nodes>      <edges>          <edge id="0" source="0" target="1" label="port">           <attvalues>           <attvalue for="0" value="[8080, 4020]"/>           </attvalues>         </edge>      </edges>  </graph>  </gexf> ``` and read it in `G = nx.read_gexf("ttt.gexf");G.edges(data=True)` you get: ``` OutEdgeDataView([('0', '1', {'connectedports': '[8080, 4020]', 'id': '0', 'label': 'port'})]) ```  You can further process this (and/or preprocess before writing the graph to a gexf file). Convert the data attributes to strings before writing and evaluate them to lists after reading.  ```python ea_new = {tuple(e): str(d) for *e, d in G.edges(data="connectedports")} nx.set_edge_attributes(G, ea_new, "connectedports") nx.write_gexf(G, "ttt1.gexf")  G2 = nx.read_gexf("ttt1.gexf") ea_new = {tuple(e): eval(d) for *e, d in G.edges(data="connectedports")} nx.set_edge_attributes(G, ea_new, "connectedports") ``` *untested*
comment
`_extrema_bounding` was deprecated and then removed in #5757  I don't understand how it is still showing up in the repo. I'll look into that.
comment
Ah, I see better now. The original function was intended to be a private function -- not called from the public api. We now use a leading underscore to denote such private functions. So the issue #5415 discusses moving it to be a private function, with the trail of PRs to deprecate it and then remove it (replacing with _extrema_bounding).  It is an alternate way of computing the distance measures. Shifting it to be private was done because it was designed to be private. In that process we added tests via the `usebounds` keyword argument. I don't think anyone has taken a serious dive into its performance or functionality. The docs suggest that its complexity should be better. But that doesn't always mean better performance on specific (finite size) networks. I think it just hasn't been explored thoroughly. If you do that legwork, we'd love to hear about your results! :)   
comment
Thanks for this report! It looks like the multigraph processing just after creating `F` loops over nodes in `G` when it should loop over nodes in `F`. (basically `F` throws out the nodes with self-loops from further consideration.) We should add some version of this example as a test, and make sure not to reconsider nodes we already have ruled out.  Thanks!
comment
I guess the workaround in the short-term is to remove the nodes with selfloops (and track those selfloops as chordless cycles) before calling the function. ```python import itertools sl_nodes = set(nx.nodes_with_selfloops(G)) Gless = G.subgraph(set(G) - sl_nodes).copy()  chordless_cycle_iterator = itertools.chain(([n] for n in sl_nodes), nx.chordless_cycles(Gless))   print(list(chordless_cycle_iterator)) ```
comment
Yes -- we typically don't assign issues. So you can work on any that look of interest (though some might require policy discussions that slow things down). This one seems like a reasonable choice. Ask questions here -- or better, post a PR with questions (can be a partial PR).
comment
>It looks like the multigraph processing just after creating F loops over nodes in G when it should loop over nodes in F. (basically F throws out the nodes with self-loops from further consideration.) We should add some version of this example as a test, and make sure not to reconsider nodes we already have ruled out.  I think this is the main part of the fix for this issue. Probably good to add a test first -- see how it fails and make sure it is reproducing the OP concern. And then implement a fix.
comment
That is a very strange result (and it does not occur on my environment). I can't think of how it would be finding a self-loop in the multigraph version.  Can you check `1 in MDH._adj[1]` should be False. and `isinstance(MDH._adj, nx.classes.coreviews.FilterMultiAdjacency)` should be `True`?
comment
If I understand correctly you are saying: setting initial node labels to the node's degree: - shifts the iteration count by one compared to the same graph with trivial node attributes - gives hashes for `DiGraph`s that differ from what is produced with trivial node attributes  So, a short-term fix for users is to always use node attributes.   And tests of the function should include comparing hashes between the case with no node attributes and the case with trivial node attributes. And these tests will be needed for both `nx.Graph` and `nx.DiGraph`.  A long term fix would involve adjusting the iteration count for the case of no node attribute, and to initialize the DiGraph node labels to be a 2-tuple of `(in_degree, out_degree)` values.  Have I missed (or incorrectly summarized) anything? How do edge attributes fit with all of this?
comment
It is not clear to me that for DiGraph we need to include both predecessors and successors in the construction. If we cover all the successors for the entire network, then we know where all the predecessors are.  Said another way, specifying all successors of all nodes is sufficient for determining the DiGraph completely. The predecessor information is encoded in the successors. So the hash shouldn't need to explicitly compute the predecessors.   Also, to unify/simplify the code for `Graph` and `DiGraph` cases, you can use `G[node]` in place of `G.neighbors(node)`. Something like: ```python def _neighborhood_aggregate(G, node, node_labels, edge_attr=None):     """     Compute new labels for a given node by aggregating     the labels of each node's neighbors.     """     label_list = []     for nbr in G[node]:         prefix = "" if edge_attr is None else str(G[node][nbr][edge_attr])         label_list.append(prefix + node_labels[nbr])     return node_labels[node] + "".join(sorted(label_list))  ``` I believe this should work for both `DiGraph` and `Graph`.  Re degree for initial node labels: I think using a 2-tuple `(out_degree, in_degree)` works for DiGraph in the same way that `degree` works for Graph. Am I missing something there?  
comment
Thanks!!   @Anemoonvis for the simple example and @MichaelMartini-Celonis for the nice explanation for why it is needed and how the information filters down through the nodes.  Nice pictures and descriptions!  I think the summary is we need a long term fix involving: - adjusting the iteration count for the case of no node attribute,  - initialize DiGraph node labels to be a 2-tuple of (in_degree, out_degree) values, - aggregate using label identifiers for successors and predecessors when handling a DiGraph.  Do we need to do anything to handle "no edge attribute" better than we currently do? 
comment
The official way is:  `G.number_of_edges(n, nbr)`  Also for a multgraph, `G.degree` counts how many multiedges are incident to each node.   The data structure is built on: `G[node][nbr]` is a dictionary keyed by edge-key (any hashable, but by default an integer) indicating which edge, and the value is an attribute dict for that edge.  So to find the number of edges between `u` and `v` you could do `len(G[u][v])`. That's often cleaner if you already have `G[u][v]` for some other reason. `G.number_of_edges(u, v)` is doing that.
comment
I am in favor of both these suggestions: 1) fix the typo `{}` -> `set()` 2) add documentation text (maybe in "Notes" section) about when an empty set is returned.  Thanks! 
comment
I merged this with main, and updated the tests to reflect recent changes to how isolated nodes with self-loops were tested.    Up until now, all tests ran on the whole graph. So the `nodes=` kwarg was not tested.  Since this PR makes the two code pathways separate for `nodes=None` and `nodes=[0]`, I added tests of `effective_size` and `constraint` when the `nodes` parameter is used.  @rossbar, I reused test code by Subclassing the test class. If there is a better way to do this with parametrized code, let me know.  That could be a separate PR or this one.
comment
I actually didn't have trouble with the formula itself -- it was the description of $p_{uw}$ in the paragraph that used $v$ instead of $w$ that made me question which was right. I have now gone to the papers (which are even worse than our docs in terms of explaining what's going on). Wikipedia is better than the papers -- but still worse than our docs. And I have determined that the two `$v$`s in the description of `p_{uw}`  should indeed be `w`s.  But I'll put that into a different PR.  This one has two approvals, though only 1 a core dev. And the author is a core dev so In It Goes. :) 
comment
Instead of editing the file you can specify a max_iter value when you call the function. The value you changed is only a default value for the input `max_iter`.
comment
Thanks for this addition to NetworkX!! What are **you** using `k_vertex_cover` for?  We'll need to make changes to the code to match the NetworkX package. And this code has many parts to try to get a handle on for a full review.  To help with that process can you do the following?  - move the code for bipartite graphs into the bipartite folder (Is all of this for bipartitie graphs?) - remove the inline typing (NX doesn't put typing in the code) - name all helper functions starting with an underscore to denote a "private" function (functions that aren't imported to the main namespace). - use `nx.bipartite.sets(G)` instead of creating `get_top_nodes` - put all the tests and the test helper functions into a single test module. Tests and helpers are distinguished by whether they start with `test_`. Each test should not be in a separate module. - It looks like all the code could also be combined into one module, too. Private functions start with underscore and public functions include a full doc_string.  If you have some functions for bipartite and others not, split them. Put the bipartitie functions into a module in the bipartite subpackage. I would think a natural place for the non-bipartite functions would be in `covering.py`. You could add them to the bottom of that file to keep covering functionality together. If you prefer to keep them separate, push back on this and say why. - Remove `assert` statements from the code. If this is a way to catch errors, use an `if/raise` pattern and raise a `NetworkXError`. If they are not intended to raise exceptions, then just remove the assert statements.  Thanks very much!
comment
It was closed in 2017 -- :)
comment
I haven't hardly even looked at this yet. But I wonder if that `or 1` near the end of `faux_inf` should be a 3. (the whole max expression gets multiplied by 3 as an over-estimate for the max of 2 * edge_flow. So why would that being zero make the alternative value `1`.  Anyway, that's why it seems to me to be wrong to have `or 1`.)
comment
Yes, a PR would help us understand and test the ideas discussed above, and especially your suggestion. Thanks!
comment
Nice demo!  And the function just below is right there to allow easy comparison.  I think its not the reason Guido let match into the language -- it is just replacing if/else clauses. But that's a good place to start.  Some things I noticed: - enticing structure at the top. Looks nicely organized. By the bottom, I've scrolled the matching phrase off the screen so I can't easily see what I'm matching for each case. :) - It's actually one indentation level **more** than the if/else structure (4 vs 3). Used to be source-is-none, target-is-none, method.  Now it is match-source-target, case-source-target, match-method, case-method.    I guess we could always use a 3-tuple for the outer match -- rather than nested match statements. So we could reduce it to 2 indent levels...    Fun -- Fun! :)
comment
Yes you are so correct.  The doc_string has it right too. I think this is "just" a typo... but more impactful than most typos. It's just wrong as it is.
comment
There is a keyword argument to indicate which name the serialization process will use to indicate the name of the node. The default value is 'id'.  That is why `graph_json` holds the names of the nodes in an attribute called 'id'.  Using the default name `id` in your case clobbers your actual attribute named `id`.  So you will want to prescribe another value to that keyword.  For example, choosing the name "my_id" can be done using: ```python # Serialize to JSON graph_json = nx.readwrite.json_graph.node_link_data(G, edges="edges", name="my_id")  # Deserialize back G_deserialized = nx.readwrite.json_graph.node_link_graph(graph_json, edges="edges", name="my_id") ```
comment
This "feature" of changing the attribute names when a collision is about to occur holds for much more than the name attributes. It is also implemented for source, target, edges, nodes and edge keys.  Perhaps there is better wording that we could use in the doc-string to explain what this is useful for and when we want to use it. Do you have any suggestions?  
comment
I've adjusted my scale for vote on this PR.  I now understand that -0.5 is more negative than I was intending.  I was -0 for adding the `is_stub`, not -0.5.  And my concerns are not blockers. With @mridul approving and @eriknw authoring, I think it can be merged.  I would suggest a name other than `is_stub` because "stub" is a new term that submitters of code to networkx will need to learn. Perhaps something like `nx_func=True`, or maybe `no_nx_func=False`. Or if longer is OK, `nx_func_supported=True`. Anyway, something that doesn't require learning a new term. :)
comment
A similar approach would be to just have the backends provide functions on their own, without any NetworkX interfacing. The backend library doesn't hve to always work through a networkx interface -- only when it makes sense to do so.  So the user would do something like:  ```python import networkx as nx import nx_parallel as nxp  G = nx.path_graph(9) result = nxp.new_feature(G) ```
comment
@mriduls your approval was long ago -- sorry for the delay while I get up to speed with this stuff. Could you check whether you still approve and then merge this if so?   Thanks!
comment
If the layout functions create a dict/array of positions during the algorithm and then stores it on the graph it might be good to return the dict in addition to storing it on the graph. I believe layout might be useful for more than just drawing. It is certainly related to planar embeddings. :)
comment
> I've left a warning block in the documentation about the default value of attribute changing in the future.  It feels like more than just a change in the default attribute value -- though that is how the change is implemented. The change (if I understand it) is to automatically add "pos" values to the node attributes in the graph getting the layout. That means we are changing the parameter `G` from an input to an input that gets changed during the function. Previous code that depends on the layout function not changing the graph will need to explicitly set `attribute=None` to work the same as it used to.    I'm not sure whether that is officially a deprecation or some other kind of change. But it seems pretty important for users to notice it. Right now it is a generic `Warning`. It does seem like a DeprecationWarning might be more appropriate. Perhaps @rossbar or @MridulS have thoughts about how to announce that this change is coming. The test currently is: ```    .. Warning:: In future releases, `attribute` will default to `pos`.```.  It should probably be specific about which version (or indicate more long term by saying something like "Eventually, in a future release..."). And it should have another sentence  with something like:  "This means that layouts will be by default written to a node attribute of `G`".  As for names, you point out that `result_attr_name` indicates that it is the name for the output, but it loses the info that it is the position being stored.  I guess something like `store_pos_attr_name`. Or how about `store_pos_as`?  
comment
One way to make the release cycles 3 or 4 instead of 2 would be to merge this PR with the deprecation warnings removed. Then 3.4 has both functions without any deprecation warnings. We advertise the changes in the release notes, etc. Then once 3.4 (or 3.5) is released, we add the deprecation warnings for the old functions.  This would give us some time with early adopters before we make the "scary" deprecation warnings nudge less early adopters. And then we finally remove the functions.  Thoughts? 
comment
> If we decide to wait a cycle or two before adding the deprecation warning, I strongly think that we should add warnings in 3.4 when the new draw API takes effect.  @mjschwenne can you repeat or reword that last phrase? We can't wait a cycle or two for warnings and also include the warnings in 3.4.  Are you saying that we should have some sort of warning even if it isn't a deprecation warning? Or is the 3.4 number a typo? I want to make sure I understand what you are thinking.
comment
Thank you for reporting this issue. Floating point inaccuracies are tricky for sure. We run into those problems in many algorithm areas, like shortest_path, min_flow, etc. Anywhere that we want to optimize or find a cutoff. Rounding can work, but is not consistently applicable because rounding can mess up the values if the resolution needed is finer than the rounding.  When possible, and especially in algorithms that avoid division, it is best to use integer values for attributes that are being optimized. If your data is floating point you can effectively round to integers while keeping the desired accuracy by multiplying by a large value and converting to integer.  ```python old_cap = nx.get_node_attributes(G, "capacity") new_cap = {node: int(1e8 * val) for node, val in nx.get_node_attributes(G, "capacity").items()} nx.set_node_attributes(G, new_cap, "new_capacity") ```  
comment
I think this is an expectation mismatch issue.  `nbunch` can be either a node or a collection of nodes. The error message says: `nbunch is not a node or a sequence of nodes.` and that makes sense so long as you think of your input as an "nbunch".  The user says: OK, the nbunch I input is not a node or a sequence of nodes. I need to make it a node or sequence of nodes.  But if you think of your input as a node, you expect a message like `the node is not in G`.  `G.degree`  probably tries to do too many things. The function interface `G.degree(nbunch)` accepts an nbunch while the dict interface  `G.degree[node]` looks up a node. Most of the time, neither of these should be used, because we rarely want to look up the degree of only one node. Most code should just iterate over the `(node, degree)` pairs by using `G.degree`. If processing involves multiple lookups or comparing degrees, etc, then we should use `G_degree = dict(G.degree)` following by e.g. `G_degree[100]` because this avoids repeated use of the "degree view" lookup. dict construction and lookups are much faster than the python classes which implement views.  We could try making the docs clearing indicate that looking up the degree of individual nodes is not performant. we could remove the function interface (it is included mostly for the rare case when people want to iterate over the degree of a subset of the nodes -- it might be faster to use `(deg for node, deg in G.degree if node in nbunch)`, I don't know), Or we could change the error message in `nbunch_iter` to include the graph considered, e.g. `nbunch is not a node in G or a sequence of nodes.`   Thoughts?
comment
I like your suggestion 2), and would like to make that appear for types other than `int` where that makes sense.  The idea is to handle the TypeError message when `nbunch` is not iterable. We can then refer to it as a node. Maybe we can use something like: ```python     if `object is not iterable` in message:         exc = NetworkXError(f"Node {nbunch} is not in the graph") ```
comment
Don't replace the use of `DegreeView` with dict creation. (I'm not sure if that is what you intend by suggestion 1).  We need the function interface of `DegreeView` to allow specification of the `weight` argument. Maybe there is another api that would be cleaner, but I haven't come up with one.  We might be able to help most people having trouble by adding a note or warning in the docs that `G.degree(v, weight="wt")` is an inefficient way of looking at the degree of all nodes. Iterate like `for node, degree in G.degree(weight="wt")` or create a degree dict `degree = dict(G.degree(weight="wt"))`.
comment
To get a divide by zero error here wouldn't we need all the node positions to be zero? That's the only way a norm can be zero.  So how would that happen? Do you have a small enough example to put into a test for this? How did you find this issue?
comment
Got it -- Thanks for reporting this and for the example!!  I was thinking the positions had been shifted so the center was (0,0). My understanding is that Gravity should pull everything to the center instead of to the origin. The docstring suggests that gravity pulls to the center.  @cvanelteren can you weigh in on this? Should we be computing gravity based on distance to the origin, or distance to the center of all the other positions? Thanks! 
comment
Thanks @cvanelteren !  Looking at that suggestion I notice that in line 1502 (where we actually do the update) we stop (break) only if the abs of the sum of the updates is small.  But doing the abs of the sum allows positive and negative updates to cancel before the abs has a chance to make them all positive. Should it be  `if abs(update * factor[:, None]).sum() <= 1e-10: ` instead of  `if abs((update * factor[:, None]).sum()) <= 1e-10: ` 
comment
It looks like the center position moves around over time, so we should recalculate it each iteration. Does that make sense?
comment
@oestej, Are you willing to check the box/button on the right side of the PR discussion that allows maintainers to make changes to the PR? I've got @cvanelteren 's changes set up and some other small improvements. I could push them if you give me permission.
comment
Yes -- this is a nice suggestion. The helper function checks if `n in pred` in its first lines and rasies precisely the exception the main function then looks for. We know which nodes will cause the exception because we know `pred` at the time of the loop. So change the lopp to be over `pred` instead of `G` and remove the try/except.  Thanks very much!!  And I think it is really cool that `adventofcode` led you to this improvement! :)  What a nice project. 
comment
This seems like a good addition to NetworkX. And the expanders module is a good place. The proposed names look reasonable except I would like "regular" to be included in `maybe_regular_expander` and `random_regular_expander`. 
comment
That's the right approach.  The stochastic_block_model function provides a graph attribute named `partition` which has a list of sets as it's value. Sets are not automatically taken care of by the default stringizer, so you need to supply a more complicated function -- and in this case, `literal_stringizer` works fine.  Looks like you solved your own question, so I will close this, but you can leave more comments an/or start a new issue if desired.
comment
Unfortunately, `{}` is a dict.... not a set.  To get an empty set you need to use `set()`.
comment
Yes -- an example in the doc_string that uses `set()` should help a lot. Thanks!
comment
Hmmmm...   If the edge weight can change during the search, then will the standard algorithms for shortest path like Dijkstra work? It seems like they rely on the static nature of edge weights.  Perhaps it would be better to reformulate the graph by splitting nodes based on the state data needed to make the paths static.  For the adventofcode-day16 example, each position may have 4 nodes in the graph depending on the direction you are facing. Turning moves you to a different node. But the edge weights are now constant and standard shortest path methods can be used.
comment
You should also keep in mind that it has always been this way. Numpy scalars are equal to Python ints if they represent the same value. The reason we see it now is only because NumPy 2.x displays numbers together with their class. So what used to show up as `333` now shows up as `np.int64(333)`.  But the Python int 33 is equal to `np.int64(333)` just as it has always been. You didn't see the difference when you printed before. Now you do.
comment
OK... I think we have agreement on changing the module name to `density.py`, the test module to `tests/test_density.py`. We also have agreement on a long term plan for what the main function will be with private helper functions for the different methods.   I agree with @rossbar that the function names can be changed in later PRs which introduce the new methods. But I would very much prefer that this happen before we make a release (which is scheduled for late March/early April). Otherwise we have to make a deprecation for `greedy_plus_plus` and wait two release cycles, etc.  If that timing is OK, function names are fine. If that might be iffy, can we go ahead and change the function name here and add the "method" keyword arg?  Either way, can we change the module names now so we don't have those changes baked into the commit history forever?  Thanks!
comment
I took the liberty of pushing up some tests.  I think this is now ready. @rossbar can you take a look at the tests?
comment
Adding a regression test shows that the current code doesn't work for 2 nodes (tries to take the `min` of an empty list at line 630).  So I added some code to handle the 2 node case.   @mjschwenne can you check that this 2-node code is correct? 
comment
You should probably address this report to the source of your clique.pyi file.
comment
Yes, the function was deprecated in #6186 and then removed two releases later (3.2) in #6941.  As a replacement, the doc_strings were enhanced to include  the one-liner code that computes the answer. There were three such functions eventually removed. The graph_clique_number is/was computed using ```python max(len(c) for c in nx.find_cliques(G)) ``` The doc_string descriptions of how to compute this quantity is in [the `find_cliques` function docs](https://networkx.org/documentation/latest/reference/algorithms/generated/networkx.algorithms.clique.find_cliques.html).    But the problem seems to be that the type stubs are not correct -- not that you want to find the clique number.
comment
Great work! Could you add a test method or two to the `tests/test_subgraphviews.py` test class for edge_subgraph views? Can be added at the very end of the file. The test should check the examples you came up with that fail before the fix and now pass. I think all you'd need is the code up to the first "print" in your initial code quotation. Then `assert 'c' not in H['a']` and `assert not H.has_edge('a', 'c')`.  And put a comment at the start of the test method with something like: `# see gh-7724`.  Thanks!!
comment
Thanks for this Report!  It is not a known/reported issue.  I can verify that this behavior is indeed a bug.  To get this strange behavior, both 'a' and 'c' must be incident to visible edges in the subgraph and there must be a hidden edge between 'a' and 'c'.  The means `'c' in H['a']` passes because the nodes are in the view and `G['a']['c']` exists and we don't check whether any edges between them are visible. Similarly, for `H.has_edge` when the edge key is not included, we don't check whether none of the edges are visible. We just check if the dict of edge-keys exists.    So, we need to 1) add a hook into those lookups to check if any visible edges are present, and 2) make sure that other strange behavior isn't also happening for other lookups.   This bug only affects edge_subgraphs of multigraphs.   A workaround is `nx.edge_subgraph(G, <edges>).copy()`. That workaround is actually faster than the subgraph view when you need to look up most of the edges more than once. Subgraph views save memory, not time. And the memory saved is the size of the subgraph as a graph (which is often pretty small).
comment
Excellent!   Thanks for hunting down this lookup in the "Inner" layer: `FilterMultiInner`. That is the heart of the bug for sure -- and I wasn't sure whether a fix would be there or would need to be made in many places where this interacts with the other view/filter classes.  Glad to see that the examples discovered and reported here are fixed so easily.  I'm not sure how to ensure that other strange behavior is not also occurring. That is what tests are for!  :)  I thought we had it covered, but clearly we didn't. Hopefully this at least gets us closer.
comment
I'm afraid that I am 0- on this change.  It changes `iter` so it passes through all the nodes of the graph instead of the nodes in the subgraph. And this only impacts `iter` and not all the other functionality. That is, you will run into similar troubles in other places if you use numpy scalars to look for nodes with python numbers as nodes. And I don't think we should fix all of these.  What is the trouble you are trying to solve?  The values `0` and `np.int32(0)` both refer to your node. Everything works as it always has -- they just print out differently. Are you trying to make the printing of nodes look the same? Then convert before creating your subgraph. If it is not the printing of the nodes that is the problem, then what is the problem?
comment
Thanks for that info about how this arose. You probably wouldn't have noticed if numpy hadn't changed their printing standard.  The reason not to iterate over all the nodes in the graph is that a graph might have 100000 nodes and we are only looking at 100 or so nodes in a particular region. Look at the 3-4 lines above this change to see that we test for the len of the node set and choose this code-path when it is faster. The same code appears in the other filter-view as well.  My main concern with this change is that we are covering one way in which different types of numbers arise, but not the other ways. I think we should either make a comprehensive dive into the code to treat the types differently, or we should continue to treat all number types as interchangeable.  But my feelings are soft (hence 0- instead of -1) and could be changed by further information about how to handle numpy 2 numbers interacting with python numbers, or how we could handle both gracefully.
comment
Thanks very much for this!  The CI test errors are because we make sure that NetworkX still works even for environments which can't/won't install numpy. So we have a "base" test environment without numpy installed. What this means for the PR is that you need to move the `import numpy as np` inside the function (wrapped in a try/except for ImportError) and add this function to the list in `networkx/conftest.py` in the section indicating `needs_numpy` (about line 200).  For backward compatibility, it would be good to make this function work even if the import of numpy doesn't work. The code is only about 25 lines long, so maybe we should include both versions (with the native-only version including your improvement of updating the out-degree, but not the numpy parts). And the choice of which to use could be based on a flag set by whether the numpy import worked...???  There are others ways to implement this in a way that it could work in both settings. Some algorithms go so far as to separate the versions into two functions -- but I don't recall any that are generators. I think for now, could you put the other option in an if-clause like  ```python try:     import numpy as np     no_numpy = False except ImportError as exc:     no_numpy = True  <setup>  if no_numpy:     <other stuff>     return G  <main stuff> ```  Do other folks have thoughts about how to set this up for numpy and not-numpy?
comment
I agree with the path forward that @rossbar puts forward.  @TLouf can you create two functions (named something like `_random_k_out_graph_python` and `_random_k_out_graph_numpy`) without doc_strings or dispatchable decorator -- just the `*_random_state` decorators as needed.  Leave the original function (with doc_string and dispatchable decorator) and have it call the numpy version for now... We'll need to figure out whether we want a keyword arg or an environment choice to be made.   I'll put together a quick change to tests to parametrize the current tests to run twice -- once with each private helper function, and I'll add a deprecation process.  Unless of course you want to have fun with those implementations.  If this is more than you are up for, let us know. If you are up for adding the "needs_numpy" entry in conftest.py, adding deprecation warnings and running tests on both private functions, go ahead and take it on. Do as much as you want and tell us what you'd prefer we do.  Thanks again!
comment
This looks like an excellent addition to NetworkX!  I haven't looked at the PR in depth yet, but have two requests on a quick look: - can you add a section to the documentation hooks that let sphinx (our doc builder) know where to pull the doc_strings from your module as it builds the docs? The file that is relevant (I think...) is `doc/reference/algorithsm/community.rst` and I think you can copy/paste/change an existing section to make it work with your code. The module name is listed in some preamble stuff, and then the function names are listed with appropriate whitespace around them. - Can you wrap your doc_string lines to something like 80 chars? Code is <88 chars on a line, so the doc_strings look good between 75-88 or so. We don't have an automated tool to do that yet I believe, but we're working on it.   Thanks!
comment
I include a comment with the term  Fixes #6899   so that merging this PR automatically closes that issue.  More on [linking a Pr to an Issue]( https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue) [Edit:  -- oops!  that phrase has to be in the original post. I added it there so they are connected.]
comment
I think the tests actually passed -- it was a red X because one test got canceled, but rerunning it allowed it to pass.  A second test could be a graph that works plus an isolated node. That will trigger the other exception added in this PR.
comment
Yes -- I was referring to the test added and then removed.  That tested the "no basal nodes" exception. 
comment
Yes -- could you add it again, and a test for a network that should work, but with an added isolated node so that it doesn't work -- and raises an exception from the 2nd code-check. Does this make sense?
comment
I thought edges are undirected and arcs are directed.  Do you understand that differently? 
comment
It looks like this file name is the commit message and PR title. I think this code needs to go in the `networkx/drawing/nx_pydot.py` file inside the repo.
comment
Please look at [the previous question's answer](https://github.com/networkx/networkx/issues/7173#issuecomment-1862152218) as linked by @Schefflera-Arboricola .  You **are** getting the classical algebraic adjacency matrix. But you aren't specifying the node order for the matrix. See that answer.
comment
I dont understand why the documentation CI test is failing even when restarted. It talks not determining the CUDA version. But other PRs are successfully passing the tests.  I'm going to push my suggestions to see if that helps. reword and change as you like.
comment
The testing is intentionally weak for the drawing tools -- intentional in that we figured it is just a temporary stopgap tool and we'd get to better testing when we made it a real thing. Fast forward 18 years and we're at the point of needing/thinking about tests.  The tests with no asserts were our "smoke-tests" to run the code and see if it started smoking or blowing up or something (syntax error).   I would suggest that you add tests as much as you feel makes sense. It will help you keep track of which features are set up which way and how they might change. But don't worry about complete testing unless you think that helps your design work. The interface will change so the tests will need to change too.
comment
Thanks @eriknw and @Schefflera-Arboricola !!  Now I'll go figure out what you did. :) This is great--
comment
These are good questions for the dispatching group:  > Should we include [graphblas-algorithms](https://github.com/python-graphblas/graphblas-algorithms) in the list of backends on the new "Backends" page? It hasn't been updated in quite a while and the docs still use outdated terminology ("plugin"). It looks like graphblas-algorithms hasn't received the focus to update it along with the dispatching system. But @eriknw is one of its core devs and he wrote the dispatching system. So I would give them the choice in this case.  > Should we include any other backends on the new "Backends" page? Have any other backends approached us about being added? Are we aware of any others? If not, it's hard to add them.
comment
Here's a big-picture question regarding the docs:  Should Backends have the separate tab on the main NX webpage? The webpage itself is about as long as the entry in the "install" tab about backends.  Could we just put the list of known backends into the install page docs? Or into a multiple-page section of the reference docs? To be clear, I am fine with this PR for this release. Nothing here is a blocker. But I'm sad that the tabs on the docs page are enough to require a "more" click to see them all on my laptop display. :)
comment
When the first caching warning shows up, the user no longer knows about caching (caching is "on" by default now -- they used to have to be turned on so the user knew they were doing that).  So the current message does not start well for its new placement.  Can we change the first sentence to something like: Converting the input graph to <backend> type and caching the converted graph to be used by other functions.
comment
And now that I said that, I'm not sure that this PR makes caching turned on by default. Is caching turned on by default? 
comment
I think I just don't have it straight in my head when this message gets shown. If caching the graph conversions is turned on by default, then I *think* it is shown the first time a user calls a function that checks the cache (the first time they call a function that could cache a converted graph). That means a user reads about backends somewhere, installs cugraph or another backend and tries it. They get the warning. But they don't know anything about caching yet. As far as they know, they just turned on the backend. And now they are getting a warning about caching being done.  When caching is not turned on by default, then the user has to install the backend, play around a bit, find out about caching and turn it on, and that's when they get the warning message. So they know that caching is a thing when they get the message.  I was just thinking that if the user hasn't heard about caching before, we should introduce it slightly at the start of the warning.
comment
For random graph generation, there is usually a distribution that is expected. For example, the degree distribution should follow a specified distribution. That is, for example, specific to the case of an undirected graph. Other generators work with a directed graph. And you have worked with the code that treats directed and undirected differently, so you know the differences there. MultiGraphs bring on their own set of issues and it is not always clear how the multiedges would affect the distributions.   Can you say something about the motivation of this PR? I am guessing that you want to use `create_using` to construct your graph from a subclass of the base classes. We believe it should be easy to use the existing code with the new graph subclass as something like:  `G = new_graph(nx.random_graph_generator(...))`  We have been designing a framework in [NXEP3](https://github.com/networkx/networkx/blob/main/doc/developer/nxeps/nxep-0003.rst) where the graph generators would return either edges or graphs. Then you would have the basic building blocks to combine generator algorithms (which usually just construct edges -- though sometimes with potentially isolated nodes as well) very flexibly without even having a graph data structure. When you are done manipulating edges, you send those into your `new_graph` class to store it.  Could you look at these options with a perspective that we previously decided **not** to include the `create_using` argument? How much does this help you, and is it worth the extra complexity of the API.  I don't actually recall the `duplication_graph` code, so will have to look at that. But the `random_graphs.py` module definitely had reasons to not include `create_using` and we'd need to revisit those reasons to consider this PR.  Since you are motivated and interested in this issue, we'd like to hear your thoughts on NXEP3 and generator API. Thanks! 
comment
I can make the recently suggested changes above if you don't have time atm. Didn't mean to dump a bunch of stuff on you. What do you think?
comment
Thanks @tillahoffmann (and @rossbar reviews)! create_using in random_graphs.py and duplication.py is merged!   
comment
Thanks for this! Could you make the first two lines wrap nicely (within 88 chars for sure and maybe 80  to look nice)?
comment
If I understand this correctly, this PR does two things: 1) turns on dispatching of nx-generator-functions if the backend's version of the function has an attribute (declaring that it's backend graph type is compatible with nx.Graph). The output can be a nx.Graph or a backend graph. 2) calls the nx-function with a backend graph input (no conversion) if no backend in the priority list supports the function for that backend graph type AND the backend graph input is a nx.Graph instance.   The impact of these changes is: 1) that the backend version of generators now can return non-nx-graph graph objects. 2) if non-nx-graph graph objects are provided as input to nx-functions that aren't supported by that backend, then the nx-function itself is called using that graph object so long as it is a subclass of nx.Graph.  I think this PR is independent of #7585 but they provide two mechanisms for handling the "fallback" case of when the input graph is a backend type but the function is not supported by the backend. This PR allows the `orig_function` to be called directly without conversion if the backend graph isinstance of nx.Graph.  The other PR allows automatic conversion to an nx graph before calling `orig_function`. Both paths could exist at the same time so long as this PRs logic comes before the fallback conversion of 7585.   
comment
So, if I understand right, we now have two related configuration settings: `backend_priority` and `backend_fallback`. The priority says what order to try backends when a NXGraph is the input. The fallback is what backend to try if a backend conversion fails. But this probably isn't a correct reading of your description.   Does the fallback option occur when **any** of the priority backends can't convert? Or after **all** of the priority backends can't convert?  And I'm not sure what you mean by 'sticky': "backends in backend_priority won't be converted if they implement a function."  doesn't make sense to me... is there a word missing?
comment
Thanks for that description -- still digesting it.  What is the difference between the backend in "fallback" and having that backend in the last spot of the "priority"?
comment
> Thus, we have:  > 1. If backend=backend_name is given then use that backend (and fail if can't) > 2. If you pass in a backend graph to an algorithm, then try to use that backend > 3. If you pass in a NetworkX graph to an algorithm, then try to convert it to use backends in backend_priority  I have trouble with Rule 2. It seems to assume that every backend has a single backend graph type. And every graph type has a single backend associated with it.  We ran into this already with nx-parallel and again with nxadb where the graph class structure is a networkx graph. We got around that. But perhaps we should consider removing that assumption. Sometimes we are trying to make backends replace functions, and other times we are trying to make them replace graph classes, and often we are doing both. Is there an interface that would separate those two approaches?  [Edit: this paragraph isn't very workable because most graph classes would need to be changed to include such a tuple. It's probably better just to make the backends track what can convert to what.] ~~Maybe each backend should support a tuple of graph classes that it know about and supports. And each graph class can hold a tuple of graph classes it can convert to. Then we remove Rule 2 and adapt Rule 3 to march along the backend_priority looking for a backend that supports a graph class that the input graph can convert to.  (Note that "supports a graph class" basically means that the backend can convert the incoming graph class to something it can use.)~~
comment
Sorry for multiple posts instead of one...  > nx.config.backend_priority = ["spam"] > nx.pagerank(G_spam)  # Uses "spam" backend > nx.pagerank(G_eggs)  # Tries to convert and use "spam" backend, then would try "eggs" backend  Does `tries to convert and use "spam" backend` mean check if the spam backend function works with G_eggs as input? (that is, does this happen by an indicator on the backend, or by trying to run the function and looking for exceptions?)  [Edit:]  Maybe you answered this already:  `backend_priority` is the list of backends to try with the given input G (of whatever class). Then if none of those "work", meaning conversion doesn't happen or the function isn't implemented for that backend, then `backend_fallback` is the backend used.  But then why not have the fallback just appear as the last entry in backend_priority? It has something to do with this idea of "sticky".  You say: `I called these "sticky" to indicate that they will try first to run without converting` and that the fallback is not sticky. So, I think that means the fallback backend will try to convert before running the function. Or maybe I am misunderstanding something more basic:  Who does the conversions? The backend? or the dispatching code in NX?
comment
Why the change to the tools directory?
comment
Thanks for this PR @Siddhesh0305  There are some things to fix with it though. It looks like you overwrote the `test_planarity.py` file with your new code.  What we need instead: - add just the method (not the surrounding class inheriting from PlanarGraph) to the PlanarGraph object itself in `planarity.py` - add a new test method in the `tests/test_planarity.py` module called something like `test_to_undirected_attr_handling(self)` and have it run the code you use as an example, only instead of printing the results use e.g. `assert is_planar` or `assert "cw" in P[0][2]`  The style CI test is failing because you used single quotes instead of doubles. In general, it is a good idea to install the `pre-commit` tools so the style-checker (linter) runs locally as you `git commit` and warns you of any style fixes before you commit them. It's described more at the end of bullet 1 of [these developer docs](https://networkx.org/documentation/stable/developer/contribute.html#development-workflow)  Ask if you have questions.
comment
It is hard to see what is going wrong when there is only the error traceback. Can you provide a small example of code that would reproduce the problem. For example, does `bng = nx.path_graph(5)` give this error? The first step is probably to figure out whether the error is due to code from pgmpy or networkx or matplotlib?  And be aware that the "path" described in the matplotlib code is not a path in the networkx graph but the path of an arrow being drawn to represent one edge in the network.
comment
Try taking a look at the [plotting documentation for pgmpy](https://pgmpy.org/plotting.html).  I don't think using the NetworkX draw function on a pgmpy model object is supposed to work. Try to use the pgmpy tools.
comment
I like the noisy-warning-with-fix approach. Thanks @rossbar !
comment
What would the instructions for installing via conda/mamba look like?  Should we keep the requirements files around just for that?
comment
One of the difficulties with using eigenvectors for measuring centrality is when the adjacency matrix has largest eigenvalue with a multiplicity greater than 1.  Said another way, there is a plane (or even 3 or more D space) of eigenvectors with that eigenvalue.  Normally the Perron-Frobenius Theorem ensures that there is only a line of eigenvectors for the largest eigenvalue (can't be less than a line because any multiple of an eigenvector is also an eigenvector). We normally select a unique eigenvector by choosing a unit vector with first nonzero element positive.  But the P-F theorem doesn't hold when the the network is disconnected (in other language: when the matrix is reducible). Then there is an entire plane (or more) of eigenvectors for the largest eigenvalue and the computed eigenvectors reflect a correct eigenvector, but only one of many. Depending on the method used, round-off error can affect which of the infinitely many choices of eigenvector may be reported. They are all correct as eigenvectors, but can't easily be used for centrality measures.  The python version of `eigenvector_centrality` is fairly robust -- that is, it reports the same results each time. But the numpy version doesn't. That is the source of what you see.  But the underlying difficulty is that the eigenvector centrality is not well defined for disconnected graphs. 
comment
Thank you for these suggestions!! I like the approach of raising an exception for disconnected graphs.   But it does mean that we would need to check whether the graph is disconnected for every call of the function. I haven't looked at how much that slowdown would be. If it is significant, maybe we could put a warning in the doc_string that results may not be effective for disconnected graphs. We could test whether the timing is significant or not by timing a two step process of `ns.is_connected(G); nx.eigenvector_centrality_numpy(G)` for various graphs G and compare with just centrality. 
comment
Hee hee...  Yes!  It sounds like it is time to write a paper! :} 
comment
I thought I had found Harmonic Diameter discussed on Wikipedia. But apparently not. So, I am not as sure about this PR as I was.  And I have found the cited article  (well -- the arxiv version) and it doesn't contain the word "diameter".  Do you have a better reference for harmonic diameter? 
comment
So, if understand the discussion, this function computes a kind of average path length where by average we use the harmonic mean instead of the arithmetic mean.   What do people think of adding this feature by adding a keyword argument to the  `average_shortest_path_length` function? The keyword could be "mean" or "harmonic", and maybe we could find a way to allow other strange measures of average like "median"?  OK... too much... :}
comment
Sorry about the long delay. We should be able to get this going again.  The main issue seemed to be the name `harmonic_diameter`.  We agree that computing average distances using the harmonic formula is a nice option to have. But it is not a diameter (which involves the max of all distances). It is a mean.  I'd like to propose the name `harmonic_mean_distance`. It'd be good to add this new function to the "See Also" section of the doc_string for the functions `average_shortest_path_length`.  It's possible that we would want it as an option to the `average_shortest_path_length` function, but I think it is better for now to get it into the `distance_measures` module.   We can refactor later is desired. And this makes it closer to these distance measures that might be closer to "harmonic" measures if we add those. @jarrodmillman what do you think and are there other issues?  
comment
Thanks @vigna !! And sorry for the long review.
comment
Thanks for submitting this issue. I agree that this is not a good error message for MultiDiGraph input. And I'm not familiar enough with the algorithm to know whether MultiGraphs should even be considered using this technique. Of course, we can always construct a way to handle multiedges. But will it help make a reasonable hash?  Do you know of any articles which build off the papers cited in the doc_string (or not) that handle hashing of multigraphs?  The easy fix (that can be expanded later) would be to check `G.is_multigraph()` early on and raise an exception. There is a decorator to make this simpler:  `@nx.utils.not_implemented_for("multigraph")`
comment
The Issue talks about nx.Graph working and nx.MultiDiGraph not working. The PR checks for multigraphs. But what about directed graphs?  If we want to rule those out until we find an algorithm, we should put two `not_implemented_for` decorators, one for "directed" and one for "multigraph". ([examples here](https://networkx.org/documentation/stable/reference/generated/networkx.utils.decorators.not_implemented_for.html))  Of course, if this algorithm "just works" for directed graphs then we don't need to do that. I haven't checked if DiGraph makes something that works.
comment
This has one approval and a nice review with all suggestions implemented (and tested).  I'm going to merge it to help other PRs that depend on this one. Thanks @mjschwenne and @rossbar 
comment
Removed a divide-by-zero RunTimeWarning that showed up after the numpy 2.0 release.  Added the default values to the doc_string parameter descriptions.  Revamped the initial position code to place the initial points within the smallest rectangular region that includes all the inputted position nodes. If no nodes are given initial positions, the region is the units square $x, y \in [0, 1]$.   I added the seed parameter to allow setting initial node positions in a deterministic way.  This was also missing from `arf_layout`, so I added it there as well.   I believe this is ready for review -- and maybe merging. :tada:  if tests passed.
comment
I added some doc_string changes to make this more easily discovered in the docs for `shortest_simple_paths`.
comment
The path lengths are different because `single_source_shortest_path_length` doesn't take the weights into account while `floyd_warshall` uses the 'weight' attribute of the edges.  You could specify the weight attribute for shortest path using something like:  ```python nx.shortest_path_length(LM, 'Brujon', weight='weight') ``` and the results agree with the results of `floyd_warshall(LM)['Brujon']`
comment
The problem with mixing numpy scalars and python ints is that they are equal when the value is equal. So Python dicts treat them as the same key in the dict. The examples you show seem to be related to the printing changes for numpy 2.0 rather than the python version used. By that I mean in numpy 2.0 printing a scalar shows the text e.g. `np.int32(0)` whereas in previous versions of numpy it shows `0`. So with the old version they **look** the same, but the type of the answer is actually different. With numpy 2.0 they now appear different when they have a different type.   When mixing numpy and python numbers you can take two approaches (as I understand it). You can treat the numpy scalars as the same as python numbers -- in which case you should ignore what the string values show as being different. The values are the same (they are equal) and the hashes are the same so dicts treat them as the same. Printing them treats them  as different. The other approach is to be very careful to convert all of one type to the other type when you interacxt with the graph. For example, make them all python numbers on the graph object, and only create subgraphs using python numbers.  There are other subtle differences between numpy scalars and python numbers -- like numpy scalars have finite size, so overflow and underflow can occur if you do arithmetic with them. That allows them to be efficiently stored in fixed stride arrays/buffers which is good. But `np.int32(2**31) * 4` gives a strange result.  This kind of difference between python numbers and numpy numbers has alwayds been there. But now they appear different when printed. That can cause confusion. But hopefully when we all get used to thinking of them as different objects even though they hash and equate as the same there will be less confusion.  I personally usually choose to use python numbers in my networkx graphs and convert the output of numpy arrays to python numbers when non-numpy manipulations are involved.  i.e.  while I used to write `A[3, 4] * 4`, with numpy 2.0 I will write `int(A[3, 4]) * 4`.  If others know a better approach please let me know. 
comment
The output you describe is precisely what networkx is intended to provide.  When djikstra is finding the shortest_path, you will always want it to be choosing the multiedge with the smallest distance between those two nodes. Write your function to search through the multiedge key_dict to find the shortest edge and return the distance for that edge.  ```py def f(u, v, kd):     print(u,v,kd)     return min(dd['weight'] for dd in kd.values()) ```  If you are going to be doing this many times with the same multigraph, you should probably create a graph with each edge having the weight that is the minimal weight between those two edges. ```py smallerG = nx.Graph((u, v, {'weight': min(dd['weight'] for dd in G[u][v].values())}) for u, v in G.edges())  ```
comment
It actually doesn't make it impossible to distinguish the multi-edges from each other. But you do have to use info from the original graph. You know `(u, v, {k:d})` for each edge in the path, and that's enough to find the shortest edge between those nodes. It is true that the format of the shortest path does not include the edge key as part of the return value. But you have all the information you need to find which edge key was chosen on every edge of the shortest path.  The one-liner code to form the path with edge keys is very similar to the one-liner code to construct a smaller graph over which to find the shortest paths (from comment above).   Something like: ```python edge_key_path = [(u, v, min(G[u][v].items(), key=lambda k, dd: dd.get('weight', 1))) for u, v in shortest_path) ```  But I would still recommend that you use `shortest_path` on the reduced graph that has only the shortest edges between each node-pair present.  ```python smallerG = nx.Graph((u, v, {'weight': min(dd['weight'] for dd in G[u][v].values())}) for u, v in G.edges())  ```
comment
I'm going to close this in favor of the discussion pointed to above.
comment
I also think it should be `edges`.  And the code itself is strange in that it uses `"link": "links",` on line 13. It makes me wonder if it was a typo.  Also, NetworkX consistently uses "edges" rather than "links".  Regarding backward compatibility, perhaps we can add the new "edges" while keeping the old "links", and early in the function check if `links` is not the default. If not, raise an exception or a deprecation warning depending on if `edges` is not the default. And copy the non-default value from links to edges if appropriate.  After the two release deprecation cycle, we can remove the "links" argument.  I guess the edges kwarg should go in the position of the links arg with the links arg at the end of the kwargs. I guess the doc_string should add the description of links as deprecated and to use edges.   Does this make sense?
comment
Yes -- as part of the deprecation, a line or two should be added to `doc/developer/deprecations.rst` under the section for the version that removal will take place. That line should say what should be removed. It shows up in the networkx documentation pages at: `https://networkx.org/documentation/stable/developer/deprecations.html`. There is also a section of the [developers guide](https://networkx.org/documentation/stable/developer/contribute.html) (subsection 7 of the section "development workflow") that mentions there should be a warning filter put into `conftest.py` (see the other similar warnings filters for deprecations and copy/paste/adapt). If you have other questions we'll cover them during PR review. Thanks very much! 
comment
This looks like there might be a confusion between 0-based indexing and 1-based indexing. Check your conversion, for example the first matrix indicates edges `(0, 1)` and `(0, 3)`, but the second matrix indicates edges `(0, 1)` and `(0, 4)`.
comment
When that test failed July 4th I restarted it and it worked. I just now restarted it, so we'll see. I think that test is only run upon merge, not upon push to PR. But I am not sure.  The longer error message says:  > If https://pypi.nvidia.com is not reachable, we cannot download the real wheel file to install.
comment
I don't see anything in the docs indicating that it works for MultiGraphs. (But it says that input G should be a directed graph when it should say undirected graph.)  Looking through the code, there is no handling of multiedges and their edge keys. I can look into the history more, but first: - can you describe why you think this should work for MultiGraphs? - how would you expect it to handle two edges between the same two nodes?
comment
You might try creating the edges iterator before the for loop.  Something like: ```python     partition_dict = partition.partition_dict     partition_key = self.partition_key     G = self.G     edges = G.edges(keys=True, data=True) if G.is_multigraph() else G.edges(data=True)      for *e, d in edges:         e = tuple(e)         d[partition_key] = partition_dict[e] if e in partition_dict else EdgePartition.OPEN  def clear_partition(self, G):     edges = G.edges(keys=True, data=True) if G.is_multigraph() else G.edges(data=True)     for *e, d in edges:         if self.partition_key in d:             del d[self.partition_key]
comment
Looks like adding `weight` mixed up the test in `networkx/tests/test_all_random_functions.py`. It just needs the parameters corrected and it should pass.
comment
One disadvantage of using `**kwargs` instead of having the user curry their function is that misspelled keyword names get silently ignored. E.g. if you use `wieght="weight"`  you will get no error and the computation will compute using 1 as the weight of each edge.  I user could easily miss this mistake and use the resulting answer as if it was computed using edge weights.  Is there a reasonable way to check that the kwargs given are valid?
comment
I agree with your assessment! As @eriknw nicely described at the dispatching meeting, the extra/misspelled keywords will get put into backend_kwargs as you indicate, but when the dispatcher calls the original function, those extra/keyword args are sent to the "method" function which will raise an error for a misspelled or extra keyword argument.  So I think my concerns about silent misspelled keywords are not an issue.  (yay!)
comment
Will this still work with curried functions that don't provide kwargs to the outer function?  I would assume so.  Perhaps we can leave a test in the test module that tests both methods by currying some keywords and passing in the rest as extra keywords in the outer function.  Is there a reason not to do that that I am not seeing?
comment
Yup -- that makes sense... you'd have to add the keywords to the lambda parameters. ```python sa_tsp = lambda G, weight, seed=None : nx_app.simulated_annealing_tsp(     G, "greedy", weight, source=4 )  path = nx_app.traveling_salesman_problem(     G,     method=sa_tsp,     cycle=False,     seed=1 ) ``` I don't think there is any reason to do that in a test though....Thanks 
comment
Nice!  This fix didn't take long.  Fixes #5817   Just so I understand better --   1) it looks like you've changed from the `interior-point` method to a default method.  Does that have any implications?  2) You nicely switch `arb_count` to be tracked via an `enumerate` call.  But it looks like you left the increment statement for `arb_count` in the code. It doesn't affect anything I believe, but maybe that line should be removed too.
comment
Yes -- when I said "broken" I meant it breaks backward compatibility -- not crashing anything. Sorry for the confusion.  I understand the rationale for either stripping whitespace from every entry or none of the entries. stripping the whitespace on the start of the first and end of the last without doing anything in between is not consistent across entries in a line. I tend to lean toward the simpler approach of not stripping whitespace at either end -- just remove the newline. And that is what is in this PR.  But, I'm worried about backward compatibility: Do we need to deprecate the current behavior? And what idiom can we give those for whom the tools work with their current file to get it to work with the new code. It seems like trouble could only occur for folks with `node_type=None` and `delimiter` set to something. For example: `line = " b,a\n"` or `line="b,a \n"`. I guess we could deprecate with a warning to strip their lines like:   ```python with open(path, "rb") as file:     G = nx.parse_edgelist([line.strip() for line in file], delimiter=",") ``` Is there a better way?  Or does someone feel this doesn't need a deprecation?  Or   ;P    maybe we should just tell people to fix the current Issue by doing: ```python with open(path, "rb") as file:     G = nx.parse_edgelist([line.replace("\t", "@#$") for line in file], delimiter="@#$") ``` 
comment
Thanks for this! It looks like the same code idiom is used in `networkx/readwrite/edgelist.py` line 250. So the fix should be applied there too.
comment
While I did not expect this result to be backward incompatible, and I haven't dived into what format of sparse array we are using, it does make sense that the new iterating functionality in sparse arrays is backward incompatible.  It used to yield 2D row arrays and now yields 1D arrays.   I think this means that iterating over sparse arrays has been incorrectly returning 2D arrays for quite a few versions. But it should return 1D objects to match array indexing. This will need to be fixed or reconciled or deprecated or something in scipy.   Is there something we can do here to avoid the issue? Yes: Changing `x, y = np.nonzero(row)` to `y = np.nonzero(row)[-1]` should do it since we don't use `x` anywhere here. But there may be other similar code that needs correcting in other ways.   Thanks for flagging this @MridulS !
comment
And I should add that I don't understand why this code seems to be failing on some platforms and not others. The change in iteration results of scipy sparse `csr_array` from 2D to 1D should be the same across platforms.
comment
Before you put a lot more effort into this, NetworkX does not provide any cpp or pxd code. It should be pure python.
comment
I agree that this is ready for reviews/approvals...    The tests seem to indicate a problem on the circle-ci build action with being unable to update apt-get.  That has nothing to do with this PR. I will hopefully be able to get to this PR next week.
comment
Maybe we can go ahead and merge this PR and open issues for testing the spring-like layout methods more thoroughly and perhaps also for having a uniform approach to `seed` arguments when the seed can be set manually for the input `pos` if people want to set a seed.   I think the test errors come from our `black` linter and can be fixed locally by installing black and running it and committing the results.
comment
Thanks for this! --  I think there are some missing words in one section. Would you prefer to fix them or I think I can make the changes.  The attracting and repulsing forces description seems backwards to me. Don't high degrees attract? Maybe I just need to look at the formula in the paper again. It's been a while. 
comment
I'll try -- but it looks like Circle CI is having trouble with apt-get. So that may be bigger than we can deal with other than a wait-for-a-fix approach.  The conflicts with the main branch are definitely within our control though. And those fixes might help the CI if they affect setup, which it looks like they might.
comment
Looks like a merge from main and a bunch of lint tweaks cleaned up the tests. You will need to pull it down to your local machine of course before making changes.
comment
You might not get notified when people give thumbs up! to your comment, so I'll post that there are 3 thumbs up for @Peiffap suggestion to update the PR accordingly. :)  Thanks!
comment
Thanks for this! We have trouble with personal websites becoming non-existant too.  What do you think of:  Cooper, Keith D., Harvey, Timothy J. and Kennedy, Ken. "A simple, fast dominance algorithm." (2006). https://hdl.handle.net/1911/96345
comment
In what sense is the graph invalid? You don't show any error or what the value of `idx3` is.  What are you expecting and what do you get?  I get the response `3`...  Which is what I expect.  
comment
Thank you for this careful description of the contributor experience and for your thoughtful suggestions. This is a relatively new feature that touches so many parts of networkx. We have been working to get it up and running and have also been making changes (as problems arise and sometimes before they arrive) to make the ride smoother. The above concerns are very helpful for us to spend some time on.  In terms of names, perhaps `@allow_backends` works too. But I've sometimes found the grammar of the keyword args confusing too:  e.g. `edge_attrs` vs `preserve_edge_attrs`. Which do I use to do what? Does "preserve" mean "make sure they don't change during the function" -- or "make sure they get converted to the backend representation of the graph".  And which refers to "all edge attributes" instead of holding a list of names of those edge attributes to be converted to the backend representation.  Luckily we're not locked into certain names -- this is all supposed to still be "experimental" and can change from version to version. But I suspect it is maturing to the point where it may get more difficult to change later... :}  
comment
There isn't enough information to know what the question is here.  Please edit the original post to explain your issue. (you can delete the default headings if they aren't appropriate)
comment
This issue is closed and we need more info. Can you open a new issue and give a small example with description?
comment
With #7432 merged, this should be rebased-on/merged-with main and will hopefully pass the dispatch tests now that nx-loopback preserves edge order when converting during the tests. I don't think there will be any conflicts on the merge, but if it is messy let me know and I'll take a stab at it. (`git merge --abort` is helpful if it gets messy)
comment
What I think this discussion is leading toward is the following: - the `compute_v_structures` function should appear in the main namespace raise a deprecation warning and call `nx.dag.colliders` - the two new function names `colliders` and `v_structures` should not appear in the main namespace, and instead appear only as `nx.dag.colliders()` and `nx.dag.v_structures()`. That is, `colliders` and `v_structures` should not appear in the `__all__` dunder variable of `dag.py` . - all three functions should show up in the docs -- and the old `compute_v_structures` doc_string should include a deprecation statement in its doc_string.  
comment
It looks to me like the doc_string does not match the function behavior. The function seems to compute triples where the two outer nodes have directed edges to the middle node. It doesn't perform checks on connections between the outer(parent) nodes.   Unfortunately the tests do not include this case where the two parents are connected. And the wikipedia reference doesn't actually use the term `v-structures` as far as I can tell.  Looking at the code, no relationshpi between the parents is explored. It is only finding triples with edges directed at the middle node. And it actually won't work if the nodes are not sortable -- which we should have caught upon review.  This function is not used elsewhere in NetworkX including in the causal DAG parts of the code.   @maichmueller can you describe how you are using this function? Should it be removed?   @adam2392 do you have any thoughts about the `compute_v_structures` whether it should examine connections between the parents and whether this function is useful now that we have the d_separation code (that doesn't use v-structures)?  Thanks!
comment
I thought scipy requires numpy...
comment
Thanks for this!  The docs are indeed incorrect here. It should return a number or iterator, not specifically an `int` or iterator.  It goes all the way back to #744 before which it is correctly called a number.    NetworkX supports `float` edge weights with the usual caveats of floating point issues. Thanks!
comment
We dont assign issues to individual developers -- you can start looking at an issue. When you have a solution open a PR. Or open a PR with a partial solution and we'll give feedback. (actually we'll give feedback either way. :)  This one might be a good one to start with. To start, you should read through the developer section of the docs, especially the contributor guide.
comment
Thanks for this!  You are correct that the copy line is not needed and should be deleted.  You comment about subclassing `nx.Graph` suggests that it is important to override the `copy` method when your subclass has additional init parameters. Removing the `copy` command from `bridges` will let your example code run, but anything else that uses the `copy` method will not work. I'd suggest overriding the copy method. Another approach would be to create your subclass and have it hold the graph object as an attribute of the new class rather than be a subclass. But that depends on what else you do with your subclass. :)  Thanks! A PR that removes the `H_copy` assignment would fix this issue.  
comment
Floating point numbers are not good choices for problems where equality is checked or required. Our standard advice is to change the weights to integers (by multiplying by e.g. 1e6 and rounding?) I'm not sure if it will work in your case, but it's worth a try:  ```python pae.ravel()[:] = round(d * 1e6) ``` and then further scaling of `distance_cutoff` and `weight=1e6/pae`.  It's hard to avoid the trouble with equating floating point values. You could also create your own subclass of float by over-writing __eq__ and __lt__ to provide some tolerance for equals as well as less than and greater than. But that's pretty extreme. 
comment
Given that floating point errors are really tricky to try to debug, I don't think `dq` should be involved in a dict key lookup.  The heapq documentation about how to make heaps with the ability to update the priority say that the dict should map the element to its position. I think we should do the same.  Given that we have a number of Issues about this function and in fact this very issue, I would like to make a 2.6.3 release so they are fixed.  Let's work on performance for the 2.7 release.  It would be worth trying the heapdict package to compare performance too.  It would require a dependency but how much speed difference is there?  BTW, what is a good test for the speed?  erdos-renyi graph with 500 nodes and p=0.5?
comment
To summarize (hopefully correctly). There are multiple ways to define uniformly chosen random directed trees in the literature. Even the definition of a directed tree varies (arborescence vs polytree for example, see [wikipedia trees](https://en.wikipedia.org/wiki/Tree_(graph_theory))). Each definition of directed tree has a different set of probabilities due to different restrictions on directions of edges. So, if we want random directed trees we should at least use separate functions for them -- and probably multiple separate functions.  The above-linked comments do mention that we could include an example in the doc_string for turning the random tree into a DFS-oriented directed tree (an arborescence). I don't see that. But I suspect that is probably the best way to help users who want to replicate what was previously available via `random_tree` with `create_using=DiGraph`. It might also be good to mention the difficulty in defining a uniformly chosen random directed tree, though it might be hard to write it in a short yet understandable manner.
comment
I agree with @rossbar that this check doesn't really have a down side -- and it allows a non-prime p to construct a graph in the same way that a paley graph would if p was prime. Since we're not checking for p prime, this seems like a good approach.  I'm going to close this, but  the conversation can continue (and PR reopened if needed). Also a new PR/issue can be opened to improve docs if desired.
comment
Does it crash when run in an ipython shell? Or is it only in the Jupyter notebook?  That code works for me in an ipython session, but my config is different from yours.
comment
Do we need the warnings in `to_networkx_graph`?  I think the warnings were intended to help a user who didn't realize that they had forgotten to install e.g. pandas in their environment. That they input a pandas DataFrame but didn't have pandas installed.  But it seems like that case is not possible. If `pandas` is not installed, how could the user get a pandas object to input?   Is there a way for users to input an object intended for the pandas conversion code and not have pandas installed?  If not, is there any danger in replacing the warnings with `pass`?
comment
I'm going to merge this cause it is only affecting the docs.  Thanks for this!!
comment
Can you describe what impact this has on your projects or how this helps you? Storing information in two places is potentially problematic if one source changes. By caching the length value we need to ensure that the `iter` process remains the same, or that the cached value gets updated.  ~~so… what’s the problem you are running into that this will solve?~~ [Edited:  I added a link to Issue #7377 that describes the problem and how this fix might help that problem.] 
comment
I've edited my previous comment and the OP to show the link to the issue where a good description of the problem is contained. Thank you for this bug report and potential fix.   I will need to check for ways that this cached value might be out of sync with the underlying data. Could we e.g. use `len(self.NODE_OK.nodes)` when that exists? When NODE_OK is a function we might need to fall back on the current iterator based approach.  **Another approach:** I suspect you will get better response time if you copy the subgraph as soon as you create it using `G.subgraph(...).copy()`. If you have to look at edges more than once this will usually be faster.
comment
Do you have any suggestions for how to make sure that an arbitrary user's functions are not resulting in a change in the length of the view?  I don't see that we can guarantee that. And so caching the length may not be correct.  As for copying the subgraph -- remember that it is only copying the subgraph, not the full graph. In terms of time, it should be faster if you end up looking at the edges more than once. In terms of memory, it will require an extra size of the subgraph -- not the size of the full graph. You might want to try it and see what the impact is.  As another workaround, maybe you can store the length in a cache that you create (assuming that you know that your lengths will not change).  ``` SG = G.subgraph(node_ok_func) SG.length = len(SG) ``` If you don't want to store it on the subgraph object, could you store it somewhere else?
comment
Here's another approach that might work.  Since `NODE_OK` is either a bunch of nodes or callable, we could compute the length based on `len(NODE_OK.nodes)` when NODE_OK has a `nodes` attribute, on `NODE_OK.length` when `NODE_OK` has a `length` attribute. And when `NODE_OK` has neither attributes, we iterate to find the length.   Users who define the length attribute are responsible for updating it when the set is a dynamic quantity. The length is stored on the object that defines the set of nodes (i.e., close to the data it describes).  Thoughts?
comment
Thanks!   Also, based on the test-failure that's showing up:  we don't include type annotations in the NetworkX source code. People are working on type stubs elsewhere. But for this PR please take out the annotations. :)
comment
I think the `__iter__` function works fine in that case, though perhaps slower than necessary. If `NODE_OK` has the attribute `nodes` then that attribute is a set of nodes so we can compute it's length. and the `len` computation is done. If `nodes` doesn't exist then we iterate, calling `__iter__` which checks if `nodes` exists, which it doesn't, so it calls the `NODE_OK` function for each node to determine whether that node is in the `_atlas` being considered.  So, in the case with  a `nodes` attribute, we are fast, and in the case without `nodes` we are accurate, but we check for the `nodes` attribute twice.  Does this make sense?  To speed it up, change the last line of your change to do the iteration directly instead of having `self` do it. We've added enough to these `__len__` methods that I feel iterating directly makes sense.  So could you replace the last line of each of the two sections with something like?: ```python     return sum(1 for n in self._atlas if self.NODE_OK(n)) ```   Thanks!
comment
Why are we changing the first part of the `__iter__` methods?  It's not clear to me that they are the same. And without a check of the performance difference I'd be more comfortable leaving them as they were.
comment
I added some comments and also expanded t he docstring of  the `show_nodes` filter. I'm sure it is not sufficient for easy understanding -- this code is pretty poorly documented. But I hope it helps explain the optional `nodes` and `length` attributes on node filters
comment
Thanks for this -- nice catch and thanks for tracking down the bug!   Unfortunately, switching `copy` to `True` in the `relabel_nodes` call can be expensive for big graphs and is backward incompatible (as the failing tests show).  What we want here is an option:  `copy=False_if_possible`. Or renaming, something like: `in_place_if_possible=True`. But I don't like either of those names.  But we can build our own in this code rather than changing the function signature of relabel_nodes.  ```python     try:         return nx.relabel_nodes(G, dict(enumerate(df.columns)), copy=False)     except nx.NetworkXUnfeasible as err:         return nx.relabel_nodes(G, dict(enumerate(df.columns)), copy=True) ``` The exception is raised before any relabeling starts, so this is safe from that perspective. Can you check that this does what you want it to do? And then make the change and push it up to this branch?  Thanks!
comment
I like the idea of `from_numpy_array` supporting a nodelist kind of argument! Especially if it could be done in an integrated way with the graph creation rather than relabeling nodes after-the-fact.  I think this is also a very small step toward @eriknw's interest in forming a standard binary/array-based representation of a graph because connecting the nodes to arrays is a key part of that IMO.
comment
This is expanded on via the suggestions @rossbar makes above.  I will close this in favor of #7412.     Thanks very much @katerinakazantseva for this improvement!
comment
Has anyone tried just removing the import and the `isinstance` check?  The call to e.g. `from_pandas_*` should raise an exception -- I suspect due to a missing attribute most of the time. The questions is whether that messes up the input object.  The type-based dispatching here didn't used to be the mechanism used. And if we don't use it we do get rid of the import being tried upon each construction.  **BUT** is that really the problem here?  The reason **for** using e.g `import pandas` as a way to check if the input is of pandas type is that importing a package that isn't installed is pretty fast.  It shouldn't matter even if you do it 1000 times. I suspect that in the case where 90% of the time is spent creating the warnings that is primarily due to the fact that the test is so small. The warnings are not taking very long on an absolute scale, and if you were doing a bigger task you wouldn't notice the time they take. But I haven't actually checked it -- that's just my intuition.  So, I will ask a perhaps silly question @albertferras-vrf : Is the problem that the failed imports take too long, or that the warnings pollute the log?  To stop the pollution of the logs, can we set a warnings filter so they don't go into the log? Does that solve your issues? Or is it too slow?  And, there is, of course, a way to write the code so the import is tried at the module level with different versions of the function being defined based on what can be imported. But I suspect that makes the code pretty ugly. So, let's identify the problem first.  Is it the speed or the logging pollution that is the issue here? Or are there two issues? :)
comment
Yes -- good thought! And a user may have written their own function. so we shouldn't just raise if we don't have one of the included flow functions.   The history here is helpful (and not apparent from the code -- but maybe from the "blame" or comments like this one). Originally the cutoff parameter was only present for some of the flow functions. This code was written and some elif didn't have the cutoff parameter. Then there was an effort made to ensure all flow_functions supported a cutoff kwarg. That effort was successful. And this logic was updated but not simplified.   I guess an even simpler way to handle this is to include `cutoff` in the kwargs dict in the previous line where it is defined. Then it is just one if statement.  Users who write their own functions already have to have the other kwargs at least in their signature (they can ignore them).  So I guess that would put `cutoff` into that category. They'd have to at least put it in their function signature (or have a ** parameter). They don't have to do anything with it though.
comment
I agree that this doc_string should be improved.  The best description of graph edit paths I have found is buried in the doc_string of a function defined within a function -- and thus not available outside of the source code.  Perhaps it can kick-start a better description for the main doc_string.  <this is from the `get_edit_paths` function defined in `optimize_edit_paths`> ```       Returns:             sequence of (vertex_path, edge_path, cost)                 vertex_path: complete vertex edit path                     list of tuples (u, v) of vertex mappings u<->v,                     u=None or v=None for deletion/insertion                 edge_path: complete edge edit path                     list of tuples (g, h) of edge mappings g<->h,                     g=None or h=None for deletion/insertion                 cost: total cost of edit path             NOTE: path costs are non-increasing  ```
comment
oops -- spoke too fast -- The code looked good, but the tests show that something is wacky...  Probably a regex issue.
comment
This has (not surprisingly) some of the same traits as #7166 for shortest_path. I think that's because both are taking logic made up of lots of if/elif/else clauses and rewriting it using match clauses. - the code is basically the same as before -- no real advantages to having it use `match`. - the code no longer "reads like english".  yes, I know that if/elif/else isn't english and match/case isn't english either. But other than the abbreviations `elif -> else if`, and `else -> otherwise do this`, the if-style is set up like an english sentence. That is, it is easy to read, though sometimes hard to hold all the cases in your head.  In contrast the `match/case` doesn't have an english sentence structure to mimic. Presumably it can do more complicated case-logic and/or variable definitions with less code. But at least so far, the advantages are not obvious over the classic style.  I'd like to find cases where there is value added to switching to the match/case style. This feature was resisted by Python for a long time. As I understand it, the advantage of different syntax was not obvious and it made code more cryptic. It didn't fit with the beauty aspects of the language. Python eventually let this version of match/case into the language because it can do things if/else can't -- or can make the logic more simple. It's not a replacement of if/else. Where the if/else is straightforward, we should still use it -- it's better. The new feature has potential advantages -- what are they? where  should it be used?  Of course, the only way to find the advantages is to try it...  So this PR is a useful exercise.  Let's keep trying it in places where the if/else is cumbersome. Look for advantages of match/case.   But I don't think this case shows improvement over the existing code. Which would you rather try to read? If you come across any places where you think there is advantage to using match/case, let's try to generalize that to a rule-of-thumb for using match/case. :) 
comment
Well -- you can use it now via the branch of the PR, but I think you mean: when will it be merged with the NetworkX repo?  We like these functions and the implementation, but are looking for time to review it. It will happen "soon".
comment
That is a reasonable result.  If nodes in the graph are not connected then you can't compute all shortest paths between all pairs of nodes. There is no path between the nodes. So the function raises an exception.
comment
Point taken -- it is certainly not an optimal result if you want the partial results given by the pairs that are connected. As you point out, the current `all_pairs_shortest_path` functions don't raise an exception in these conditions. I'm +1 for making that happen with these functions too.  Thanks for your input @wsh2836741  
comment
No need to open a new PR. Its a busy time of year, but we'll get to it. Thanks!!
comment
The current Johnson method doesn't handle ties.  I'm not sure whether it is worth adding it here. If you want to, go for it. But if you'd rather get this merged, we can put that in later.   - Can you remove the "print" part of the examples? The >>> form in the examples prints the results so no print statement should be needed. That also makes it easier to read those commands on the doc-pages. - Many of the doc_string lines are longer than 88 chars.  I can shift this, but if you want to you could  do it.  Looking good!
comment
Rats!  I didn't look at the PR closely enough.  For each of the new functions you should include an entry in the appropriate `.rst` file in `doc/reference/algorithms/shortest_paths.rst` If the pattern from the list of other functions is not clear, ask questions (here or in a new PR).  Those changes should be made in a new PR. The tests in the new PR include a test that builds the docs, and another test that loads the html pages to a temporary url (the test appears as "document artifact"). Look on the "Details" link for that test to go to the documentation webpage with that PRs changes included.
comment
It should return all shortest paths. It could be that you are running into round-off trouble or maybe another subtle distinction. Can you give a small example that has at least two shortest_paths but only returns one? Then we can track down the cause.
comment
Ahh... you are using `all_pairs_shortest_path` which finds a shortest path between all the pairs of nodes in the graph.  So it's not finding all paths. It is finding all **pairs**.  Sorry that I missed that in your original post.  You want:  `list(nx.all_simple_paths(G, 1, 3))`. The `list` is needed, because there may be a huge number of simple paths and so `all_simple_paths` is a generator which yields results one at a time.  If your graph gets big you should probably use: ```pythohn for path in nx.all_simple_paths(G, 1, 3):     # handle each oath one at a time ``` so you don't run out of memory making the list.
comment
How about:  `nx.all_shortest_paths` https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.all_shortest_paths.html
comment
Very good -- that would perhaps be a nice new feature.  If you click on the "Source" link at the top of the documentation link, you can see that most of the work is done by calling djikstra's predecessor_and_distance function. Then there is a helper function to build the paths from the predecessors.  That's the sticking point.  It might be fairly easy to rewrite that helper function for your special needs -- then call the dijkstra function and your special function to construct the multiple paths. We'd love to include such a function and make a `single_source_all_shortest_paths` kind of function. It would then be easy to create in turn an all_pairs version of that. Thoughts?
comment
I used to know the specific defn of lexicographic BFS, but I have recalled my innocence. :) I think it might be that the order of the nodes within each level of the BFS are sorted in a lexicographic order.  If this is true, can we use the `sort_neighbors` feature of the NetworkX bread_first_search functions?  Also, I'm not convinced this should replace the existing method for `is_chordal`. I get that it should work. But the old method worked too. Is there some reference or computation that claims this is a better method for computing `is_chordal`? I'm fine with making this PR just be about the `perfect_elimination_order` stuff -- and having the change to `is_chordal` be considered in a separate PR. The first functions will almost certainly get merged faster that way.  Thanks for this!
comment
I guess we should add a sentence to the warning message about how to handle cases where they do want to mutate the graph directly. All they need to do is clear the cache.  So perhaps add a final sentence like:  ``` If you manually mutate the graph and you use caching, you should probably use `G.__networkx_cache.clear()`. ``` 
comment
I like the idea of a function to clear the cache so if we change how we want to handle it, the changes in code will be less widely spread. Should that be a method on the Graph? That's keep the function close to the data it is manipulating. But I'm open to a function if there's an advantage to that over a method.
comment
I think we are running into a problem of expectations. Each of us (Rick, Erik, Dan and Mridul) have different ideas of what the cache is supposed to store. We're confounding storing graph objects, storing function results and storing partial results that could be used for future computations. While it might be possible to provide all of these with a single caching mechanism, it is also possible that it'd be better to provide separate mechanisms for the different types of caching.  As for caching converted graphs, the difficulty as I understand it is that some backends -- like cugraph -- convert all the node and edge attributes into the new structure, while other backends --like graphblas -- only convert one edge attribute.  So some will naturally only want one version of the graph cached while others will want an option for more than one cached representation.  Yet others will likely want to cache partially computed results.  I'd like our caching mechanism to be cross-backend friendly. That is, one backend should not interfere with another's cache.  Then each backend is responsible for updating their cached objects. I guess I'm envisioning a system where NetworkX provides a way for the backends to mark each cached object as needing to be cleared when certain NetworkX graph mutations are made. Then the backends don't directly clear the whole cache. They clear "their" stored objects as needed. And NetworkX clears cached objects from any backend when graph mutations are made. Which cached objects? The ones that have been tagged as being cleared for that mutation.  For example: we could have a dict of cache keys to be cleared when nodes are added/removed, another when edges are added/removed. I'm not sure how to handle the cases when node/edge attributes are changed.  I think the current setup tries to clear the whole cache when any mutation is made. And that might be the right implementation. But we will be losing the opportunity to cache e.g. a graphblas matrix representation for attribute "edge_length" when we update the attribute "edge_capacity".  Is this a big deal? I don't know. :)  But I guess the first check of our design could be "does it work well for cugraph, graphblas and an imagined scipy_sparse backend"?
comment
I agree with all those comments @eriknw  The goal here is to get a minimal caching interface in for 3.3. I was responding to what @rlratzel asked about caching once for each graph vs potentially caching more than once. And then I got carried away with looking forward.  I have been (hopefully) more productive too.  I went through your list of dispatchable functions (~4 comments up). Here are the ones I think are really just helper functions.    Two groups of such are:  1) functions ending in "_recursive". They are the same implementation as the version without "_recursive".  The signature should be the same. The algorithm is the same. So I don't think there is any advantage to making them dispatchable. (might be no harm either... but could lead to confusion later.) 2) functions ending in `_numpy`. They are using a matrix manipulation algorithm rather than graph traversal. But the signature should be the same as the version without `_numpy`. Here, the algorithm is not the same. So perhaps there is a reason to keep it dispatchable. But in the spirit of having backends provide the algorithm for a given function signature, if feels like these shouldn't be dispatchable to me. Again, there might not be harm in making them dispatchable except that somebody writing a backend might try to implement them both.  To be clear, I'm not saying these need to have dispatching removed. I'm saying that these are functions I flagged as not needing dispatching because they either duplicate another function (e.g. `_recursive`) or are really just helper functions.   My list of functions (including the module info on previous line): ``` │   │   ├─ treewidth  │   │   │   ├─ treewidth_decomp   │   ├─ asteroidal  │   │   ├─ create_component_structure   │   ├─ clique  │   │   ├─ find_cliques_recursive  │   ├─ coloring  │   │   ├─ equitable_coloring  │   │   │   └─ pad_graph   │   ├─ planarity  │   │   ├─ check_planarity_recursive  │   │   ├─ get_counterexample_recursive ```
comment
I have approved this PR and my recent postings only continue conversation. They don't add to anything needed for this PR.  It sounds like @rlratzel looked through this fairly closely too. 
comment
@rossbar can you take a look at this PR?  Some attendees of GTC have requested more info about the caching data shown in Rick and Mridul's talk there. So, it'd be good to get this in.  @jarrodmillman once this is approved and merged, can we make another rc (or a full release -- though we probably need to get the numpy issues working for the full release... If that starts taking a while (more than 2 more weeks or so?) maybe we can push those into 3.3.1 or something.)
comment
Is there a reason to split out the deprecations.rst change from the PR itself? Not a big deal, but it seems easier to keep them together -- but I might be missing something.
comment
Yes -- please: - include the deprecations.rst change in this PR - add a note in deprecations line of doc/release/release_dev.rst - add a test of the deprecation (see below)  The tests seem to need more work... I don't see any tests of the `generic_bfs_edges` function. Adding them could be done here quickly by copying the `bfs_edges` tests and revising them to run `generic_bfs_edges`. But there is probably a better way to do that. In the interest of getting this PR merged, let's bump the job of creating tests for `generic_bfs_edges` to another PR.  The test of the deprecation can look something like: ```python         with pytest.deprecated_call():             nx.generic_bfs_edges(self.G, source=1, sort_neighbors=sorted)  ```
comment
There are two existing functions to bring pandas data into networkx format. `from_pandas_edgelist` forms a graph from a dataframe where the dataframe is storing each edge as a "row" in the data frame. `from_pandas_adjacency` forms a graph from a dataframe where the dataframe is storing the adjacency matrix in a dataframe form. Both of these function deal with creating a graph based on edges stored in a dataframe.  This new feature should allow addition of node attributes that are stored in a pandas dataframe. Since the one-line code to do so is so short, we thought it would be best to add a note about this feature to the doc_strings of the two `from_pandas_*` functions. So, instead of making a new function, we are looking for an addition to the doc_strings. But if someone thinks it should be a new function this is a good place to discuss that.  In the doc_strings, this isn't really an example and it is not a straightforward "See Also" (which usually points to a function). This is more like a "tip for users". So I think it could go between the "See Also" section and the "Examples" section. It should say something like: "If you have node attributes stored in a dataframe ``df`` (separate from the edge info dataframe), those attributes can be loaded to a graph ``G`` using code similar to: `G.add_nodes_from((n, dict(d)) for n, d in df.iterrows())`."  Whatever the format and language is for that note, it can appear in both of the two functions described above.  Is this more clear?
comment
The second code block you sent works on my machine. I get G to be an undirected triangle on nodes 1, 2, 3. What is your question?
comment
Yes, that line to build the `df_nodes` looks good to me. :}
comment
The axis can be set however you like after the function is done -- right? We usually try to set good defaults but let people change whatever they like using the fundamental matplotlib tools. We usually try to avoid adding keyword args which replace matplotlib code because our code would soon become filled just reproducing all the possible options matplotlib provides. Is there something I am missing here?
comment
I'll take this as a request to change the default then. I don't want to get into the business of providing the options that matplotlib provides. Long long ago there was a version that kept the axis ticks on by default.  But we changed it to what I think might be a better default. I think that the numbers on the axes are usually irrelevant and get in the way.  Perhaps we should put a note in the doc_string (or somewhere else better?) telling people how to do this?  Or maybe StackOverFlow is enough?   Thoughts?
comment
I can "justify" accepting this change along-side our general policy of not implementing matplotlib style features within our code kwargs as follows:   This `hide_ticks` kwarg makes our already existing non-default style choice for matplotlib optional. Our existing default behavior of hiding ticks was implemented after many years of showing ticks which was problematic -- users couldn't find how to turn them off. At that time, we debated whether it was worth turning off the ticks -- it is the better look for most pictures of networks. Turning them off is fairly involved and you need to know details about matplotlib to do it. But it goes against our preference for leaving style choices to the matplotlib interface. In the end we decided to implement this style choice even though users could make that choice via (somewhat involved) matplotlib commands.  This PR allows users to turn off that style choice. So I am supportive of this PR.  Thanks @rossbar for your work with this (including bringing it to the community meeting agenda).  And Thanks @tillahoffmann for the PR (sorry this took so long! :)
comment
Also, what do people think about moving the backend documentation out of the "Utillities" section and into its own section?  The Utilities page is getting pretty long and it might be nice to have a page just for Backends that we could point people to. 
comment
As I hinted above, my knee-jerk reaction about subclass-ing dict is based solely on my experience with performance issues. That experience may not apply here. For example, maybe performance doesn't matter for the config dict.  Let's see...   Are there guidelines for when to use an attribute and when to use a method?  It seems like an attribute is documented in the class documentation while a method has its own documentation. Why is that? Maybe something like attributes tend to be data structures/values, while methods tend to provide functionality.  Do you have some experience with that kind of rule-of-thumb? My intuition is to have a dict attribute and document that in the class documentation. But I could easily be swayed here. 
comment
@rossbar what do you think about having `nx.config` but having the backend related keys in a dict `nx.config['backends']`?  What are the tradeoffs between using subpackages and dict hierarchies. NetworkX has leaned toward flat namespaces with "dicts all the way down" for storing/organizing data. I can imagine config options for visualization, for backends, and for some of the more complex algorithms, but probably only one of two of those. So there shouldn't be many config options overall. Are you worried about namespace collisions, ease of finding the options, documentation, or "all of the above"? :)
comment
I think the original description is correct.  Each node's label is created by combining the node labels of nodes within i-hops of the node.  Since those nodes at distance i have their labels created from nodes at distance i, the node labels are influenced by all nodes within $2i$ hops of the given node. The fact that the doc_string used 2i instead of i in so many places made me suspect that the 2i is correct and not just a typo. My investigation leads me to think the 2i is correct.  What do you think?      If it really should be 2i then we should change back the switch in that text from #6598.  Other changes in the doc_string may or may not need adjusting too.  And maybe we should add enough text to make this aspect of the description more clear. 
comment
So, you are saying that each iteration only collects the labels from the neighbors... and not the neighbors-neighbors when i=2 and the nbrs-nbrs-nbrs when i=3... etc. OK. clearly I misunderstood.  Another change that would be helpful is to go through the doc_string and use `i` for the iteration number everywhere. I think that `k` and `n` are used where `i` would be more consistent. And `n` is already being used for a node so shouldn't be used for the iteration count. (and maybe I've got this wrong too and they should be k, n, i but it looks strange to me.)
comment
Sorry this has taken so long to get back to -- and Thanks for responding so quickly to the new Issue! :)  It looks like this PR is now conflicting with main in one of the doc strings. Can you merge/rebase the main branch into this one?  Go ahead and make the changes needed to fix #7330 and flag me -- I will try to make it a fast review this time. :)
comment
Closed by #6601
comment
Thanks @ebokai for this great reproducing example code! 
comment
The error message seems to say that you can use colons as long as you put them inside double quotes. I interpret that as meaning you need your string to have this line: ``` - "brand:Honda" ```  This is a workaround for a problem in pydot.  If you can use the pygraphviz library instead it would not have that restriction. NetworkX has a similar interface to both those routes to graphviz. :)
comment
I think the random number generation in asadpour is fine. But calling `traveling_salesman_problem` doesn’t present a way to provide the seed to the asadpour function directly.  One of the tests shows that it can be called with a seed by overriding the default method with a function that calls asadpour with a seed.  But…  the test that is causing troubles doesn’t do that. So no seed is set for that test despite the comment saying it is.  Perhaps a previous version did call the seed.  I think the easy fix is to change the test to use asadpour with a seed. Longer term maybe we should provide a seed argument to traveling_salesman_problem. But I’m not sure.  If we don’t do that we should provide an example in the doc_string of how to call it with a seed.  Thoughts?
comment
Whoops!  Sorry @mjschwenne I stand corrected. The seed is set in the test function. Not sure what I was looking at. :}  I have extracted the test function and run it many times with the same results. I have also added a print statement to show the next random number generated after it finishes and I get the same number from (0,1) many many times within a for loop. I'm not sure how best to debug this. @jamesjer can you repeatedly run this code in the environment the test is run in?  ```python import networkx as nx import numpy as np edge_list = [         (0, 1, 100),         (0, 2, 100),         (0, 5, 1),         (1, 2, 100),         (1, 4, 1),         (2, 3, 1),         (3, 4, 100),         (3, 5, 100),         (4, 5, 100),         (1, 0, 100),         (2, 0, 100),         (5, 0, 1),         (2, 1, 100),         (4, 1, 1),         (3, 2, 1),         (4, 3, 100),         (5, 3, 100),         (5, 4, 100),     ]  G = nx.DiGraph() G.add_weighted_edges_from(edge_list)  def fixed_asadpour(G, weight):     return nx_app.asadpour_atsp(G, weight, seed=19)      for i in range(15):     nx_app.traveling_salesman_problem(G, weight="weight",method=fixed_asadpour) ```
comment
The design seems to be that changes to the `colors` dict [occur between the yield](https://github.com/networkx/networkx/blob/9cc8b422a512e7e7819238d597fd6815ec9c1c8e/networkx/algorithms/coloring/greedy_coloring.py#L381) and the `next` call to the generator. The generator does not have control over that logic. But the calling function does.  Note also that it is "ok" to have a generator that never terminates (e.g. `all_prime_numbers()`).  Is there a better way to handle this coding situation? Or maybe I should also ask -- what problem does this cause for dispatching?
comment
I think this points out how interrelated this function is to the calling function. It might make sense to make these strategy functions private. But they have been public for quite a long time.  Could we create a new category of functions -- those that are public but for which the dispatching is not appropriate?  Also, why don't the other strategy functions give this same kind of trouble? It looks like none of them are "infinite generators".  But we should support infinite generators (IMO)... I think.....???
comment
This code uses a numpy random interface. Not a python random interface. So we should use `@np_random_state(5)`. The documentation line about preferring `PythonRandomViaNumpyBits` for new code is supposed to be referring to the choice between numpybits and the old `PythonRandomInterface`. That choice determines how we use numpy random when the code uses the python random library.  Those classes wrap the numpy RNG to make it follow the Python random api.   In this case the code uses the numpy random interface. So we should just use `@np_random_state`. No wrapper interface is needed.  Then replace `np.random.choice` with `seed.choice`.  I think this is correct and in alignment with our expected behavior.
comment
I think your proposed solution is fine for this PR.  I'll put this topic it in [the "community meeting"](https://scientific-python.org/calendars/) agenda for Wednesday. But I think changing all the np_random_state, or changing `create_random_state` are out of scope for this PR.  Luckily I think (based on [the numpy docs](https://numpy.org/doc/1.23/reference/random/new-or-different.html) that we only have to create 4 methods in a subclass of generator to make this work.    Then again, maybe we should just deprecate use of `RandomState` and get rid of a whole slew of issues. :)
comment
Could we create an issue that tracks the different suggestions for refactoring?  If/when that issue gets too long or disjoint, we could convert it into a NXEP with a file in `doc/developer/nexps` and make changes there via PRs.   :)
comment
See also [this function.](https://github.com/networkx/networkx/blob/f9170cc404bd4bd1c482e976f8f7677b1b60a1cd/networkx/linalg/laplacianmatrix.py#L150).   I think this might be a place where improving the doc_string of the existing function (and maybe even its name) could be better than the new function.  We can get the number of spanning trees by using the existing function with unweighted edges.  If that is really true and works, then an example in the doc_string of the existing function could show people how to get the number of spanning trees. It discusses it already, but maybe it could be made more clear.
comment
I'm +1 for the two proposed name/place changes.  `number_of_spanning_trees` is more clear, especially if there is a mention in the doc_string that this is related to the total spanning tree weight. And moving it to the `tree` subfolder also makes sense, though we could keep/add the function to the doc string for the laplacian module even though the code is not in that folder.  And let's cross reference in the "See Also" sections of the function doc_strings.  If moving folders makes deprecations really tricky, we can leave it in the laplacian subfolder and add it to the doc_string for the trees module.  Am I making any sense right now? :)
comment
Another way to get this in place perhaps quicker would be to add a new function `number_of_spanning_trees` with the code. and keep the deprecated function `total_spanning_tree_weight` around with a deprecation warning on it. People can use the new function name and when the deprecation ends we remove the old name.  The code should be in the new functions location with the old location's function simply calling the new one (perhaps with some restrictions on parameters if the new one adds features that the old one doesn't provide.  Maybe this comment should have gone into #7100 but I think this PR is the nexus for all these comments. @peijenburg are you interested in changing #7100 to adding a new function and changing the old function to a shell that does what the only old one did by calling the new one?  I could try to help with that if you need it.
comment
See #7100 
comment
Let's leave off the absolute value -- round-off error is something we have to live with for floating point computations. And who knows if someone might actually need to compute this for weights that give a negative result.  And it does make sense that the theorem's formula holds whether the matrix is positive definite or not, it's just not as clear how valuable the result is. Thanks!
comment
I'm working on yet another review... :}
comment
Thanks for these changes -- I will take care of the deprecations.  I wasn't expected the name change from root to leaf. I was expecting the name root to stay (`root` is a common term for trees, with a tree that has a root called a `rooted tree`). But the description of the meaning of "root" should shift from incoming edges to outgoing edges. The arborecenses include, for each node, a unique path from the root to that node.  The code currently computes the paths following successors (out-edges) and using `out_degree` of the nodes when computing the laplacian. Doesn't that mean the root is a node from which outgoing edges exist connecting it to all the other nodes?  Let's figure out what the code computes -- then we can align the docs with it.
comment
aha! Thanks for pointing out my error... I had the adjacency entry A_ij representing an edge from j to i instead of i to j. This is the reverse of how it is defined in NX. So when we take the out_degree we are summing each row instead of each column. That also sent me back to the paper you cited and the wording may be clarified there as well.  As you have said, the paper looks at roots that are "sinks" in that all edges from other nodes go toward the sink.  They are roots in the sense that every node has a path to the root. They emphasize this directionality by calling the graph a "sink-rooted digraph" if it has such a root. Of course, they could have written about "source-rooted digraphs" instead with similar results. The other name that they carefully qualify in this discussion is the analog of "tree" for digraphs: they discuss the number of "reverse-arborescences". The adjective "reverse" indicates that these are the arborescences of the graph with reversed edges.  I am fairly sure that the "sink-rooted digraphs" which have "reverse-arborescences" are completely analogous to "source-rooted digraphs" which have "arborescences".   The theorem in the paper gives the number of "reverse-arborescences" for a given "sink-root". But the same method applied to the reversed digraph would give the number of "arborescences" for a given "source-root". Reversing the graph changes the Laplacian Matrix in two ways: it transposes the adjacency matrix (off-diagonal entries of L) and it changes the degree diagonal entries to be in_degree instead of out_degree. Another way to say this is that the degree values come from the column sums of A rather than the row sums.  Switching the direction of edges and the out-degree to in-degree leaves L the same. And the sink-root becomes a source-root. So, the number of reverse-arborescences in a digraph given a sink-root is the same as the number of arborescences in the reversed digraph given a the same node as a source-root.  The terms reverse-arborescence and sink-root are the opposite direction of how we use the term root and arborescence elsewhere in NX.  For BFS and DFS traversal, if we start at a root, and move in the direction of the edges, we should traverse the tree.  The BFS and DFS traversals would end/backtrack at a leaf. There could be many leafs but only one root. Thus our roots are source-roots. And discussing this function in terms of reverse-arborescences makes the wording more convoluted, especially for people coming to this from other places where arborescences are used.  Can we switch to use the term "root" and "arborescence" for this PR to mean source-root and arborescence? I think that means we need to compute L using the in_degree (or `A.sum(axis=0)`). Everything else in the code would be the same. But we'd want to switch back to the term "root" instead of leaf. And for minimal changing of tests, we should reverse the directions of the edges in the tests.  Thoughts?     Thanks!
comment
The merge conflicts occur when another PR that touches the same code this PR touches is merged. That means `main` changes in a way that git can't figure out whether to use your changes or the other PRs changes. In this case a PR added a function `random_spanning_tree` which appeared on the same lines as this PRs changes. So I went through to tell git how to resolve those conflicts.    The changes I made do mean that you will need to pull those changes down to your local branch before making further changes though. Hopefully we are close enough on this that it won't happen again on this PR. :)
comment
Thanks!
comment
I think the aspect ratio comes into play anytime you take ratios of x-values and y-values. Think of it as translating from x-units to y-units in each of these ratios. For example, in the ```numpy.arctan2``` function: ```np.arctan2(change_y, change_x * aspect_ratio)```   The only other spot I found is when computing the perpendicular vector, but I might have missed something: ``` - change_x * aspect_ratio / change_y```
comment
See also #5882   
comment
I believe the changes in #7010 fix this issue of label placement along curved edges. Are there aspects of this we should address/highlight before closing this issue? Thanks
comment
Would you be able to add the example from #7291 as a test for this so we make sure it doesn't reappear? It should go in `readwrite/tests/test_graphml.py`. I don't see why there are errors about strongly_connected in the tests. They certainly didn't come from this change. Thanks!
comment
Is it possible that your main branch is from a month or two ago? If so, can you try to pull the upstream main branch into your patch-3 branch? (or into your local main branch and then merge that into your patch-3 branch)
comment
This example uses `nx.grid_2d_graph` which creates nodes that are 2-tuples of ints (positions in the 2d grid). So interpreting each 2-tuple of ints as a node, this suggests that the node cuts are (sets of) pairs of nodes. That could be true since this is a lattice structure -- that is, I think removing individual nodes won't create a cut. But removing pairs of nodes next to each corner or the grid would cut off the corner node.  The corners would likely be the smallest cuts possible. Any other part of the grid/lattice would need more nodes cut. So this result makes sense to me.  At least they are interpretable as node cut-sets.  Does this make sense?
comment
Could `__repr__` and `__str__` be the same as it was without the dispatchable decorator?  - is there any reason to include more? - are there side-effects from getting those from the wrapped function and attaching them to the wrapper?
comment
@eriknw could you approve this PR if it is good to go from your perspective? I don't want to mess up anything you are working on that this might touch.
comment
I agree that the generator nature of this function is sufficient to allow users to determine how many layers to compute.
comment
There is a lot going on with this code that is not part of NetworkX. And I'm guessing that a core dump is due to some other library rather than NetworkX.  But...  I do notice that you are making a subgraph (which is a view to the original graph). You should at least consider doing a test to see if it is more performant to add `.copy()` when you create the subgraph. Making the copy will make a new graph with only the edges of the subgraph. It obviously uses some memory to do that. But the advantage is that it no longer has to consider every neighbor in the original graph and eliminate those not in the subgraph. Instead it just has the nodes/edges in the subgraph. I can imagine a copy helping you here for speed at the cost of some memory -- or helping because the original graph does not need to be shared between processes (if you make the subgraph copy before splitting off the parallel process).   This suggestion might end up performing worse... but you should try some test runs with and without `.copy()` just to see what happens.
comment
This might be "just" a matter of setting a better stacklevel for the warnings. They didn't have a stacklevel before.
comment
It looks like one option given by the github interface (triangle menu on the green "Squash and merge" button) is "rebase and merge". Does that look like it does what you want to preserve the history?  I'm fine with you setting up testing here in this PR, unless that is more difficult for you. I'm fine with wading through the commits for review.
comment
Hmmm...  I see what you mean about the option being disabled -- not sure why I didn't see that before. Anyway, easy to do when the time comes.  Also -- I get what you are saying -- the code in the tutorial/introduction may not need tests.  Would that still be true in the notebook format? It seems like it would.  I guess the tests just check that we haven't changed the main repo results and just forgotten about updating the tutorial. I don't think having character-consistent output is a motivating factor though.  @jarrodmillman what are your thoughts about whether the tutorial needs testing since that functionality is tested elsewhere?
comment
I turned on the option to "rebase and merge" when merging a PR.  If that is a problem let me know and we can turn it off. But it should allow splitting of a PR for blame purposes. :)
comment
I'm sure there must be some difference between k_core and k_peak. But I can't find it.  Our [Cores docs](https://networkx.org/documentation/stable/reference/algorithms/core.html) say:  `The k-core is found by recursively pruning nodes with degrees less than k.` This PR describes k_peak as:  `A k-peak of a graph is a maximal subgraph that contains nodes of degree k or more within each other.`  What is the difference between peak_number and core_number? And what is the difference between k_core and k_peak?
comment
So, is the description in the k_core docs wrong?  `The k-core is found by recursively pruning nodes with degrees less than k.` The phrase "recursively pruning nodes" suggests that degrees are recomputed at each stage. The k_core function is more than just getting rid of nodes whose whole-graph degree is less than k.   Can you find better words to describe both what a k_core is and what a k_peak is -- so that the difference is clear?  I guess in a related topic -- do you have any suggestions for situations where someone should try k_peak as opposed to k_core?
comment
But the current code is a generator, so it shouldn't take much time (over the suggested code) to use: ```python (c for c in enumerate_all_cliques(G) if len(c) >= min_length) ```
comment
I guess I should also ask you to merge with the `main` branch so you can get the circle-ci tests to pass and we can see what the documentation looks like in the "artifact" test (by clicking on the details for that task). Thanks!
comment
Yes -- you are right that moving imports into the class body don't work. Sorry-- I meant to take that out of my post.  And by "merge main", I meant to merge it into this branch:  ``` git ch <this-branch> git merge main ``` I think it should just "fast-forward" the merge (leaving your commits the same but basically rebasing them onto the latest version of main) which will capture the new settings that allow the docs to build.
comment
The documentation circle-ci check is still not passing because a few of the gallery examples draw multigraphs but don't specify any connectionstyle. Can we make this backword compatible for drawing multigraphs? That is, "backwords compatible" meaning that it still runs and draws something similar to what it used to -- not that the drawing is exactly the same.  I like the self-loop rotation for a multiedge self-loop. The edge labels for self-loops in some examples are on the node (bottom of the loop) and I think they used to be on the top of the loop. Is that expected?
comment
Yup -- sorry I didn't before.  I found them by looking at the "Details" of the documentation circle-ci check. Then looking at the "Build Docs" log. The examples are:  examples/drawing/plot_unix_email.py examples/drawing/plot_directed.py  And the idea is not to fix those files, but to make the new function work with the syntax that is in those files. Otherwise this will not be backward compatible and will break other peoples code too.  We can make breaking changes but we need to deprecate for 2 release cycles and that's about a year. During that time we need a version that works with the old drawing functions. So I'm hoping we can just make it work with the old syntax. And if that takes a special case in the code, we can do that and remove that special case after deprecation.
comment
Let's see if we can get this reviewed over the next week or two. :} 
comment
We have talked about making an effort to get this merged in the last two community meetings. No promises of course, but we do really want this in -- we just haven't made the time to work through it yet.  My laptop wrist-rest now has a sticky note with "7010" written on it -- so that ought to help right? :}
comment
This looks good. The reasons for the generator approach are old and no longer relevant (sets were slower and _adj was only used in the base classes).  I am not sure that we need a deprecation -- it is true that the type of the return value is changed from generator to set. But that will only break code that is calling `next` directly on the return value. Anything using `len` or looping over the result will work quite nicely. Searching github for nx.common_neighbors shows no cases where it won't just work (of the first 3 pages of results -- I can look further). But I tend to lean toward "just putting it in", so I'm biased here. :)  Are there cases where a set would not work but in a silent way?  There are quite a few other functions in the `function.py` module that might be similarly sped up by large factors. That code has not been looked at since the sets have been improved and it's pretty clear that performance wasn't the intent in some choices (like using `neighbors(G, u, v)` instead of `G.neighbors(u, v)` in `nonneighbors`).
comment
The non_neighbors looks good too.   Do you want to change all the `G.adj[` and `G[` to `G._adj[` in this module? Or am I trying to put too much into this PR... :} Thanks!
comment
> This changes behavior. Graph.__contains__ has a try/except statement,  Nice catch!    I guess that raises the question of whether a single try/except with a loop inside is better than many try/excepts inside a loop. That should be a python dependent question maybe answered elsewhere -- but I didn't find in a quick search.
comment
Actually, `NodeDataView` is a subclass of `set` and it is supposed to act like a set when the graph data is hashable. But, as always, floating-point data can be difficult due to round-off errors. Thus the class is perhaps well described as a very finicky set in the doc_string: ```     The main use for this class is to iterate through node-data pairs.     The data can be the entire data-dictionary for each node, or it     can be a specific attribute (with default) for each node.     Set operations are enabled with NodeDataView, but don't work in     cases where the data is not hashable. Use with caution.     Typically, set operations on nodes use NodeView, not NodeDataView.     That is, they use `G.nodes` instead of `G.nodes(data='foo')`. ``` If your data values are hashable (say integers), doesn't that work?  Can you give an example where there aren't floating point issues or unhashable issues where a problem arises? This is similar to trying to use a python set on your data. In many cases it won't work for you.
comment
Yes -- it looks like the `contains` routine for that class is broken.  We test contains explicitly for `(node, data) in G.nodes.data(...)` because that is how is it intended.  But apparently only indirectly (in the `test_pickle` test) for `node in G.nodes.data(...)`. That test passes, but for the wrong reasons.   Checking `node in G.nodes.data('weight')` when `node` is in `G` currently returns `True`. So I guess this is a set with nodes allowed membership but never reported on the `__iter__` membership list.  That pretty clearly breaks the rules for sets.  I'm pretty sure that is not what was intended, but it was a long time ago and while I wrote it I can't vouch for any specific intentions here. Certainly, that code hasn't been exercised much. Luckily it is very unlikely to have affected users -- they would need to be checking for node membership (which includes equality as sets) in a node-data view. All of our examples and tests check node-data pairs for membership. As the name suggests, it should be all about node-data pairs.  That said, who knows, maybe someone has written their code to create a NodeDataView and then use it to check node membership in a graph. So, while this probably doesn't matter in practice, let's figure out: - should we return False for `node in G.nodes.data("weight")` when `node` is in `G`? - or should we raise an exception for `node in G.nodes.data("weight")` when `node` is in `G`? And do we deprecate this behavior or change it?  I guess I'm in favor of an exception, and an immediate change... but I could be persuaded otherwise. Switching from returning True to returning False could be surprising to folks who are using its current behavior. While raising as exception with a note that they probably wanted to check membership in the graph, or in a NodeView rather than a NodeDataView would be less silent -- and hopefully clear.  All of this has the usual caveats that we learn to live with in NX: if you have a node (1, 2) and you also have a node 1, you can run into confusing results. In this case, if node `1` has a node attribute with value `2`, the tuple (1, 2) is a node and also a member of a NodeDataView (and also potentially an edge if `1` is connected to `2`).  
comment
I'm not sure we should put much effort into this feature (not sure there is even a use for a set-like object of `(node, data)` pairs). 
comment
Unfortunately, it is best to read pseudo-code and algorithm descriptions and not to look at the GPL code if you suspect you will want to write a non-GPL version of the algorithm. It is very hard to make sure that code you write is not based on GPL code if you have read that code. That said, if you can clear your mind and start from scratch and build your code only using other sources (and not memories of the GPL code). Then you can probably get that to work in a courtroom.    Perhaps someone else has a better description. But I think the perspective of the people behind GPL is that all the code related to GPL is GPL code. And we shouldn't be writing non-GPL code after having read the GPL code.   It's a pain, and restricts code reuse. But that is what copyrights are all about. We need to give proper power over usage to the authors of the code.
comment
Let's move forward on this. The code is pretty short.  The Kneser graph is of interest and quite understandable.  I have another question : you move some other functions (or it looks like that in the diff). Did those function change? Or did they get moved? What is your intent with them?  (If it is to alphabetize them then maybe we should back those changes out to reduce churn in the blame. finding a function definition within a file should be easy without the alphabetizing (maybe?)
comment
Thanks for catching and reporting this.  The `seed` parameter is not set for all but one test of `TestFastLabelPropagationCommunities`. That makes it random and will sometimes fail.   possible fix in #7242 
comment
The randomness is coming from the graph creation rather than the tested function. The `random_tree` creator works fine for seed integers 0 -> 14, but `G=random_tree(10, create_using=nx.DiGraph, seed=15)` produces the error shown in the failing tests.   I believe the `random_tree` and `binomial_tree` graph creation will need seed values too. A fix is on the way. 
comment
I've been looking at this due to Jarrod's comment above that we try to get it in soon. The main issue I see is that the code tries to split `G` into a specified number of components. But the underlying functions to compute the betweenness raise an exception if G is not connected. So, we need to compute the betweenness values on each component separately.   This overlaps slightly with the issue of whether the values should be normalized or not. Since we are going to have to compute the betweenness values separately for each component, and then compare them to find the highest betweenness edge. If they are normalized based on the component, they will have different normalization factors. So, I'm thinking that we should not use normalized scores. [Even in the case of edge rankings that don't require connected graphs, normalizing should not affect which edge is maximal. So there is no reason the user should need to specify whether we normalize the ranks or not. And that also reduces the input arguments by one -- Yay! :}  I'm close to making these changes to this PR, but if you have an idea or perspective that would help please let me know.  Also, I'm thinking I can push directly to the PR because it is fixing the test errors and this is a rewrite of a previous PR. If anyone would prefer that I make a PR to the branch that is this PR, let me know. 
comment
To help with the CI test errors for configs that don't use `numpy`:  The CI tests are set up to test without any additional packages. This helps us ensure that the main parts of networkx work without any requirements. That means we have to add some special handling of the imports and tests whenever numpy and scipy (and others) are used.  This should be added as a section of the contributor's guide. So I'll put some text here and I intend to put it into the CONTRIBUTING.rst file after including any feedback you might add in comments below.  How to test functions/modules which use numpy/scipy/matplotlib, etc:  - See the contributing guide for standard import names, e.g. import numpy as np - Put the import statements inside the functions which use them. That delays the import until it is needed. - In the tests, use np = pytest.importorskip("numpy"). Note that this function is pronounced "import-or-skip". If it can import numpy, it attaches that module to the name np. If it can't, all tests in this scope are skipped. Please this importorskip call so that it affects only tests that call a function involving numpy (or that use numpy for the test). The idea is to skip functions that will fail because numpy isn't available. Usually this is either near the start of a test module, near the start of a test function, or near the start of a test class. That will skip tests in that module, function or class respectively. - Add a line to the networkx/conftest.py section which defines "needs_numpyorneeds_...`.  Hopefully I have remembered everything. This should fix the CI failures due to missing numpy.
comment
The contributors who have credit for their commits will be automatically included in the list of contributors. If some of the people haven't made specific commits (or if you want to make it happen manually instead of automatically) you can add people to the list in `doc/developer/about_us.rst`. 
comment
Sorry that I wasn't more clear.  The `pytest.importorskip` needs to be in the test-module... not the code itself. We don't want the code to skip the test (no test may have been started). So we choose which tests to skip by placing the `importorskip` where the tests are.  For the code, we just move the import inside the function, so the function can be defined even when numpy is not available, but calling the function raises the import error. 
comment
What sizes are you looking for in these functions? I don't usually think of 7000 as a very large number -- but I don't know how fast these things grow...  :)
comment
That's interesting. I got a recursion depth error when I ran n=7000 a few days ago. But there is a random nature to this, right?  But I'm fine with keeping the recursion in for this PR.  Many of the networkx graph applications I see are much larger than 10000 nodes. Hence the concern. But this area of research may focus on smaller trees.  Let's focus on numpy or not and on containers of nodes vs containers of node-pairs.  Thanks and sorry for the diversion.
comment
Yup -- another release of scipy has messed up our tests. I've put the fix into our main branch. You should at some point merge from the main into your repo, but we can leave it as is for the moment. The tests are being run because they don't use numpy anymore -- and they are passing! :} 
comment
Also, since this is a new function, can you please make the keyword arguments "keyword only" by adding a *, between the positional arguments and the keyword arguments?
comment
I made a PR to your branch with suggestions for doc_strings, and other small changes for the deprecation part. See what you think. https://github.com/vigna/networkx/pull/1
comment
If you want to update any of the changes I suggested in that PR on your PR, either comment here or go ahead and change them.  Looking through the tests, I see a few places where `random.seed(43)` is used. In some cases, the seed in the next line is set to 43.  In other cases, the seed in the next line is set to "random".  I believe in all cases the line is not needed and should be deleted.  And each line where `seed=random` is used should be changed to `seed=43`.  Does this make sense?  The last issue is whether we should be creating an edge list using lists of nodes (which are eventually paired) or lists of 2-tuples of nodes. This is the question I called  the node-pair issue above. The construction of `t12` is quite cryptic -- every other entry is `0`, so the 2-tuple edges  would be `[(0, t2_nodes * i + t1_nodes) for i in range(j)]`.  The other changes needed to make this collect 2-tuple edges instead of collecting nodes (which then get turned into 2-tuple edges at the end) are fairly straight-forward. I believe the time difference is negligible in my quick timing. (n=100 and n=400 nodes for a single random_unlabeled_rooted_tree).  Thoughts?
comment
Thanks for all the changes. I have two more questions:   (always more :) - The random integers generated in `_select_k` are much bigger than what normally fits in a int64 integer even though the final value is relatively quite small. Would there be a downside to doing the cum_sum as floating point values? The upside is that numpy random number generators could be used with this code (not necessary, but nice). Something like:  ```python def _select_k(n, seed):     r = seed.random()     cum_sum = 0     for k in range(1, n):         cum_sum += comb(n - 1, k - 1) * (n ** (n - k) / ((n + 1) ** (n - 1) - 1))         if r < cum_sum:             return k     return n ```  - The parts of the tests that use `seed=43` seem to be testing the random number generator decorator, which I hope is tested elsewhere, rather than the random_trees code. They also test the case that `number_of_trees` is specified. But they don't do that fully -- e.g. the returned trees could all be the same tree and the test would pass. Are there other aspects I am missing?  How about: ```python     t = nx.random_unlabeled_rooted_tree(15, number_of_trees=10, seed=43)     first_t = nx.random_unlabeled_rooted_tree(15, seed=43)     for i in range(10):         same = nx.utils.misc.graphs_equal(t[i], t_first)         assert (i == 0 and same) or not same         assert nx.istree(t[i])         .... ```
comment
Do you have some reasoning for why using floating point cum_sum would make the output incorrect? The inequality is the same, just divided by the very large integer.  Ahh... I see that the low probability event of identical trees could occur. Thanks for that. And nice to hear about the extensive chi-squared test. I don't feel strongly about leaving the comparisons of t and s in. :}
comment
But we only have `n` different possible results...  not `n**n`.  For example, with n=20 there are only 20 possible outcomes, each with a probability.  What precision do we need for the cutoffs between those 20 different outcomes? Using integers we expand the space to about `20**20` different outcomes and select among those. Using floats we expand to about `2**52` different outcomes and select among those.  But we are only selecting between 20 items. So it is hard to see that a difference in the cutoffs would be much different.  Perhaps what I should do to answer my question is to figure out whether the chi-square result would change based on round-off error in the cutoffs for `cum_sum`. That is, how sensitive is the final distribution to cutoffs in the `_select_k` distribution.  This sounds like I am going down a rabbit hole... :} :}     The purpose is to figure out whether we can make this work for someone using a `numpy` random number generator instead of the python `random` library. That's a bigger question than this PR. But I was hoping that maybe we could avoid the issue in this PR by switching to floats.  Perhaps the right approach is to merge this and find a workaround for that situation later.  Another approach would be to only allow the python random library for this function.
comment
I'm doing calculations on the _select_k distribution.  Using `random.randint` with these huge integers, and then repeating with `random.random()` or `numpy.random.default_rng.random()` and converting that to the integer k via computation of the cum_sum divided by the max value (n+1)**(n-1) - 1.  The probs come out about the same with both methods... but "about the same" is a tricky concept with these kind of rare events.  For example, By printing the `cum_sum` cutoffs I can see that for e.g. n=20, the chance of getting 15-20 is essentially zero.  In fact it **is** zero for the method using floating point. And it is below 1e-16 for the "exact" method. In practice I have never seen a value bigger than 8.  ``` cumsum values (Integer random method) 1 5242880000000000000000000 2 10223616000000000000000000 3 12464947200000000000000000 4 13099991040000000000000000 5 13226999808000000000000000 6 13246051123200000000000000 7 13248273776640000000000000 8 13248480165888000000000000 9 13248495645081600000000000 10 13248496591032320000000000 11 13248496638329856000000000 12 13248496640264755200000000 13 13248496640329251840000000 14 13248496640330988288000000 15 13248496640331025497600000 16 13248496640331026117760000 17 13248496640331026125512000 18 13248496640331026125580400 19 13248496640331026125580780  cumsum values (Floating point random method)  (3rd column is the effective cutoff as an integer) 1 0.39573395701665076 5.24288e+24 2 0.771681216182469 1.0223616e+25 3 0.9408574828070871 1.24649472e+25 4 0.9887907583507289 1.309999104e+25 5 0.9983774134594573 1.3226999808e+25 6 0.9998154117257666 1.3246051123199999e+25 7 0.9999831781901694 1.324827377664e+25 8 0.999998756504721 1.3248480165888e+25 9 0.9999999248783125 1.3248495645081602e+25 10 0.9999999962789208 1.324849659103232e+25 11 0.9999999998489512 1.3248496638329856e+25 12 0.9999999999949979 1.3248496640264754e+25 13 0.9999999999998661 1.3248496640329252e+25 14 0.9999999999999972 1.324849664033099e+25 15 1.0 1.3248496640331026e+25 16 1.0 1.3248496640331026e+25 17 1.0 1.3248496640331026e+25 18 1.0 1.3248496640331026e+25 19 1.0 1.3248496640331026e+25 ``` I think the question is how much precision do we need for these probabilities of very rare results (and not so rare results which also have errors at the same precision starting at about `k=9`).  Should we add some code to allow folks to use `numpy.random` with some very small changes in probabilities, or force them to use the `random` library -- effectively making it impossible to use a single seed to control all random number generators?    I've tried looking at other similar cases, and it is very rare to have a general language (other than python) which allows computation of these kind of numbers without some special arbitrary precision library. Python allows it, but that doesn't mean we should use it if we can avoid it. In what sense do we need it? What is our sensitivity to these probabilities?
comment
Just as I thought we were through with random trees for a while, it has come to my attention that the deprecated `random_tree` supported directed graphs. These new random tree creation functions only create undirected graphs. Do you have any insight into how random directed trees should be constructed?  The old random_tree sets the directed edges to align with a DFS from a node. That seems to construct an **arborescence** -- which is the natural extension of a rooted tree to directed edges. But another definition of a directed tree (both from [wikipedia trees](https://en.wikipedia.org/wiki/Tree_(graph_theory))) is that the edges can have arbitrary direction -- so long as the undirected version of that graph is a tree. That is also called a polytree.  My instinct tells me that we should stick with creating aborescences when directed trees are requested. But my instincts are often wrong when it comes to these things. :)  @vigna do you have any ideas on this?
comment
Is this the same as: ```python     PG = nx.path_graph(200, create_using=nx.DiGraph)     # CG is a DiGraph with one edge direction of each pair of nodes     CG = nx.DiGraph(nx.complete_graph(200).edges)     for G in (PG, CG):         H = G.copy()         in_degrees = sorted(H.in_degree())         out_degrees = sorted(H.out_degree())         nx.directed_edge_swap(G, nswap=40, max_tries=500, seed=1)         assert in_degrees == sorted(G.in_degree())         assert out_degrees == sorted(G.out_degree()) ```  This approach only checks the degree sequences and not whether the right number of edges were swapped. If we want to get the right number of edges swaps, you might be able to get a list of them using: `G.edges - H.edges` and `H.edges - G.edges`.   But I'm not sure we can check for the right number of swaps... I believe it is possible for the algorithm to randomly select a swap that un-does what a previous swap did.  
comment
Just as undirected edge swaps have conditions on the edges chosen (need a 4-node combination with a-b and c-d but not a-c or b-d), the directed edge swap does too. The docs for `directed_edge_swap` describe it as finding 4 nodes with edge formulation `a->b->c->d` and no other connections between b or c and those 4 nodes. Then the swap results in `a->c->b->d`.  If you make a complete graph with all possible directed edges, it won't be able to swap -- as you say.  Even a "complete" graph with one direction chosen between each pair of nodes won't allow swaps because the nodes b and c will be connected to each of the 4 nodes a,b,c,d.  We need a graph with fewer directed edges to allow swaps of this form. Note that edges and "anti-edges" (node-pairs without a connection) are *both needed* for edge swaps. So swapping is hard with a sparse graph and also hard with a dense graph.  We need something in the middle.  A directed path graph works for directed edge swap, in that it does allow the edge swap to occur. Maybe the best approach to finding graphs that could be used for a test is to stick with small graphs. They may have very few swaps, but you can identify them and see if they happen easily.  The change could be as simple as what is listed in [a comment by @rossbar](https://github.com/networkx/networkx/pull/5663#pullrequestreview-1018715617) where you check that the edges are no longer equal and thus that a swap did occur. More complete testing could involve checking how many swaps were made -- but I think that is hard due to the random numbers inherent in the process. Still, if you set the `seed` input to a specific value you could see how many edge changes were actually made and compare that to the `nswaps` requested.
comment
For these tests, I think the more obvious check using `G.edges == H.edges` should work.  I'm not sure how much the `nx.utils.edges_equal` is used to compare two graphs (where the above syntax works better). That function is usually used for comparing a list of edges with `G.edges`.  At least I think that is true. Note that for a set of edges, `G.edges == set_of_edges` also works well. But I bet there are a lot of places where this utility function is used instead.  For now, I would suggest just using the direct equals operator and we can talk about updating the utility script.
comment
Testing a random graph isn't all that helpful. It would be better to test a graph with a specific configuration that you are concerned will be handled correctly by the function. Random graphs give a different result every time you run them, so it is very hard to track down what the cause of the error is. You can specify a "seed" to make it the same result every time, but that is no longer random. It is just some arbitrary graph that you usually don't know much about.   So, if you want to expand beyond a `path_graph`, I would suggest choosing a specific graph that has a different topology and working with that.  One option would be a complete graph with only one directed edge for each edge pair. ```python G = nx.DiGraph(nx.complete_graph(N).edges) ```  You should think about better graphs to test edge swaps. I just made this one up as something that looks very different from a path -- not because of some feature about edge swaps.   Thanks for taking this on! :}
comment
Hard to say much with this much info. We know it seems to be changing from run to run more than expected. And we know that another implementation doesn't do that. - does your network have weighted edges? Does it have multiedges (more than one edge between two nodes)?  - have you tried increasing the number of iterations? If the results have not converged that might be a reason for variation from run to run. 
comment
That's not very good.  Thanks for the file to try it out. It seems to work for me (with 10 iterations) and gives slight differences from run to run, but the general shape is similar: mostly the middle nodes are red and the ends of the strand are blue (or vice versa).  See if this code is different in some way to what you are doing: ```python import networkx as nx import matplotlib.pyplot as plt plt.ion()  G=nx.read_edgelist("Downloads/graph.txt") pos=nx.spectral_layout(G) (red, blue)=nx.community.kernighan_lin_bisection(G)  plt.clf() nx.draw_networkx_nodes(G, pos=pos, nodelist=red, node_size=10, node_color='red') nx.draw_networkx_nodes(G, pos=pos, nodelist=blue, node_size=10, node_color='blue') nx.draw_networkx_edges(G, pos=pos) ``` The drawing looks like this -- with other random starting values (can be controlled by e.g. `seed=42`) giving similar results. ![Figure_2](https://github.com/networkx/networkx/assets/915037/f6844d00-b17a-4b0e-bb88-4bb9e5b6c2db) 
comment
The Schultz index, and Gutman index (second type of schultz index) are related to the Wiener Index and Kirchoff Index (https://hrcak.srce.hr/132323).  All four (and others) are used as markers of chemicals based on the molecular network of atoms connected by chemical bonds.   The functions provided here can be improved and updated. The file structure, doc_strings and tests seem pretty good. I think they should be moved into the module `wiener.py` and probably connected to the kirchoff index if we ever get that.  Maybe they should all go into a distance module as suggested in #6975 but for now we can put these into `wiener.py`.
comment
These measures involve both distances between nodes and the degree of the nodes. Can you say why you tink they should all be contained within the ditance_measures.py module?
comment
Thanks for that.  It should go in the tests folder next to the `planarity.py` module.  So, in this case, `networkx/algorithms/tests/test_planarity.py` sorry for not being clear.  I also now realize that this creates a difference in the treatment of `add_edge` and `remove_edge`.  In the current code, `add_edge` and `remove_edge` are both handled by the superclass `nx.DiGraph`.  Adding half edges is handled by the subclass `PlanarEmbedding`.  Since this is removing half edges should we create new methods instead of these -- something like:  `remove_half_edge_cw` and `remove_half_edge_ccw`?  Another question:  what happens if removing that edge makes it no longer possible to have a planar graph?  Yet another question:  what about removing nodes?  Is that not needed?    As an aside, I think these features were not included because they led to headaches -- and a user could recreate the graph with the removed edges if needed. But that was obviously not documented nor enforced.   What should the api be here?
comment
I'm afraid there is a lot more checking to do before this PR will be merged. For example, you stated that removing edges does not break the trait of the graph being planar. I agree... :} But it might break the representation that `PlanarEmbedding` provides to check that a graph is planar. For example, the code says that one must be careful not to call the method to connect components if the components are already connected, especially if one half edge already exists and you are adding the other half edge. So, what does that imply about breaking a component into two components?  Is it enough to check local traits?  You can tell from comments in the code that it was written in a fragile way. As long as you do "normal" things, it will be correct. But it doesn't protect you from yourself. Another example if that it subclasses `nx.DiGraph` but changes `G.is_directed()` to return False.  That will break lots of code in NetworkX if you try to use it as a networkx graph. This fragility impacts us as maintainers because we have to figure out the impact of seemingly small changes.   Another example is that adding an edge to the graph could easily make it no longer planar. But we have no mechanism for tracking that. This PR will provide `G.remove_edge` but not make `G.add_edge` correct (nor block it from happening). Users will be more tempted to use `G.add_edge` if they are using `G.remove_edge` somewhere else.  It'd be great to expand on this class to make it editable -- with checks for each edit to ensure that the attribute structure is maintained. Or make the class "frozen" as suggested above. Then people are expected to change the graph in its form as a standard networkx graph. Once changed they can go back to a planar embedding to check traits there.  Both would help this code become less "fragile". But the `is_directed` feature will still make it hard to use/maintain.  Can you convince us that this change does not break the PlanarEmbedding data structure? Are you willing to take on making the class fully editable?  Can you describe a use case where making these changes to the code and maintaining them is better for the users than having them use the networkx graph to make graph changes and then convert the new graph to a PlanarEmbedding to check planarity. How slow is that anyway?   I think those are three things I would want to see for this.
comment
We talked about this in a community meeting recently and we are OK with having remove_edge/node added to the class, but we'd really like it if the other morphing methods would raise exceptions. We are worried that people will see that they can remove and edge and then try to add an edge.   Could you add methods for the following that simply raise an exception stating that this is not allowed for a PlanarEmbedding?  The methods are (obtained from the function [frozen](https://networkx.org/documentation/stable/_modules/networkx/classes/function.html#freeze) which turns off all of these). ```     G.add_node     G.add_nodes_from     G.add_edge     G.add_edges_from     G.add_weighted_edges_from  ``` If that's too much for now let us know and we'll find a way to get it in.  [Edited: if we then in the future want to make one of these work, we still can. But it completes this improvement in PlanarEmbedding] Thanks for all of this!
comment
Yes -- it sounds like that is a bug.  Thanks for tracking down the `remove_edge` location for the bug. Are you able to open a PR to fix it? If not we will get to it, but we'd love a PR -- or even a description/patch of what changes are needed.  Thanks!!
comment
This question is quite old, and I think a complete answer requires looking at the reference paper to see how they count the number of edits and in particular how they check for isomorphism with ```G2``` after the edits. But here's a partial answer:  The edit paths you report all have 3 node edits and 3 edge edits. That means the length of those two lists is 6 total for all of the edit paths. They are equivalent despite the first being the "identity path"(? a term I just made up).  But, as you point out, the two graphs are isomorphic, so the edit distance should be 0 if distance is the number of changes needed to make G1 isomorphic to G2.  I suspect the algorithm actually computes the number of changes needed to convert ```G1``` to ```G2```. Said another way, it doesn't count the 6 edits as part of the edit cost.  In fact if you look closely, these 6 paths consist precisely of the isomorphism mappings from ```G1``` to ```G2```. They are simply renaming the nodes and edges and thus have no cost.  This question shows that the documentation needs more information about what is returned here (i.e. what is an edit path?).  That should be sufficient to close this Question.
comment
Thanks for this comment! As you may have gathered, our concern with pydot is that it doesn't seem to be maintained anymore. We've started to have bug reports that might just never be fixed.  We've been working pretty hard on getting pygraphviz to be easier to install on Windows. But there are understandable limitations when using a program with such a storied history like Graphviz.  Our support for pydot depends on comments like this one. So thank you.
comment
Are you saying you want to render the graph in the terminal window? (we have ascii representations of graphs #5602 )  If you mean you want to render the graph from a terminal-based script, we will continue to support `pygraphviz` which creates dot files.  We are definitely not cutting off support for interaction with GraphViz.   And if you don't need the graphviz features, the matplotlib-based tools we offer will continue in the long term.  So, basic image creation is supported. But we aren't able to support all the features of a full-fledged graph visualization tool.  Perhaps we should update the comments in the NX/Reference/Drawing section to make it more clera that basic rrawing will continue to be available via networkx. :}
comment
I believe matplotlib can save the figure without drawing it interactively. Maybe use `matplotlib.pyplot.savefig` instead of `matplotlib.pyplot.show`.  There may be other ways too. They definitely allow image file creation without any interactive UI.
comment
You created an undirected graph.  Use the `create_using=nx.DiGraph` optional argument to `from_pandas_edgelist` to make it from a DiGraph.
comment
It looks like you are expecting "subgraph monomorphisms" instead of what we call "subgraph isomorphisms" which could also be called "induced subgraph isomorphisms".  The monomorphisms let you find a subset of the nodes and edges of the original graph.  The subgraph isomorphism requires that the subsets of edges include all edges from the original graph connecting the nodes in the subgraph -- that is, the result must be an induced subgraph of the original graph.  Change your line as follows and it should work: ```python subgraph_isomorphisms = list(GM.subgraph_monomorphisms_iter()) ```  
comment
I like your coded names like ```read_pajek_communities```.  But does this only read .clu files? And what are .clu files? Are they a subset of pajek specification?  If it really is for .clu files only then perhaps ```read_pajek_clu``` is a better name.   You should include in the docstring a quick description of what .clu files are and how they relate to pajek files.
comment
I agree that it's good to change the existing names to ```read_pajek_net```, etc., and then have the alias ```read_pajek``` point to the net version. Looks like you've got most of ```.clu``` close to ready and we can leave .vec and .per for later. :)
comment
Thanks for thinking of providing this and for making the efforts to learn github and its quirks. There are still a number of parts of this PR that are missing: tests, connection to the docs, more description of the algorithm including connections to other similar functions. But I don't think it is worth pursuing those changes until we decide whether KQI is going to be part of networkx.   This looks like a straightforward formula for measuring knowledge content. And the paper claims it is a worthwhile measure. But there are thousands of these kinds of measures of different aspects of graph theory -- and KQI is, as far as I can tell, not used elsewhere or cited widely yet. It may become the dominant measure of knowledge content in the future. But at the moment it is 1) easy to implement by the user, and 2) not yet widely used by or cited in published literature.  I would prefer that we wait to include this measure in networkx until it has become established in the knowledge quantization field. Again, thank you for thinking of providing it to NetworkX.
comment
Maybe look at line 9242 column 48 and see if there is a strange character or a different structure than expected. The different graph formats are quite different so it's not such a surprise that some files have the invalid token and the others don't.  Without more information,we can't really help. You are in the debugging stage. Look where the errors say there is a problem and see what part of the graph is causing trouble. Then make a very simple graph with just that part. You might figure it out from that. If not, post the small example here and we can recreate it to see if something is wrong with the files or the code. :}
comment
Great!  This should be merged after v3.2 is released and included in the release (along with removal of the deprecation warnings and `conftest.py` entires) of v3.3.
comment
Based on my comments in #6533 I think there is more needed here. And I'm not seeing now what would be deprecated. The main problem is: ```python G=nx.complete_graph(5) G.remove_edges_from([(0, 1), (3, 4)]) print(list(nx.all_node_cuts(G)))  # returns [{0, 1, 2}] ``` returns `[{0, 1, 2}]` when it should also include `{2, 3, 4}`. Similar looking case works correctly: ```python G = nx.complete_graph(5) G.remove_edges_from([(1, 2), (0, 4)]) print(list(nx.all_node_cuts(G))) # returns [{1, 2, 3}, {0, 3, 4}] ```
comment
Yikes -- that took a bit. But I think I found an error in `nx.all_node_cuts`. Luckily it wasn't in the heart of the algorithm, but in the setup for which nodes get considered. A change from removing the potential node_cut set `X` to removing the currently considered node in that set `{x}` did the trick. I was following along with the article and it says "if x_i != v_j and v_j is not adjacent to x_i". It does not say that v_j should not be in the set X.  Anyway, it now works! With the fix already here for complete graphs, we have solved all the issues raised in #6533.  Ready for Review
comment
TLDR; Issue #3025 points out a few bugs which were fixed in #3039 They left the complete graph case, but it seems  thatis not handled correctly.  Looking through the blame history of this function, it seems that bug reports such as #1875 have been fixed by adding extra code to handle those cases rather than fixing the underlying algorithm. Special code for e.g. cycles was also added but has since been removed without removing the code for complete graphs. I'm not sure why this approach was taken. Was the underlying algorithm investigated and found to be unable to handle those cases?  How do we know the underlying algorithm is correct?  I've been playing with it and it seems to work fine.   The tests use `node_connnectivity` to give the size of the minimal cutset. The doc_string for `node_connectivity` mentions that the connectivity is the number of nodes that need to be removed so that the graph is disconnected **or the trivial graph** (trivial graph is one node, no edges). So, I believe the author considered a node cut-set to be the nodes which when removed leave a disconnected or trivial graph.   By the definitions of node cut or vertex cut that I have found, it only is a cut if the resulting graph is disconnected. That is different from what is implemented here **I think**.   But, are there graphs that are not complete graphs that have "no node cut-set". That is, there is no set of nodes which when removed leave the graph disconnected?  I don't think so... because if you consider an edge that isn't present in the graph, then removing all but the two nodes on either side of the missing edge will leave the two nodes -- a disconnected graph. I conclude that the only case where we might result in the trivial graph is if we start with a complete graph.   The [wikipedia article on connectivity](https://en.wikipedia.org/wiki/Connectivity_(graph_theory)) states that "In particular, a complete graph with n vertices, denoted Kn, has no vertex cuts at all, but the vertex-connectivity is n − 1."  So I am left thinking that the original implementation on this function was correct, and the addition of specialized code to handle complete graphs was added (incorrectly) to produce "cuts" which lead to trivial graphs rather than disconnected graphs. That is, the added code made the node_connectivity equal to the size of the node cut-set. That trait is true for all graph except for complete graphs.  OK... enough of this train of thought writing. :}  I think we should remove the code that handles complete graphs specially and fix the tests to check for equality of node_connectivity with the length of the cut sets unless the graph is a complete graph.    Thoughts?
comment
I agree. Make the code return an empty set when the input is a complete graph. I think this just means deleting the special case handling code, but we better check that. :) And let's change the current tests for complete graphs and change them to check the empty cut return value while also checking that node_connectivity is n-1 in that case. 
comment
I've finally gotten back to this and found some other problematic cases. I don't trust this functoin at the moment. :}  ```python G=nx.complete_graph(5) G.remove_edges_from([(0, 1), (3, 4)]) print(list(nx.all_node_cuts(G)))  # returns [{0, 1, 2}] ``` So it found {0, 1, 2} as a node cut that leaves nodes 3 and 4 (which are disconnected). But it doesn't find {2, 3, 4} which is also a node cut that leaves nodes 0 and 1 (which are disconnected).  Am I missing something? Have I mixed up something with the definition of node cut?
comment
Thank you for this bug-report. There was indeed an incorrect handling of complete graphs. And it helped us find a bug in the algorithm more generally. #6558 fixes those two bug.
comment
I agree that returning `None` should indicate that there is no minimal separator. And returning an empty set means that the empty set is a d_separator.    This changes (corrects) the current behavior, so a note should be added to the API changes section of `doc/release/release_dev.rst`.  I don't think it should need a deprecation, since it is correcting a faulty previous behavior. But I'd like @rossbar to verify that I'm not missing something.
comment
It looks like those two functions use `@not_implemented_for("directed", "multigraph")` which is almost certainly a bug that should be written: ```python @not_implemented_for("directed") @not_implemented_for("multigraph") ``` See #6904 for others that should be checked too. 
comment
There is a difference between the integers you see in the matrix and the integers you see as the nodes of the graph. In the sparse array, the indices are the row number of the node, while in the networkx graph the integers are the objects that represent the nodes. The order of the rows is determined by the order those nodes were added to the graph.  To simplify things, you should provide a `nodelist` argument to `nx.to_scipy_sparse_array` function.  To match what you seem to expect make it `nodelist=range(12)` or something like that (untested).  Then the row numbers will match the python objects you are using for nodes. :)
comment
Thanks for this!   But I think that issue was not a "bug" so much as a misinterpretation of the results. The row numbers in the matrix are not necessarily the same as the node values in the networkx graph. In this case the row number is the order that the nodes were added to the graph, not the value of the node.  Using a `nodelist` argument to set the row order is usually helpful for any matrix representation of the graph. You don't need it if you remember that the order reported by `list(G)` is the row order when you don't provide a nodelist. But it definitely an cause confusion if you aren't ready for that.  I'll close this, but the conversation can continue. :}
comment
Thanks @MridulS and @rossbar !   This looks good.  I think some `is_*` functions should return True or False for empty_graphs. It depends on the trait that they are considering. Not all traits are pointless concepts for an empty graph. :} But I could probably be convinced otherwise. :}
comment
Interesting that the syntax warning says the octal sequence is `\420` when the actual string says `\4200`
comment
Nice catch!!  -- the dispatch graphs do not have type `nx.Graph`.
comment
Thanks for the quick changes and the complete responses to questions.   Are you able to give a description of why this queue is expected to eventually empty? It seems like it'd be easy to get into an oscillation where nodes are shifting labels back and forth repeatedly -- and each time they change they'd be adding their neighbors to the queue.  I agree with your proposal to correct the documentation for `asyn_lpa_communities` while leaving the functions as is, and probably not implementing a "proper" LPA implementation unless the literature finds a compelling reason for it.  Those changes could be in a separate PR, or you could add them to this PR. It might be easier to review as a separate PR, but perhaps slightly more work for you. Let us know which you intend to pursue. And someone else might chime in too if there are other perspectives.
comment
We forgot to put a link into the docs for the new function.  A separate PR adding it to `doc/reference/algorithms/community.rst` should be made. Sorry that we didn't catch that. The new line goes [here](https://github.com/networkx/networkx/blob/main/doc/reference/algorithms/community.rst#label-propagation) Thanks for catching this and letting us know! 
comment
Yes, generally in NetworkX the directed edge (i,j) is an edge from i to j. Almost uniformly in the graph theory literature this is the standard.  See [wikipedia's description](http://en.wikipedia.org/wiki/Directed_graph)   I've seen some literature on Markov processes that traverse edges based on a matrix that is the transpose of the matrix definition we use. There may be other literature areas which use entry (i,j) to represent the connectivity from j to i as well. So we need to include our convention in our documentation.  @Midnighter are there other places it would be good to mention this? 
comment
Thanks for this idea -- but I'm afraid that if we try to enable every possible argument to each matplotlib function we will explode. :)   I would prefer that we make the user convert the node color values to their normed versions first. This can be done in the graph using `nx.get_node_attributes/nx.set_node_attributes` along with e.g. `mpl.colors.Logrithmic`.  Or once you have the `node_color` values, you can simply use `mpl.colors.Logrithmic` to get the `normed_node_color`. And then call the draw function. Is there functionality that this approach does not enable?  (untested)  ```python norm = mpl.colors.Logrithmic() normed_node_colors = norm(node_colors) ```  To change the values stored in the graph attributes: ```python norm = mpl.colors.Logrithmic()  # or any other normalization-function constructor wts = nx.get_node_attributes(G, "weight")  # as a dict norm_wts = norm(list(wts.values()))  # as an ndarray normed_weights = dict(zip(G.nodes, norm_wts)) nx.set_node_attributes(G, normed_weights, "normed_weights") ```
comment
Hello and welcome to NetworkX's Github...  Thanks for your first PR, and congratulations on your project.  There are many aspects of this that will be required before acceptance. You will need to put the module in an appropriate place within the NetworkX codebase. And that depends on what type of algorithm it is (and whether it is an algorithm or an example of a problem that can be solved using NetworkX).  Your description for the Github PR should be "edit"ed (link on the three dots in upper right of the first comment) to provide a paragraph description of what the function does, why it is of interest and an introduction to the core developers of the problem. (doesn't have  to be very long).  Links to free versions of cited results would be helpful too.  Take a look at the CONTRIBUTING.rst guide for basics. You will need doc_strings to match the sphinx format that our other functions do, an entry in the networkx docs to point to your function (once we figure out where it should be).  It's be good to have examples in the doc_strings. You should separate the tests into another testing module. Your code should have the typing removed. You should make helper functions names start with an underscore (they are private functions) and you should list the functions you want as public functions in a variable called `__all__`.   See the other modules to use as a template.   I'm guessing this could go in the `algorithms/matching.py` module, but see if that fits. If so, you would add the code there and the tests to the associated test file. 
comment
I'm commenting here based on a chat comment from yesterday's community meeting.  This is an 8 year old PR that adds sampling functions for edges and nodes. Basically, the functions let you select a set of N random nodes (or random edges). Most are chosen uniformly random, but there's also functionality to prescribe probabilities to each.  I think this can be done now with something like  ```python rng = np.random.default_rng() sample_nodes = rng.choice(G.nodes, size=15, p=[n/sum(G.nodes) for n in G.nodes]) ```  So perhaps this should be closed -- or maybe find a place in the docs to point this out? Or suggest a nx-guide topic on this?  Do we have a place to suggest nx-guide topics?  Maybe in the mentored projects?
comment
I'm going to close this issue. I think sampling interfaces  generally and the NX random walks functions make this easier. Comment here if you are still interested in this.
comment
This PR proposes 4 functions.  - minimum_dominating_set has been refactored into PR #1464 - 2 of the other 3 are 2-3 line use of the 3rd (maximum_clique) (a max independent set is the max clique of the complement of G, and the minimum vertex cover is all nodes not in the max independent set) - the  maximum_clique algorithm used here is an order of magnitude slower (as reported above) than computing all the cliques and finding the largest one -- which we can do with existing nx functions.  So I think we should close this PR in favor of the PR for minimum_dominating set. If someone wants to reopen it, leave a comment.
comment
Some reasons `common_neighbors` is only defined for undirected graphs include: - there are different types of neighbors in a directed graph (call them successors and predecessors), and it would be tricky to make a clean API that allowed specification of which type of neighbor is to be calculated. - the computation is a one-liner that can easily be morphed by the user to get the combination of types of neighbors that are desired.  I cannot find any publications that specifically mention Jaccard Coefficients in a directed graph -- though my search was not extensive. I do find articles on link prediction in directed graphs and it seems a popular way to score the likelihood of a link `(u, v)` existing is to consider how many successors of u are predecessors of v.  This seems naively better than looking at common successors of both u and v because we are considering whether v is likely to have a new predecessor -- not successor.  It seems to me we should stick to the literature when defining the Jaccard Coefficient. And that our function for common_neighbor should either be restricted to undirected graphs (as currently coded) or provide a way to specify which combinations of directed neighbors are considered. Perhaps the best approach would be to improve the documentation by adding some text and an example showing the one-liner that would provide the number of nodes that are e.g. both a successor of u and a predecessor of v.  Do you have a different perspective on this? 
comment
More discussion of this issue is in #5825   The failing tests in this PR show the potentially vast nature of this change. Existing code may expect to remove edges using multiple 2-tuples in the same edgelist. I'm less confident that this is a good way to implement this feature.  It might be better to come up with a different signal for the case of removing all the edges between 2 nodes... e.g. if Python had a symbol All which was available like None, we could let (u, v, All) mean that all edges should be removed -- and we'd have to ensure that no edge key was ever set to All. But Python doesn't have that feature...  We could try adding that feature, but it would not be familiar to users and there ought to be a better way...  Maybe `(u, v, key_maker)` where key_maker is a Callable that returns the keys desired to be removed, or None to indicate all keys. Then `(u, v, lambda:None)` would indicate to delete all the nodes, while `(u,v,lambda: [3,4,5])` would be a short way to indicate 3 edges without listing all 3 separately...  But I'm not convinced these are the best options...  The other alternative brought up before is to make a new method (something like: `MG.remove_all_edges_between(u, v)`) that simply calls `super(MultiGraph).remove_edge(u, v)`.  This wouldn't allow removing all edges between 2 nodes as part of an edgelist with `remove_edges_from`. But in some ways that might be helpful -- it separates this special treatment from the usual one-edge-at-a-time case. 
comment
I vaguely recall a discussion of this (most recently in #2510 and #2612 stemming from #2465) where we decided that the `all_pairs_...` versions should return an iterator that could easily be made into a dict. That was because the dicts that are created can be huge, especially when returning the paths. And for memory many people would want to process them one at a time rather than store a large data structure holding all that at once.  My quick peek suggests that the `all_pairs_shortest_path` returns an iterator no matter what the docs say (hmmm... seems the docs are wrong.  I am disappointed that the `single_source_shortest_path_length` returns a dict. At the moment, I think that should be an iterator as well. But I can read myself stating the opposite in those issues above. :)   If someone needs a dict they can easily make it one by wrapping it in a `dict` call.   I think the `shortest_path` and `shortest_path_length` are meant to return different things depending on the inputs. Those are for ease of use. And sometimes they are returning a single path and other times a whole collection of paths. So the return value can be int or list or iterator if I understand correctly.  So... I'm sure that `all_pairs_...` should return iterators. I currently think the `single_...` should also return iterators. But I stated the opposite in #2510, so maybe someone else should make that call.  There is also backward-compatibility to take into account, but this PR is more about what it should be -- we can worry about backward compatibility later. :) 
comment
Upon reading this thread again today, I notice that making the `single_source_...` functions return iterators might make it more difficult to create the `all_pairs_...` outputs because each inner function would need to get wrapped in a `dict`. Otherwise we'd have an iterator of `(node, iterator)` pairs. So should the `all_pairs...` functions `yield (n, dict(iterator))` or `yield (n, iterator)`?    It is certainly most important for the `all_pairs...` to iterate -- the output is quadratic in number of nodes otherwise. The choice for `single_...` is more subtle than I realized in my comments above. It still might be good to have them iterate. But can we make it easy?  And certainly the `single_source...` and `single_target...` functions should do the same thing.
comment
Yes -- you are correct that we can have `all_pairs...` return (n, dict(iterator))`  I think that is the way to go if we turn `single_...` into returning an iterator.  And: Yes, a performance analysis would be helpful. Before we worry about whether it should be an iterator or a dict, we should know the impact of each option. But we probably don't need to do a memory-size performance test -- we can figure out the size of a dict with would be.  As for CPU time, I suspect that the time difference is a small percentage of the time for computing `single_...` with a large graph (assuming the memory doesn't get full -- because then it will slow down considerably).  I'm guessing that long ago we figured out that the dict returned by `single_..._length` uses O(n) memory for n nodes. That's not too bad. So that could be a dict. The `all_pairs_...length` uses O(n^2) memory. So that could return an iterator.  But then `single_..._path` uses O(n^2) in general, but smaller amounts in many cases.  It's not clear whether, from a memory perspective, we should make a dict for paths or not.  But the `all_pairs_..._path` functions use O(n^3) memory, so I think we should definitely return an iterator.  (This is all assuming that CPU time diff is negligible.)   And of course we need to keep backward compatibility in mind too.  We don't want to switch what we return and make a bunch of people have to change their code just so they keep up with the latest version. There should be some payoff before we make a change.  I would argue that making `single_target...` match the return type of `single_source...` is a desirable API choice that should be done.  We would want to "deprecate" the current behavior for a while before actuating the change (at least I think that's how this is supposed to be done).   :)  So...   what change do we need to make to have a consistent API? And are there any outputs that really should be switched?
comment
I think the `shortest_path_length` output should match the underlying code used. So, if the inputs to `shortest_path_length` indicate a source and target, we return a number. If a source, but no target, or a target but no source, we return whatever `single_...` returns. and if neither source nor target is provided, we return an iterator like `all_pairs...`.  I think this is what you suggested also.  And I think it is what the function currently does (and `shortest_path` should be similar.)  It looks like the only change, other than the doc fixes in #6528 that we need is to switch the output of `single_target_...` to match the output of `single_source...`. I just did a quick dive-in to see how the inconsistency occurred and when `single_source...` got changed from returning an iterator to a dict, we just didn't change `single_target...`.  It was an oversight (and it was mine :)  So, I think it could be considered as an API bug or at least an API wart that needs to be fixed.  I scanned through the `weighted.py` versions of these functions and they seem to be consistent with our decisions here. I didn't check every function with its doc_string though.  So, let's make a PR to switch the return value of `single_target...` to a dict instead of an iterator. This corrects an oversight in #2612 .  I'm not sure if it will require a deprecation due to backward incompatibility, because it fixes an API error. But I'll put that on the topic list for our next discussion of such questions.
comment
If you mean changing `single_target_shortest_path_length` and `shortest_path` for the case of `source` and `target` being `None`, then this makes sense to me. I will create a PR to add deprecation notices for these changes. We will need to wait until v3.3 to release these changes.  But go ahead and get the PR ready so it will be easy to merge at that time.
comment
And can you make the changes in the doc_strings be in a different PR, so we can get that one merged soon. :} 
comment
Thanks for this!  It makes me wonder about our other Wikipedia links — which often end with a close paren. ;)
comment
I have tried to fix the conflicts (another new distance measure function has been merged into `main` in the same lines of the file that you put this new function.  My pushing these changes means that your local repo will need to pull the changes down from your github account before you can make more changes.
comment
I would much prefer the name `effective_graph_resistance` over kirchhoff_index.  There are so many things named after Kirchhoff it is hard to know what that refers to and is harder to pick up in a search based on the term. (I'm also biased against naming ideas after the creator instead of using an expression for what it means. But I know I'm biased that way so push back if you prefer kirchhoff_index and I'll likely back off.)  Thanks for this!
comment
You have run into a name collision. There is a [Python builtin library named html](https://docs.python.org/3/library/html.html) that networkx uses.  You have another python module named `html.py` .  Any python program that uses this python feature (the html builtin library) will not run for you. It is a restriction of the python import command. If you want to name your module `collections.py` you won't be able to use both Python's collections module **and** your module.  Python let's you hide other libraries -- which can be seen as a bug or a feature -- seems like a wart to me.  But you should probably rename your file to something more informative. perhaps `orgchart_html.py`?
comment
This just links the existing doc_string to the documentation. So I'm merging it...
comment
To get the tests working, you need to set up the import and tests so that this function doesn't impact the rest of networkx when numpy isn't even installed.  You've done some of it, but   How to test functions/modules which use numpy/scipy/matplotlib, etc:  - Put the import statements inside the functions which use them. That delays the import until it is needed. - In the tests, use np = pytest.importorskip("numpy"). Note that this function is pronounced "import-or-skip". If it can import numpy, it attaches that module to the name np. If it can't, all tests in this scope are skipped. Please this importorskip call so that it affects only tests that call a function involving numpy (or that use numpy for the test). The idea is to skip functions that will fail because numpy isn't available. Usually this is either near the start of a test module, near the start of a test function, or near the start of a test class. That will skip tests in that module, function or class respectively. - Add a line to the networkx/conftest.py section which defines "needs_numpy" or" needs_...`.  Hopefully I have remembered everything. This should fix the CI failures due to missing numpy. 
comment
Also, since this is a new function, can you please make the keyword arguments "keyword only" by adding a *, between the positional arguments and the keyword arguments?
comment
The invalid sequence comes from your docstring where the latex code is interreted as containing escaped special characters instead of as latex code. If you don't need special characters in your doc_string, take out the `r` in from of `r"""` at the start of the doc_string.  If you do have a special character in there somewhere we will have to do something like double `\\` in the latex part of the doc_string.
comment
Ack -- I think I got it backwards...  The doc_strings work with `r"""` and single `\` in the latex code. So, instead of replacing `r"""` and adding double backspaces, it is better to add the `r` to the `is_regular_expander` doc_string.  Sorry for the confusion. Now the doc_strings latex code looks ugly.  To fix it, replace all the double backslashes with single backslashes.  Then add `r"""` at the start of the doc_string for `is_regular_expander`.  I can fix it if you prefer.
comment
You are correct -- the tests used to work.  But we were getting warnings at import time, so a new test was added last week to check for warnings at import time -- and you were unfortunate enough to get caught by it.  Sorry about that.  I have got it working by making every doc_string that has latex in it a "raw string" (using `r"""` at the beginning).  I think this is ready to go, but I've only quickly checked the resolutions of @rossbar's suggestions.
comment
Is this ready to merge/review? Are there other things to add?
comment
This seems like a good idea, but one that might not scale well. If iteration order options occur too often it will be very difficult to make the set of possible solutions that the given solution is one of.  In some slightly more difficult examples, you might not even be able to know whether you got all the possible correct answers.  Similarly for the approach of generating both graphs and then checking if they are equal. Checking that two graphs are equal is (if I understand you correctly) the graph isomorphism problem which can be done for small graphs but not so well for large graphs.  So, this works well when it is small, but I don't know how widely we will be able to use it.
comment
The two docs build failures can be fixed by merging with or rebasing onto the latest networkx `main` branch. It's not related to your code, but pulling in the fix will allow you/us to look at the documentation changes to see how they look. If you prefer just say so and we'll merge it into this PR.
comment
Looks like [ETE](https://github.com/etetoolkit/ete) is a mature established open source python library focused on trees. I can't find any other satisfying those criteria. They do have [an old issue #103](https://github.com/etetoolkit/ete/issues/103) that talks about interoperability with networkx and gives example code for translation before closing the issue. I think this mention and link between this PR and that issue could help interested people find both of these attempts.  But I agree reluctantly that it'd be good to have someone invested in adding ete support participate. The maintenance load is small -- until it isn't.  The "other hand" here is that connecting with ETE could reduce the need for specialized tree data structures (and maybe some algorithms) in NX. But without a person connecting the two making that work is hard.    How about we close this (it'll still be here). And we reopen when someone with ete experience can comment.   ???
comment
This dinitz implementation was largely added via #1978 and there may be comments there which are helpful to understand the implementation. The cited paper may also help understand any difference between this implementation and the original dinitz algorithm.  Hopefully it also addresses the complexity. But I’m not sure it gives complexity for the algorithms you are wondering about.  There is also a comment there with code used to time the results (there may be better timing tools today — that was 8 years ago).  The comments there suggest that Edmonds-Karp is the faster implementation for the cases looked at.   Does your version of the code produce the same results as the current version?  Have you done any time tests? Why do you think the current algorithm implements Edmonds-Karp instead of Dinitz (how should I be looking for the difference between the algorithms?)
comment
Thanks -- that description is very helpful for me to see where this PR is coming from.
comment
Thanks for those timings and the checking into other threads about these algorithms. I deleted a duplicate message of those results and I'll include the testing code for long term usage in the details below.  Now its time to review the code, bugs from before, and surrounding stuff.   I'm not sure what you meant by "overhead". Can you explain what surprised you?  <details>  ```python import scipy from time import time import numpy as np import networkx as nx from scipy.sparse import rand from scipy.sparse.csgraph import maximum_flow from networkx.algorithms.flow import edmonds_karp from networkx.algorithms.flow import shortest_augmenting_path from networkx.algorithms.flow import preflow_push from networkx.algorithms.flow import dinitz from scipy.sparse import coo_matrix, csr_matrix """ n = 1000 density = 0.1 for k in range(100):     m = (scipy.sparse.rand(n, n, density=density, format='csr',                            random_state=k)*100).astype(np.int32)     G = nx.from_numpy_matrix(m.toarray(), create_using=nx.DiGraph())     Edmonds_Karp_max_flow = nx.algorithms.flow.maximum_flow_value(G, 0, n-1, capacity='weight', flow_func=edmonds_karp)     Dinitz_max_flow = nx.algorithms.flow.maximum_flow_value(G, 0, n-1, capacity='weight', flow_func=dinitz)     assert Edmonds_Karp_max_flow == Dinitz_max_flow """ """ n = 1000 density = 0.1 m = (scipy.sparse.rand(n, n, density=density, format='csr', random_state=42)*100).astype(np.int32) G = nx.from_numpy_matrix(m.toarray(), create_using=nx.DiGraph())  begin = time() for itr in range(3):     flow = nx.algorithms.flow.maximum_flow_value(G, 0, n - 1, capacity='weight', flow_func=edmonds_karp) end = time() print(f"Edmonds Karp: {(end - begin) / 3}")  begin = time() for itr in range(3):     flow = nx.algorithms.flow.maximum_flow_value(G, 0, n - 1, capacity='weight', flow_func=shortest_augmenting_path) end = time() print(f"Shortest augmenting path: {(end - begin) / 3}")  begin = time() for itr in range(3):     flow = nx.algorithms.flow.maximum_flow_value(G, 0, n - 1, capacity='weight', flow_func=dinitz) end = time() print(f"New Dinitz: {(end - begin) / 3}") """ """ n = 500 np.random.seed(42) a = np.zeros((n, n), dtype=np.int32) for k in range(n - 1):     for j in range(-50, 50):         if j != 0 and k + j >= 0 and k + j < n:             a[k, k + j] = np.random.randint(1, 1000) m = csr_matrix(a) G = nx.from_numpy_matrix(a, create_using=nx.DiGraph())  begin = time() for itr in range(3):     flow = nx.algorithms.flow.maximum_flow_value(G, 0, n - 1, capacity='weight', flow_func=edmonds_karp) end = time() print(f"Edmonds Karp: {(end - begin) / 3}")  begin = time() for itr in range(3):     flow = nx.algorithms.flow.maximum_flow_value(G, 0, n - 1, capacity='weight', flow_func=shortest_augmenting_path) end = time() print(f"Shortest augmenting path: {(end - begin) / 3}")  begin = time() for itr in range(3):     flow = nx.algorithms.flow.maximum_flow_value(G, 0, n - 1, capacity='weight', flow_func=dinitz) end = time() print(f"New Dinitz: {(end - begin) / 3}") """  def make_data(density):     m = (rand(1000, 1000, density=density, format='coo', random_state=42)*100).astype(np.int32)     return np.vstack([m.row, m.col, m.data]).T  data01 = make_data(0.1) data03 = make_data(0.3) data05 = make_data(0.5)  def networkx_max_flow(data, primitive):     m = coo_matrix((data[:, 2], (data[:, 0], data[:, 1])))     G = nx.from_numpy_array(m.toarray(), create_using=nx.DiGraph())     return nx.maximum_flow_value(G, 0, 999, capacity='weight', flow_func=primitive)  def scipy_max_flow(data):     m = csr_matrix((data[:, 2], (data[:, 0], data[:, 1])))     return maximum_flow(m, 0, 999).flow_value  begin = time() for itr in range(3):     networkx_max_flow(data01, nx.algorithms.flow.edmonds_karp) end = time() print(f"Edmonds Karp: {(end - begin) / 3}")  begin = time() for itr in range(3):     networkx_max_flow(data03, nx.algorithms.flow.edmonds_karp) end = time() print(f"Edmonds Karp: {(end - begin) / 3}")  begin = time() for itr in range(3):     networkx_max_flow(data05, nx.algorithms.flow.edmonds_karp) end = time() print(f"Edmonds Karp: {(end - begin) / 3}")  begin = time() for itr in range(3):     networkx_max_flow(data01, nx.algorithms.flow.shortest_augmenting_path) end = time() print(f"Shortest augmenting path: {(end - begin) / 3}")  begin = time() for itr in range(3):     networkx_max_flow(data03, nx.algorithms.flow.shortest_augmenting_path) end = time() print(f"Shortest augmenting path: {(end - begin) / 3}")  begin = time() for itr in range(3):     networkx_max_flow(data05, nx.algorithms.flow.shortest_augmenting_path) end = time() print(f"Shortest augmenting path: {(end - begin) / 3}")  begin = time() for itr in range(3):     networkx_max_flow(data01, nx.algorithms.flow.dinitz) end = time() print(f"New Dinitz: {(end - begin) / 3}")  begin = time() for itr in range(3):     networkx_max_flow(data03, nx.algorithms.flow.dinitz) end = time() print(f"New Dinitz: {(end - begin) / 3}")  begin = time() for itr in range(3):     networkx_max_flow(data05, nx.algorithms.flow.dinitz) end = time() print(f"New Dinitz: {(end - begin) / 3}") ```  </details>
comment
I'm comign back around to this PR. It looks like you have done a number of timing tests on different graphs and different algorithms.  It looks like `shortest_augmmenting_path` is consistently the fastest.  Also the new dinitz is faster than the old.   In your original post you asked about `shortest_augmenting_path` and how it differs from Edmonds-Karp. Have you figured that out yet?  Is this code ready to go? Anything else you want to include here?
comment
Thanks @YVWX
comment
There is no specific reason not to do it for undirected graphs. Go for it. :}
comment
We need to ad  more than documentation changes to make this work for undirected graphs. The types of triads are not the same for directed and undirected. As the OP in this Issue mentions, there are 4 types of undirected triads possible. So the code would be similar to what we've got, but different. Another question is whether anyone is interested in undirected triads. I haven't seen any literature about it, but I haven't looked either. :)
comment
There seems to be a way to develop a triadic census for undirected graphs (referred to very briefly in [these ucinet help pages](http://www.analytictech.com/ucinet/help/hs4335.htm).  It essentially says: convert the graph to directed using `G.to_directed()` and then run the existing algorithm. Only 4 triad types will show up in the census -- the ones that only consist of double-direction edges.  Given that the literature on this idea/concept is essentially non-existent, and what is there is a very simple approach to using the current function. I think the best approach to handling this issue is to put a one-line comment in the doc_string that creating a triadic census for an undirected is best done by converting the graph to a directed graph and then running the provided function. Maybe also mention that only the following triads will arise: 003, 102, 201 and 300.  This change should be easy to implement.
comment
go for it. :)
comment
The doc build is failing silently. I'm not sure why it isn't failing loudly. The problem is that the `extendability.rst` file has the wrong function name in it.  My mistake in #4890  Will fix that soon. I don't know why that would affect animation, but who knows at this point.
comment
You will need to add the package `scikit-learn` to the requirements file `requirements/example.txt`.
comment
@yamaguchiyuto any thoughts on this?
comment
The functions in `nx-parallel` do not need to be independent of networkx. This use case is an example of how we could use part of networkx to make a parallel function similar to another networkx function. At the moment, networkx does not provide parallel computations. So your suggestion: 1) looks like one reasonable way to parallelize the functionality you want. 2) doesn't use the `joblib` interface to parallel code that nx-parallel does.  3) could form a skeleton of how to implement this in nx-parallel 
comment
There are a few different definitions of a path depending on treatment of these kind of corner cases.  In NetworkX a path can contain one or even no nodes. I'll close this as we won't implement it, but the conversation can continue. :)
comment
It is not "clear" that a single node or no nodes are not a path. To people using the literature that defines a path as a sequence of nodes, those are perfectly good paths. Removing them from consideration does not make it more clear.
comment
It looks like the gexf file is providing default attribute types of `int` while your data is not integer.  It is hard to see what is going on with so little code. Can you at least show the full cycle of wriet to read? And maybe construct a simple (1 edge?) example that does the same thing so we can run it on our machine without a big download?  If you are not specifying the type of the data, then check the writing and the reading function's optional arguments to help align the types. Is there a specific reason you are using gexf, or was it the first one you tried. If there is a reason to use it then you will need to dig into how to specify the types better. If not, you could try another format that isn't so particular. 
comment
At the very least we should explain what we do and give examples in the doc_strings for how to do other things. Perhaps our "operators" should not copy node or edge attributes at all, and just have the doc_strings give the code to move attributes for people who want the attributes moved.  Another extreme version could provide a keyword argument that lets the user choose between no-attributes, shallow-copy attributes, and deep-copy-attributes.   To be clear, the current behavior is that attributes are shallow-copied. That is any containers are copied by copying the pointer to the container rather than copying that container.    This saves memory and time compared to deepcopy where the containers and any containers they contain and so forth would be copied.  And this shallow-copy method works for most attributes.  Even the attributes which are containers work this way unless you expect to be able to access attributes on the original graph as separate things from the newly created graph's attributes.
comment
You don't need the `to_multigraph` methods stuff.  Just let the user use `H = nx.MultiGraph(G)` or similar.
comment
It looks like handling multiple targets was introduced in #3138 and at that time the if-structure was changed from ```python if source == target:     return [] ``` to  ```python if source in target:     return [] ``` The current post's corner case was not discussed at the time.  The previous behavior is different because with only one target, when that target equals the source, you don't get a simple path. It is a cycle. But with multiple targets, we now have the case in the OP -- what about paths to the other targets?  A workaround is to call the helper function directly: ```python list(nx.algorithms.simple_paths._all_simple_paths_graph(G, source=1, targets={1, 2, 3}, cutoff=10)) ``` or to call with `target=set(target) - {source}`.  But I think the fix is pretty easy. We can let the code do it's thing with the full set of targets.  But we could retain the spirit of the original code (avoiding extra computation in a corner case) by making the check: ```python targets.discard(source) if not targets:     return _empty_generator() ```  Does this make sense?
comment
Indeed -- at least based on a definition that a path is a sequence of nodes.  But I guess that also means that a list of no nodes `[]` is also a path.  Isn't the empty sequence a sequence?  These corner cases are rarely described in a satisfying manner in textbooks/references.  But `[source]` is not a path if we go by [the definition e.g. wikipedia](https://en.wikipedia.org/wiki/Path_(graph_theory)), that `In graph theory, a path in a graph is a finite or infinite sequence of edges which joins a sequence of vertices.`  So, I think we should decide based on whether users will be more troubled by having to remove the single node paths (and zero length paths) when they don't want them vs by having to add the single node paths when they do want them.  Backward compatibility suggests we should not report these paths with less than 2 nodes in this PR.  But I am open to discussion about whether we should have another PR that deprecates the current behavior and adds single node (or less than 2 node) paths in a future version.  I'm -0.5 on adding them because I believe more users will want to remove them than users who will need to add them.  In the meantime, we should add a sentence or note to the doc_string that this function does not yield paths with less than 2 nodes, so those should be manually added by the user if desired.
comment
You are sooo right...  And the `is_simple_path` also recognizes a single node path. So in our simple_path functions we have: - `list(nx.shortest_simple_paths(G, 0, 0))` returns [[0]] - `nx.is_simple_path(G, [0])` returns True - `list(nx.all_simple_paths(G, 0, 0))` returns [] - `list(nx.all_simple_edge_paths(G, 0, 0))` returns [] - `nx.simple_cycles(G)` yields single node paths when a self-loop exists.    So, the evidence for yielding single node paths is much stronger than I realized.  @MartinPJorge do you have an opinion about returning single node paths?  I think I am now +0.5 on reporting single node simple paths.  If we do change the `all_simple_paths` and `all_simple_edge_paths` to yield the single node paths, I think we would need to deprecate the current skipping of those edges. Does anyone think we should fix this without deprecation (e.g. because the current behavior is wrong)?  Also, looking briefly at the current code, this might require that we change how the `cutoff < 1` check is done. And maybe we could get rid of `empty_generator` now and make the functions be generators which use `yield from ...` instead of `return ...`.
comment
Great -- I think we have consensus on changing #6694  1. report the single node paths (and show this in doc_string examples) 2. add a test that these functions report single node paths. 3. If you want (otherwise we will do it in another PR) add tests to the other functions mentioned above to ensure that they treat single node paths as paths. Those tests may already exist. But if they don't it'd be good to add them.   @plammens Can you do 1) and 2) and let me know if you want us to do 3)? Thanks!!
comment
More evidence: ```python G = nx.empty_graph(0)  # or any graph with 0 as a node nx.has_path(G, 0) ``` outputs   `True`
comment
> So at some point in time this was considered an intentional feature instead of a bug.  Yes, until this thread -- and sometimes during this thread -- it was considered as an intentional feature instead of a bug. We are definitely walking back what we had decided before in this case.  > if we consider [0] a simple path in the format of a list of nodes, then the equivalent format as a list of edges should be []. Therefore this should return [[]], correct?  I'm not sure what the right definition of an edge-path for the path `[0]` is.  If there are no edges, it is an edge path??  That is, is `[]` a path? If it is, then you're right that this line of code should return `[[]]`.  I can also see an argument for `[]` with a note in the doc_string that empty paths are not returned. Is there any source for a definition of an edge-path that indicates this is correct?
comment
The failing CI test is not related to this (nightly build isn't available for cugraph).  This looks like the right approach.  I haven't had a chance to look at the details yet, but the big picture looks ok
comment
Could you merge or rebase to the current `main` branch?  If you don't have the resources to do that, let me know and I should be able to get to doing it in the next day or two. This error required us to change where we were looking for the cugraph stuff.
comment
The Pajek manual describes the CLU, VEC and PER file formats. It basically says that a CLU file has a single header line (with the number of lines in the file) followed by a community identifier for each node each on it's own line. It seems like this could be read by something like this: ```python with open("filename.clu") as file:     communities = {i: int(c) for i, c in enumerate(file.readlines()) if i > 0}  nx.set_node_attributes(G, communities, "community") ```  For writing a clu file, we write each node's community on a separate line. ```python comm = nx.get_node_attributes(G, "community") header = f"*vertices {len(G)}\r\n" values = "\r\n".join(str(comm[n]) for n in range(1, len(G) + 1))  with open("filename.clu", 'w') as file:     file.write((header + values).encode(encoding)) ```  If this is correct, I'm -0.5 on including this functionality in NetworkX because it seems like something that users can do easily with basic python and doing so avoids having to maintain 4 more functions. The VEC and PER formats are similarly describing node attributes with a list of values in order from 1 to len(G). So, we would be avoiding 12 new functions.  We already support the Pajek network files. Should we support the node-attribute files, CLU, PER, VEC?
comment
I think we should go ahead and close this PR.  Thanks for the effort you put into it! But I think we will just include the network reading information to keep it simple and avoid maintenance headaches.
comment
Be careful. The inner functions stop their algorithm when the target is reached. For `all_shortest_paths` we can't do that or we won't get all the shortest paths. We don't provide a `target` to the inner functions because we don't want to stop the search when the target is reached.  Does this make sense?
comment
Yes -- that is correct.  I'll close this, but further comments or a new issue/pr can continue.
comment
The functions now raise for a multigraph, but they still don't raise when self-loops are present. (except for `k_truss` for some reason.)  Also note that this changes the previous behavior -- before this, a MultiGraph object with no multi-edges worked. So, if we decide to do it this way, we'll need to deprecate that behavior for two minor version releases. That means we can't use the `not_implemented_for` decorator until the deprecation period ends.   The wording in the doc_strings does not align with NetworkX terminology either. It talks about parallel edges instead of multiedges, for example.  *sigh*  Seems like `k_truss` at least checks for no self-loops. but I don't see any tests for either selfloops or multiedges.
comment
I think your example for parallel edges does give the correct paths, but not the correct edge id because you need a `MultiGraph` to store multiple edges between `1` and `2`.  So the graph you created just overwrote the original edge and there is still only one edge between `1` and `2`.   If you change the first line to `G = nx.MultiGraph()`, I think it gets the result you want.  As for the optional `core_number` input argument, this is to avoid computing the core_numbers many times for the same information. Since that it is the time-consuming part of the other functions, a user that wants to use more than one of those functions can compute `core_number` once and then use the result to compute the other values. They should not expect to create their own core_numbers. It just saves some work if the work is already available to the user.
comment
I think this looks good as it is right now.   It is true that people can get the wrong results if they submit a `core_number` that is not accurate. And by messing with this in strange ways they can provide a core_number input that is valid for a multigraph (so long as it doesn't have any multiedges). But that actually gives the right answer! :}  To make it give the wrong answer you have to change the graph in some way between computing the `core_number` of G and computing e.g. it's `k_core` while using that `core_number`.  But this is natural. If you change the graph the k_core will change, so I would hope that people would only use the input `core_number` if they haven't changed the graph.  The term `multigraph` versus `parallel edges` in the doc_string doesn't bother me. I want to avoid `MultiGraph` because that refers to a class, but `multigraph` is fine. Another related term is `multiedges` which I know is used in other places. I don't think we use `parallel edges` very often. So this shift to `multigraph` makes the docs align better with the rest of the package.  (I think the term "parallel" in "parallel edges" is strange because edges aren't always straight. So brining in terms from geometry might imply things we don't mean. Playing with the words -- and making up new ones like multiedge -- is more fun anyway.)  :)
comment
OK... Thanks for this!   With two approvals and a thumbs up by the OP, I'm going to merge it. :)
comment
Floating point numbers are tricky that way.   Changing the weights to integers also works (as does multiplying the floating point by 10000 and converting to int -- which is often good enough).
comment
This is much clearer--thank you.  But it still is not clear how you are assigning the edges in a network.  It looks like you start with a dataset with three variables for each observation ==> two are continuous variables and the third is a binary classification. Then you use an AI method (which one?) to classify the observations into two groups. The clustering output shows these groups and they are close but not the same as the original groups. Is this correct?  Then you show the network -- but there is no explanation for what the nodes represent, how the edges are chosen, etc.  
comment
Much more complete information here...  Thank you!  The original description had the sentence: "Then using the data from the spectral clustering to generate a graph for the data points."   If I understand correctly, the "graph" is constructed using the affinity matrix by computing k-nearest-neighbors.  But maybe here you mean "graph" (as in draw) a scatter plot of the feature space showing the clusters. In general, stay away from the word "graph" to mean a scatter plot when you've got graph theory in the mix. Graph Theory graphs can be called "networks", while graphs of datasets can be called scatter plots (or whatever other plot they are).  What is the right visualization of the network that will make this algorithm more clear?  I think you have formed the network using distance in the feature space between data points. When you plot it you use a spring_layout for the nodes (which are the data points).  Maybe it would be better to use the feature values as the coordinates of the nodes.  Then you will show the essentially the scatter plot with edges between neighbors.   Another approach would be to plot the nodes using laplacian eigenvector values for two main eigenvalues. That gives a spectral layout which may (or may not depending on the meaning of "spectral clustering") show how the clustering is being determined.   Perhaps both pictures of the network would be helpful in getting the reader to understand 1) where the matrix/network is coming from, and 2) where the clustering is coming from once the network is formed.  What do you think?  Or maybe you can explain why you chose to show the axes and positions you chose -- what is your pedagogical goal and how are you working to attain it?
comment
I have looked into this description and agree with many items. The newly cited paper provides a better description of how to handle the question of selfloops and multiedges than the current citation in NetworkX. Unfortunately it is also different from our current function and that will cause backward incompatibility. Note that the current implementation is not incorrect given the less precise description in the currently cited paper.  I also notice that the name involves 3 authors and neither paper is written by `strogatz`.  I suggest that we implement this change as follows: - retain the current function `newman_watts_strogatz_graph` and add words in the doc_string to describe how self-loops and multiedges are avoided. It should also "see also" the new function described next. - create a new function (perhaps `newman_watts_graph` ?) that returns a MultiGraph and handles the selfloops and multiedges as in the cited paper suggested by this PR.  What do you think? It avoids backward incompatibility by keeping the simple graph interpretation available, yet makes the analytically tractable version available too.
comment
To get this to pass the documentation build tests, we need to include a change to the testing system that has laready been merged into upstream `main` branch.  If you want an exercise in git, you can try to "rebase" this PR onto the current `main` branch. That means you get git to apply the commits from this PR to the current `main` branch and make this branch point to the new "rebased" code.  Sometimes rebasing is as easy as: `git pull upstream main --rebase`. But if the upstream  main changes lines that you have also changed, you'll have to update the commits in this PR to work with applying them to the new main branch. That is called "resolving the conflicts". Sometimes that is quite time consuming. I think this one will be "easy".   But I'm confident that we can just merge this and the documentation will work. This PR only changed some words in the docs -- not it's structure or format. :)   
comment
I don't think we want to exclude the case when triangle inequalities are violated. As described by @mjschwenne, it is the `cristofides` function that requires the triangle inequality to hold. The traveling salesman solution **can and should** include a node multiple times if that is what the graph structure requires to visit all the nodes (think of a network with a long path sticking off the side -- the tour must include going out and going back in along this path).  We are not claiming to return a Hamiltonian Cycle.  Perhaps this can be made clear in the doc changes in #6995  I think that will be sufficient to close this issue unless others feel differently.
comment
The main loop uses `nx.all_neighbors`, but you have restricted the routine to undirected graphs so might be better to use G[node] to get neighbors. So the main loop could become:  ``` for node in G:      if _is_complete_graph(G.subgraph(G[node])):         return node ```  Note also that some "corner" cases may exist.. I think isolated nodes might be found by this routine. Maybe that is OK... I don't know enough about the applications. Similarly, nodes of degree 1 would qualify.  You should spell check the doc_string. The tests could be expanded to include making sure the error(NetworkXError) behavior is correct.  Thanks! 
comment
I think the tests are failing because "an arbitrary orientation of the edges is selected".  Perhaps the test could check the absolute value of the matrix?  So, I don't think you need to rearrange the commits unless it helps you with debugging.  Is there a way to know (or maybe even determine) the order of the edges and cycles in the matrix? That questions makes me realize that this feature has similarities with the laplacian and adjacency_matrix code.  Should cycle_basis_matrix be a module inside the linalg directory? I'm fine with leaving it in cycles.py but fine with moving it too. 
comment
Why doesn't the `cycle_basis_matrix` function use the existing `cycle_basis` and `utils.misc.pairwise(_, cyclic=True)` functions iterating over cycles and adding entries to the matrix? Is this algorithm different? better? cited anywhere?  Wouldn't this work:  ``` edges = {e: i for i,e in enumerate(G.edges())}                                            basis = cycle_basis(G)                                                                    M = np.zeros((len(basis),len(edges)))                                                     for row, cycle in enumerate(basis):                                                           for e in pairwise(cycle, cyclic=True):                                                        if e in edges:                                                                                M[row, edges[e]] = 1                                                                  else:                                                                                         M[row, edges[(e[1],e[0])]] = -1      return M, edges, basis  # give the order of rows and cols to the user.                                          ``` 
comment
This should be replaced by #4822  Pick up the conversation on that thread. Leaving this open for now in case the rebase of that one is too confusing.
comment
Looks like you need to add an import of richcore into the ```__init__.py``` file in the algorithms directory. Also, then your test can simply call ```nx.extract_rich_core```
comment
I think the sigma values you return are actually the sigma^+ values, not the sigmas. Which should be returned?  It also looks like the normalization of weights is not needed (or can be done after all computations). Calculating strength, rank and boundary node can be done without scaling edge weights and the results for rank and boundary node are the same. Scaling strength by minw can be done after computations if desired, but is not necessary.  Am I understanding this correctly?
comment
Needs tests and updates to reflect reviews above.
comment
Can you provide a short description of the goal of this feature? What are you trying to achieve relative to the existing algorithms for topological sort (that is -- why create an incremental system)? The next question is whether you are implementing the published algorithm's data structures as well as program flow. I haven't read it, but your question about whether it should subclass ```nx.DiGraph``` might be answered by whether the paper uses a graph-like data structure.  Thanks!
comment
Can you add some tests of this feature?  See the tests folder.  Any comments about whether this ought to be a universal option for all methods?
comment
It looks like we could allow both backend and flow functions to use the values in the kwargs so long as they don't collide. Is this correct? So the backends just need to choose keyword names for their flow solvers that don't match the ones used by the 3 networkx flow solvers.  But maybe I am missing something.
comment
After some discussion and further thought, here's an idea that seems promising:  Use `functools.partial` on the solver functions instead of allowing kwargs to be passed in dynamically. We would need to make it easy for users to figure out how to do this. Perhaps an example or two in the relevant function doc_strings.   I think this issue is somewhat separate in that it is more about getting the stubs to work correctly. So we could open a separate issue with the `functools.partial` idea. We could also let this issue be the marker for that work.   As far as getting the type stubs working, I think they should indicate what the current configuration is. Yes -- it is possible to have a collision. Authors of backends implementing flow functions should be aware of these potential problems and name parameters to avoid the collision. The type stubs should be able to indicate this somehow even if there is a potential collision.  Other thoughts? 
comment
It sounds like we have addressed the OP question: how to make type stubs for the flow func interface? With the answer that it should have `**kwargs` for these functions and not `**backend_kwargs`.   So, I'm going to close the issue, and we'll continue the surrounding conversations elsewhere. We can reopen if there is more to cover. 
comment
The test errors all seem to relate to a print statement left in place. Could you also add line breaks to keep the code below 80 chars per line?  How likely are people to want the "variant" forms of the UCINET DL format? What is the best way to handle that? Are there easy converters from one variant to another? 
comment
Since the pypy standard build doesn't include numpy, you'll have to make the tests skip your module when numpy is not available. Any module that uses numpy should have the code to skip tests at the bottom, for example the [bottom of graphmatrix.py](https://github.com/networkx/networkx/blob/master/networkx/linalg/graphmatrix.py) 
comment
Some pointers to data structure descriptions for planar graphs: A Dynamic Data Structure for Planar Graph Embedding, Roberto Tamassia https://www.ideals.illinois.edu/bitstream/handle/2142/74258/B37-ACT_83.pdf?sequence=2  Doubly connected edge list data structure for planar graphs. https://en.wikipedia.org/wiki/Doubly_connected_edge_list 
comment
Replaced by #7096 
comment
Yes -- that should overcome the style complaint of the "linters" :}
comment
Thanks @rossbar! That's great!! They look much better already. :)   But you are right that there is probably still work to do for completeness and formatting.  @akshayamadhuri   Let us know if you want to keep going with this PR.  We can use a separate PR if you prefer. 
comment
The way I remember it, the filter classes were created to enable us to implement subgraph views, etc. And the Filter functions (in `filters.py`) were meant to cover the commonly desirable filters and also as examples to how users could build filter functions to do unusual or exotic filtering of edges and nodes. So the `Filter*` classes are not expected to be used directly by users, the filters themselves are expected to be used and morphed by users.   That said, the other "core views" like `AtlasView` and `UnionAtlas` are in that same category -- probably shouldn't be used directly by the users. So either all of those should be included in the docs, or none of them.  I guess I lean toward providing at least the simple kind of docs we have for those classes (and the simple docs created by this PR). I don't see a downside to it (but maybe I am missing it).  I'm sure there are people who dive into the code and want to know more about the underlying data structures. Maybe it is worth including full documentation for those folks, but I'm not sure. They probably want to read the code anyway.  The downside is too much effort spent making those doc_strings complete and well written.  I'm open to suggestions for how to proceed. It makes sense to me to get this PR clean and formatted, but not to worry about including full doc_strings or the "core views" (including `Filter*`).
comment
To get the "style" check to pass, click on the associated "Details" link and it will show you what the style programs think you should change. In this case, a space appears starting the doc_string some of the functions.  Similar style "failures" show in the `cores.py` PR as well.
comment
I think the middle error was fixed in #5550 recently.  So that’s helpful. ;}  The other two cases could maybe raise a NetworkXPointlessConcept exception explaining that there are no edges upon which to build communities.   But I’d prefer that we return an object that makes sense for the request rather than raise an exception.  That way when people apply this function to a large collection of graphs, and one happens to e.g. have no edges, it doesn’t break the process with an exception.  I know they can just use try/except. But it’s probably better for them to just get an empty list back…   Thoughts?  The first and 3rd case can be handled as one case by testing for `G.size() == 0` in which case my suggestion is to return `[{n} for n in G]`. That would be an empty list for the null graph, and each node in its own community for the no edges case. I think these make sense…???    We’ll need tests too, of course.
comment
Looks like in the documentation check deatils under "Build Doc" and then searching for "Animation", line 1375 or so in the Build Doc details identifies an error that the `======` lines aren't long enough around your title for the doc_string. But then it gives a number of warnings that suggest it is having trouble even loading networkx. I'm assuming it is a problem related to this test example -- perhaps the fact that it doesn't use networkx anywhere. But I can't really tell without trying to build locally.   Have you tried building the docs locally? Does the animation work there?
comment
Do we need to include `nbconvert` without restriction on version number?  It looks like this removes nbconvert completely from the file.
comment
There is another typo in a comment on line 172:   It has the formula as "-" Beta when the code clearly uses "+".  Thanks for this!
comment
Excellent @vigna  Can you suggest some wording here relating it to the eigenvector centrality? Or is that difficult....  Maybe something like:  For strongly connected graphs, as $\alpha \to 1/\lambda_\mathrm{max}$ and $beta=1$ this approaches the result for eigenvector centrality.
comment
It can be merged once it is clear what the good resolution of the issue is. Can you suggest some wording for the docs to make this all clear? The discussion above makes me believe that the change to the documentation should not be `beta=1`. But there's a lot of other information you added that is not included in the doc_string. If you can write out what you think this change ought to be then it would help move this PR closer to merging. 
comment
You could load the branch by making a git "remote" to it, but you still couldn't push to it. You could make a PR to the PR-branch, but that still relies on the original author responding.  Another approach would be to pull the branch from a git remote and then add some commits, push it to your fork of networkx and make a new PR. But maybe that's more than needed.   How complex are the changes you are thinking about? Can you make a suggestion in the comments? I can make changes to the PR if you want to make a suggestion. 
comment
Ahh... got it.  I missed that. :) Thanks!
comment
Thanks for this! And for the detailed numerical analysis. The theory requires  alpha <= 1/lambda_max to avoid singular results. But in the limit alpha -> 1/lambda_max we can look at alpha*A as a multiplication operator that shrinks all vectors except the eigenvector corresponding to lambda_max. And it keeps that vector the same length. Thus repeatedly multiplying by alpha*A should turn any vector into its component in the direction of that eigenvector. Scaling the result to a unit vector reveals the same eigenvector for any starting guess that isn't orthogonal to the eigenvector. For Katz centrality we start with the vector beta. So certainly the current documentation is wrong. If beta=0, it is orthogonal to all eigenvectors. And we won't end up with the stated result of the eigenvector as the Katz centrality. It is also true that beta=1 does give the desired result.  But Beta=2 also works... as does beta=0.2 or any beta>0. (So long as beta as a vector isn't orthogonal to the eigenvector).  So I see two possible fixes -- the suggest correction to the doc_string of \beta=1, or the correction \beta >= 0.  We could also be more precise and say that this result holds in *the limit as* alpha -> 1/lambda_max.  But I'm not sure that is causing any confusion. It is the incorrect value of beta=0 that is causing confusion.  Also, the sentence doesn't say this is the only case where Katz Centrality matches Eigenvector Centrality, so I'm fine with the suggested language here (and in the @BrunoBaldissera PR).  Thanks for the excuse to take a romp down the lane of linear algebra and infinite series. :)  Let's fix the doc_string!
comment
Nice! I think almost the same changes are needed in lollipop_graph, but scope-creep could end up making this really big. I believe these docs and tests are largely based on the lollipop docs and tests, so the changes should be very similar too.  I'm fine to let that go to another PR -- maybe considering all of `classic.py`.   Whatever you want to include here is fine with me. :) Thanks! 
comment
See #7034  I hope that fixes the error. 
comment
A patch release could be within a month or maybe a few weeks, especially if we can nail down what's happening with this bug.
comment
I added the label "type: API" because I think we should warn users in the release notes that this changed. Not that anyone will have done this, but you never know. :)
comment
To fix CI, merge the upstream main with your branch. Steps are hopefully something like: ``` git remote add upstream https://github.com/networkx/networkx git fetch upstream main git ch Add-tadpole-graph   # if needed git pull upstream main --rebase ``` The last line will try to apply your commits since you diverged from `main` on top of the current `main` branch. If you have changed a line that was also changed on `main` you will get a "conflict" and you'll have to follow the instructions to edit the file by hand to specify which changes you want to keep. That can be a mess sometimes.  Can you try it and see if everything "just works"? If it doesn't, I can work through that.  Thanks.  The last line can also be:   `git merge upstream main` which I believe is the same as `git pull upstream main --no-rebase` and that can sometimes be easier to deal with the conflicts. But it adds an extra commit if I remember the details right.  I think the conflicts are also on another of your PRs. I have on my list to work through those conflicts. :}
comment
Yay!  I should have noticed that plot_ is needed... :}
comment
This was also suggested in #6435 and even longer ago in #1170 before we even had a logo. :}  So +1 for a favicon. :}
comment
I'm not sure that all the machinery with compose functions is needed. I think @hagberg's code above is almost enough. It uses shortest_paths while the power function needs all_paths. But I think pulling the breadth first search part of shortest_path into this function and changing it to track all paths would give an efficient algorithm--more network oriented than algebra oriented but that's intentional. 
comment
It seems that the definition of graph power from Wikipedia and MathWorld is unclear whether self loops should be included. Both sites use the same definition but it does not say whether the result is a simple graph or not. The examples (figures) on both sites suggest that self loops are NOT included in the power graph. A quick search did not yield any power graphs with self loops.  Does anyone have a good source for the definition? If not, we can use the Wikipedia version and my interpretation including examples is that power graphs should not include self loops.  On Fri, Mar 6, 2015 at 6:23 AM, Mridul Seth notifications@github.com wrote:  > While calculating the power graph, the kth power of G has vertices > adjacent if the _distance_ between them is less than equal to k. And > _distance_ is the shortest path between two vertices. While calculating > number of walks this implementation has considered a walk from 0->1->0 > having a distance of 2, but the shortest distance from the vertex 0 to > vertex 0 is zero. > For example >  > G=nx.path_graph(4)pow(G,2).edges() should return > [(0,0),(1,1),(2,2),(3,3),(0,1),(1,2),(2,3),(0,2),(1,3)] > pow(G,3).edges() should return > [(0,0),(1,1),(2,2),(3,3),(0,1),(1,2),(2,3),(0,2),(1,3),(0,3)] >  > http://en.wikipedia.org/wiki/Graph_power > http://en.wikipedia.org/wiki/Distance_(graph_theory) >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1307#issuecomment-77544753. 
comment
Sounds like the definition you are using can be stated as: the graph with adj matrix equal to the power of the adjacency matrix of the graph.  I see why you think of using matrix operations to compute it.  Which graph power operation should we implement? I like @jfinkels argument about expander graphs. Are there any use-cases for the wikipedia definition?  On Fri, Mar 6, 2015 at 11:57 AM, jfinkels notifications@github.com wrote:  > We're not trying to compute the shortest path distance metric, to which > you refer, we're trying to compute the graph power. In the definitions to > which I linked above, a walk of length two, regardless of the initial and > terminal vertices, yields an edge in the graph's square. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1307#issuecomment-77593434. 
comment
I think it is more than just logical versus counting number of walks.  One definition adds a 1 (or any nonzero) to the main diagonal before raising to a power while the other does not. By leaving the diagonal zero the "walk" must keep moving each timestep. By adding the 1 it allows walks with pauses at any node.  Consider the graph with two nodes connected by one edge. G**2 has two self-loops. But does it have the edge connecting the two nodes?  In one definition it does and in the other it doesn't.  On Fri, Mar 6, 2015 at 12:44 PM, jfinkels notifications@github.com wrote:  > @dschult https://github.com/dschult Yep, maybe I didn't make that clear > enough initially: the adjacency matrix of G *\* k is A *\* k, where A is > the adjacency matrix of G. >  > If you just want to know if there _exists_ a walk from a source node to a > target node within a certain number of steps, you would use Wikipedia's > "logical graph power". If you want to know how _many_ walks exist, you > can use my more general definition. (Mine is more general because if you > know the number of walks, then you know if there are greater than zero > walks; this is basically what I'm doing at > https://github.com/networkx/networkx/pull/1307/files#diff-414eaa09098b00206eecbff1d545bbf4R212; > something similar happens in from_scipy_sparse_matrix, due to changes I > suggested in other pull requests.) >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1307#issuecomment-77602047. 
comment
The definitions you cite are for regular graphs, and it's not transparent that the printed definitions there are what you are computing (though they may be). Perhaps we can find a different name for what you are computing because it doesn't seem to be what many people call the power graph. 
comment
That definition matches the wikipedia and mathworld definitions (where the examples show the result as a simple graph).  1) does it allow self-loops? (it doesn't say the result is a simple graph, though maybe it is implied and the examples may be needed to know). G is a simple graph, but the definition doesn't say whether G^k is. 2) The distance is "at most" k which implies that we make the diagonal entries nonzero before raising the adj matrix to a power.  If I understand correctly, that is not the definition you have implemented. But maybe Im missing something.  On Sun, Mar 8, 2015 at 6:56 PM, jfinkels notifications@github.com wrote:  > Here's a definition of graph power from Exercise 3.1.6 of _Graph Theory_ > http://www.amazon.com/Graph-Theory-Graduate-Texts-Mathematics/dp/1849966907 > by Bondy and Murty: >  > The _k_th power of a simple graph _G_ = (_V_, _E_) is the graph _Gk_ > whose vertex set is _V_, two distinct vertices being adjacent in _Gk_ if > and only if their [shortest path] distance in _G_ is at most _k_. >  > So there's a concrete, written definition from a contemporary graph theory > textbook. If you want to rename my "power" function, maybe I would call it > "walk power". I have updated this pull request with that name. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1307#issuecomment-77780418. 
comment
This Issue is largely closed by #5908.  That `number_of_walks` function provides all-pairs number of walks of given input `walk_length`. This provides the functions from this PR: `single_source_number_of_walks`, `all_pairs_number_of_walks` and `number_of_walks`, and it is faster since it uses scipy instead of pure python.  Other items here:  `walk_power` which creates a multigraph with the number of multiedges indicating how many walks go from those two nodes. But I think it is better to represent the number of walks with a number than with multiedges. A user could construct the walk_power graph using nested for loops something like: ```python walks = nx.number_of_walks(G, walk_length=5) MG = nx.MultiGraph() for n, nbrs in walks.items():     for nbr, nw in nbrs.items():         for _ in range(nw):             MG.add_edge(n, nbr) ``` It's not a one-liner, but it is fairly straightforward and I think it would be rarely wanted.  So, I'm going to close this issue unless someone comments soon.  :) (it can be reopened of course)
comment
Closing this "walk" issue. As previously described: it has been largely implemented and the other functionality is declared not worth implementing.  We can still post here and reopen if needed.
comment
Yes, this is probably a one-liner that could be included in the doc_string of to_node_dataframe:  ```python # df already created and G a graph you want to update/add the node data attributes G.add_nodes_from((n, dict(d)) for n, d in df.iterrows()) ```
comment
@j6k4m8 that is the [already existing function](https://networkx.github.io/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html): ```nx.from_pandas_edgelist()```  This Issue deals with moving data between dataframes and the **node** attributes. 
comment
Well.... It will be easier to do that after #4217 is merged. And to be clear, this issue is fixed by adding a simple example that shows the one-liner to the doc_string of the function `to_node_dataframe` (that's why you have to wait until the other PR is merged).  It might be more immediate to contribute, for example, by fixing #4432 which is also a documentation issue.
comment
This one needs #4217 to be merged first. Try looking at the [issues with label "Documentation"](https://github.com/networkx/networkx/issues?q=is%3Aopen+is%3Aissue+label%3ADocumentation)
comment
Thanks for reporting this!! Is the `nxleft` and `nxright` in your example code (in the assert statement) supposed to be `left` and `right`?  Also, can you describe how you know these are isomorphic graphs? Do you have the isomorphism? Or is that result based on previous networkx results?  
comment
I can verify that `could_be_isomorphic` and `fast_could_be_isomorphic` return apparently false results for this case. And I've traced it to the computation of `triangles`. One node is only connected to itself by a self-loop (node "46" in right which is isomorphic to node "8" in left).  Those nodes are getting a different number of triangles in the two graphs when using `nx.triangles`.  The self-loop is the only edge involving those nodes in the graphs.   `nx.triangles was changed 3 months ago in #6258 so I think an error was introduced there -- something about self-loops but even worse, something that counts the triangles differently depending on relabeling the nodes. 
comment
Thanks soo much for reporting this bug. It would have been hard for me to find for sure.  The error in triangles comes down to using `n is not node` to rule out self-loops instead of the correct `n != node`.   I guess the result is: we should never use `is` to compare nodes. Always `==`.
comment
I agree that this is a nice layout -- and it is nicely implemented here (short and understandable).  But this is not a layout on the unit circle. The unit circle has radius 1. And this layout has nodes at many different radii. Looking at the picture you draw the nodes look more like they are laid out on the spokes of a wheel. There might be a better description of those straight lines coming out from a point. Is there a history/literature for this layout? I can imagine that it is common enough that a standard name doesn't exist.  I don't think `arc_layout` describes it very well though because the nodes are not lying on an arc.  Thanks for this!  I like the addition. I'm just wanting a different name/description. :}
comment
Thanks for that history.  It **does** appear that the term [Arc Diagram](https://en.wikipedia.org/wiki/Arc_diagram) or "arc layout" are already used for the general 1-d layout with edges drawn as arcs. I'll look for a good term for this...  Does this work when there are more than 3 types? Or is it really just for tri-partitie graphs?  I imagine that with 4 or 5 groups the edges would cover each other and make the connections hard to see.  Maybe in that case, the nodes should be on e.g. 4 short lines in the corners of a larger square. Then the within-group lines don't show but the connections between groups do.  Similarly for 5-partite with short lines of nodes in each corner of a pentagon.  But maybe thisPR method works well with 4 or 5 groups too?   I'm beginning to think this might make a good [gallery example](https://networkx.org/documentation/stable/auto_examples/index.html):  (see the code for these in the repo under `examples/drawing` or a different group of examples if you prefer.)  What do you think of that?
comment
I agree that in order to be 2-edge connected, the graph needs to be both connected and to have no bridges.  Can you add a test of a simple case to make sure we don't accidentally let this slip back in?  Thanks for this! 
comment
I'm going to close this issue as not implemented. We can discuss further here and reopen if interested.
comment
Can you look at #6438 and see if that helps with these issues? I'm pretty sure it addresses the second problem, but I think it might also at least partially address the first. It would be helpful to have someone with more intuition about this take a look at it. :}
comment
Let's take a stab at a new algorithm published in 2019 by van def Zander and Liskiewicz: [http://auai.org/uai2019/proceedings/papers/222.pdf](http://auai.org/uai2019/proceedings/papers/222.pdf)  The algorithms takes linear time instead of potentially quadratic time (no moral graph is required to determine d-separated) and discovers or determines d_separated as part of the minimal_d_separator process.  So the new `d_separated` function would be a natural step of the `minimal_d_separated` algorithm.   It seems like the condition for d_separated involves the closure of X with respect to Z intersected with Y. If that is the empty set, X and Y are d_separated by Z. Determining minimal_d_separation involves finding both the closure of X wrt Z and the closure of Y wrt Z. If Z is in the intersection of the two closures, Z is minimal. The bulk of the effort then is spent finding the closures. But that is a linear process and I think not too convoluted, though I haven't worked through it all.  What do you think?
comment
Yes -- I was also thinking the first PR for the new algorithm would only be for DAGs. And it doesn't need the special treatments like: some nodes must be in Z or some nodes can't be in Z (the sets R and I) unless you find that easier.  Thanks!!
comment
That looks good to me for an API.  (no need to have a function find_d_separated without minimal right?)  I would also prefer if the API could allow X and Y to be either single nodes or iterables of nodes. I always have trouble switching from input {4} to 4 and back again when switching functions but keeping the same inputs. The logic would be ```python if u in G:     u = {u} ``` Note that when u is an iterable, this doesn't change it. (maybe u needs to be a set---I haven't checked.)  I think this is already clear, but just to make sure: the new PR will change the current API in that: - minimal_d_separated checks for both minimal **and** d_separated. - find_minimal_d_separated will return None if no separating set can be found.  I think both of those API changes are bug-fixes (based on these discussions) so no deprecation will occur, but we will need to flag the change in `release_dev.rst` as an API change and separately as a highlight that the algorithm was improved from quadratic to linear.  Thoughts?
comment
This is almost certainly due to the fact that you expect the rows and columns to be ordered in alphabetical order when they aren’t ordered that way. Since you did not supply the `nodelist` keyword argument, the order of the nodes in the rows is the same as the order of nodes reported by G.nodes.  That order may not be alphabetical.  In this case ‘H’ is added to the graph before ‘G’ so the order hasH before G. The resulting matrix is in fact correct with that ordering.
comment
This looks good...  Can you check out the docs (starting at the [bipartitie reference to find_extendability](https://2163-890377-gh.circle-artifacts.com/0/doc/build/html/reference/algorithms/bipartite.html#module-networkx.algorithms.bipartite.extendability))  These are available from the final test "Details" link. The final test is a "Build Artifact" -- that is, something created during the Circle CI test -- it is a build of the documentation webpages.  Anyway, you can see what the docs will look like in the online documentation and change things if you want.  It looks like the docstring needs different underlines on the sections: ``` Parameters     ----------     G : NetworkX Graph ---------- ``` to similar but different: ``` Parameters     ----------      G : NetworkX Graph ``` Similar changes to the other sections will help them format nicely too.
comment
I'm not sure what exactly is causing tis error, but it is certainly not your code. So, the way I would try to solve it is to merge the networkx main branch into your PR. It is covered in the CONTRIBUTING.rst file at the base directory of the repository. But in short:  ```bash # add the remote if you haven't before...  git remote add upstream https://github.com/networkx/networkx.git git remote -v  # check that it worked  git branch -v  # make sure you are on the branch you want to update git pull upstream main  # there may be some conflicts when merging if someone has made changes on the main branch to files you are changing. # edit those files to make sure you don't un-do whatever someone else did that conflicted with your work. git status  git add ... git commit -a git push origin <branchname>  # or just `git push` if you aren't doing other stuff in other branches you don't want pushed ```  
comment
Two minor changes based on the latest update:  - can you add a couple of sentences after the paragraph describing what this function does (after the lemma). These should probably be a new paragraph and they should describe what kind of feature k-extendability is supposed to measure and why someone would want to use it? - the style checker found an extra space at the beginning of one doc_string (after """ and before the first word.
comment
I find quite a lot of literature using searches:  "extendable graph theory" and especially "k-extendable graph theory". So, I think this should be included. I'll try to look for a good article to point to for the docs.  So, while extendability doesn't show in the searches, extendable does. And extendability is the measure of how extendable a graph is. :}   You can nounify anything!! (see how I verbified noun there?)  I'll try to: - [x] look for a good article to point to for the docs. - [x] look through the public functions proposed and see if there's a better naming choice to align with the literature.
comment
A good canonical article to link to for k-extendable graphs is: `M. D. Plummer. On n-extendible graphs. Discrete Mathematics, 31:201–210, 1980 https://doi.org/10.1016/0012-365X(80)90037-0`  As for names, the original article uses `n-extendability` to be the trait of being n-extendable. So the *n-extendability* of G is True iff G is *n-extendable*.  The quantity in this function is the maximal n such that G is n-extendable. I think a better name for the function would be: `maximal_extendability()`  Thoughts?    (These two items fulfill my missions stated above, so I have checked them in the comment.) 
comment
I believe the sum of the node degrees is twice the number of edges (each edge adds 1 to the degree of two nodes). So this formula **is** computing the number of edges. The definition of `m` seems to be ok, at least for this case.
comment
When handling a weighted graph, `G.size(weight=weight)` will give you the sum of all the edge weights in the graph. Dividing by 2 gives `m`.  Thus `m` is the number of edges when we have an unweighted treatment and the sum of all edge weights when we have an unweighted treatment.
comment
The core developers assign labels now, because it controls where the title shows up in the release notes. We should probably include that in the description of the labels test
comment
New wording suggested are appreciated. Thanks!
comment
Yes -- that is a good way to have the official article references as published and then also have the arxiv link available. Thanks!
comment
I added a release note about `is_semiconnected` removing the `topo_order` kwarg.  And I added paragraph description of the algorithm because I remember someone saying that the doc_string should include the relationship between semiconnected and topological sort.  (maybe that was Jarrod?)
comment
I think this might be the fix for #6412  Can you see what is written there and comment on your perspective?  Is this the same question/issue?  And, can you add a test to ensure this correction isn't removed in a later change?  The tests are in the `networkx/algorithms/community/tests` folder. Ask if you have questions... Or push what you think might be ok and we'll give suggestions.  Thanks very much!!
comment
I've added a test for whether selfloops in the input graph affect the resulting partition. The fix does not get rid of all impact of self-loops. It corrects the code to make the treatment of self-loops the same for directed and undirected graphs (and corrects it to the published algorithms). But there still is a small amount of impact. The original self-loops in the original graph are treated as pre-formed communities that have already been condensed and now have a self-loop edge. I've added a paragraph which explains this and suggests that people who have edge weights that don't represent pre-formed communities remove the self-loops before using these function.  As the tests show, when the self-loop edges have small weight their impact is minimal, but if the self-loop edge weights are large it changes the resulting partition quite a bit.
comment
The test seems to work. And it seems like a reasonable approach. So it looks good to go ahead and decorate more graph creation functions
comment
Thanks for the bug report and and thanks for the fix. :) This problem has been around for a long long time, and shows how little use the recursive version of the function has received. I can verify that the bug is present -- users should not trust results from the recursive version of the code until it is fixed.   See #6898
comment
Don't we need to fix the bug in the code if we leave this as deprecated. It currently returns incorrect results. I'd prefer just to get rid of it -- or perhaps to deprecate it but call the non-recursive function during the deprecation cycle.  The example from #6897 shows that the recursive version of the code gives incorrect results. And [#6897(comment)](https://github.com/networkx/networkx/issues/6897#issuecomment-1704320361) points out the error.  This error has been there for a very long time -- suggesting that people are not using this function (which they shouldn't). 
comment
The failures here are not related... And geospatial example somehow doubled the size of its array.  And because I tried to use a github comment "suggestion" instead of just pushing to this PR, ended up with two commits instead of one and my test was hidden (unless you click "show resolved"). So here's that text:  >I'm wondering if we could add an example that doesn't use datetime. I tried quickly to take this example and make the dates just a year as an int e.g. '2012'. To get it to work, I had to make time_delta=4 instead of time_delta=5. Did I miss something? Or maybe there is an "off-by-one" error in the 5*365 value (or in how time_delta is used).
comment
It might be helpful to look at the second reference paper where they make some small improvements on the Tarjan approach. See the link in my comment above. I looked at the original paper and it does indeed use an integer much like `cnt` to identify when each node is visited. And it also computes the min of the `lowlink[v]` and the `Number[w]` in the case that the neighbor `w` has a number. But looking at the more recent paper, those details are changed. (which I think makes sense because we are looking for the lowest value to put into lowlink(v).) The pseudocode is quite readable and largely matches what appears in our functions.
comment
I find that papers are much easier to follow than lecture notes because they are self-contained... everything is defined in one place rather than having to find the previous set of lecture notes, etc.  But lecture notes are easier to find because people post them openly on the internet while papers are published by journals who want us to pay for them :} so they make them hard to get for free.  Can you tell what the function name `dsc(x)` is an abbreviation for? What is the intuition behind distinguishing between `low` and `dsc` (between the `root` and the `cnt`)?  These notes certainly have the pseudocode you are talking about with the `low(x) = min(low(x), dsc(y))` on slide 29. But it doesn't describe why the minimum should be this way instead of `min(low(x), low(y))`.  That page also shows that `low(y) < dsc(y)` because they are equal early on, the `dsc` value doesn't change, and `low(y)` can only decrease. I've constructed two versions of the function -- one with `min(root[w], visited[y])` and the other with `min(root[w], root[y])`. They produce the same results for many randomly generated graphs I have created.  The paper giving "improvements" over the original Tarjan algorithm claims the minimum can be over `root` values. Can you explain why it might be better to have `visited[w]` in the min? 
comment
Won’t it get to the final root value faster if we set it to the lower value?  We are trying to find the lowest value — and I can imagine that this would speed it up. But I can’t wrap my head around what part of the search it would save us from going through.
comment
I am leaning toward removing this function. We never wanted to spend much time maintaining "recursive" versions of functions. They typically shouldn't be used in Python if an efficient non-recursive version exists. But we keep them around because they are sometimes instructive -- so to the extent that there is some pedagogical potential we leave them in place.   But if they are producing incorrect results and it's not clear what is best done to fix them I think we should probably just remove them.  In this case we should remove `strongly_connected_components_recursive` and have people just use `stongly_connected_components`.   Thoughts?
comment
Let's make the change to remove the recursive function. So I'm closing this in favor of #6957
comment
Looks like both contextily and nbconvert changed versions on our CI between the success and failures. My recent update here leads to an nbconvert exception.  I see that they released 7.9.0 12 hrs ago and then 7.9.1 just after this ran, so I'm going to start it again to see if 7.9.1 fixes what we need.
comment
Yes -- the new release for nbconvert is causing the 'no attribute "exporter"' error in 7.9.1 and not in 7.8.0. Should we pin contextily and nbconvert versions for a week or so?    We might see more release issues as package push to release for Python 3.12 support.
comment
I think the exporter package/module is still there. But they switch the `__init__.py` file to list specific entries to import instead of star.  So -- PR 2040 https://github.com/jupyter/nbconvert/pull/2040 no longer imports the exporter module. Only its contents.  So the code that is using it (inside nb2plots?) just needs to remove `.exporter` from their code and it should work.
comment
This might be good to include in #6948 since that is close to being merged and would result in a conflict because that adds a new keyword arg `first_label` and changes the default of the `label_attribute` arg.  It would be more in the style of networkx to name that arg `label` rather than `label_attribute` too, but that's probably bike-shedding. :}
comment
Thank you for the timing and memory comparisons. And good to also include the comparison for a single pair of nodes.  TLDR: I prefer this pseudo-inverse method over the determinant method.   The time gain and flexibility of computing for multiple pairs is worth the rather small impact on memory.  Details: Comparison could be tricky: - The current version uses determinants to find the result for a single pair of nodes. (less memory, more time) - This PR solves the whole circuit/graph essentially finding the result for all pairs of nodes. (less time, more memory)  The first method saves some memory relative to the second, but at a large computational cost. There is a general rule of computational liner algebra that determinants are often great for proofs but often bad for computations. In this case, it is a further disadvantage that the determinant route doesn't even give results for all pairs -- only for one pair. You must compute the determinants for "L without node a" and "L without nodes a and b". So to do all nodes you must compute lots of determinants. I looked at the determinant method to figure out whether it might be converted into an all-pairs method, but I don't see a good way to do that.  Looking at the time/memory comparison for a single pair of nodes (the environment favoring the current code), I don't believe the memory cost of the PR methodology could be more than twice that of the original (scales nicely). The time advantage of the PR is present, but not large in this setting. But the memory cost is also not large. Indeed the memory savings might be saved elsewhere (e.g. by eliminating the copy of the original graph).  But the advantage of the PR is that you've basically solved the entire problem to get the first pair result. This leads to very large time savings very quickly.    Perhaps we could make up for that memory impact by e.g. rewriting `nx.laplacian_matrix` to allow a function `weight` instead of forcing a string attribute (see #4085). That would eliminate the need for copying the graph. But I haven't checked that copying the graph is significant in this method.  
comment
I like that calling pattern too. `nx.is_tournament` (it is the one tournament module function imported to the main nx namespace) and `nx.tournament.<fn>` rather than an import at the beginning followed by `tournament.<fn>`.  This is the way we'd like users to refer to these functions, so it's good to have the examples follow that idea too.  Thansk @davidbonin92 !
comment
Thanks for this!    It looks like `spectral_bisection` has been left out of `doc/reference/linalg.rst` Can you add it to the section on algebraic connectivity?
comment
Could this go into the `distance_measures.py` module in algorithms? Is that what this is used for? Maybe a sentence or two in the doc_string about what this measures would be helpful too (right now it says how to compute it, but not what it is trying to measure).  Thanks!
comment
This test failure is very strange.  It only happens on the Ubuntu system with Python 3.11.  The runs with other systems work with other versions of Python on that system work. And the test just has G with 3 edges with integer weights. Nothing fancy... check if the `nx.is_negatively_weighted` function works.   BTW the check for negative weights hardcodes the edge attribute named "weight" while the rest of the code uses an input parameter `weight`. You should probably have a test for the case of edge attributes named something else.  But that doesn't help figure out why this is working on Ubuntu and working on Python 3.11 but not working on Ubuntu with Python 3.11.
comment
Thanks @MridulS !!   And, yes, we should think about how to make it easier for new contributors to figure it out.  But why did only the Ubuntu test with Python 3.11 fail?  If that test handles the loopback differently, maybe we can name the test somehow to indicate that it is testing the loopback feature. ??
comment
I'm going to close this since the Issue was closed. Comment here or reopen if desired.
comment
Is this available in `nx.readwrite.text.py` with functions `nx.write_network_text(G)` and `nx.generate_network_text(G)`?  I think supporting the "rich" ascii features has been started in #5821 and an invitation to build on that to get it merged with NetworkX. :)
comment
I pushed a commit to add keyword args for these geometric generators. Feel free to undo any or all of these changes.  I chose `pos_name` as the keyword arg. It is as short as "position" and conveys the idea that this is an attribute name without using the attribute word (which is long and if used as "attr" is confusing as to whether you mean the attribute-value or the attribute-name).  That means I also added `weight_name` for two of the functions. They already have a keyword arg "weight". So I hope this is clear and also not confusing that we don't follow the convention elsewhere to use "weight" for the keyword that specifies the name of the attribute.  I got carried away and so added "seed=42" to a number of `thresholded_random_geometric_graph` tests. I also cleaned up some test function doc_strings. I hope those aren't too annoying -- undo any that you don't like.
comment
This sounds like the right organization of this info.  Thanks @rossbar 
comment
I think for the most part you would still simply want to check whether a value is different from the default. That is, I would favor API design that doesn't distinguish whether someone used keyword or positional syntax to provide the input value.  But -- there are tools that can do it in [Python's `inspect` library](https://docs.python.org/3/library/inspect.html). In particular the `inspect.Parameter` and `inspect.Signature` objects.  But I would not say that these are an easy way to check for positional/keyword passing.
comment
Would it help to have examples where the sequences are *not* graphical sequences?  If that is better for another PR we can leave this as it is. I think the examples here all look good.   For example: ```python >>> nx.is_valid_degree_sequence_havel_hakimi([1, 2, 2]) False ```
comment
Yes, that is what I was thinking. I'm hoping that they would be very simple and short examples. 
comment
Thanks for the updates.  To add the non-graphical examples, one way to proceed would be to add 1 to the last entry in the sequence. That requires making `sequence` a list instead of a generator. Then adding something like this should do it. ```python     >>> sequence[-1] += 1     >>> nx.is_multigraphical(sequence)     False ```
comment
This suggestion for the default behavior of `label_attribute` would make `nx.join` match the behavior of `nx.convert_node_labels_to_integers` which is very similar in it's relabeling goal and interface.  Looking at that function brings up the idea that maybe this should also have a `first_label` optional keyword argument for people who want the nodes to be integers starting at some offset from zero.  I could add both of these changes to #6908 depending on this discussion.
comment
Nice idea of computing in 2D and then projecting onto a best fit line! :}   Another approach could be to pick one node as the "root" (position 0) and set the position of the others to their weighted shortest path distance to that root node. You could use [single_source_djikstra_path_length](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.weighted.single_source_dijkstra_path_length.html) to get the lengths even though it is a complete graph the weighted nature might show something. I have no idea if this works or is even close to what you want. :)
comment
@oddoes for simple graph it looks like you used `nan` (which is a `float`) but set the dtype to `int`.  That's why it shows a large integer value instead of nan.  I can't easily follow the other questions. Can you put the code you are asking about inline with your comments? To do that, use \`\`\`python at the beginning and \`\`\` at the end.  ````python ```python G.add_edge(2, 2, weight=3) G.add_edge(1, 2, weight=4) ``` ````
comment
I think adding `is_tournament` to the main namespace makes sense.  But I would want the code for that function to live with the other tournament code.  And if people are going to discover the function via ipython's tab-completion feature, then we need to make sure they can easily see enough of the doc_string to be able to understand what a tournament graph is. I haven't looked at that. Maybe it is already just fine. :}
comment
This was designed this way to avoid backward incompatibility.  But of course now it is an inconsistency. We should probably do something....  The thinking was as follows: MultiGraphs should have G.edges yield 3-tuples because that is the unique representation of each edge. But we have code that could be much easier to maintain if we return 2-tuples because then the multigraph and graph objects can both work with the same code...  It is useful in this sense to have an option where the same syntax for Graph and MultiGraph yields 2-tuple edges.   In addition, the v1.x code all used ```G.edges()``` and always got 2-tuples with this syntax. So, we left ```iter(G.edges())``` unchanged in the transition to v2.0. But we implemented G.edges as an object that yields 3-tuples.   It'd be great to find a consistent useful easy interface...  Actually, consistency is more than you might suspect because ```G.edges``` produces a ```MultiEdgeView``` which yields 3-tuples while ```G.edges()``` returns a ```MultiEdgeDataView``` which yields 2-tuples.  So, if you think of them as different beasts, then no illusion of consistency arises.  But the names are so close to each other that it is easy to start thinking of them as the same thing...  The "How do we handle multiedges?" question rose up very early on...and it will likely continue to be an issue. What' sthe best way to handle G.edges and G.edges()?
comment
Yes, this problem occurs in undirected G.edges and other places as well. It is not restricted to strings either. ```G.out_edges((-2, -3, -4))``` returns an empty edge view too if none of the entries in the tuple are nodes AND the tuple itself is not a node.   Summary:  If nbunch is a container or iterator that is not itself a node and none of its elements are nodes, then the edge view suite of methods return an empty edge view instead of an error.    I could say that it is a feature -- not a bug. But I see how this could cause trouble.   I think we want:      G=nx.path_graph("fee")     G.add_edge("fee", "paid")     print(G.edges("foo"))     # [('f', 'e')]     print(G.edges("fee"))     # [('fee', 'paid')]     print(G.edges('bar'))     # currently [], but maybe should be something else?  I suppose we could track whether a container of nodes has ANY element in the network and if it doesn't raise an error like ```NodeNotFound```.  Other ideas?
comment
I think the existing API can be used in a more restrictive way to do as you say: - Use G.succ[u] to get edges (actually neighbors, but you can make edges) of a particular node u - Use G.out_edges(container) to get edges from any of a container of nodes.  You can treat the API that way now. But it is not enforced. So, we would simply enforce ```G.edges``` and friends to  either provide a container of nodes, or None. G.edges(3)   -->   G.edges([3])     OR    G.adj[3] if you want neighbors G.edges((2, 3))   --> G.edges([(2, 3)])
comment
This is probably due to the order in which nodes are stored in sets. We could try replacing "sets" with "dicts" in ```strategy_smallest_last``` in [the code](https://github.com/networkx/networkx/blob/3351206a3ce5b3a39bb2fc451e93ef545b96c95b/networkx/algorithms/coloring/greedy_coloring.py) and see if that fixes this issue. The dict acts as a set and you can set all values to e.g. None.  Comments in the code suggest that sets are used for fast removal and dicts have that property as well.
comment
Thanks for the quick turnaround.   And -- That looks like a nice example! :) I think that would be reasonable addition. Would it be better with the height (value of the time series) shown? ```python pos = [[i, v] for i, v in enumerate(time_series)] ``` I don't know what is typically used for this kind of analysis. So we should go with whatever is pretty standard. If nothing obvious has become standard than we can do what you think best conveys the information. Thanks!!
comment
The code looks good, but the example only ends up showing 1 figure with both pictures superimposed (you can see what the rendered docs look like via the CI - tests (37 checks on your PR page). Near the bottom is one called "document artifact". Click on the "Details" link for that test, and you go to a webpage of the dully rendered docs after your PR. Navigate to the Gallery and select your example.  One example that also has two pictures is in the basic section called simple_graphs.  I think all that is needed is for you to create a new figure via `fig = pyplot.figure()` or to create a new axis `ax = pyplot.gca()` or maybe you have to put a special character between the plots `# %%`   (I believe the # is to make it a python comments, and the %% breaks up the sphinx docs. But I'm not an expert in these things. sphinx-gallery is the tool we use, but I don't think you'll have to go down that rabbit hole. Try these things first.  You can also build the docs locally if you install everything in `requirements/doc.txt` and then go to the `doc` directory and use `make html`. It takes a while. :}  
comment
We should probably make these parameters be keyword-only, so that the user is required to specify them -- it is too tempting to rely on order and get switched.  We are planning to look at adding that "feature" to many functions this summer.
comment
Yes -- I think that would be a good improvement. And probably good to get the deprecation in before v3.2.  It would/could also show where in our own codebase we get the order of these arguments mixed. :)
comment
This function should be called with an embedding instead of a graph. I don't think we should get into the game of testing every input to make sure it is the right type.  So I'm -1 on checking for this error and raising.  Why did you think that you **could** provide a graph to this function? Maybe we should change whatever caused **that**.
comment
The style can be handled locally by installing `pre-commit` so that it runs e.g. black, ruff, etc either locally or as part of the commit process.  `pre-commit install` Then you can run locally (files must be `git add`ed before this acts on them) `pre-commit run` Or just leave it as is and it will run automatically when you do `git commit`.
comment
The self-loops argument refers to whether self-loops should be added when two connected nodes are contracted (see the docs for that parameter). It does not relate to self-loops in the original graph.  The return value with `copy=True` and `copy=False` should match. If they don't, that's definitely a bug.  The doubling of self-loops from the original graph is a "feature" or a "bug" depending on how you look at it. The edge interacts with the node in 2 ways: incoming and outgoing.  So, the new graph shows both of those interactions and thus two edges. If you want to track which is which you will need a MultiDiGraph and you'll need to look at the keys of the resulting edges. That is a "feature", not a "bug".  But if you don't want both of these edges, then you'll need to either remove one (or both) of the resulting self-loop edges, or you could use a DiGraph instead of a MultiDiGraph.  That's the extra work required ("bug"-like behavior) for people who don't want to track both in and out directions of self-loops on a contracted node.   The reported "bug" is, in my view, a "feature" for some users, even though it requires clean up for those users who don't want the extra information.   If there is a difference between `copy=True` and `copy=False` then we will need to fix that. A simple example would help a lot.    
comment
When `copy=True` you should look at the return value of `nx.contracted_nodes` for the results. In those examples, you aren't capturing the return value, so it disappears.  Testing G2 will not show any changes because the changes are being done in the copy of G2, not in G2 itself.  Try something like this: ```python     G2 = nx.MultiDiGraph()      G2.add_edge(1, 1)     newG2 = nx.contracted_nodes(G2, 2, 1, self_loops=False, copy=True)     count2 = newG2.number_of_edges()       lis2 = list(newG2.edges(data=True))  ``` 
comment
Hmmm... I am getting 6 edges for both of these examples...  What version of networkx? `nx.__version__`
comment
Can you check your results. You show getting 6 edges, but you only show 4 edges.   I get 6 edges and they show 6 edges. (This is all for the list_G3b case). I copied and pasted your code... but the results you show are as comments, so maybe they got copied over incorrectly?
comment
Yes, that is correct... directed edges which start as selfloops count as two edges when `self_loops=True`.   >The doubling of self-loops from the original graph is a "feature" or a "bug" depending on how you look at it. The edge interacts with the node in 2 ways: incoming and outgoing. So, the new graph shows both of those interactions and thus two edges. If you want to track which is which you will need a MultiDiGraph and you'll need to look at the keys of the resulting edges. That is a "feature", not a "bug". But if you don't want both of these edges, then you'll need to either remove one (or both) of the resulting self-loop edges, or you could use a DiGraph instead of a MultiDiGraph. That's the extra work required ("bug"-like behavior) for people who don't want to track both in and out directions of self-loops on a contracted node.  >The reported "bug" is, in my view, a "feature" for some users, even though it requires clean up for those users who don't want the extra information.  At least we have verified that the issue is restricted to selfloop edges (and doesn't depend on the selfloop keyword). Also that it doesn't depend on the `copy` keyword. 
comment
Good -- Thanks @MridulS and @sirichandana-v   Let's move other possible changes to a different PR to get this one in. Other items: - In the doc_string of `join`, indicate that trees are represented by networkx graphs. - The name `join` is a terrible name.  It means that `nx.join` doesn't give any context to what that function does.  We should at least make it `join_trees` or something like that.  That would require deprecation of course.  I can make an issue with these items when this is closed -- unless someone wants to do them here.
comment
I think the sequence must have `n-2` values with numbers between 0 and n-1.  This has 5 values, so n=7. But the first number in the sequence is 7.  So this is an invalid sequence... Right? I guess the error message could be more helpful. But first -- let's address the expected behavior question... Should this return a tree?
comment
Actually, it was implemented and then removed. But that was close to 12 years ago before the git system was used so the comments describing why are harder to find. It looks like there used to be a graph generator which returned the graph with maximal s-metric for a given degree sequence. That is no longer provided so I'm guessing it had a problem that was not fixable.  If you know an algorithm to provide the maximal s-metric graph, or to compute the maximal s-metric itself, please let us know.   Meanwhile, we should probably at least make the default value `False`  :)
comment
We already had a version of the code that generated the maximal s-metric graph for a given degree distribution according to the method in that paper. Based on [the "blame" report for the smetric.py module](https://github.com/networkx/networkx/blame/main/networkx/algorithms/smetric.py) it was put in place a year after the original code for smetric to be normalized. But then was removed a year after that. The older version was left as a comment, so I'm quite sure that there was a problem with the method to find the maximal s-metric.  So, doing what you propose would not get us to a reasonable normalization for the s-metric.   I think it would be better to find a newer paper that discusses the s-metric, how it could be normalized, and what it is useful for. Then construct a normalization based on that new paper.
comment
[Here is a reference paper ](https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=152129) following up on the 2005 s-metric paper. It was written in 2008. It points out that the algorithm in the 2005 paper for finding a graph with maximal s-metric has not been proven to end. And in practice it appears that sometimes it takes so long you have to give up. If it finishes, then the resulting graph has the maximum value of the s-metric.  That is likely to be the cause of the removal and commenting out of code. The algorithms we have for finding the normalization factor aren't assured of finishing. We could approximate the value of s_max, but approximations aren't very useful for normalizing -- the resulting number could be greater than 1. So we probably shouldn't even provide a normalization option.  We currently do worse than that!  We make `normalized=True` the default -- and it always results in an exception.  Instead we should remove the normalized option (first deprecating it). During the deprecation period, we should check the `normalized` input parameter and if it is False, raise a warning that this option will be deprecated in version 3.1, that the default value is now False, and that users should just remove the `normalized=False` input in their code. We will also need to add a test of the deprecation warning, a filter in conftest.py to ignore the deprecation warning when CI testing and a note in the `doc/developer/deprecations.rst` file to remind us to remove the offending code once v3.1 comes.
comment
Digging down into the discussion on scipy linked to above, it seems that each doc_string will need to do the import at the top of that doc_string. At least that will satisfy the automated testing system. But it **doesn't** satisfy the idea of cut-and-paste examples and they work. It is very rare that you cut and paste an example from the top of the doc_string code. So most cut/paste examples will need to add the import.  I can still believe it is easier for newcomers to see the import, understand that they didn't copy/paste that part of the code and add it in. But the nly way to make out examples copy/paste friendly would be to have the import statement at the start of every section of the examples -- and to refuse to interrupt the examples with explanatory text -- which would be a shame. Explanatory text is often best in the middle of an example.   Still, I'm +0 on including the import statements in the examples to help newcomers.  I find myself using copy/paste/remove-the-import when I copy from stackoverflow, or other sites. But it's better for me to have to delete than for a newbie to figure out that they have to add. :}
comment
I think you will have better luck using the `pygraphviz` package than the `pydot` package. Pydot has not been maintained much lately and this colon trouble keeps affecting people. I'm sure there is a way to make that work (other Issues in this github repository have discussed it). But it is likely to be faster to install `pygraphviz` using the conda-forge channel. Then try `nx.nx_pygraphviz.graphviz.layout` instead of nx.nx_pydot.graphviz_layout`.
comment
Ack!   Sorry -- feeling dumber... :} The name of the module is nx_agraph not nx_pygraphviz....  That's what I get for not actually trying the code I suggest. It also looks like I put a period when it should have been a "_"...  The [docs for the interface ](https://networkx.org/documentation/stable/reference/drawing.html#module-networkx.drawing.nx_agraph) are probably more reliable than my code, but I think this works... (I have tried it... :)  ```python position = nx.nx_agraph.graphviz_layout(G) ```
comment
I think this is a pydot issue more than a GraphViz issue.  Is that correct? If so, it should have been fixed in pydot long ago.  At some point we will just have to let pydot go unless they update their software. If it is a GraphViz issue then I think they will respond and fix the issue. 
comment
I agree that if the `*path_length` version of the function simply calculates the path and then computes its length that there isn't a good argument for a NetworkX function providing the length.  And example in the `*path` function should be sufficient.  That is the case with the `dag_longest_path_length` function.  The argument for the `*_path_length` functions in the `shortest_path` suite of functions is that it saves a bunch of time and memory if you don't have to collect the paths...  If the length is all you are after, then you can compute the length more efficiently.   In summary, it seems like the `*_length` functions should be provided when the algorithm to compute them is significantly more efficient than computing the paths followed by the length of those paths.  Thanks for bringing this up!
comment
This will be automatically closed when the associated PR is merged. (the link between the two is shown in the right sidebar on github under the heading "Development".
comment
I left an extra space in my suggestion. fixed now (maybe :)
comment
Re-looking at this I thought of two silly things that aren't blockers: - should `default` be keyword only  (`,*, default=None` in the signature)? - should a note be added to the release notes Fine with just merging too...
comment
This is puzzling -- (so perhaps a bug).   There is a cycle with weight zero. `list(nx.simple_cycles(G)` reports `[[0, 4], [0, 7, 4], [0, 7], [0, 7, 5, 4], [0, 5, 4], [2, 7]]` Manually checking these shows no negative cycles, but the `[2,7]` cycle has weight 0. I suspect that is the trouble.    Perhaps goldberg radzik is checking for non-positive cycles rather than negative cycles while bellman ford is looking for negative cycles.... But I haven't found the difference yet -- reporting early in case this helps you.
comment
I found the offending `<=` which should have been `<`.  #6892 fixes the issue.  Thanks for reporting this! 
comment
The pygraphviz issue is related to windows environment if i understand correctly. There are two ways around this: - use `make html-noplot` instead of `make html`. - submit the change as a PR, wait for the CI tests to run, scroll to the bottom of the CI tests and select the "documentation artifact" link to go to the webpages as they would appear after the PR is merged.  I think the easiest for you is the first one since you probably have everything else set up for local building of the docs. If it doesn't work or is too much, I'll try building it on my machine.  Thanks!!
comment
Hmmm.. It can't find the relatively new function `girth`.  That sounds like it is using an older version of networkx instead of the local version you are working on. Do you have the PYTHONPATH environment variable set to the head of the repo you are working on? I don't know what kind of environment you are working with -- but it seems like `python -c "import network as nx;print(nx.__file__)"` might help you track down where the networkx you are using is located. 
comment
Arg -- yes, I see the problem.  Maybe we make two private helper functions: the current edges function and the current multiedges functoin. Then we make a new public version of the edges function which checks for a multigraph and calls either the private multiedge or private edge function.  Finally, change the multiedge private function to call the private edge function when it is looping through the different connection styles.   This is just thoughts put down in email. Are there better ways to arrange this code?
comment
I think the tests are failing because there isn't any return value from `draw_networkx_edges`. It should return the drawn objects and probably now doesn't return anything -- or maybe there isn't a return command at all. Either way `None` is the returned value. So the pictures are the same, but the next bits of code (that try to handle the returned value) raise exceptions because it is `None`.
comment
See also #3813   which has some interesting ideas that might work for placing labels.   
comment
I think this is not correct....  :(      But maybe I am missing something?    A cycle is a path (with other conditions). And the examples you show are not paths -- they are walks (you can walk on a path back and forth.... but the path does not repeat the edge --- your walk on the path repeats the edge.)    A cycle is a path that begins and ends on the same node.  A simple cycle has no other node repeated.  Any cycle (not necessarily simple) in a network can be described as a sum of the cycles in the cycle basis.  I have looked at the wikipedia link cited above -- but it says that the basis elements are simple. It doesn't say that the cycles created from sums of those cycles are simple.
comment
This is often a problem when using floating point attributes -- and not just for this algorithm.  The trouble is that the choice of cutoff is not clear and seems to depend on your application.  So, on the one hand we encourage integer valued attributes for this reason (multiply by 1e6 and round). But on the other hand, maybe we should be making this work more easily for users by adding kwargs to tune the precision of "==" throughout the library.  Thoughts anyone?
comment
What should it return in this case? Have you been able to shrink the network smaller than this and reproduce the same issue?   It looks like you are creating edges with negative weight. Is that really what you want?
comment
If I understand correctly the negative weights might be the problem.  To maximize you could try using reciprocal weights (multiplicative inverse) instead of negative weights (additive inverse). I'll look more closely to see whether negative weights should work.   I'm not surprised that there are (small) cases where this code takes forever to run. But this is small enough it might be useful to figure out what is causing it to take so long.
comment
Yes -- Of course you are right about the reciprocal weights...  I was (and probably still am) confused.  I tried removing each edge and the infinite loop is avoided. (just as you say about removing ('T1', 'T2')). I tried truncating the edge weights after the 4th decimal place. That also avoided the infinite loop. Even truncating at the 11th decimal place gets rid of the infinite loop.  (To be clear -- I don't know that it is an infinite loop -- it might just be really long.)  So, this is almost certainly an issue of round-off error in the edge weights.  To track it down will require tracing the code to see why a stopping condition is not being met. Based on hitting Cntrl-C midloop it seems to be part of `find_entering_edges` which continually increases m until an entering edge is found -- yields that and resets m to 0.  If it is finding the same edge over and over it would cause this symptom.  Not sure why it would be doing that though...  In other places in the NetworkX code base we have run into problems with using floating point values in edge weights.  It looks like this is another of those places.  A workaround is to replace the edge weights with rounded versions, or (what amounts to the same thing) scaling the weights and rounding them to integers.  In this case: `d[f][t]['capacity'] = round(sent_vol[f, t], 10)` seems to work.
comment
The `reduced_cost` function is the culprit.  That's where we should figure out how to use a tolerance. ```python def reduced_cost(i):     """Returns the reduced cost of an edge i."""     c = C[i] - pi[S[i]] + pi[T[i]]     return c if x[i] == 0 else -c ``` The last line seems to try to avoid this problem with 0 switching to -0. But it isn't enough when edge weights are floats.
comment
The doc_string for `networkx_simplex` has a note:  ```     This algorithm is not guaranteed to work if edge weights or demands     are floating point numbers (overflows and roundoff errors can     cause problems). As a workaround you can use integer numbers by     multiplying the relevant edge attributes by a convenient     constant factor (eg 100). ```  I'm inclined to let that warning be enough... users should not using floating point values for edge weights where equality is important. Similarly floating point values are not good choices for nodes.  If we need to check equality using approximation ideas it will potentially add a lot of extra checking to our code. I think this is potentially a problem in just about any function that optimizes any attribute (like weight/capacity/demand).
comment
Thanks for pointing out this issue with round-off when using floating point representations of numbers.  As I'm sure you know, since we know the results should all be positive, as a workaround you can take the absolute value of the results: ```python result = {u: {v: abs(value) for v, value in udict.items()} for u, udict in res.items()} ``` You could also `round` the results to a number of digits useful for your project.  You should definitely **not** count on floating point values to equate to zero unless you process them to indicate what you mean by equalling zero. (Indeed, using `==` and `!=` between floating point values is not a good approach.)  Similarly depending on the sign of floating point values is suspect when values are close to zero -- as is depending on the sign of `inf` and `-inf`.  I don't think you can use floating point numbers without running into these difficulties.  The difficulty in processing all returned floating point values in the library to try to avoid potential confusion of this type is daunting (and computationally wasteful for most cases). So, I'm -0.5 on trying to fix these generally in the library's functions. I'd prefer to rely on the user knowing or learning about the pitfalls of floating point -- though I am quite aware the problem and sympathetic to those learning that lesson.
comment
Thank you for this! And for the (fairly small) example that shows the error message. I'd love an even smaller example to include in the tests, but this allowed me to trouble-shoot and understand the problem.  As you say, this is a problem with round-off switching the sign of an entry in the eigenvector and then taking the sqrt of the result. To fix it we should use `np.maximum(p, 0.)` or `np.abs(p)` before taking the sqrt. Quick tests confirm the internet claim that `np.abs(p)` is faster. Are there other reasons to choose `np.maximum`?  I put a fix in #6866 
comment
I found examples as small as 50 nodes by taking your example and repeatedly removing one node at a time while checking to see if the nan's still showed up.  The problem is that round-off is not reliable/repeatable/predictable so even with the small examples, it often doesn't happen that the round-off switches the sign of the values close to zero.  In that process I came to understand that this isn't really a test-able issue in the sense of CI / unit tests.  It is sufficient to make sure it doesn't happen without testing that it doesn't happen. So the PR is merged now.  As far as placement of the `np.abs`, the difference will only matter at the level of round-off which is inherent in every step anyway. So normalization is only "true" up to round-off error anyway.   Thanks --
comment
But the docs state that the source is the starting node for the DFS, not the node for which you want the predecessors.  So, while I think the returned value is correct, I can see that there is a misunderstanding in the wording of the doc_string.  We could change the name of the optional keyword argument from `source` to `starting_node_of_DFS`. But I think that may be more than what is called for.  How about the description becomes: ```     source : node, optional        Specify starting node for depth-first search.         Note that you will get predecessors for all nodes in the         component containing `source`. This input only specifies         where the DFS starts. ``` Thoughts? Is there a better way to convey this?
comment
I’m trying to run the benchmarks with this branch. I use the command as stated in the README file, but it complains: - no asv*.json file found (when I’m in the repo home directory - No  `__init__.py` file if I’m in the benchmarks directory  I was able to run it after copying the json file to the repo home.  I wonder if the import of `benchmarks.utils` should be changed to allow running asv from the `benchmarks` directory.  Or maybe there is a way to give the location of the json file while running it from the repo home.  It seems like we need to add pandas to the benchmark requirements file…  we use it to get the Stanford Facebook data.  ‘Is there a way to get the report table to show the `str` version of the graph instead of the `repr` version? That would effectively show density differences rather than memory locations…   But I realize it might not be possible.  This is nice!  ;)
comment
Yes -- this works for me locally!   Yay!  I see what you mean about asv setting up the environments with pandas for you. Part of the benchmarks process includes creating environments to run them in. That's where we need pandas -- not in the environment where asv is run.  And I haven't played with multiple commits yet. But the html output seems to work fine too. 
comment
I think all concerns/questions with directories and requirements have been addressed here. So I'll merge it unless someone says something in the next day or two.  Thanks Mridul!!  Putting a framework in place is huge! Now we can tweak what gets run and how it gets used. :}
comment
This looks like another one of those changes that mess up our doc_strings. When numpy changed their output format we had to handle those differently.  I don't recall if we only tested doc_strings with specific numpy versions, but I think it was something like that. And we changed all the doc_strings format in one big commit I think.  So, we should talk about how best to handle it.  My initial thought is that we drop the `--doctest-modules` on much of the testing matrix -- At first that can be for whenever numpy >= 2.0. But then we'll need to switch it at some point from dropping it for newest numpy to dropping it for the older numpy -- and we make that CI change when we make the doc_string change.   I guess the idea is that the results of doc_string tests/examples are rarely dependent on version of numpy. So as long as we're testing it on one version of numpy, it is likely working on the others and we test the others using the test modules.
comment
The [GML format technical format](https://web.archive.org/web/20190303094704/http://www.fim.uni-passau.de/fileadmin/files/lehrstuhl/brandenburg/projekte/gml/gml-technical-report.pdf) shows the official syntax to include newlines as part of "whitespace" along with spaces and tabs. So I think this is a bug and we should read strings that cross newline characters (i.e. multiline strings).  Can you please add a test in the `readwrite/tests/test_gml.py` file? That way we won't lose this feature in a later PR. Thanks!
comment
Something seems to be wrong with your setup. There should definitely be an attribute `nx.single_source_shortest_path_length`. That has been part of NX from the beginning and it doesn't change in v3.1.  Can you (say, in a new cell) import networkx and then type `nx.<tab>` to activate the tab completion. This should show you the attributes of the library. Maybe there is a conda way of checking the installation too, but I don't know it.
comment
I wonder if the functionality for these two functions could be embedded into `dag_longest_path` and `dag_longest_path_length`. We would need to add two new input arguments. And I haven't looked closely at the code to see how much duplication this would avoid. The new code handles the case when either `source` or `target` are provided, and the older code handles the case when both are `None`.  I suggest this to build on the api developed for shortest_paths where we have functions to return the path that change their output depending on whether `source` or `target` are provided in the function call. 
comment
I agree with @rossbar that this sounds like a good feature. I'd argue for the keyword to be `weight="weight"` as the default way to refer to edge attributes is `weight` in many other places (like `to_numpy_array`). But I don't feel strongly about it.  And I agree that there might be some messy corner cases.  Distinguishing between 0 and non-edge might be a different PR. But could be done with a keyword argument like `nonedge_sentinal=0` so people could use `nonedge_sentinal=float("nan")` for example.
comment
Interesting that all the `to_<datastructure>` functions use `weight="weight"` keywords, while the `from_<data_structure>` functions either don't have it, use `edge_attribute` or use `edge_attr`.  We should probably make this more uniform. The `edge_attr` case is interesting because it uses value `None` for no attributes, just like we are thinking, and `True` to mean load all the available information for each edge.  We could use that option for compound dtypes perhaps. I'm not sure what's involved there.  And just to be clear, the default value for non-edges should be 0. That is standard. We just want to provide a way for people to change that if desired.
comment
The tree edges each get reported twice. Once when the edge is traversed in the forward direction (when v is first discovered). A second time when the node `v`  has been fully explored and the process moves back to exploring u -- this is reported as a reverse traversal on that same edge.  So, the (0, 1, 'forward') means that node v is being prepared for exploration -- it should be reported on a dfs_preorder at this time. Then the (0, 1, 'reverse') means that node v has been explored completely and we are returning to node u -- node v should be reported on a dfs_postorder at this time.
comment
We should make sure that "number_of_walks" creates the adjacency_matrix with `weight=None`. Otherwise if the graph has a weight attribute it will be used to compute the number of walks and we don't want that. I'll make a PR for that.
comment
Other notes: - The default value for `partition` in the `tree/branching.py` functions is `0` or if you prefer you can use `nx.EdgePartition.OPEN` - I do not find a function to reduce a weighted multiedge set for two nodes to a single edge weight for those two nodes. There is an idiom used in a few examples though: ```python G=nx.Graph(MG) for u, v in MG.edges():   # parens edges() are needed to suppress 3-tuple edge reporting     G[u][v]["weight"] = min(d.get("weight", 1) for d in MG[u][v].values()) ``` where `min` can be replaced by `sum`, `max`, `np.mean` and other reduction functions.
comment
To fix the Mypy problem:  In `networkx/algorithms/connectivity/edge_kcomponents.py` Line 15:  remove this line -- it's an internal import from networkx Line 235: change `bridges` to `nx.bridges` 
comment
Here are some crude timing results:     Full test suite for networkx: (very crude test using pytest time reporting)       - without dispatch decorators (before PR #6688): 48.14s          - with dispatch decorators: 48.83s       - in nx-loopback test mode without dispatch decorators (should take longer due to converting graphs): 56.37s       - in nx-loopback test mode without dispatch decorators (should take longer due to converting graphs): 154.24s      Time a function call that takes very little time: `G=nx.path_graph(5); %timeit nx.shortest_path(0,3)`       - without dispatch decorators: 2.67 µs       - with dispatch decorators: 3.3 µs     Increase number of nodes:  `G=nx.path_graph(50);%timeit nx.shortest_path(G, 0, 47)`       - without dispatch decorators: 21.9 µs       - with dispatch decorators: 22.3 µs  Timing of import of networkx:        Change to new git branch, then `python -X importtime -c "import networkx"`       - without dispatch decorators: ~70 +/-2 ms    (200+/-4 ms for first invocation after switching branches)       - with dispatch decorators: ~72 +/-2 ms    (204+/-4 ms for first invocation after switching branches) The first time an import occurs, a py_cache is stored which speeds things up for subsequent imports. 
comment
This sounds like a good idea to me. It will cover what seems like a common case and make the code easier to read too.
comment
The NetworkX standard way to include node attributes for weights is to use an optional keyword argument `weight=None`. Then if weight is None, we treat each node as having weight 1. If weight is a string we use the node attribute with the name held in the string:  e.g. `weight="distance"` where the graph holds a node attribute called "importance".  This way, graphs with e.g. multiple different node attributes can compute the cd_index for different weighting without constructing a new graph.  Does this make sense?
comment
Thanks @rossbar!  I think you are right -- it is worth looking at whether this needs to be a `datetime` attribute, or perhaps it could be more general.  For example, the node `time` attrs (and `time_delta`) could be integers... And indeed any set of comparable objects. The current code uses time units of years and uses the `datetime` attribute specifically by referring to the `timestamp` of that `datetime` value -- but I think it could just use the datetime objects themselves instead of converting to timestamp.    In the interest of making this module a starting point for other time_dependent algorithms, can we pull the time handling into an optional argument that is a function which inputs the graph and outputs a dict keyed by node to a time value that could be any format you want (datetime instance, timestamp/float, integer). The type of the value would need to work (in this case) with the type of the input `time_delta` and support `<` and `+` operators.  What do you think @evgepab @eirinikafourou ?...  I can push a draft for your review if you don't want to take this on.  Also, was there a reason you decided to use the `timestamp` attribute instead of comparing/adding `datetime` instances? I don't know the datetime class very well, so want a head's up if there is a quirk that timestamp gets around. Thanks!
comment
I was thinking that the default time-function could provide exactly what you use now. But that it would allow the user to specify alternate ways to produce a time value if desired.  And, right now, I'm not sure my idea for a function that returns all the node values is the best approach.  Maybe it should just be a node attribute that holds the value. Then the input function would not be needed. Let me play with that here for a minute: Right now the node attribute "time" is a `datetime` object. Can we use those objects along with the `time_delta` value to make all the comparisons that are needed? If so, is it possible for the user to put a float or integer in there (or the timestamp) and make it all work?  My quick look suggests that this could work and would be simpler than my suggestion. Removing the `.timestamp()` in two places and the `* 365 * 24 * 60 * 60` for the time_delta would make it work for arbitrary time-related numbers or datetime objects or timestamps. The scale of the time_delta can be left up to the user -- the default value can be scaled by the value for a "year", or the default value could be a `time_delta` object from the datetime module (maybe??).  You would then also be able to remove the check for all time objects being datetime.  Maybe this is a better suggestion than an additional input argument. But maybe there another way to do that which I am just not seeing. What do you think would be a good way to make this work for the cases you show and also for an arbitrary time value? I think the goal is to not force people to use `datetime`, but to allow and maybe even encourage it through the examples used.
comment
Yes -- it would be better to have them able to provide any attribute that will be used to represent time. I can imagine, e.g. a graph with multiple time related attributes and people might want cdindex for each of these attributes.
comment
Thanks @eirinikafourou and @evgepab !! 
comment
Do we have any evidence yet that this speeds up the calculation? It looks like a fair amount of extra code and that usually ends up slowing things down. My comment in the original issue about looking up one neighbor not speeding things up made me think of that again here. How is this version doing?
comment
To speedup the subgraph part, you might try making a copy of the subgraph. Subgraph Views are not very performant. When traversing the graph, you still have to look at all edges and reject the ones that are not in the graph. If memory is not a bottleneck, and if you expect to look at an edge more than once on average, it is likely to be faster to add `.copy()` to the end of your subgraph call.  This does take time of course -- because you have to create that new graph to hold the subgraph. So, no guarantees... That also would mean you should have some logic to handle the case where `nodes is None` so that you don't make a copy/subgraph of the whole graph.  :)
comment
Yes -- that is a good way to handle the case when nodes are provided.
comment
I see the ```local_bridges``` function is using ```weight=None``` to indicate a function that always returns 1, but there's got to be a better way to do that. It does always return 1, but edge attribute lookups are usually inner loop items where performance matters. I'll try to figure out a good way to improve local_bridges to avoid this issue.  Similarly for ```shortest_simple_paths```.   And I think ```_bidirectional_shortest_path``` just ignores the weight argument so no decorator needed.  Looks like the remaining 2 errors are for ```nx.johnson```.  The title of the PR is "initial implementation".  Is there more to be done? How much of the codebase did you look through to find appropriate functions?  Looking elsewhere can be a different PR.
comment
I'll close this as it is an upstream issue in scipy. Thanks for reporting!
comment
Ack! I think my suggestion was not helpful -- I think it doesn't work for Multigraphs and it doesn't do the right thing for DiGraphs either...  :{     But this PR passed all the tests, so I wonder if I'm understanding it incorrectly?!?!  The reason I think this suggestion should not work is: - The multigraph edges are 3-tuples (u, v, edgekey) and this code turns that into a frozenset so we lose the info about which is the key and which are the nodes. - When turning it back into a tuple we unpack it to `u, v` which will raise an exception if it is a 3-tuple.  Maybe the other suggested method would work -- but I already see a mistake in that one too: - the if statement should add the other directions for the **undirected** case, not for the directed case.  Also, I see an improvement that avoids the checking for multigraph or not: Iterating over `G.edges` results in 2-tuples for non-multigraph and 3-tuples for multigraphs. (Unlike `G.edges()` which yields 2-tuples for both multigraph and graph.)  Would this work? ```python     G_edges_set = set(G.edges)     if not G.is_directed():         G_edges_set.update((v, u) for u, v in list(G_edges_set)) ```  And it looks like we need better tests -- of simple multigraph and directed graphs...   Like path_graph(4) and path_graph(range(2, 7)) with one repeated edge added for the multigraph case.  Maybe 4 nodes is even more than we need. :)
comment
Thanks for this!  I prefer these operations to not copy the graph/node/edge attributes. We could add an example to the doc_string showing them how to update the resulting graph with attributes if desired.  Some code like the following, which adds the minimum attribute for each node across all graphs could work... but maybe there is a better example. We could say in the doc_string that similar ideas work for `set_edge_attributes` and setting the graph attributes.  ```python         >>> g = nx.Graph()         >>> g.add_node(0, capacity=4)         >>> g.add_node(1, capacity=5)         >>> g.add_edge(0, 1)          >>> h = g.copy()         >>> h.nodes[0]["capacity"] = 7           >>> j=g.copy()         >>> j.nodes[1]["capacity"] = 3          >>> graphs = [g, h, j]         >>> ghj = nx.intersection_all(graphs)          >>> inf = float("inf")         >>> attr = 'capacity'         >>> new_node_attr = {n: min(*(anyG.nodes[n].get(attr, inf) for anyG in graphs)) for n in gh}         >>> nx.set_node_attributes(gh, new_node_attr, 'new_capacity')         >>> gh.nodes(data=True)         NodeDataView({0: {'new_capacity': 4}, 1: {'new_capacity': 3}}) ```  This would fix #6750 for intersection_all.  Would should/could also fix the `union_all` function. And I think we should check the `compose_all` function. But those could be in a different PR if you prefer.  Other comments: 1&3:  Thanks for fixing the multigraph behavior, making it work in all cases and fixing the tests as well... :) 2: Good catch!  There should be different handling of multigraph and graph in that spot. 4: (see discussion above)  I also noticed while looking through this that we don't check for taking the intersection of a DiGraph with a Graph. I suspect that should raise a ValueError. I think these operators should not be trying to handle input lists with both DiGraph and Graph.  Could you add code to check whether all are directed or not directed? And I guess that should include small tests too (could be very simple graphs).
comment
I think we should include these restrictions on mixed directed/undirected operations. And so we should also adjust the doc_strings to reflect that restriction. Probably just removing the current language about "not recommended" and add a section saying the functions raise a NetworkXError when the graphs are not all the same is_directed and is_multigraph value.  :)
comment
This looks nice!  Would it be worth adding an inline Latex example in addition to the indented/offset example?   Maybe at the end of the paragraph add: like $E=mc^2$. (there has got to be a more network-y formula to use as an example... :)
comment
The tests are failing now because we test some configurations that don't have numpy/scipy installed. To make the test system skip any functions that use numpy/scipy we use the function `pytest.importorskip()` which is pronounced "import or skip". That is, if it imports, great!  If it doesn't, then skip these tests. You can use it inside a function/class to skip just that part. But in this case I think we just run it at the top of the module because none of these will work without numpy/scipy to make the adjacency matrix.  ```python np = pytest.importorskip("numpy") sp = pytest.importorskip("scipy") ``` Actually, I think you don't call np or sp explicitly in the tests. So you probably don't need the "np = " or "sp = " part.  That just provides the name for the imported library if the import is successful.
comment
I like this new interface. It is quite clear and easy to follow.   It looks like the documentation rst file needs to be updated.  As for whether to implement a generator version, I think the generator would still need to compute the whole matrix just to yield the first part of the result. So we will have memory issues even if we yield parts of the dict-of-dict format. The memory issues stem from using the matrix approach to this problem — but I still think it is the best approach to finding the number of walks between pairs of nodes. So I’m leaning toward using the current PR and not a generator, but I might be missing something.
comment
Hopefully we will get a `matrix_power` function in scipy.sparse soon. The expression `A**n` (when A is a sparray) computes the element-wise power. Because A is a matrix of 0 and 1, it might be surprising, but `A**n` is just A.  :)  So, some of the tests' expected values need to be updated.  The variable walk_length is described as if it is the number of nodes in the sequence. But the length of a walk/path/trail is the number of edges. So I think we should describe walk_length as the number of edges and point out that this is one fewer than the number of nodes in the sequence. So, now the walk length can go to zero and it makes sense.  I changed this PR to use dense arrays and numpy's matrix_power, so we can merge this. To use SciPy, we will need to wait until we stop supporting versions of scipy.sparse that don't have matrix_power -- and that could be a year or more. I made the suggested test fixes and added to and changed the wording of the doc_string.  I also added the check for a positive walk_length.
comment
I like the exclamation point `!` better than the `v`. The `v` somehow registers as a letter to me, disturbing the distinction between node symbols and edge symbols.  But really, I'm good with whatever you think works well. :}
comment
As for "how to do a deprecation", that sounds like something that would be a good section in [the contributors guide](https://networkx.org/documentation/stable/developer/contribute.html). I'll try to put down what I understand to be the process here and use this PR as a guinea pig if you are willing.  Deprecating code: - running the code should raise a `DeprecationWarning` with a message that says how to update the code to avoid the deprecation (if such a method exists) and which release the deprecated code will be removed.  This process typically involves:     - `import warnings` somewhere appropriate (at the top of the module or in the function if only one function     - `warnings.warn(msg, warnings.DeprecationWarning)` but I think `raise warnings.DeprecationWarning(msg)` also works and I'm not sure which is preferred.     - add a test that the warning occurs when calling that feature.     - add the warnings filter to `conftest.py` in the function `set_warnings` (other examples are in that code).     - add a couple lines to the deprecations section of `doc/release/release_dev.py` to show which PR, which function(s) and enough info to help a user looking for a solution. Look at previous release notes, e.g. `release_2.8.rst` for examples.     - add  a line to `doc/developer/deprecations.rst` to remind us to remove the code when the time comes to do that.  The guidelines for picking a release to remove the code are in this file at the top.  Ask is anything isn't clear or doesn't work. :}
comment
Also, since this is a new function, can you please make the keyword arguments "keyword only" by adding a *, between the positional arguments and the keyword arguments?
comment
Actually, it is even worse than we realized. The `draw_networkx_edges` function was changed in significant ways in November to make all edges be `FancyArrowPatch` instead of `LineCollection`.  Can you see if a similar change could work with the FancyArrowPatch? 
comment
Is there more expected for this?   You've added the label/webpage urls for a new version, but no changes to code? Does it all work the same as before? Also, the GEXF webpage does not mention version 1.3 at all...
comment
Thanks for the explanation. I didn't have that link. It is interesting that the github GEXF primer you point to only talks about 1.3 on the Wiki and not in the repository. The Wiki examples all use 1.3draft instead of 1.3. I wonder where Gephi is getting the specs for 1.3 to export 1.3 by default now...  
comment
Yes, we need the specs for the format (and hopefully what changed between versions).  Otherwise it is very hard to make code that mimics what Gephi is doing.  I'm not sure why they are using a format that don't seem to be created yet for their file structure.
comment
Fixed by #6744
comment
Using the `GG.subgraph` method should cut down the work in finding edges. The `nx.cycle_basis(GG)` function returns lists of nodes that are the path of the cycle.  It is worth looking into...    The current return values of `nx.minimum_cycle_basis` are not very useful. I'm not sure why it returns the cycles with unordered nodes.   We will look into it more.  Here are some partial fixes: `nx.find_cycle` might work with something like this: ```python cb = nx.minimum_cycle_basis(GG) cbasis = [bl for b in cb if len(bl:=nx.find_cycle(GG.subgraph(b)))==len(b)] ``` which can be written more clearly: ```python cb = nx.minimum_cycle_basis(GG) cbasis=[] for b in cb:     cycle = nx.find_cycle(GG.subgraph(b))     if len(cycle) == len(b):         cbasis.append(cycle) ```  Does `nx.cycle_basis(GG)` work for you? Or do you need a minimum? Maybe the following works? (max of a collection of lists uses `len` to get the longest one.) ```python [max(nx.cycle_basis(GG.subgraph(b))) for b in nx.minimum_cycle_basis(GG)] ```
comment
Sorry -- my comment was not clear.  I mean to keep the left-eigenvector equation prominent, but to make the additional sentence include an inline equation (not prominent) about `$AX = \lambda x$`.  Also, I think the documentation is failing due to a change in circleci that is fixed in the main branch. So you may need to merge (or rebase:   `git rebase -i upstream/main`) to remove that red check.  Thanks!
comment
Great -- just need another approval and we're ready to merge.
comment
@adam2392 Can you verify if the assumption is that input set `z` is a valid separating set?
comment
I agree that strong statements in the doc_string are preferred over an extra keyword argument. Maybe even an example in the doc_string where we show people how to check that the set is a d_separator before using it in `is_minimal_d_separator`.
comment
This [wikipedia page describes Edmonds algorithm](https://en.wikipedia.org/wiki/Edmonds%27_algorithm) pretty well.  It looks like `find_optimum` can choose if an edge is "acceptable" and does so only if it is looking for a branching.  As the code is written, a branching does not require each node to be covered by the edges in the branching. That is, it doesn't have to be a spanning branching. In that case, a maximum branching should never choose to add a negatively weighted edge. It would prefer to have no edge at all to make a maximum branching. That is correct logic. But for a minimum branching then, the same logic suggests that you should only consider negatively weighted edges. Because if there are no negatively weighted edges, then having no edges at all is a branching with lower weight. That is how the code currently works.  In this sense then, the `minimum_branching` often does have no edges.  BUT...  this is a very strange way to define a minimum branching.  I can't find that term defined anywhere. But it seems to me that the branching should not be allowed to just choose not to connect to a node.  There ought to be a constraint (like spanning) that requires the problem to add edges where it can.  In this code, that is done by `spanning_arborescence` -- but then the result is thrown away if the final resulting branching is not an arborescence.  That behavior (throwing the branching away) also makes sense and is what the current code does.  I think what @lkora wants is the result of the `minimum_spanning_arborescence` **before** checking to see if it is an arborescence.  That can currently be obtained using: ```python Ed = nx.tree.branchings.Edmonds(G) T = Ed.find_optimum("weight", 1, "min", style="arborescence", False) ```  But I'm not sure what to call this.  It is not the minimum weight branching --- that would have no edges in it. It is not the minimum weight spanning branching --- it doesn't span all the nodes. It is some sort of **minimum weight, maximal node-coverage branching**.  @lkora does this description make sense to you? Is this is fact what you were expecting from `minimum_branching`?
comment
@mjschwenne  Do you want to take a stab at adding a function `minimum_maximal_branching` that mimics the others but this time, runs `find_optimum` using the arborescence node but returns the result without checking if it is an arborescence (the code in the previous comment)?  We should beef up the docstrings to explain the meaning of these subtle yet important distinctions too.  If you'd prefer, I will do it (just let me know), but it might be an easy first-accepted PR.
comment
Yes, I agree that we should not get into setting thresholds for edge weights being included or not.  Perhaps a minimum branching doesn't even make sense as a concept.  But I think the minimum_maximal_branching makes sense even when it isn't an arborescence. It differs from a spanning arborescence even when the graph is not weakly connected. But I think if the graph is strongly connected it would return the spanning arborescence.  ```python def minimum_maximal_branching(G, attr="weight", default=1, preserve_attrs=False):     ed = Edmonds(G)     B = ed.find_optimum(attr, default, kind="min", style="arborescence", preserve_attrs=preserve_attrs)     return B ```  We have Kruskal's algorithm and there is a function `kruskal_mst_edges` which appears to work with directed and undirected graphs.
comment
It looks like setuptools is installed with the other packages in the install packages section. At least it is listed with the other packages. ``` 69    setuptools     58.1.0 ```
comment
Yes -- that looks like a correct description of what has happened with the documentation of this function. And I agree that it seems straightforward to make it handle edges without an edge attribute.  It'd be nice to include tests of input graphs without edge attributes for some edges and show the function working and maybe even check that results match a case with weight specified as 1... ???  
comment
That is certainly a bug.  I can verify that the code treats sets of edges without caring about the directed nature of the edges.   The place to fix this is in `intersection_all` in the `networkx/algorithms/operators.py` module. The line setting `G_edges_set` has two potential fixes. 1. Add two lines after the `G_edges_set` is created: ```python if G.is_directed():     G_edges_set.update((v, u) for u, v in list(G_edges_set)) ``` or 2. use `frozenset` to make each node-pair into an unordered set. ```python G_edges_set = {frozenset(e) for e in (G.edges(keys=True) if G.is_multigraph() else G.edges)} ... later ... R.add_edges_from((u, v) for u, v in edge_intersection) ```
comment
Thanks for your update.  I'm sorry you ran into this issue. Floating point values involve round-off error.  But it is pervasive.  To be able to avoid such problems, we suggest avoiding floating point values when round-off may cause problems. One way to do that is to multiply the number by 1e6 and convert to an integer. This is effectively rounding the value, but it also converts it to a form (integers) that doesn't involve round-off error.
comment
Fixed by #5908 
comment
Now having looked at the tests file more generally, I do agree with you that the `astype` calls should now be replaced with using the `dtype` argument. That doesn't have to be in this PR, but while your head is still in that file and you've got a branch and pr, could you switch those `astype` to `dtype arguments?   Thanks!! 
comment
Yes -- I think a release note -- and I'm close to a simple example for this too.  Can we hold off on merging for a day? I'll remove my approval and approve it again when I've got that sorted out.
comment
I found a test for the deterministic behavior: a graph with 2 cycles that when you change the names of two nodes gives a different (though equally valid) result.    And I added a comment to the release_dev.  And, as usual it seems, I ask whether this should be a deprecation.   :)
comment
Nice work folks!  I have a question for you:  A chordal graph is defined only for undirected graphs with no self-loops (as far as I can tell). So, should a graph with a self-loop automatically make `is_chordal` return `False`?  Even more corner-case-y: should a graph with 2 nodes that has a self-loop return `True`?  What is the right way to handle selfloops? It seems like their existence violates the standard definitions of `chordal`, so should we raise an exception like we do with directed graphs? I'd prefer to extend the definition of chordal to graphs with self-loops if we can find a way that works. I think the restriction to considering cycles with 4 or more edges was done to allow triangles. We're trying to build triangles by cutting up the bigger cycles via chords.   Now, how would self-loops fit into that world?  Maybe they just don't fit. Or maybe we just ignore them.   If they don't fit, we should probably raise a NetworkXError (with a better message than the existing one which just says it is not chordal, and it doesn't say anything about self-loops... that could be confusing to someone who doesn't realize their large graph as a single self-loop edge).  If we ignore self-loops on the reasoning that they don't cause simple cycles that can be made shorter by replacing part of the cycle with a chord, does everything work out?  We thus define a chordal graph as being a graph with no simple cycles of 4 or more edges that don't contain a chord.  The "simple cycles" part means the self-loops can exist, but they don't participate in any cycles.   Would this re-definition lead to trouble with other chordal-related functions?  I think the "ignore self-loops" approach would simply involve removing lines 332 and 333. But it should also add lines to the documentation (probably just two lines per function: the line saying selfloops are ignored and a blank line separating it from the others). And we could turn on that selfloop test again, only have it check that the selfloop doesn't affect the outcome rather than checking that it raises an error.  Thoughts?
comment
The Graph class is based on dict-of-dict-of-dict storage. That doesn't rule out self-loops, and it is somewhat costly to e.g. check every edge addition to make sure self-loops are not added. Instead we chose to let the user police themselves. If they want a simple_graph they can either 1) check the edges they are adding before adding them, or 2) add edges and then remove self-loops when they are ready to do a calculation that requires a simple graph.   We do try to make it easy to work with simple graphs. For example, `nx.number_of_selfloops()` can be used to check whether it is a simple graph. And eliminating self-loops is fairly easy: `G.remove_edges_from(nx.selfloop_edges(G))`  But the short answer to your question "why is Graph not a simple graph" is that we avoid checking every edge that is added this way. And we try to help the users who need a simple graph (which Graph can be -- just doesn't have to be).
comment
I'm actually trying out the dispatch testing on this branch locally -- haven't done any dispatch testing locally before. :}  I have a question about how to  turn on the dispatching. The [docs on Graph Types](https://networkx.org/documentation/stable/reference/classes/index.html#backends) says I should "register an entry point" which seems to point to the networkx_plugins.  How do I register an entry point? I can create the entry points dict. But what do I do with that to make it an entry_point?  Hopefully I'm just missing some terminology or step in the process.   And is there a way I should be running the tests locally even just with the basic backend of a networkx graph. The tests pass on the CI on github, but when I run them locally it complains that I don't have any networkx.plugin. ``` networkx/classes/backends.py:84: in items     self._items = entry_points()["networkx.plugins"] E   KeyError: 'networkx.plugins' ```  Thanks! 
comment
Thanks  @jim22k for that info!  I think this is ready -- I have one question: Could all the `@_dispatch(stuff)` calls in cluster.py become `@_dispatch`? I think the default values cover all graph and name arguments.
comment
Ahh... Good point. I think we are "due" for a review of the namespace configuration. We'll have potentially many changes like moving imports from the subpackage to the main package at that time. I hope that can be fairly soon -- maybe at the summit?  It might be good for one of us to take a first pass at it to see what is involved.  If that should be done to fix this PR I guess we'll need a new PR now... 
comment
Great!  Thanks for this.... Can you remove the codespell integration. Leave the spelling changes except remove the one that is `perfor... (#4536)` flagged above.  :)
comment
This might be better with inputs `_plain_bfs(adj, source)` rather than `(G, source)`. That way functions that need to search a directed graph backwards can use it by inputing `G.pred`.  It does mean each place we call it we'll have to use update the call from using `G`.    I'd be interested in seeing a comparison between calling it with `G.adj` and calling it with `G._adj`. `G.adj` is a view on `G._adj` so is perhaps slower, and since this function never updates G there is no risk of exposing the internal data structure to corruption.  (See #4895)
comment
I was wrong --- These functions are already restricted to being undirected (the one in connected.py) and directed (the one in weakly_connected.py).  So we should just input G as we currently do. As you point out, we'd have to call it twice (or with a `UnionAtlasView` of both G.pred and G.succ) and this is a better interface.  Looking at the `DiGraph.__init__` method you'll see that G.succ and G.pred are views of the dicts G._succ and G._pred.  So, implementing this idea of using `G._adj` internally will amount to replacing `G.adj[u]` or `G[u]` with `G._adj[u]` and similarly replacing `G.succ[u]` with `G._succ[u]` and `G.pred[u]` with `G._pred[u]`.  The views are nice because they protect the user from messing up the data structure, but they are slower.
comment
Hmmm... I'm not sure this makes sense for negative weights...  Can't you just go back and forth across one of the negative edges and get as negative a cycle as you want? Or are we limiting this to simple cycles?  Anyway, I was assuming we would use Dijkstra -- which would require positive weights anyway.   @boothby do you think this PR is ready then? I could approve it, but would feel better with someone else looking at what I did rather than approving it myself. :}
comment
Oh my!  That article is looking for the negative "cost" cycle with the fewest edges -- not with the least cost... We don't even look for that with our shortest_path suite of functions. I guess if the count of edges represent the objective and the edge weights represent the constraint -- we are getting the least length of those cycles with negative cost.   I would restrict any PR I do to positive weights and use dijkstra to find the shortest paths.  
comment
This looks great -- I had not recognized that the "tied" predecessors (multiple predecessors for a node on a bfs) is exactly what shows an even-cycle. That brings this together nicely.   I also like the splitting into even and odd cycles -- which is in the discussion of girth on wikipedia. And the break-else-continue-break construct is something useful I have not seen before -- with a good comment to make it more clear. The benchmark results for versus various commits is also nice to see. :} I assume you have tools to construct that. :}  Can you explain the new test graph you added? It looks like three overlapping cycles length 3, 3, 4 with some pendulums added. Does it test something specific, or just a good simple case?  Comments on the algorithm: - can we process the levels as we create them? That is, look for odd-cycles after each level of the check for even cycles? Does that help cut off the search? - I can't help thinking that a BFS generator would allow us to cut off the BFS earlier. Toward that goal, would it be better to use the `nx.bfs_layers` generator instead of `nx.predecessor` which completes the BFS up to depth_limit? I'm thinking it might allow us to cut off the BFS at the first cycle found. For even cycles we'd have to compute the `pred` info manually by intersecting the neighbors with the previous level in a similar way that we do in the odd-cycle check. We are backchecking on each level, but we might be doing fewer levels.   On a related note, should we also benchmark against a graph that has both long and short cycles e.g.: ```python # spiky cycle N=large integer G=nx.cycle_graph(N) for i in range(N):     G.add_edge(i, N + i)     G.add_edge(i + 1, N + i) ``` 
comment
I agree that we should create a `bfs_labeled_edges` function to match `dfs_labeled_edges`. It allows nice flexibility for the traversals. I have thought that before, but this seems like a good time to do it. I had always thought it should live in `traversal/breadth_first_search.py`, but could maybe be convinced otherwise. I also think it should work with directed and undirected graphs -- and probably multigraphs too if  that doesn't prove to be too nasty.  As I think about edge types I'm confused by the BACK_EDGEs. At first I couldn't get them to happen. But I think now that I've convinced myself that they will not occur for undirected graphs.  And they definitely can occur for directed graphs. For the undirected graphs, you would always traverse an edge outward before it would be reached going inward.   So, while I very much enjoyed(!) the triplet of edge labels indexed by `dv-du`, I think it isn't needed for the undirected case and for directed case we'll need to check if dv-du is negative rather than -1. Does this make sense?  I'm still thinking about whether to have the labels yield the distances -- it is certainly convenient, but also (as you mention) likely to have a way to compute it based on the previous edges.  I'm leaning toward a pure version in traversal, and then the `girth` code (and maybe a doc_string example in the `bfs_labeled_edges` doc_string?) can show how to compute the distances.  But I'd like to think/play about it some more. :}
comment
This is related to #4895
comment
A simple path is a list of nodes where each pair in the list corresponds to an edge. If the graph is an undirected graph then adding the edges (0, 3) allows a path that includes edges (0, 3) or (3, 0).  If the graph is a directed graph then the edge (0, 3) allows a path the node order (0, 3), but not (3, 0).  That is what it means for "an edge to be in the graph". In a directed graph, we pay attention to the direction when considering edges. In an undirected graph, we don't.  A DAG is a directed acyclic graph, so the paths never return to the same node (no cycles exist in the graph).   A simple path is a path without repeated nodes. A simple path in a directed graph pays attention to the direction of the edges, while in an undirected graph the path doesn't not have to follow the direction of the edges.  > By definition, a path in a directed graph is a sequence of edges having the property that the ending vertex of each edge in the sequence is the same as the starting vertex of the next edge in the sequence. A simple path only means there is no repeated vertex in the sequence. What do you mean there is no way?  The term "ending vertex of each edge" and "starting vertex of each edge" shows how the directionality plays a role. For a directed graph the path [0, 3, 2] must have edges (0, 3) and (3, 2). Your graph doesn't have an edge with starting vertex 0 and ending vertex 3.   That's why [0, 3, 2] is not reported by the function `all_simple_paths`.   > is 0<-3->2 a path from 0 to 2? > If so, is it simple?  No, (0, 3, 2) is not a path from 0 to 2. Look at the definition (the wikipedia page has it). For (0, 3, 2) to be a path, the edges (0, 3) and (3,2) must exist in the graph, and they don't exist in this directed graph.  I the edge (0, 3) did exist in the graph, then the path **is simple** because (as the wikipedia page says) if the nodes in the path are distinct then the path is a simple path. (0, 1, 2) is a simple path in your graph. (0, 3, 2) is not a simple path because it is not a path in the graph.  Hope this helps clear it up. 
comment
The Graph Theory literature is quite established and clear about this -- and the Wikipedia page agrees with that literature -- as does our library. The edges for a path in an undirected graph are an unordered set of 2 nodes. While the edges in a directed graph are ordered pairs of nodes. A directed path is just a path in a directed graph.  The causal DAG literature is notable in its sloppy handling of graphs as being both directed and undirected. The term causal path refers to a path in the directed graph under consideration. While non-causal path considers the edges as undirected and finds a path in the obviously related undirected graph. Some people refer to the graphs in the causal literature as Mixed Edge Graphs, but that is really a different definition of a Graph where the edge set can contain both directed edges and non-directed edges.  Our function naming convention is not (usually) a set of commands like "find this". We try to describe what is being returned in as concise a way as possible and no more concise. The function `all_simple_paths` finds undirected paths if the graph is undirected and directed paths if the graph is directed. Adding the adjective "directed" would be incorrect. The function finds undirected paths when the edges are undirected. The output depends on your input.   To expect an undirected path when providing a directed graph suggests that the literature you are working from is not being careful in their communication. And there is nothing wrong with that -- but you need to realize that when you reach out beyond that literature to the rest of the world. 
comment
The quoted textbook is considering an undirected graph.  How do we know that?  Because it says that edges are "two-element subsets of V".  Those are undirected edges. Directed edges are ordered pairs of nodes.  Similarly for the wikipedia page. The undirected description uses {v1, v2} notation to denote sets, while the directed description uses (v1, v2) to denote the ordered pairs.  I think it is useful to look again at the wikipedia page. We both seem to suggest that it backs our claims -- and it very well might back both of our claims. :}  Perhaps we are agreeing in the end. But be careful -- the subtlety of using {} vs () makes a difference. That's the sense in which I use the word careful. In the causal inference literature, authors often switch between treating edges as ordered and treating them as sets. And that's the sense in which I mean that there is nothing wrong with not that approach -- sometimes ignoring the difference between two types of objects can lead to new ideas and new advances. It's best, of course, to be flexible and be able to think about edges as being the same and also turn around and think of edges as being different if they are directed vs not directed. In NetworkX we use Graph and MultiGraph when the edges are undirected and DiGraph and MultiDiGraph when the edges are directed. And most of the functions work with either -- applying the appropriate methods and returning the appropriate results depending on what type of graph is considered. 
comment
This is great!   Thanks @rossbar for putting this together.  And @jarrodmillman for laying the groundwork for this in NX and also helping Scipy get to this point. And to @stefan for putting lazy_loading together and then in place in Scipy. And to everybody else who helped along the way.  This is a small yet significant victory for better import interfaces. :}
comment
Yes, .nodes() are in the order in which nodes are added to the graph. Actually the edges also have an order to them based on the node pairs.  The first node of the edge is reported in the order that nodes were added. And the second node of each edge is in the order in which that edge was added among all edges starting at that node.  It is a dict-of-dicts.  dicts are ordered in python by order the key is entered. The edge (u, v) is added with u as the key to the outer dict and v as the key to the inner dict pointed to by u. Then when edges are reported we iterate through the keys `u` of the outer dict and for each one, we iterate through the keys `v` of the associated inner dict and put them together to yield `(u, v)`.  That's the order that is the same every time. It is not the order in which the edges were added, but it is determined by the order in which nodes and edges are added.
comment
Are you going to check whether this interferes with Johnson's algorithm?
comment
It looks like that discussion ended [here](https://github.com/networkx/networkx/discussions/4797#discussioncomment-740900) with a request that you construct a separate function for the length check version so that it wouldn't affect the runtime of the case `max_len is None`.  Perhaps as a first check that this is true, you could time some graphs with your version and having `max_len=None` and the existing networkx version.  Is there a difference in runtime?  If there is a difference, you could in-line the function call (python can be slow to call functions) `if nbrs and max_check_len(path):` becomes: ```python if nbrs and (max_len is None or len(path) < max_len): ```
comment
Fixed in #6461  :)
comment
It seems to me that the documentation pages have a "Releases" tab at the top of each page. So maybe this isn't needed?  One downside (as I understand it) is that linking in Readme.rst to the stable (or latest) version of the release notes adds an extra step when uploading to Pypi so that the link on resulting page on Pypi (built from info on Readme.rst) goes to a specific version's release notes instead of the generic "stable" version which changes over time.  Another downside is the more crowded Readme page.  I think this PR does show that the Github Releases feature is suboptimal because it doesn't give the full release notes -- only the list of commits and contributors. Maybe we should stop using that feature? Or maybe we should make it provide the full release notes as stored in the repo. (I don't know if that is possible and I'm reluctant to spend dev time on it.) 
comment
The tournament module is quite clear in the documentation that rather than check `is_tournament` for each function, it is the responsibility of the user to provide a tournament graph as input. So, my impression is that we don't want to make each function test for a tournament graph before it runs its code. Perhaps that warning language needs to be spread to all the function doc_strings as well. I haven't looked at how long it takes to run `is_tournament` recently either.  But I don't think there has been any confusion with the current setup.  This is part of the reason why we have a separate namespace for these functions. So this may relate to #5542  although that Issue also talks about bringing only the `is_tournament` function into the main namespace. With a separate namespace, it is much less likely that people will mistakenly run these functions on a non-tournament graph.    
comment
We are happy to include a submission along those lines if it is well tested and documented. This [wikipedia article](https://en.wikipedia.org/wiki/Hamiltonian_path_problem#:~:text=An%20early%20exact%20algorithm%20for,in%20the%20path%2C%20and%20undecided.) hints at some algorithms to try.
comment
If I understand you correctly, the dfs_edges function already does this.  It iterates over the edges so after yielding an edge it waits for you to process it before continuing to the next edge. This saves memory and also allows you to stop whenever you like.      for (u, v) in nx.dfs_edges(G, n):         if switch_open(u, v):             print('Stopping DFS')             break
comment
OK... I think I understand...  But just to check:  - is this different from [the restricted_view approach](https://networkx.github.io/documentation/latest/reference/generated/networkx.classes.function.restricted_view.html#networkx.classes.function.restricted_view) where you can hide nodes or edges depending on a function output? - I'm perhaps not up to the latest on StopIteration, but doesn't raising a StopIteration inside interrupt_func mean it will be raised in dfs_edges where interrupt_func is used, in which case it would not continue to run the dfs_edges?  Clearly my understanding is incomplete, I recall something changed with how StopIteration is handled inside nested functions/generators... - We need more clarity in the docs on what is required of the interrupt function. 
comment
Thanks for that.  There is a relatively[ poorly documented feature](https://networkx.github.io/documentation/latest/reference/classes/index.html#module-networkx.classes.graphviews) behind the hood for these "filtered views" of a graph.   ```python def dfs_edges_interruptable(graph, should_traverse):    filtered_graph = nx.subgraph_view(graph, filter_edge=should_traverse)    yield from nx.dfs_edges(filtered_graph) ``` Or perhaps use the idiom:  ```dfs_edges(subgraph_view(graph, should_traverse))``` 
comment
This looks good. Thanks!  I especially like that you've moved subgraph_view into the main networkx namespace.  - The ```EdgeView``` output in the examples should be OutEdgeView for a directed graph's view. - The 1st 3 lines of code in reverse_view (which raise an exception if not directed) should be removed now the you've got ```not_implemented_for``` in place. - Can you move ```reverse_view``` to the same line as ```subgraph_view``` in ```__all__```. Just to even out the lines there -- and they are both imported into this module, so makes sense to keep together. - Can you remove blank lines after ```Examples``` and ```Parameters``` headings? - Change ```nx.graphviews.subgraph_view``` to ```nx.subgraph_view``` in examples. Same for ```nx.reverse_view```.  Thanks!
comment
See also #5882 
comment
The nodes might be oitlines with linewidth 1.  Try setting linewidth to something small in draw_networkx_nodes? (untested) ```python nx.draw_networkx_nodes(G,  pos, node_size=0.1, linewidths=0.1) ``` 
comment
`callable()` is a [Python builtin function](https://docs.python.org/3/library/functions.html?#callable) It returns `True` if the input argument is function-like...  that is, if it can be called like a function is called. So, objects that won't work include, e.g. 5, "string", "a string of a function name", and others. Objects that are callable include functions, methods, classes, etc.  So, callable is a Python builtin, not a networkx specific feature and that's why you couldn't find it in the documentation. 
comment
I don't understand your `filter_func`.  It doesn't even return true or false...  it just defines a lambda function and then returns `None`. Am I missing something?    What is the motivation of this feature request?
comment
Wow -- that discussion is from a loonngg time ago...  And it's actually not about determinism of the cycle_basis so much as why the algorithm wasn't choosing the optimal basis (as measured by shortest cycles).  Your issue is about preserving the order of the nodes so that the results don't depend on the set implementation ordering. Sets in python provide an arbitrary order that can differ based on their history. They are random, but the order can change in ways we can't easily control. So it's a reasonable goal to move away from sets if you want the same cycle_basis for the same inputs. 
comment
The examples in the doc_strings can indeed be separated by text...  They just need a blank line after any output (so the testing software can tell when to stop comparing). I'm not sure that they need a blank line before the example too, but it often looks nicer that way. 
comment
Does it work to use `nx.bipartite.degree_centrality(G)`? Now that we have lazy loading that should be the preferred way to access the bipartite subpackage. And it cuts out one more import statement from the example. :}
comment
I don't think the array should have a 1 at indexes 0,2 and 2,0.  The order of the nodes in your nodelist have index 0 and 2 pointing to node values 0 and 3. And there is no edge in the dict_of_lists between 0 and 3. But maybe I am missing something.
comment
It is important to distinguish between a nodes label (it's number in this case) and its position in the matrix. The order of the nodes in the graph is not 0,1,2,3,4,5,6,7,8,9,10,11,12,13. It is: 0,1,3,4,2,5,6,7,8,10,11,9,12,13.  When you export, the graph is represented correctly in any order you use. But you have to understand that e.g. the array will look very different if you change the order of the rows and columns. The number representing a node is not the row number for that node. And for the dict_of_list format, the value you see in the dict-of-lists **is** the number representing the node.
comment
The test errors are due to a change in availability of the code coverage tools in our git actions testing quite. They are not due to anything in this PR.  The PR #6635 will (hopefully) fix the testing problem when it gets merged. I'd suggest waiting until that fix is merged and then merge/pulling the resulting upstream networkx repo into your branch. This will likely have to be done with every open PR, so if you try it please let us know how that works for you.
comment
I agree. The import statements should be removed and the calling structure should be: ```python     nx.community.louvain_communities(G, seed=123) ```
comment
Hmmm...   It works for me on the dev branch. Which version of networkx are you using `print(nx.__version__)`?
comment
Yes -- I think a PR to fix that example is the next step. :}
comment
Aha -- yes, the "stable" documentation for 3.0 didn't have this update and the "latest" changes the imports/calling. The will get fixed when v3.1 is released (soon).    Thanks for reporting this!
comment
This looks fine for content -- but the ISMAGS Object section has no space before it (the References section runs right up to it, while the other sections get some breathing room).  I don't mind merging this as is, but is there an easy way to make the space between sections?  (maybe just a blank line at end of doc_string? but maybe different autodoc magic in the .rst file?)
comment
[Line 572 of networkx/readwrite/gexf.py](https://github.com/networkx/networkx/blob/34d77cf4ed2aa6b69a34f617607d46b572faf45f/networkx/readwrite/gexf.py#L572) is where the difference between version 1.1draft and 1.2draft is processed in NetworkX.  Indeed, if a color is specified and there is no alpha value associated with that color, the value `"None"` will appear in the resulting file.  One could say this is simply enforcing the requirement in v1.2draft that all colors have an alpha value. But I agree that this is somewhat limiting, especially when shifting from older color systems that don't include the alpha value.  One fix would be to change line 572 to provide a default value of 1.0 for the alpha value of the color.  Do we see any down side to making that shift?  I would be fine with a default for alpha and not for red/green/blue because I doubt many people leave out only one of red/green/blue. They might leave out all 3(4 including alpha) but then the "None" value seems like an OK result in that case.  This might be an easy fix.
comment
`all_simple_paths` is a really really hard problem. Even writing your own version in C will likely max out your hardware with a somewhat large graph.
comment
Thanks very much!
comment
I have "block"ed MasterC4t from the NetworkX organization. We can unblock if needed.
comment
Looks good.  At some point we should update the "funding" section of the About Us to include CZI multiple times.  It doesn't have to be in this PR though.
comment
Did you mean to close this PR @PurviChaurasia ?
comment
I like the idea of having simple_cycles for undirected graphs. We've got `simple_cycles` for directed graphs. So this is much the same, though I haven't looked hard at that code with this in mind yet. We've had a number of people asking about searching through all cycles, so this would be a reasonable way to address that long term issue.  Of course, tests and docs and stuff are needed. And the `chordless` naming scheme gets switched to `allow_chords` but then used as `chords`, so some fixing is in order.   The paper you point to is 20 years newer than anything in the references for the directed case `simple_cycles`. And this improved version handles cycle reversals and unsortable nodes. So, I'm in favor of fleshing this out.
comment
Too bad to lose the search for chordless cycles... but leaving that till later makes sense to me.  It seems that the undirected solution can be built from the fundamental cycle basis. But not much is talked about how to enumerate that. I found a link to an old paper that discussed how to enumerate the elementary cycles from the cycle basis. It's [by Gibbs in 1969](https://dl.acm.org/doi/10.1145/321541.321545) and it seems straight-forward. It'd be cool  to try putting together a version of that to compare with. I'll try to take a stab at that.
comment
Here is some code for tests. I had the Gibbs algorithm in mind with these tests, but they might be applicable to other algorithms too. ```python     simp_cycles = undirected_simple_cycles_from_basis      testG = nx.cycle_graph(8)     cyc1 = tuple(range(8)) + (0,)     assert list(simp_cycles(testG)) == [cyc1]     testG.add_edge(4, -1)     nx.add_path(testG, [3, -2, -3, -4])     assert list(simp_cycles(testG)) == [cyc1]      testG.update(nx.cycle_graph(range(8, 16)))     cyc2 = tuple(range(8, 16)) + (8,)     assert set(map(tuple, simp_cycles(testG))) == {cyc1, cyc2}      # print("Starting New Test")     testG.update(nx.cycle_graph(range(4, 12)))     cyc3 = tuple(range(4, 12)) + (4,)     expected = {         tuple(cyc1),         tuple(cyc2),         tuple(cyc3),         tuple(range(5)) + (11, 12, 13, 14, 15, 8, 7, 0),  # cyc1 + cyc2 + cyc3         tuple(range(4, 9)) + (15, 14, 13, 12, 11, 4),  # cyc2 + cyc3         tuple(range(5)) + (11, 10, 9, 8, 7, 0),  # cyc2 + cyc3         # cyc1 + cyc2 is disjoint     }     {         (0, 1, 2, 3, 4, 5, 6, 7, 0),  # cyc1         (8, 9, 10, 11, 12, 13, 14, 15, 8),  # cyc2         (4, 5, 6, 7, 8, 9, 10, 11, 4), # cyc3         (4, 5, 6, 7, 8, 15, 14, 13, 12, 11, 4),  # cyc2 + cyc3         (0, 1, 2, 3, 4, 11, 10, 9, 8, 7, 0),  # cyc1 + cyc3         (0, 1, 2, 3, 4, 11, 12, 13, 14, 15, 8, 7, 0),  # cyc1 + cyc2 + cyc3     }     answer = set(map(tuple, simp_cycles(testG)))     # print("Answer",answer)     # print("Expect",expected)     assert answer == expected     assert len(answer) == (2**3 - 1) - 1  # 1 disjoint comb: cyc1 + cyc2      # Basis size = 5 (2 loops overlapping gives 5 small loops     #        E             #       / \         Note: A-F = 10-15                                             #    1-2-3-4-5                             #    / |   |  \   cyc1=012DAB -- left                                     #   0  D   F  6   cyc2=234E   -- top                          #   \  |   |  /   cyc3=45678F -- right                                              #    B-A-9-8-7    cyc4=89AC   -- bottom                     #       \ /       cyc5=234F89AD -- middle                                            #        C              #     # combinations of 5 basis elements: 2^5 - 1  (one includes no cycles)     #      # disjoint combs: (11 total) not simple cycles     #   Any pair not including cyc5 => choose(4, 2) = 6     #   Any triple not including cyc5 => choose(4, 3) = 4     #   Any quad not including cyc5 => choose(4, 4) = 1     #     # we expect 31 - 11 = 20 simple cycles     #     testG = nx.cycle_graph(12)     testG.update(nx.cycle_graph([12, 10, 13, 2, 14, 4, 15, 8]).edges)     answer = set(simp_cycles(testG))     assert len(answer) == (2**5 - 1) - 11  # 11 disjoint combinations ```  And the code I'm using. Two functions:  - one to convert from sets of edge_ids to cycles written as tuples of nodes. - the other to find the simple cycles as lists of edge_ids and use the 1st function to yields the cycles as tuples of nodes.  ```python from collections import defaultdict import networkx as nx   def path_from_set_of_edges(edges):     # construct adjacency of the cycle     nbrs = defaultdict(list)     for u, v in edges:         nbrs[u].append(v)         nbrs[v].append(u)     # print("nbrs: ", nbrs)      # Build edges starting at first edge     start, tip = cycle = list(edges[0])     while tip != start:         # cycle guarantees 2 nbrs         u, v = nbrs[tip]         tip = u if v == cycle[-2] else v         cycle.append(tip)     return tuple(cycle)   def undirected_simple_cycles_from_basis(G):     edgelist = list(G.edges)     edge_num = {}     for i, (u, v) in enumerate(edgelist):         edge_num[u, v] = edge_num[v, u] = i      # potentially long     basis = list(nx.cycle_basis(G))     #print("cycle_basis:", basis)      # convert cycles to sets of edge numbers     Basis = [{edge_num[e] for e in nx.utils.pairwise(C, cyclic=True)} for C in basis]      # loop over Basis cycles adding to all previously found     B = Basis.pop()     yield path_from_set_of_edges([edgelist[i] for i in B])     all_linear_combs = [B]     while Basis:         B = Basis.pop()         R = []         new_LC = [B]         for T in all_linear_combs:             comb = T ^ B             new_LC.append(comb)             # if comb is disjoint cycles T and B (T & B empty) cycle is not simple             if T & B:                 # if comb contains an existing cycle it is not simple                 if any(comb >= V for V in R):                     continue                 R = [V for V in R if not (comb <= V)]                 R.append(comb)         yield path_from_set_of_edges([edgelist[i] for i in B])         yield from (path_from_set_of_edges([edgelist[i] for i in C]) for C in R)         all_linear_combs.extend(new_LC) ``` The large memory comes from saving all the linear combinations of basis cycles as sets of edge_ids.  It should be much thinner RAM-wise if we save which basis cycles participate in each cycle, but slower of course. With k basis cycles, we have 2**k -1 combinations to look at, so the enumeration of cycles is exponential in the number of basis cycles no matter what we do. I'm hoping that storing 2**k-1 indexes to the cycles rather than 2**k-1 sets of edge_ids will be thinner. But that is not this code. This code stores the sets of edge_ids for every linear combination of basis cycles.  
comment
I think to avoid the slow views you would want to keep one graph for path lengthening and another graph for blocking. Subgraph views are rarely performant unless you only look up each edge once. They save memory, not time. Would keeping two versions of the graph also allow e.g. removing edge `(b, c)` in your example above by removing it from one graph but testing whether the result is chordless by using a version of the graph without the edges removed?    Maybe we make a copy of the graph to hack at while finding cycles and use the original graph to check for blocking and/or chords? Does that even make sense? Not sure if it si possible.  I like the term `chordless` rather than `induced` because it actually describes what it means (once you learn what "chord" means). The word "induced" is used often for subgraphs, but it confuses many people. I personally think that is because the word doesn't actually mean anything specific on its own. The word "induced" is not a natural word to describe: "all the edges in the graph between these nodes have to occur in the subgraph". Also, you have to think of a cycle as a subgraph of the graph (which it is -- but often isn't thought of that way) and also understand/recall what an induced subgraph is. That said, anyone comfortable with the term "induced subgraph" will be able to work out that a cycle being an induced subgraph means that the only edges between the nodes of the cycle are part of the cycle. So, while my preference is for "chordless", I could be convinced or overruled without too much trouble.
comment
I like the approach of returning nodes to represent cycles and leaving multiedges out of the process of finding the cycle-nodes.  Finding all cycles in simple graphs is hard enough. The multiedges "just" duplicate some cycles many times. Hard to see that as a useful feature -- and producing such a list is certainly possible from the cycle's list of nodes.
comment
The whole multigraph thing is messy. I agree that 3 parallel edges should create 0 chordless cycles, and 2 parallel edges should create 1.  But its a digon.  And the digons are iffy when talking about cycles anyway.  Would the whole suite of functions be easier to understand/use/maintain if we just state that we aren't going to report any digon cycles?  The user can pretty easily construct them the way they want to...  and we aren't doing digon cycles along paths right? (I mean the path 1-2-3 has cycle 1-2-3-2-1 if you allow digon cycles.)  I'm leaning toward ignoring both multiedges and digons.  But you have a much better perspective on this.
comment
Yes -- I agree that a new function `chordless_cycles` would be better than having it be an option on `simple_cycles`.  Nice work! I'll do a review of it -- and hold off on the docs till that goes in.
comment
I actually meant that I would hold off on the docs until you were done.   This works well!   Thanks for getting this ready for review! :)
comment
Can you check and fix the doc_string where needed for the return being `dict` or `iterator`? This is the same file and it seems reasonable to fix both here.
comment
Thanks for this!  Do you have some ideas about why this is faster than what we compared 3 years ago? Some things I see: - You've made it so that only nodes not in `seen` get added to the list for the next level. This avoids the need for `nextlevel` to be a `set` because the same node is never added to the queue a second time. That's what allows you to get the preserved node order.   - I think eliminating nodes in seen from going into the queue may also lead to the large speed up for dense graphs. You have essentially pushed the collection of the next level nodes to the next iteration of the loop. That allows processing the new nodes as soon as you get them from `adj[node]`. The current code collects them one level earlier, but still processes them later, so they must get stored in "nextlevel" in the meantime. Basically, the current code adds the same node to the queue many times and that increases processing.  - It also looks like you have eliminated the set/list creation needed in the current version for "nextlevel" and "found". But I can't think that takes much time since that is only done once per level. - In the first level, you avoid all the checking of nodes in seen and adding them to the queue by preallocating them.  Again I don't think that saves a huge amount of time.   I wonder if checking the speed of the current code with a change of `nextlevel.update(adj[v])` to `nextlevel.update(adj[v].keys() - seen.keys())` would be comparable.  But I doubt it. The new code has a "just-in-time" feel to it.  In that sense, could you try in your new code changing: ```python            for v in adj[node]:                  if v not in seen:   ``` to ```python             for v in adj[node].keys() - seen: ```  Nice work!  Thanks!
comment
Interesting!  Thanks...   So, I think that means the new code with `for v in adj[node].keys() - seen` is the fastest of these tests -- but I'm losing track of which base is being used for the 1.25 to 107 times faster. :}  Whichever is the best, lets go with that. :)
comment
This looks even better than the previous version. It is easy to read with no figuring out whether the deque is getting emptied correctly.  The main part of the speedup in this PR is making sure not to add previously seen nodes to the queue. This version also removes some extra data structures by refactoring the loop. Should be easier for people to review too.  I reaffirm my approval of this PR. :}
comment
@mriduls and @rossbar  Might there be a chance to get this reviewed and into v3.0?  It could speed up many algorithms that use shortest_path_length. Also I think the changes are short enough to review fairly easily.  If not that's OK.  We will expand these ideas to e.g. shortest_path and maybe others and can make it a plus for the whole module in v3.1. :)
comment
I agree that it is less worthy. But what do the timings say?  It does cut the final level once all nodes have been seen. But rather than converting the input to a list, we should change all the calling functions so they create a list rather than a dict. But I'm not sure that is worth doing either.  How can we tell if it makes a difference?  But that patch should be a different PR anyway.
comment
I'd be interested in seeing a comparison between using passing `G.adj` into this function and `G._adj`. See #4895 for an issue discussing this.  Because `G.adj` is a view on `G._adj` it is slower, and since this function never updates G there is no risk of exposing the internal data structure to corruption. Also, this code might really benefit from the direct access to the dict-of-dict storage.
comment
This is ready for another set of eyes. I think it is ready for merging, with perhaps discussion/thought about 1) whether to include the changes of `G.adj -> G._adj` in this PR or to make them more pervasively in the codebase as a separate PR. 2) whether  to include the other BFS improvement PRs together with this one, or to keep them separate.  I am in favor of including the `G.adj -> G._adj` change here as a proof of concept and experiment for how and whether it impacts people.  I am in favor of keeping the various BFS improvements in separate PRs because it makes review of each one easier. But I admit there are positives for doing it the other way on both of these issues.
comment
Hmmm...   Selfloops are such a crazy corner case...    The `k_truss` function does "work" when selfloops are present. I mean it returns an induced subgraph with one definition of the right property.  But it might not match what people expect. The code ensures that every edge in the induced subgraph has k-2 nodes that are connected to the two nodes on the end of the edge. That normally implies that the edge participates in k-2 triangles within this induced subgraph. And the standard definition of a k_truss is that it counts the triangles an edge participates in.  But with selfloops, the two nodes at the end of the selfloop edge are the same!  So every neighbor of the node with a selfloop is a neighbor of both ends of the selfloop edge.  Does this count as a triangle? u - v - u and back to u along the selfloop edge is a strange triangle.  But it does fulfill the property that the edge has a shared neighbor between the two ends of the edge(!!).  It reminds me of the question: if one angle of a triangle has angle measure 0-degrees then is it a triangle? One edge has length 0 and the other two are really the same edge. There's no good answer.  I guess that means we could say that the returned subgraph is a k_truss as long as you are willing to count triangles generously.  But I prefer the approach that we should **not** be counting these self-loop paths as triangles.  What do you think?  If we go with the standard definition/intuition of what a triangle is, then the documentation could be changed. I would prefer to say that the results are potentially confusing if the graph G has selfloops. So, update the current language about not implemented for directed graphs or multigraphs (take out the parallel edges in favor of the term multigraphs). Then add a sentence saying that if G has selfloop edges, the results may give surprising results.   But I think a maybe better solution would involve raising an exception when self-loops are present. You can check if selfloops are present using `if G.number_of_selfloops() > 0:`  or if you prefer: `if any(n in nbrs for n, nbrs in G.adj.items()):`  Then raise a `nx.NotImplementedError` with a message saying the function doesn't work with selfloops.  So -- we need to choose between backward compatibility by updating the docs to explain that selfloops will give an answer that might be confusing. Or change the code (making it not backward compatible) to raise an exception when G contains selfloops.   
comment
I'm not sure if this is related, but I'm getting a circle-ci workflow error message: [https://github.com/networkx/networkx/actions/runs/4450093865](https://github.com/networkx/networkx/actions/runs/4450093865)  The error is basically: ``` ##[debug]Fetching JSON: https://circleci.com/api/v2/project/gh/networkx/networkx/9388/artifacts ##[debug]Successfully read CircleCI API token ##[debug]Artifacts JSON: ##[debug]{"message":"Bad request."} Error: Cannot read properties of undefined (reading 'length') ```
comment
Yay!  Thanks for this input and sleuthing. We need to change to radians with an `r` or degrees!  And we should find a way to easily make the default layout for the latex/tikz functions. Perhaps a separate function could provide the `pos` argument... something like `tikz_circular_layout(G, radius)`  I'd prefer that over an argument to `to_latex` that influences the default layout. Let's just pick a good default and make it easy for people to make `pos` for tikz.  Thoughts?
comment
So, shall we set the default to what it was before only this time do the radians correctly and set the radius to 1 (or maybe 2)?  The current default is not good, and I'd love for it to look reasonable.
comment
Looks good!   I guess prettier pre-commit wants different white-space. But it's good to see that the code is in place and works. (at least the page loads and the script is in the html) :)
comment
You can click on "Show all checks" and then the "Details" link to the right of any check that has a red X indicating that it didn't pass the tests. The "style/format" test usually means that you didn't run the "precommit" suite of style checking programs on your commit.  That can be automated by installing precommit (see the contributer guide at the end of step 1 for how to do that). It looks like in this case, the style checker is complaining about extra parentheses.   But it also looks like you may have other errors showing up and I'm not sure why... You can run the test locally using `pytest --doctest-modules weighted.py tests/test_weighted.py` to check for those test errors.  Then commit any changes and push to your branch and it should update the PR and rerun the tests.
comment
First, can you fix the precommit errors?  Second, other functions use `negative_edge_cycle` and apparently one of them (it looks like `nx.capacity_scaling`) relies on the old error behavior -- rather than returning `False`.  So, this change is more delicate than expected.  You will need to find those parts of the code, understand why they used to give a NetworkXError, and make sure that they now give a reasable other type of error.  If that is the case, then you just have to change the error type that **those tests of the other function** look for.  For example if it now raises a `NetworkXUnbounded` error instead of a `NetworkXError` then you will need to change the test that is currently failing to make it look for a `NetworkXUnbounded` error.  Hope this makes sense.
comment
I think you should comment out the failing test on line 441 of `algorithms/flow/tests/test_mincost.py`.  It should no longer raise a NetworkXError.
comment
Can you provide  short code snippet that makes this change needed?  Or ideally, a test that fails for the current code and that this fixes... I feel like this doesn't provide enough context for me to figure out why it is there if I see it next year. 
comment
This looks good! I like the removal of e.g. `nx_comm` and the import cruft is nice to get rid of.  Are there other places where communities are used? For example, in the `examples/` folder? Or elsewhere in the codebase? There are fancy ways to use *nix commands like `grep` or `find` or both to find the word "community" -- and I'm sure there are other ways to do that (like with Github's search and then looking at the "code" results). Can you find any? 
comment
Yes -- please remove the explicit import of community from the test files. I think we shouldn't have to worry about HTML or release docs.   Thanks!
comment
Why do you think that `isomorphvf2` should be imported directly into that namespace? It may have used identical names to other names already in the namespace. For example, `GraphMatcher` seems to be defined twice -- once with keyword options for `node_match` and once without those keyword options.  It's probably better not to import isomorphvf2 directly into the namespace. We could revise the namespace and our roadmap suggests that this might happen sometime "soon". But it will take some discussion.
comment
You can still comment on this PR even though it is closed. And we can reopen it if desired. 
comment
This Issues was closed in #5641
comment
Yes -- a string is a container. And containers can be nodes, or pairs of nodes (edges), or even paths in some contexts.  And yes @navyagarwal including some clarifying examples in the documentation would be good! :)
comment
The error message says it all:   >=3.8 NetworkX requires Python 3.8 or later.  (note that most of nx will work with 3.7 and 3.6 but any functions that use the newer features of python will not work. And if you are installing with pip you want them all to play together nicely.  To force it on an old version of python, download from git and install by hand.
comment
I think the fix will be to add a check a check for G being empty (`G.size() == 0`) early in the function. If it is empty then return `False`.  Then you should add a test in the `tests/test_weighted.py` file where it test the `negative_edge_cycle` function.
comment
Fixed in PR #6473 
comment
Thanks @Qudirah  for the nudge on this Issue. and Thanks @sengels-tum for reporting this!!  That's a bug. The `__len__` method in `UnionAtlas` was assuming distinct keys in the two dicts it is taking the union of. When they are not distinct, it gives the wrong answer. **oops!**  PR coming up.
comment
This has been a thorn in our side for a while. So: Thanks for this!  Can anyone think of a reason for people to want the average shortest path for a weakly connected directed graph to be computed this way? I'm ready to change it -- I had a colleague in my office last week who was confused by it.  Can we change it for v3.0 without a deprecation?  I think it is just wrong/useless at the moment. But this will change from returning a useless value to raising an exception.  Still.... seems like that is a pretty safe change.
comment
The alpha value should be used when it is present, but the "get" function returns `None` by default when the key is not in the dict. So, we need a second argument to the `get` function which provides a default of value 1. See dict.get docs. 
comment
Looking at the `dag_longest_path` function a little more, it looks like that function first finds the path length and then builds the path as a last step before returning.  Then the current `dag_longest_path_length` calls that function and takes the result and builds the length from the path.    I propose that we "just" refactor the two functions so that either the `_path` version calls the `_length` version and then builds the path, or they both call a helper function that returns the data structures from the search and the main routines either build a path or compute a length.   I don't think it makes sense to have the length, not return it and then have the user compute the length.  Also, the path functions elsewhere typically have a `_path` version and a `_path_length` version. So maybe its good to keep that here?  Anyway, I hope to get a PR that would make this better (and as a side effect probably won't deprecate the function).
comment
The error only occurs if you give a keyword name to the input G.   We need to fix that...  but try it with input `G` instead of `G=G` and it should work just fine.  Perhaps we should make that input a position-only input. But maybe it would be possible to make the backend code find keyword arguments as well as position arguments.  (if len(args)<1 look for a keyword `G`? But then we can't apply it to functions that name the graph something else -- if there are any such functions :})
comment
We'd have to make any function that uses the `_dispatch` decorator make the first input position-only. Would it be more flexible to allow the `_dispatch` decorator to include as an argument the name of the possibly keyword input variable? It could default to `G` and not change much code anywhere -- and we'd have to document that anyone decorating a function with first argument called something other than `G` will need to use this argument for the decorator.  Then inside the decorator code we check if `len(args)<1` and if so, we look up the keyword arg. (of course, maybe we want to allow dispatch to position arguments other than  the first position...  but one step at a time here... :}  Not sure what is best...
comment
Sounds good.  I think it is pretty universal within NetworkX that the graph goes first and it is named `G`. I guess that might be the first fix then...  Change `_dispatch` to check if `len(args)` is zero and if so, look at `kwargs` for an entry called `G`.  That seems easier than going through the codebase to add position-only syntax for each function that takes a graph as the first argument.  :)
comment
The traveling salesman problem **on a network** cannot be guaranteed to only visit each node once.  Consider the path on 3 nodes (0, 1, 2). The route starting at node 0 would be (0, 1, 2, 1, 0).  One way to guarantee that each node is only visited once is to use a complete graph. But that's not really the TSP on a network.  That's the full TSP. And I don't think that's what you are talking about.   If you have edges with weight 0 and that makes it cheaper to go through a node twice (because the route is better), then I think you **should** expect to have a node repeated on the tour.  Consider the cycle on node (0,1,2) with weight zero on edges (1, 2) and (2, 0).  Then the best route is (0, 2, 1, 2, 0) and not (0, 1, 2, 0).  So that is another case where you don't expect each node to appear once. 
comment
Sounds like you are looking for a Hamiltonian Path if that helps any. Thats a very hard problem. We have functions for hamiltonian paths with tournament graphs. But the general problem is really hard. (And so is the TSP problem -- the TSP algorithms only provide approximate solutions anyway.)   An exhaustive search for Hamiltonian Paths could work, but would be slow if your graph is even medium sized. ```python # n is the number of nodes # dist is a 2d numpy array of distances between nodes def min_hamiltonian_path(n, dist):     return min(itertools.permutations(range(n), n), key=lambda perm : sum(dist[u, v] for u, v in nx.utils.pairwise(perm))) ```
comment
The trouble here is that the string representation of the nodes include the delimiter (a space by default).  Note that the string representation of a string with spaces in it will include quotes and thus not cause delimiter troubles even with the default delimiter.  The fix is to assign a delimiter that doesn't occur in the string representation of the nodes.  For example: ```python nx.write_weighted_edgelist(graph, filename_edgelist, delimiter="|")  graph = nx.read_weighted_edgelist(filename_edgelist, delimiter="|") ```
comment
Check out #5736 which could be a function that solves this issue once we rename it as the primary function.
comment
Your examples also bring up questions about what it means to be a lowest common ancestor when the graph is a DAG. With a tree, there is a unique LCA for each pair. But with a DAG you can have edges between nodes at the same "level" resulting in multiple choices and how do we compare "5up 3down" to "6up 2down"?  I'll try to construct a simple example to show the trouble: Multiple LCAs:   `G=nx.DiGraph([(0, 2), (0, 3), (1, 2), (1, 3)])` `LCA(2, 3)` can be either 0 or 1.    0: 1up 1down   1: 1up 1down So, we should not expect a unique answer for this LCA problem.  Multiple LCAs with different total numbers of nodes in the paths: `G=nx.DiGraph([(0,2), (2,4), (0,5), (1,4), (1,3), (3,5)])` LCA(4, 5) can be either 0 or 1.  0: 2up 1down   1: 1up 2down  Combining the two examples we can get different total lengths: `G=nx.DiGraph([(0,3), (0,4), (1,3), (1,2), (2,4)])` LCA(3, 4) can be either 0 or 1,  0: 1up 1down  1: 1up 2down  It's not clear to me exactly how the LCA is defined for a DAG. We return **a** node that is a LCA.
comment
I agree that 1) LCA may have multiple solutions and so this algorithm finding one of them is a useful function. 2) adding a second function that returns **all** the lowest common ancestors is probably also useful. There is [a post on Baeldong.com](https://www.baeldung.com/cs/lowest-common-ancestor-acyclic-graph) that describes a fairly simple algorithm to implement this second function for DAGs.
comment
I opened an issue requesting a function for all_lowest_common_ancestors. I think that means this one can be closed (with a pointer to it for those interested). 
comment
Looks like the latex commands within dollar signs need each slash doubled to indicate a latex slash.
comment
Too bad this feature was lost (or not as easy). The FancyArrowPatchs give us many nice features.  It looks like the colorbar features of Matplotlib include a ColorbarPatch.  Can you try the answers in [this stackoverflow answer](https://stackoverflow.com/questions/18658047/adding-a-matplotlib-colorbar-from-a-patchcollection) to see if something similar can work here? 
comment
The arrows are a PatchCollection, not a LineCollection. So you have to use matplotlib code for  PatchCollections to make the colorbar. The workaround is only 3 lines of code instead of 1 line for the LineCollection, right?
comment
Yes -- the reason `create_using` is a keyword even though the function only works with undirected graph that aren't multigraph is so that a user could build the graph using their custom graph subclass.  I am almost certain there are other graph generator functions that could use this reasoning and don't -- i.e. they don't have a `create_using` keyword input.   The ideas from NXEP 3 might make this no longer relevant (use the edges_and_nodes version of the builder and pass that to your subclass), but for now `create_using` is the best way to plug your graph subclass into the graph generators tools.  I also notice that this function is old enough not to use `not_implement_for` to restrict which graph type is allowed. :)
comment
Looks like there are 3 imports in `betweenness_subset.py` that are from the same module and could be combined into a single import statement. I'm not sure why ruff's isort would spread a line with 69 chars into a 3 line statement, but if it is going to do that we might as well combine it with the other two. (unless there is a style rule that says we shouldn't import more than one function at a time(?))  And I'm not sure whether that should be done in this PR or another one. Probably doesn't matter...
comment
This PR proposes to deprecate the current force directed methods and replaced with a couple of options. Could you give a very short description of what these methods are, how they differ and when to choose which one?   I'm not looking for much -- even just "we try each one and see which looks best (the eyeball norm)" would be fine. I'm not up on the literature here -- it looks like one of the methods follows a paper that's been implemented by Gephi. Perhaps the answer to my question is just that we should add a link to the wikipedia article on [Force Directed Graph Drawing](https://en.wikipedia.org/wiki/Force-directed_graph_drawing).  Also you've got ```generalized_fruchterman_reingold_layout```.  Is this the same algorithm in some special case? How does it relate to the existing function.  Do we need both codes? Can we add a test in some case that checks for the same result?  Thanks!! 
comment
Thank you for a very good summary and comments.  We recently merged yet another force directed layout function using the Kamada-Kawai forces (based on shortest paths) and minimizing with ```scipy.optimize.minimize```.  Clearly there are many force directed methods. :)  Also, it seems that there are different algorithms for minimization as well as different algorithms for computing forces.   I tried to resolve the conflicts between this branch and master using the web-based editor and that wasn't as effective as I'd hoped, e.g. I notice that spring_layout is now defined twice.  Maybe you can take one of them out and commit, or revert my two commits and resolve the conflicts yourself. Sorry about that.  I think its fairly hard to write more complicated tests for these algorithms because of the approximate nature of the minimization. What you have looks good.  I do notice that you have two FIXME comments. Would you like to resolve them?  As far as a name for the function ```force_directed```, maybe you can create a name that captures the idea of your method for minimization.   Thanks! 
comment
Thanks for the polite nudge...  The only trouble I have with this change is that it removes the old method which users may have come to count on. I guess it is three functions with one public facing and the other two support (private) functions.  I am ready to merge if we:  - put the previous functions back in place. - rename in some way that makes sense to make both available. - explain in the docstrings for each that the other exists and may be helpful.  Do these suggestions make sense?
comment
Yes, the suggestion in the text of the deprecation warning text of this PR is: `isinstance(obj, str)`  :)
comment
We don't usually use the "assign" function in Github to assign Issues. Please open a PR with the fix for this issue and we will review it. Thanks!
comment
It looks like an extra file got included in the `git commit`.  You should be able to do something like: ``` git reset HEAD~1 git restore get-pip.py git add networkx/generators/line.py git commit git push --force origin documentation-fix-on-inverse_line_graph ```
comment
The default values are chosen intentionally -- though I can believe that some are based on historical defaults rather than some sort of "ideal" way to choose the weight argument. I haven't looked specifically at these functions to see the histories. But if a method can use edge weight but often or famously doesn't, then the default `weight=None` makes sense with detection of "None" meaning make every edge have weight 1. If the function always or by design uses an edge weight, then `weight="weight"` is a good default. And  then occasionally, a function may have been implemented without edge weights, and then the edge weight keyword argument in implemented later. For backward compatibility, it makes sense to leave the weight parameter defaulting to all edges having weight 1 (that is, `weight=None`).    From a user perspective, if you are running your graph through many different functions, I would recommend that you not use default values for `weight`.  It makes your code more clear and avoids any confusion between functions.
comment
Nice!!   There are a lot of pieces there -- but at least we can make a list now. :}  I believe the `simple/lazy/pagerank_directed_laplacian` are all available now in `directed_laplacian_matrix` with a keyword argument for which version you might want. (looked at that earlier today while reviewing a different PR).  It is not in the community subpackage, but I think is where it should be especially since the proposed laplacian centrality would use it also.
comment
This is a potentially important step toward parallel work using NetworkX. Decisions made here can affect a lot of code long term. So it is good to implement for one function and see what issues it brings up. I can imagine this becoming a large support issue if we implement it for many functions. There are so many ways to set up parallel systems: synch vs asynch, one machine vs remote connections, etc.   I wonder if we could start by making this an example. Then people don't expect it to "just work" in the way they want it to. They expect to have to switch it from Pool.map to Pool.starmap and they have the code easily accessible to take it and change as needed. (people are reluctant to play with code from the package in a way they aren't for example code.)  What do you think? Am I just acting like an old fuddy duddy? Is this type of usage easy enough now?
comment
I like giving this format a name and "network_text" is my favorite of the suggestions above. :}
comment
This thread is moving quickly -- but I wanted to get in a note about the name `generate_network_text` and the term "generators".  Our read/write subpackage uses a naming convention where `generate_*` creates and returns a string of the output. `write_*` writes the result to a file rather than returning a string. We similarly use `parse_*` to read from strings while `read_*` reads from files.  ~~Perhaps it is confusing that `generate_*` is not a generator. It creates the string and returns it. When we selected this naming convention, the term "generator" was not as commonly used as it is today.  Perhaps we should change the naming convention.  Or, for that matter we could make all the `generate_*` functions into generators. But I think `graph_str` as a function is fine -- and could be renamed `generate_network_text` without making it a generator.~~
comment
I was wrong about the functions `generate_*`. You are right that they yield one line at a time of a text representation of the graph. Sorry about the distraction...  Let's bring the discussion back to the rich text option.
comment
I think the documentation is quite clear here: ``` The `filter_node` function takes one argument --- the node --- and returns `True` if the node should be included in the subgraph, and `False` if it should not be included. ``` I really don't think we should need to check this for the user.  
comment
I think there is some duplication of the filtering due to the process of printing. If you use `for n in SG.nodes:` you will see only one filter call per node of `G`. But if you print the `SG.nodes` attribute it must create the string representation of the object which involves filtering the nodes. ```python G=nx.path_graph(9) def sn(n):     print(f"filtering node {n}")     return n<4  SG=nx.subgraph_view(G, sn) for n in SG.nodes:    print(n) ```  with output ``` filtering node 0 0 filtering node 1 1 filtering node 2 2 filtering node 3 3 filtering node 4 filtering node 5 filtering node 6 filtering node 7 filtering node 8 ```  Similarly, constructing a list by hand only filters once per node.  ```python ml=[] for n in SG.nodes:     ml.append(n) print(ml)  # output: [0, 1, 2, 3] ```  But converting `SG,nodes` to a list seems to iterate through the nodes multiple times. ```python ml=list(SG.nodes) ```  produces output ``` filtering node 0 filtering node 1 filtering node 2 filtering node 3 filtering node 4 filtering node 5 filtering node 6 filtering node 7 filtering node 8 filtering node 0 filtering node 1 filtering node 2 filtering node 3 filtering node 4 filtering node 5 filtering node 6 filtering node 7 filtering node 8 filtering node 0 filtering node 1 filtering node 2 filtering node 3 filtering node 4 filtering node 5 filtering node 6 filtering node 7 filtering node 8 ``` I'm not sure why this would be true.  Perhaps because `len(SG)` also requires filtering. I'll look into that more.
comment
It looks like `list(seq)` calls `len(seq)` to figure out how big a list to allocate before iterating over `seq` to get the values. So,  while having the `__len__` function is convenient for users who want the length.  It may cause multiple iterations through the filter when you don't expect a python function to call `len`.  For expensive filters, care would need to be taken.  We could remove the convenience of providing `__len__` and document to users that to get the length they should use something like `sum(1 for n in SubGraph)`. Then `list()` would use it's backup estimate of list length when the object has no length hint.  We could also add a `__length_hint__` function that just returns the length of the graph, or something like that. That would help list guess the size (incorrectly and then adjust as it needs to) without causing the full iteration process to be needed.  Not sure what makes the most sense here... 
comment
I have no doubt that we could reduce the number of times that the filter is called. But there will be a cost. For example, removing `__len__` would be one way. But it will cost if users want to use the `len` function, and it will cost when Python functions like list, dict or tuple use `len` to estimate the size of memory to allocate. I think this may be worth pursuing, but only if we keep a wide angle on our goals. The goal is not to reduce the number of times a node is filtered. It is to reduce the time needed to do common actions with a subgraph.  Hopefully reducing the number of filter function calls will also make other things more efficient too. :}
comment
I'm interested to hear what the origin of this Issue was.  What are you trying to do with the `subgraph_view` function?
comment
That's helpful to know.  The views are more straightforward for subgraph_view when there is a set of nodes to filter against. That is, when you use the provided `nx.filters.hide_nodes` or `nx.filters.show_nodes` and their cousins for edges. Those special cases store the nodes to show in a attribute of the filter function and it makes the filtering process more efficient.   All of this indicates that the subgraph view interface is too complicated and should be simplified or maybe removed. I don't think that will happen any time soon, but if it affects other people that would be good to know about. I'm going to close this issue -- but further comments will be seen and followed up on.  Thanks!
comment
Did you try `nx.to_latex_raw`?  I'm not sure if that's what you want though...
comment
I'm going to mark this as closed based on #6360. Thanks so much for the help with a much better default layout scheme.
comment
That's funny...  The typing.Protocol class changes the return value of the type() function, breaking our type-checking... :}  Perhaps we need to go back to ducktyping where we just try to create a graph with `create_using` and if it doesn't break we assume that all is OK.  OK...  more seriously, What does the `typing.Protocol` say about how to check the type of the subclass? In particular, we want to check if `create_using` is a `type` class that we could use to build an instance.
comment
That seems like the right fix: In empty_graph, replace the `elif type(create_using) is type:` with ```python    elif isinstance(create_using, type): ```  I guess the workaround in the meantime is to make sure `create_using` is an instance by adding () to get:  ```nx.to_networkx_graph(ts_edges, create_using=Graph())```
comment
Thanks for the offer -- I guess I should have waited... :}  Anyway, can you see if #6244 does what you want? I added a test that broke before and this fixes it, but not sure it is the best test...
comment
Why not check the edges before sending them into the graph object? Something like: ```python ok_edges = (e for e in edges if (e[0][1] < max_lag and e[1][1] < max_lag)) G = graph_func(ok_edges, max_lag=4) ```
comment
You can still do the checks each `G.add_edge`...   And, of course, you can overwrite the `__init__` function to make max_lag available to the add_edges_from method before G.graph is created.  Lots of options... when you are the creator...
comment
They don't....  And I would have said the same thing about "why does edge creation need to be after graph attribute loading?".  It doesn't...  Someone might need to look at the graph attribute in order to set the edges. And someone might need to look at edges in order to set the graph attribute...  But almost always, there isn't any interaction between the inputs to the Graph constructor.   Perhaps there maybe shouldn't be any inputs allowed during instantiation. Just create the graph and then add the features you want. But we liked the idea of allowing lots of convenient features.  Perhaps too many...  As you are finding... they may get in each other's way. But we don't have all the possible features that we could -- for example, nodes can't be specified in the "data".  Basically, we put as many features as we could fit, and we're relying on the users to police themselves.    If someone (like you) wants to do fancy stuff, they can do fancy stuff... Change the order of operations for your subclass if that helps your subclass. :}    
comment
I'm not sure I understand this change completely. It looks like a stub file is added... But I'm not sure what it's purpose is for lazy-loading.  Does the lazy-loader package use stub files to identify subpackages to import?  The next thing I don't recall/understand is how the namespace structure of the library affects the lazy-loading. Are we trying to lazy-load everything in the library? Or only the subpackages? Or only the external dependencies? What's the goal and what's our strategy?  Hiding the networkx functions from the top namespace is a pretty big change. Every user will have to learn the directory structure of NetworkX in order to use it. In that sense it reduces the usability of the library. Additionally, if the directory structure is to be learned by the users, we better make it worth learning. The current structure was designed for ease of developers, not users.   Are there positive tradeoffs to deepening the namespace which offset these concerns? (Other than making the lazy-loading easier to configure?)
comment
Yeah -- it's rather confusing and a little annoying that some of the functions require being called via the subpackages/sub modules and others don't.  I can understand why it is that way for e.g. `bipartite` functions where the names can be the same -- just a bipartite version. But even with that case, we could easily make sure the names don't collide to enable a flat namespace, or we could make a tree of namespaces for all functions.  What is a good criteria for when to subdivide a namespace?  You example of skimage and the 18 subpackages is helpful for me with this discussion. It clarified for me that you are looking to be able to get the function quickly via `dir` and presumably tab-completion. It also helps me see my problem with using such a system: If I want to find `circle_perimeter`, I don't have an easy way to find that function in skimage using `dir` or tab-completion. I would have to search each subpackage's namespace and presumably there are subpackages of those subpackages. The names of the subpackages are not sufficiently revealing for me as a novice user to know where to go. With a flat namespace, I can hit tab, get 900 options, type `circl` and probably get only 5 options and pick the one I want.  Of course, that's assuming I know the name -- and I found that from a google search to the docs so probably on the documentation page it says which subpackage that function is in.  In any case, as far as I know, no one has looked at the namespace/package/subpackage organization of NetworkX in a long while. That's perhaps a testament to Aric's good design choices, and probably also to a lack of attention/maintenance. It's certainly reasonable to open that can of worms for a discussion. :)
comment
I think all of those modules can safely be removed. If someone wants a module from a subpackage they should be able to specify the subpackage and the module. That at least reduces the top level from "all functions and all modules/subpackages throughout the entire code tree" to "all functions and the primary subpackages". I did not check whether there were more that could be removed. I only looked at the listed ones being removed. It looks good to me.  
comment
The impact of this change for developers reviewing PRs is that new functions or modules will need to be added to `networkx/__init__.pyi` as well as (`<whatever subpackage>/__init__.py` or an `__all__` from a module that is imported). Is that the only change in how to make a new function for networkx?  Should we add something to `CONTRIBUTING.rst` about how the import system works? We don't have anything there that I could find at the moment.  But this will make it non-standard -- whatever that means -- and  maybe a description is helpful.   :)
comment
Clicking on the "show all checks" and then one of the failing tests (and then scrolling down through all the testing output to find the error (usually near the bottom)) it shows that the formatting of the centrality values contains either too many digits or not enough. The Examples error checking needs the output to be character-by-character identical to what is produced.   Try using a similar way of printing the results as that used in [eigenvector.py](https://github.com/networkx/networkx/blob/main/networkx/algorithms/centrality/eigenvector.py). The formatted string determines how many decimal places of accuracy to compare. 
comment
No... the tests should be in a directory called `tests` inside the directory where the module lives. So in this case:  `networkx/algorithms/centrality/tests` and the file would be: `networkx/algorithms/centrality/tests/test_laplacian.py`.  sorry for not being more clear the first time. 
comment
The best approach is to have a simple enough example that you can compute it by hand. But that is not always possible.  The next best thing is to generate the results locally and add them as a test.  This doesn't actually test that the result is correct, but it checks that it hasn't changed due to some other aspect of the code. We usually try to have at least one test that is simple enough we can generate the true answers by hand. But as stated above, that is not universal in our tests.
comment
Q1:  The `pytest.importorskip` function is poorly named. It should be `pytest.import_or_skip`. The idea is that running this command tries to import e.g. `numpy` and if successful, it stores that imported module in the local name `np` or whatever you assign the output of the function to.  If unsuccessful, all the remaining tests in this test file will be skipped by pytest.  So, we use it for modules that require an optional library. If that library is not installed, then these tests are skipped. Typically, if you don't actually use `np` or `sp` in the testing code, you would not assign the output to a name: `pytest.importorskip("numpy")`.  That ensures that the tests are skipped when needed, and also indicates that the tests themselves don't actually use the numpy library -- though the actual functions they test do.  Q2:  To include the function name in the full name_space, you will need to add it to the `__init__.py` file inside the `centrality` directory. That then gets loaded when `algorithms` runs its `__init__.py` file which gets loaded when networkx runs its `__init__.py` file. 
comment
I have not had a chance to look through the code in any kind of careful way yet. (2 more weeks of the semester so there is light at the end of the tunnel! :}  I do wonder about the normalized/non-normalized return value. It looks like the normalized returns the % of energy lost when you remove a node. And it looks like the non-normalized option returns the remaining energy after removing the node.  But that is not what the docs say.  It might be better to return "full energy minus after_removal_energy" rather than just the after_removal_energy.  But I might be missing something.  Another fairly big picture question is whether there will be other functions for this module. The name "laplacian.py" is pretty specific and unlikely to attract other measures of centrality. Would this function fit into another module (e.g. eigenvector.py -- though that isn't quite what this is)? Or might there be a larger category of centrality measures that this could fit into? For example: spectral centrality measures?  Are there centrality measures which use the spectrum of a different network matrix?  This really isn't a big deal, but thought I would try to pick your brain a little. :}  I'll try to make a set of actual algorithm/code suggestions soon. Then once I approve it, we'll get another approval and then merge. Thanks for the many tests. It looks like a mix of by-hand and computed examples in there.  What is the test graph "E" and where did it come from?
comment
I think I see the graph E in figure 2 of that paper.  That's very helpful to have those published values -- especially if your implementation agrees with them. :}  And now that I looked at the tests more closely, I have a suggestion to be done before merging... Can you move the tests outside the TestClass structure?  It's something we've been trying to do more of in the last 1-2 years. The test class isn't used in a significant way. The `setup` method is being run before every test and so it has to set up all the graphs for all the tests when it is only running one test.  Can you transition all the tests into test functions instead of test methods? You'll need to build the graph inside the function. But I think that should be easy because you set up the test methods well -- each one uses one graph (I think). That's the hard part. Then it's only a matter of unindenting to make them functions and removing the `class ...` statement near the top. Does this make sense? 
comment
I tried using the self-assigned feature to make sure I didn't forget about this issue in case you didn't come back after a break. It doesn't have any impact over what you should do -- just a tool for me to keep track of issues that are important to me. Sorry for the confusion.  (Actually I'm just trying it out and it hasn't really worked for me so far... so in that sense it really shouldn't have any impact on what you do with this. :}  
comment
It looks like a new version of matplotlib in the CI tests is failing,. That is not your PR, it is the new testing configuration. We'll work on that -- you don't need to.    The first test failuer is an issue with "black" (out code style fixer) It says that `test_laplacian_centrality.py` contains some style changes. You should be able to run `black tests/test_laplacian_centrality.py` locally to fix that -- or if you prefer: `black --diff tests/test_laplacian_centrality.py` to show the differences without making the change automatically.  My suggested code puts the nodes in nodelist as the first rows/columns of the matrix. ```python nodeset = set(G.nbunch_iter(nodelist)).   ## nbunch_iter drops any nodes not in G. "set" removes duplicates if len(nodeset) != len(nodelist):        ## if we lost any nodes then nodelist isn't good...     raise NetworkXError("nodelist contains duplicate nodes or nodes not in G") full_nodelist = nodelist[:] + [n in G if n not in nodeset]. ## now add the nodes from G at the end of the list. ``` That puts the nodelist nodes first with the other nodes at the end. The ordering in the matrices is determined that way.
comment
I went ahead and made some changes that had been discussed. And that led to some that hadn't been discussed.  Discussed stuff: - changed nodelist preparation code - remove casting as csr_matrix. All computation now done with ndarrays since we need dense for eigh anyway. This makes `getcol` available and used `fill_diagonal` for setting the diag entries.  Other stuff: - The normalized code was correct, but the non-normalized version was computing `1 - new_energy` rather than `full_energy - new_energy`.  This fix simplified the code somewhat too. - No tests were present for normalized=False and weight=None. Added some. - wrapped the doc_strings to same as what `black` would provide. Reworded some. - other minor changes for error msgs and spacing.  Feel free to push back on any of this. Make a push to the branch, or comment here in the PR.
comment
We get a second approval (set of eyes) and then we can merge. :}
comment
Another function like that is `graph_number_of_cliques`.  It is literally `len(cliques)` if the input `cliques` is already computed and it is `sum(1 for c in nx.find_cliques(G))` if the cliques input is `None`. Removing these functions (I am sure there are others in this module) and replacing them with examples in the `find_cliques` doc_string seems like a good idea.
comment
Is this different from `nx.shell_layout(G, [[center_node], [n for n in G if n != center_node]])`?
comment
That was the last thing we talked about -- I think everything has been done. I haven't looked through the tests thoroughly yet, but I think we're close to being able to merge it.
comment
Those two failed tests are due to a change in the testing environment that has been fixed on the main branch. Can you merge (or rebase) the main branch into this one?  That should fix those failing tests.
comment
@kpetridis24 can you also look at @rossbar comment/question (at the bottom of the PR comment stream) about the default_label changing from `None` to `-1`? Thanks!
comment
The commits from each PR get "squashed" when we merge the PR. So it comes to the main branch as a single commit. The comments are all still there. That's why those commits show up in the list for this PR. You could probably do some fancy git mangling to rebase your branch on an updated version of networkx/main and remove extraneous commit numbers and edit commit messages. And you'd probably learn a bunch about git while doing so. But you don't need to for this PR. :} Thanks for this. I'll have a look.
comment
I think we want the code, and not the result of the code, to show in the error message. It's intent is to help the user figure out how to change their code.
comment
I've been using this chart to help with debugging #5557 so I appreciate being able to refer to a chart rather than try to retain these difficult to remember codes for each triad type.  But, that also means I've been looking at the chart more than I was before.  I think one graph is not correct and some others have orientation that runs counter to the abbreviation names.  For example '021D', '021U', '021C' are named for the orientations "Down", "Up", and "Cyclic".  But the drawing of the triad doesn't have the edges pointing down or up.  The incorrect graph is "021U" which actually shows a "021C" triad. Take a look at the cited slide-deck for a talk on triads (not a paper...  it'd be nicer to have a paper than a slide-deck for a talk).  Additionally, I think adding the node labels and colors distracts from the idea that triads are an unlabeled concept.  It might also be confusing to have the colors because [there is at least one paper](https://arxiv.org/abs/1802.01481) on using colored triad census -- where the nodes are colored.  @0ddoes can you check and correct the graphs for this example?  Also, what do you think about the node colors and labels?
comment
Hmmm... I seem to be in a picky sort of mood.... so see what seems reasonable, and push back on what isn't...  - the order in which you add edges (left or right side of the figure) doesn't seem to be consistent. For 012 and 102 you have it on the left (which looks good), but then 111D and 111U have it on the right.  And then 201 has the double edge on the left, and 210 has it on the right. As long as its consistent the OCD in me feels good... but if there is a reason for the choice, that's fine too. - The titles get confusing for me... sometimes I wonder if the name above the picture is the name, or the one below the picture?  Maybe instead of a title, you could put the name (as `ax.text` in matplotlib) in the center of the triad. Position (0,0) with options `bbox=dict(boxstyle="square,pad=0.3", fc="none"` and any others you like (or change these). The `bb` is for bounding box and draws a nice box around the text.  - It would be nice to add a sentence or two about the naming convention: Number of double edges, single edges, nonedges in the triad, and then what each letter stands for. - `visualisation` should be `visualization` to match locale spelling for the repository. 
comment
The figure isn't showing up anymore. I tried tha code locally and it seems like the `subplots_adjust` may be causing nothing to show. At least replacing that line with `fig.tight_layout()` gave a pretty good looking image IMO.
comment
It looks like you need to run `black` on your python code (see comment above).  Have you done that?
comment
Rather than a boolean input using a fixed attribute to hold the weights, we want this to allow input of the attribute name that should be used as a weight, and even allow a weight function.  See the treatment done for weight arguments in the `algorithms/shortest_paths/weighted.py` module.
comment
I resolved some conflicts with the main branch (which removed deprecated code). So you will need to pull the changes from your fork to your local machine before you edit your files more. Hope that's not trouble and if you need help sorting that out let us know.
comment
This looks good.  I checked out how the documentation would look after this PR.  You can do that by clicking on "Show all checks" on the right side of the box that says "All checks have passed".  Then scroll down to the 3rd to last check which is run using "CircleCI" and says "document artifact".  Click on the "Details" button and you are brought to a version of the webpage for documentation as it will be after this PR.  It's a little bit of clicking to find your function: Reference (at the top) then algorithms and "distance measures".  And then you can look at each function and how it renders wit its doc_string.  All your doc_strings look good.  But I do see that "resistance_distance" has some unformatted notes at the bottom.  Would you be willing to change the formatting near lines 672.  After the "Overview discussion:" we need a blank line and then indent the two bullet lines below it.  Similarly, after the heading "Additional Details:", only those also need a star in front to make a bullet in the webpage.  If you have had enough, just say so and we'll stop here and merge this. The other change can be done in another PR.  Thanks very much!  Job well done! :}
comment
I think the tests just timed out on the hardware they were use. (There are time limits for some donated hardware.) I have restarted the first and it did not cancel this time. Will restart the other two and hope for the best. 
comment
Ack -- looks like you merged with the v2.8 branch instead of the main branch.  I think we can fix that.  I'll try later today.  The changing of the default weight looks good.  Can we have a test to make sure it works even if the name of the weight attribute is "cost' (or anything else that is not "weight")?  Go ahead and change and push when you've got it. I'll fix the v2.8 branch stuff after that. (or before it if I get to it before you get to it)
comment
You might consider spending your time investigating some of the many issues that are outstanding or on improving the documentation. The time spent by you to go through flake8 reports would be much better spent on improving the repository in other ways. We have done a lot with flake8 and can revisit that work, but the value added of doing it is limited. It would be really cool if you could spend that time learning about network algorithms and/or the issues that have been raised by our users.
comment
It seems that temporal networks have so many different approaches that it's hard to settle on a single approach.  We do indeed have a temporal-isomorph checker code that you have already found. I am not aware of other temporal network algorithms that have been explored sufficiently to have a body of literature--but they could be out there and we'd love a pull request or even just a pointer to such algorithms.   That said, many features of temporal networks can easily be implemented in NetworkX using the base classes through edge attributes that store the time of interaction (for example). Nodes can similarly store information about time in their attributes.  
comment
I don't know of any others. But other people might... Anyone?
comment
I'm going to close this issue as people will still be able to find it searching for temporal networks. If/when the field settles down a little we can revisit algorithms and data structures to add.
comment
Check the version of numpy that you are using.  We support v1.20 or greater. ```python import numpy as np print(np.__version__) ``` Install a recent version and update the other libraries to recent versions too. The `_rand` value was most recently changed for numpy in May 2019, so you are apparently using a very old version of numpy -- and thus probably other libraries too.
comment
I can verify that these two graphs should not return an isomorphism and that the vf2pp code does. I can also verify (using `nx.vf2pp_all_isomorphisms`) that the isomorphism yielded is the isomorphism between the non-multiedge version of those two graphs. The VF2 version of the code verifies that the non-multiedge version of those two graphs are isomorphic with that same isomorphism.   Side note: in my looking at adding VF2++ subgraph isomorphism, I have noticed that some parts of the existing isomorphism code indicate they are checking MultiGraphs, but only check for self-loops. So that might be a place to start debugging this.  Here is some code to reproduce my findings: ```python import networkx as nx g = nx.MultiDiGraph({0: [1, 1, 2, 2, 3], 1: [2, 3, 3], 2: [3]}) h = nx.MultiDiGraph({0: [1, 1, 2, 2, 3], 1: [2, 3, 3], 3: [2]}) print(nx.is_isomorphic(g, h))  # False print(nx.vf2pp_is_isomorphic(g, h))  # True  list((nx.isomorphism.vf2pp_all_isomorphisms(g, h))   # output:  [{0: 0, 1: 1, 2: 3, 3: 2}]  G = nx.DiGraph(g) H = nx.DiGraph(h) list((nx.isomorphism.vf2pp_all_isomorphisms(G, H))   # output:  [{0: 0, 1: 1, 2: 3, 3: 2}] list(nx.isomorphism.DiGraphMatcher(G, H).isomorphisms_iter())   # output:  [{0: 0, 1: 1, 2: 3, 3: 2}] list(nx.isomorphism.DiGraphMatcher(g, h).isomorphisms_iter())   # output:  [] ``` Thanks very much for this report!
comment
Thanks for the Issue and for your careful writeup with good examples.  This looks like the same behavior that occurs for dicts.      d = {n: n for n in range(4)}     for n in d:         del d[n]     RuntimeError: dictionary changed size during iteration  It happens when iterating over ```d``` and over ```d.keys()```, etc. Basically, any time you iterate over a dict, you can't change the dict. As you mention, you can/should use ```list(d)``` or similar to make sure not to change a dict you are iterating over.   I don't think we can make that work without failure unless we 1) don't use iterators/views or 2) don't use dicts.  That said, I don't quite understand why the Graph and DiGraph methods are OK with deleting selfloops. It looks like the inner dict of our data structure does not get iterated over so the changing size of that dict doesn't interfere with the iteration. I would claim the lack of failure for Graph and DiGraph is a lucky happenstance. It is still changing a data structure involving hashes while iterating over that structure.  So.....  What do you think of the following? We add documentation to remind people that when iterating over a NetworkX Graph while removing nodes and edges, you should use e.g. ```list(G.selfloop_edges())``` or ```list(G.edges)``` just as you would when changing a dict while iterating.
comment
I think we could close this.   Another option is to change `visited[child] = None` to `visited[child] = True`. Setting to `True` is perhaps easier to understand as adding child to the dict while setting to `None` might mean turning off a flag held in the dict.  If this is "fixed", it should be changed in any other places in that module where the same idiom is used: probably a search for `= None` will quickly give the places where it occurs.   
comment
Hmmm...  How would that differ for multigraph and graph? Can't you just use the same function?
comment
Hmmm...  You are right -- the code explicitly checks for multigraphs and rules them out. We should probably look at that more closely. I can't think of a reason why that should be done.  A workaround is to convert the graph to a `nx.DiGraph` for the `lowest_common_ancestor` calculation: ```python nx.lowest_common_ancestor(nx.DiGraph(final), "HP:0003388", "HP:0011968") ``` 
comment
I think of this method (converting the MultiDiGraph to a DiGraph so it produces a lowest_common_denominator) as a workaround. I'd rather figure out why the algorithm doesn't work for a `MultiDiGraph` and fix it.  It seems like it should.
comment
Can you figure out whether this code for LCA algorithms works for multigraphs?  Maybe take out the "not_implemented_for" line for multigraph and try it on a few carefully constructed MultiDiGraphs. If there are inconsistent results we could start with a test that shows the problem -- that might make it possible to adapt the code to work for MultiDiGraph. If there are no inconsistent results then it might be possible to remove the "multigraph" restriction.  But it is very simple to covert the MultiDiGraph to a DiGraph. So it might be sufficient to add that to the doc_string and maybe include an example of doing that.  There are definitely places in the code where `degree` is used and other places with `dfs_preorder` used. The degree could easily be a problem for MultiDiGraphs.  For example, the in-degree > 1 check would probably change if we had a MultiDiGraph instead of a DiGraph. 
comment
I believe the time spent on this would be much better spent elsewhere in the library.    Too much of this discussion involves style choices, and it will be very hard to get universal agreement on style issues. Also, it will be hard to get a PR that only has changes that are not a personal preference of the author. This is where an automated tool (like isort or black) is helpful because we all agree to follow the decisions of the authors of that tool rather than our personal preferences.  Also, (and this is a personal style choice which may help give an example of what I'm talking about) I would argue against import lines that bring in a single word from the imported namespace: e.g. `from networkx import Graph`. My reason is that wherever that name is used in the module, it is not locally indicated what namespace `Graph` comes from. The user/developer has to have the import statement available to know whether this is imported from networkx, or provided in the module as an alternative to networkx, or provided by some other library.  The idiom `import networkx as nx` costs 3 characters for each usage of `nx.Graph` instead of `Graph`. But it gains in understanding (once you know the standard abbreviations: ns, mpl, np, sp, pd) about which graph object this is. One might think that a developer always has access to the import statement because it is at the top oft he module, but code from a module often makes its way to the internet in other ways -- via examples, answers to forum questions, documentation. I find myself too often having to do extra work to figure out the import structure when `nx.` or `np.` would tell me a lot. 
comment
Good idea! :}
comment
You are using NetworkX 2.5 and that function was introduced in v2.7.  v2.5 was not built for python 3.9, so you should probably upgrade to a newer version.  Perhaps v2.8.8?
comment
I can verify that there is a bug in `dfs_labeled_edges` for the reporting of reverse edges at the requested `depth_level`. The fix is to add an else clause for the check on `depth_now > 1`. And there are almost no tests currently for "reverse" edge reporting in the test suite. So we should add those. 
comment
This may not be a bug afterall...  more like a question about the definition of a "reverse" edge.  A "reverse" edge is one in which both u and v have been visited and the edge is in the DFS tree. But with the depth_limit, we don't actually visit node v when we move forward along (u, v)... we reach the depth limit and stop that branch.  Because we haven't visited that node, some people could argue that we shouldn't report the "reverse" edge.   One result is that for a fixed depth_limit, the preorder of nodes reports one depth more of nodes than the post-order.  But we can define the labeled DFS edges in either manner. For depth_limit 2, we could report the forward and reverse edges that we would-have-seen without a depth_limit for the edges that we touch.  Or we could report the reverse edges only if we actually visit the node at the far end of the edge. I can't find any good definition of what constitutes a reverse edge (also called a back-edge) when there is a depth_limit to the traversal.  @ikarsokolov can you describe the application you are working with and why you expect there to be a reverse edge for each forward edge when the depth_limit is set?    I can see an advantage to ensuring that "reverse" edge is reported for each "forward" edge. You gain information about when you stopped looking at a node -- but you don't have information about whether the DFS had finished visiting that node or you reached the depth_limit. We can easily limit the impact of this change to just this function. What is the right thing to report?
comment
With our code, you can get the "examine" trait of an edges by looking for `etype in ("forward", "nontree")`. So, I'm not sure it helps all that much. The difference I see is that the boost code has a "finish_vertex" moment. That's really what you are looking for: a point when you can be sure the tree from that node has been explored completely. That's what we use "reverse" to indicate.   But we have to figure out whether we consider the subtree-from-that-node to have been fully explored when the depth_limit stopped the exploration. Currently, we don't indicate that we stopped exploring the tree-from-that-node. I think it would be good to report the edge as "reverse-depth_limit" to make a note that a reverse edge should be indicated (we are done exploring that node), but that it isn't the same as a normal "reverse" edge. I'll put together a PR for this.  In the meantime, a workaround is for you to track the depth yourself, and when a "forward" edge reaches the depth_limit, the corresponding "reverse" edge is guaranteed to immediately follow that "forward" edge. So, you don't have to enter blacklisted mode at all. You know that no subtree nodes will be reported. Something like:  ```python if etype == "forward":     if v in excluded_nodes:         if depth < depth_limit:  # no need to blacklist the subtree if depth == depth_limit             blacklisted_subtree = True ```
comment
It does seem to be true that LazyLoader does not put the module name into the namespace while the regular import does.  My small example requires a package with a subpackage: ``` MyStuff:  |  __init__.py  |->    lib           |  __init__.py           |  somestuff.py ``` The outer `__init__.py` file is: ```python from .lib import * ``` The inner `__init__.py` file is: ```python #from . import somestuff from .somestuff import * print(somestuff.stuff)  # < -- NameError: name `somestuff` is not defined ``` The module `somestuff.py`:  `stuff = "stuff"`.  To construct the error run this script: ```python import networkx as nx ms = nx.lazy_import("MyStuff") #import MyStuff from MyStuff import lib    # or: `from MyStuff.lib import stuff` ```  The error only occurs for a subpackage, and does not depend on the relative reference`.somestuff` (the error still occurs with an absolute reference `MyStuff.lib.somestuff`).  The NameError goes away when you change the relative reference `from .lib import *` to an absolute reference `from MyStuff.lib import *`, or when the package is imported before the subpackage is imported -- shown here by uncommenting any one of the lines of code that are commented out above.  It also goes away if you don't try to LazyLoad.  (Note that the relative reference inside the subpackage does not matter. It is the relative import of the subpackage that determines the behavior.)  It seems that LazyLoader does not include the name of a module in a relatively referenced subpackage in the namespace of the subpackage when `from pkg import subpkg` or `from pkg.subpkg import *` form is used.  The regular (nonLazy) importing process does include the name of the module in the subpackage namespace.
comment
@seberg, the link you gave to a Python discussion/issue is similar, and certainly related to subpackages, but there the lazy load occurs in a sub package. And the full package is already imported explicitly before the subpackage names are requested.  Still, it does seem that both are related to how the LazyLoader is handling adding names to the sub_namespaces.  I suspect numpy could import the module names explicitly alongside importing their contents and the problem would go away. But that could make `__init__` much longer and is a shame to have to do simply because someone might lazyload your package. The best fix would be upstream in the LazyLoader code. I can raise an issue there if this discussion agrees that it is the best way forward.  If you want to try the `__init__` workaround, change each section like: ```python from .type_check import * from .index_tricks import * from .function_base import * from .nanfunctions import * from .shape_base import * ``` to  (and this could instead group the two lines for each module if preferred): ```python from . import type_check from . import index_tricks from . import function_base from . import nanfunctions from . import shape_base  from .type_check import * from .index_tricks import * from .function_base import * from .nanfunctions import * from .shape_base import * ```
comment
@RazerM Thanks for reporting this. We have a PR that removes the lazy load from networkx until this is worked out. We hope to have that in a release 2.8.5 very soon.  Hopefully we can get it fixed in Python's LazyLoader, but it's hard to know. As far as we know, this is a problem in NetworkX v2.7 - v2.8.4  Meanwhile the fix is to import numpy itself before importing from a subpackage of numpy. ```python import networkx import numpy from numpy.lib import recfunctions ``` 
comment
See #5466  and #5514.  Perhaps we need a warning when `arrows is True` and G is undirected **and** a warning when connectionstyle is specified and G is undirected but arrows is False.  Probably we need something when arrow style is specified and arrows is False also.  But the large number of issues we have about these three input parameters suggests that we need to find a better way to specify the information provided by `arrows` `arrowstyle` and `connectionstyle`.  We have too many issues stemming from this API choice.  We want to allow the user to select between LineCollection and FancyArrowPatch. And when using a FancyArrowPatch we want to allow the selection of a connection style and also the arrow style.  And we want different default values for directed and undirected graphs for all choices.  Directed uses FancyArrowPatch with single arrowhead and no connectionstyle by default.  Undirected uses LineCollection by default, ignoring both arrowstyle and connectionstyle.  At least that’s my understanding from a fairly quick look at the code.  Can we make it so that FancyArrowPatch gets used when arrowstyle is not ‘-‘ or connectionstyle is specified? And `arrowstyle` defaults to ‘-‘ for undirected and to ‘->’ for directed. This would eliminate the `arrows` keyword which is confusing.  The behavior would be: - undirected case: FancyArrowPatch when either arrowstyle or connectionstyle is specified? Otherwise a LineCollection is used. - directed case: FancyArrowPatch when either arrowstyle is not specified as `-` or connectionstyle is specified? Otherwise a LineCollection is used.  Does this miss something? Is there a better way to construct this API?
comment
Would changing the name of `arrows` to `fancy_edges` alleviate this confusion? When `fancy_edges` is False, the arrowstyle and connectionstyle are ignored. Directed graphs default to `fancy_edges=True`, Undirected graphs default to `fancy_edges=False`. Hmmmm.... maybe its the same problem.  But what do you think? Would that be less confusing? 
comment
I think it would be fine to work on this -- but you should realize that the final API decisions haven't been made. It might be helpful to see what could work though.  It looks like `arrows` is only used to help set a variable called `use_lineCollection`.  Maybe that should be the name of the input variable instead of `arrows` or `fancy_edges`. Whatever we call it, it should have 3 possible inputs: None, True and False. None means we determine a default from `G.is_directed()` and based on whether connectionstyle or arrowstyle are set. True or False mean we use those values (but should check whether connectionstyle and arrowstyle make sense in these cases and raise a warning (or exception?) in those cases.  Changing the code is not too hard, but making those decisions might be. :)
comment
Closed due to being fixed in #6098 
comment
Sorry this took so long to respond to... I didn't catch it earlier.  The LCA code is suspect. See #4942 for a fairly long discussion that lead to us changing the `ancestors` call you point out to a call on `G` instead of `dag`.  That fixed the bug but we never understood why. It made sense to me why it would work, but I couldn't make sense of the paper describing the algorithm. Others tried to find a workaround, another paper, something that would make this function match a known algorithm.    In the end @jamestrimble posted some code for a [naive version of the function](https://github.com/networkx/networkx/issues/4942#issuecomment-957337481) and said that he didn't have the time to put it into a PR.  He also has a cryptic comment that there is a link on a wikipedia page to a paper with essentially this algorithm.   If you would like to wrap that code into a PR that adds that method to the `lowest_common_ancestor.py` module, add tests, etc, that would be great!  The next step would be to figure out if and why the results are different. I suspect we want to replace the current existing code with this simpler method that is apparently also much faster. We can help of course -- and we will get to it eventually if you don't do it. 
comment
This can be closed due to #5736 and #5883 Thanks!
comment
Looks like this new test is not a valid test. (Added in #6176)  The path graph with 4 nodes does have a valid edge swap:  (0,1) and (2,3) can be swapped. So, this should not test that the swapcount is 0. Of course, sometimes it will be zero. 
comment
I'm a little confused by all the pep8 stuff.  Shouldn't running black once fix all that?  Is it true that black doesn't mess with comments and doc_strings but that flake8 and pycodestyle do? How can we make this easier for submitters? 
comment
I'm looking at this PR again, and I'm getting more excited about its inclusion.  We haven't really made edge and node attributes first class entities in NetworkX.  We allow them and use them, but functions like ```nx.get/set_node/edge_attributes``` are pretty minimal. The functions in this PR increase our set of tools for manipulating node and edge attributes.    I wonder if they should be housed with ```get/set_node/edge_attributes``` instead of in ```convert```? They aren't converting a graph. They are exporting attributes. I'd like to see functions to import attributes (or at least document how to import them. But that doesn't have to be in this PR.  
comment
I think the funcs should have reasonable defaults -- like writing all attributes in the ```edge/node_attr_dicts```. Then the ```to_node_dataframe``` output has columns corresponding to the attributes on the nodes. And the ```to_adjacency_dataarray``` output has adjacency arrays for each attribute on the edges.  I'm pretty sure we could get a better name for ```format_adjacency```.  Perhaps ```adjacency_array_to_dataarray```  (or perhaps not) :)        Also, using "n1" and "n2" and "name" for the 3 axes in the xarray could be made more specific:   perhaps something like "source", "target" and "adj_name". But maybe there's others that are better?   
comment
In support of the functions in this PR, here are the one-liners that they replace.   **First:** ```to_adjacency_dataarray``` with two adjacencies to stack... : ```python da=xr.concat(     [       xr.DataArray(np.expand_dims(nx.adjacency_matrix(G, weight="weight").todense(), axis=-1),          dims=["n1","n2","adj_name"], coords={"n1":list(G), "n2":list(G), "name": ['weight_adj_matrix']}),       xr.DataArray(np.expand_dims(nx.adjacency_matrix(G, weight="capacity").todense(), axis=-1),          dims=["n1","n2","adj_name"], coords={"n1":list(G), "n2":list(G), "name": ['capacity_adj_matrix']})     ],     dim="name" )  ``` Notice that the wrapping code around the ```nx.adjacency_matrix(G, weight=...)``` is identical and very specific to naming rows and columns based on the Graph nature of the array. The general DataArray is much more flexible of course, but a graph adjacency version only involves these features -- and it may be helpful for users (and us) not to have people construct their own version of adjacency arrays.  Notice that while it is a one-liner, it's a long one-liner that duplicates the same cruft for each layer of the xarray. The code in ```to_adjacency_dataarray``` hides much of the one-liner cruft in ```format_adjacency```.  **Second:** ```to_node_dataframe``` with two attributes to stack...: ```python # setup   nx.set_node_attributes(G, {n: n**2 for n in G}, "n-squared')  df = pd.DataFrame([     pd.concat([           pd.Series(d, name=n),            pd.Series({"n-fourth": d['n-squared']**2}, name=n)     ])      for n, d in G.nodes(data=True) ]) ``` This is also a one-liner (though I removed some error checking of the ```func``` by inlining. Again, it would be possible for each user to figure out how to do this. But I think it might be helpful to  standardize for users a good way to store node information in dataframes. (Just as we do for ```to_pandas_edgelist``` and ```to_pandas_adjacency```.)     It would take me a long time to come up with these one-liners and I would need to learn a lot about xarray or pandas. I admit that I would need to learn a fair amount about them just to create the ```funcs``` needed to customize these attributes. But, I think with default functions that simply write out the node/edge attributes on the Graph people can ease into understanding how this works. 
comment
This change does work -- and you'd think it might be faster, but it looks like it is actually slower -- even for large sets.   Probably sticking with builtin python objects is faster than using the same tools via the NetworkX views. The collections.abc tools are nice, but not optimized for speed -- rather for ease of implementation I suspect.  Anyway, if anyone is thinking of propagating use of `keys` instead of sets throughout the library (and there are many other places it could be used), it probably isn't worth the effort. :{  It is probably slightly less readable too.
comment
Getting rid of the comment is a Good Thing! One more "ToDo" out of the codebase! :)
comment
I can verify that this happens when there are two paths of sufficient length from a root to a single node.  I could believe that the "corner case" of a long loop like this isn't being caught.  Any idea how to chase it down?  I suppose we could cut it off early by forcing any (i,i) pair to have ancestor i. 
comment
OK... Thanks for that info... and Thanks for reporting the bug!! 
comment
Very strange.  After running the above to verify the bug, I then remove the node "E" and it works and then add the edge ("E","A") and it still works.  ```python import networkx as nx  G = nx.DiGraph()  G.add_edge("A", "B") G.add_edge("C", "A") G.add_edge("C", "D") G.add_edge("E", "A") G.add_edge("F", "C")  # Should return "C", but returns None nx.lowest_common_ancestor(G, "B", "D", default=None)  G.remove_node("E") nx.lowest_common_ancestor(G, "B", "D", default=None)  # output 'C'  G.add_edge("E", "A") nx.lowest_common_ancestor(G, "B", "D", default=None)  # output 'C' ```  So, I suspect node order is playing a role here.  removing and adding 'E' changes the order of nodes 'F' and 'E', and changes the edge order as well.  hmmm...
comment
In the Bender paper, there must be a typo or mistake in the statement of either the definition of L_x or the definition of the DAG D.  I think L_x should be the ancestors of x in G.  I believe that still satisfies Lemma 4.3!! Then the same pruning of the all-pairs LCA_T computation brings that from O(n^4) down to O(n^3). So, the complexity is the same even though the number of ancestors stored in the lists is larger.  What do you think?   Does changing line 276 to `           my_ancestors = nx.dag.ancestors(G, v)` make the code work? [Edit: there was a typo in this code -it was the code in the current version, not the suggested replacement.That is now fixed.]  I'm fine with moving to another paper since this one clearly has some troubles. But it has some nice numerical experiments that show it works... so I think there must be a typo of some sort.  I tried looking for the cited code implementing this "simpler" algorithm. If you want to contact an author, I would suggest that person who hosted the code (Pavel Sumazin) who is now at Baylor College of Medicine.  Contactig Bender at SUNY Stony Brook is also good of course. 
comment
Any idea how much the savings in time or memory is? Seems like it would be potentially storing a set of all nodes for each node which would be O(n^2) in memory -- which is on the order of a dense version of the DiGraph itself. But what is the time savings?  Could be substantial I think because you have to do the lookup for each node of each pair -- so you are collecting the ancestors n times.  Since this is a naive algorithm, it seems like collecting a dict of ancestor sets at the beginning might be a good way to go.
comment
Nice speedups!  :)  Do you understand why the number of LCAs is reported as different for the current and naive algorithm?  You are correct that the line number is now `272` and the suggestion is mistakenly written as exactly the same as the current code !oops!.   It should replace `dag` with `G`: ```python my_ancestors = nx.dag.ancestors(G, v) ```   Note that I don't have a proof that this is the correct implementation -- just another way to try to make sense of the Bender paper's description.  The Naive approach is looking better -- more readable and also performant. We could check it's scaling by repeatedly doubling the graph size I suppose.
comment
Check out #5736 which could be a function that solves this issue once we rename it as the primary function. Basically, we are giving up on the Bender paper and going to a more obvious (and actually faster) method.
comment
The graphml reading function creates a MultiGraph or MultiDiGraph. So, each edge has an "edge key".  Printing `G.edges` shows each edge as a 3-tuple with the two nodes for that edge, and an edge key to identify which edge it is. You can remove the edge keys if you don't want multiple edges between nodes by using: ```python newG = nx.Graph(G)               # or nx.DiGraph(G) for directed case ````
comment
I can verify that this is a bug!  And your solution should work well. I suggest that we choose C to be the maximum weight plus 1, though there might be a better choice. This ensures that integer weights stay integers to avoid round-off (another problem with using reciprocal edge weights).   We need to create a PR with the fix and also a test using the example provided above to ensure that it is getting the correct answer. If you'd like to do that, it would be great (and faster) but we will get to it if you don't do that. 
comment
If I understand correctly, eulerian doesn't involve egde weights on the graph itself -- but this algorithm for "eulerizing" (is that even a word?) a graph tries to add as few edges as possible by creating an edge weight. But that edge weight is not related to the edge weights on the original graph.  Adding algorithms that deal with a weighted definition of "eulerian" sound like it is moving into new territory. The concept of eulerian wants all edges, so the total weight will be the same for every eulerian path. But now we are talking about eulerizing the graph with an optimality condition based on edge weights.  Can you explain more what you mean by "to travel all edges at least once, at minimal cost" and how that relates to "eulerian" which is defined to go across each edge exactly once?
comment
That sounds like it should have a different function, unless there is an easy way to find the edges which eulerize and are minimal. Typically the computation needed for weighted problems is more than for the unweighted problems.   Wikipedia calls that problem [the Route inspection problem](https://en.wikipedia.org/wiki/Route_inspection_problem) and sketches an algorithm for it. 
comment
Yes -- @rfulekjames please make a PR to fix the bug! We can help with any first-time questions you might have. Thanks! 
comment
In the details of #1102 it seems that the initial flagging of functions that don't have the cutoff functionality correctly included the `shortest_augmenting_path` (as well as some others) but the `cutoff` functionality was added in a commit a few days later. It looks like an oversight that these functions did not get removed from the list of functions that do not support the cutoff input parameter.    I believe there may be others in `flow_funcs` that also do now support (and long have supported) the cutoff feature. Can someone check those functions and make a PR to correct the list of functions that don't support the cutoff feature?  ~~I'm going to transfer this discussion to an Issue so we make sure to fix this.~~{already is an Issue :}
comment
I agree that the name of that variable needs to be changed. That suggestion looks good to me. 
comment
I don't think a new Issue for checking docs for correct statements about `cutoff` is needed. We can do that in this issue. And if the PR doesn't complete that process, that's OK. We can have multiple PRs if needed to fix this Issue. I guess I see fixing the docs and fixing the code as part of the same process.  Both are correcting this mismatch between which functions are providing a cutoff feature and which are listed as providing a cutoff feature.
comment
Great! You can connect this issue to that PR using the "Development" option on the right (under milestone). That makes a link so this Issue is automagically closed when that pull request is merged.
comment
The random spanning tree algorithm doesn't know about or maintain any information about the 3-partite structure of your network. That said, if you give it a graph G that respects the edge restrictions you are looking for, the spanning tree will be formed with only those edges and thus respect the edge restrictions too.
comment
Perhaps you want a DiGraph instead of a Graph.  Directed graphs make a distinction between (u, v) and (v, u). Undirected graphs treat these as the same edge.
comment
In the details the error for one of the tests said that a server that is needed to install graphviz was not available. I restarted the tests to see if that helped. But it doesn't help the errors with the doc_string examples of the node_link_data.  The Details link "coverage" test failure shows that the "graph" attribute of the node_link_data is returned as an empty list instead of a dict. I think that is the issue. Not sure why that would work on some systems and not others.
comment
While we're at it, the names T1_out and T2_out should be changed toe T1_tilde and T2_tilde in the modules candidates.py, and state.py tests/test_candidates.py and some doc_strings in vf2pp.py and feasibility.py. I think many of these changes are in the directed version PR #5972  so we could also wait until that gets merged.  This PR is about changing a keyword argument and not just changing a variable name, so it's also fine with me to merge this as is, and wait for the variable name changes. :)
comment
It seems like this should be included in the upstream library `flask` rather than trying to get every Python library in PyPI to import a second library in order to fix flask.
comment
I think these results might be due to the recent changes to bridges.py #5397  If you update your version of NetworkX to v2.8 does that get rid of these issues? 
comment
Yes -- the `NodeNotFound` does not work for `has_bridges`, as you demonstrate. :} In fact, `has_bridges` does not even use the `root` argument at all!!  I think it needs to be put into the `nx.bridges(G, root)` call -- it must have been left out. That means we should also create a test for `has_bridges` being called with a non-node.  Thanks very much @dtekinoglu ...  You are uncovering interesting traits of the codebase.
comment
Nicely done!  You have identified the problem and suggested a good solution.  Bridges are defined as edges which increase the number of components of a graph when they are removed. So it seems good to have an option for the user to specify which component they are look at for bridges. Hence while my initial reaction was that we should just remove the `root ` option, some reflection makes me feel that it is worth making available. We just need to lat people know that we are basically only considering the connected component containing `root`.  And your suggested solution looks good to me.  Yes, it will be extra work to find the connected component containing `root` and limiting results to edges in that component. But if we do that extra work only when `root` is provided that's OK. The user is choosing to do a harder operation and it will just take longer.   Here are two ideas for limited the edges we consider. Both start by computing the nodes in the connected component `cc`: - compute the chains using chain_decomposition, and get `chain_edges` as we currently do. Then instead of looking for any edges in `G` that don't match the requirement, we should only look at edges from `G.subgraph(cc)`.  Something like: `for (u, v) in G.subgraph(cc).edges():` - make a new graph containing only the connected component and use the code as is with that graph. Something like `G = G.subgraph(cc).copy()`  There might be other nice ways to do it too.  Is any more obviously better?
comment
Nice!  And a very good example that is small enough to be a good test.  The way to get the edges in G for a set of nodes is to form the "subgraph" of G induced by those nodes. Now, the subgraph is actually more than you need if you only need the edges, In NetworkX that is: `H = G.subgraph(cc_root)`.  I guess your method copies all the edges and then removes the ones we don't want. The subgraph method actually creates a view of the subgraph. So it doesn't use much memory when your graph is big. But if you look at all the edges many times it can be slow because it has to check that both ends of the edge are in the node-set for the subgraph. But since we only iterate over the edges once, it should be no slower -- and possibly faster than copying and then removing nodes.  So I think the subgraph way would remove the copy of H, and replace `H` with the subgrqph indeucded by the connected component.  Another note: `if root:` is a dangerous way to test because if `root` is `0` or `False` or anything else that evaluates as "false" in an `if` statement, your code will ignore `root` just as if it was `None`.  So, the idiom we use to check if `root` has been specified is `if root is not None:` rather than `if root:`. (longer but arguably more clear)  ```python if root:     H = H.subgraph(nx.node_connected_component(H, root)) for u, v in H.edges(): ``` You should see if there is a difference in performance or in readability between the "remove stuff" approach and the "subgraph" approach.   Actually, as I look, I wonder if it would be better to make H a subgraph before doing the `chain_decomposition` in case that saves some work in that function. But in that case, you would be examining the edges of the subgraph more than once, so its best to make a copy of the subgraph (which creates a new graph that matches the subgraph nodes and edges.) ```python if root:     H = H.subgraph(nx.node_connected_component(H, root)).copy() ```  Again, see what you think is more readable and whether there is a large speed difference.   :}  Nice work!
comment
All tests pass!!  Whoo hoo!
comment
My notes say that it is indeed better to save the labels and degrees as dicts (thanks @mriduls for the nudge on that). Also, a **really** helpful utility from NetworkX is the `groups` function which takes a dict of nodes to labels and returns a dict of labels to lists of nodes. There are a few ways to build these dicts but something like: ```python G1_labels = nx.get_node_attributes(G1, label)  # a dict keyed by node to label G2_labels = nx.get_node_attributes(G2, label) nodes_by_G1labels = nx.utils.groups(G1_labels)  # a dict keyed by label to list of nodes with that label nodes_by_G2labels = nx.utils.groups(G2_labels) ```  To avoid computing so much for F_M(l), it's best to store the nodes that have not been ordered yet as a set. ```python V1_not_in_order = set(G1) # Get the labels of every node not in the order yet current_labels = {node: G1_labels[node] for node in V1_not_in_order} ```   Then if you remove the nodes from this set as you put them into the order, it is convenient.  The best way to implement `arg_min` and `arg_max` is to use the `key` feature of `min` and `max`: ```python rare_nodes = min(groups(current_labels).values(), key=len)  # argmin_F_MoL max_node = max(rare_nodes, key=G1.degree)  # argmax_deg ``` The `key` argument of min/max takes a function which determines the value to sort upon. By default it is the object itself (which gives the min/max), but if you specify `key` you get the argmin/argmax from the min/max functions. Think of it as: find the min of this set, using the function `key` to find the value of each element.   So, my code for this portion is: ```python order = [] used_degrees = {node: 0 for node in G1} while V1_not_in_order:     # Get the nodes with the rarest label     # groups() returns a dict keyed by label to the set of nodes with that label     rare_nodes = min(groups(current_labels).values(), key=len)      # Get the node from this list with the highest degree     max_node = max(rare_nodes, key=G1_degree.get)      # Add the root node of this component     order.append(max_node)     V1_not_in_order.discard(max_node)     del current_labels[max_node] ``` Feel free to rename, restructure, etc.  Use as you like.    Just for completeness I'll put the rest of my code. This part is not tested as well so "Beware of Dragons Here!" This fills out Algorithm 2. ```python     # Initializing F_M(l) = label_rarity    the rarity of each label in V2     label_rarity = {a_label: len(nodes) for a_label, nodes in nodes_by_G2labels.items()}      # consider nodes at each depth from max_node     current_nodes = set()     for node, nbr in nx.bfs_edges(self.G1, max_node):         if node not in current_nodes:  # This checks for when we finish one depth of the BFS             current_nodes.add(nbr)             continue         # process current level's nodes         _process_level(order, current_nodes, label_rarity, used_degree)          # initialize next level to indicate that we finished the next depth of the BFS         V1_not_in_order -= current_nodes         current_nodes = {nbr}      # Process the last level     _process_level(order, current_nodes, label_rarity, used_degree)     V1_not_in_order -= current_nodes ```  Where the function `_process_level` is used twice, so I put that code into a function. It includes Algorithm 3 ```python def _process_level(order, current_nodes, label_rarity, used_degree):     """Update order, label_rarity and used_degree"""     while current_nodes:         # Get the nodes with the max used_degree         max_used_deg = -1         for node in current_nodes:             deg = used_degree[node]             if deg >= max_used_deg:  # most common case: deg < max_deg                 if deg > max_used_deg:                     max_used_deg = deg                      max_nodes = [node]                 else:  # deg == max_deg                     max_nodes.append(node)          # Get the max_used_degree node with the rarest label         next_node = min(max_nodes, key=lambda x: label_rarity[G1_labels[x]])         order.append(next_node)          for node in G1.neighbors(next_node):             used_degree[node] += 1          current_nodes.remove(next_node)         label_rarity[G1_labels[next_node]] -= 1 ``` You should probably not trust this code until you check that it makes sense... :}
comment
A more compact way of coding the computation of set T1 might be: ```python T1 = {nbr for node in m for nbr in G1[node] if nbr not in m} ``` But this might get expensive if you had to do it a lot (maybe don't worry about that for now). It might be easier/better to build T1 and T2 as you go adding neighbors to T1 each time you add a node to m, and similarly for T2. But this may be premature optimization. You'd have to remove them when you remove a node from m, or maybe store these in the queue with the mapping.  So, it's not clear that this would be better...   Let's figure that out after we get a working version...
comment
Regarding cutting rules: The paper is solving many different problems, including ISO (are 2 graphs isomorphic), IND (is one graph isomorphic to an induced subgraph of the other graph) and SUB (is one graph monomorphic to a another graph -- that is, can you map nodes from G1 to G2 so that the image of G1 contains a subset of the nodes of G2 and a subset of the edges between those nodes).  Note that the difference between IND and SUB is basically that IND requires all edges from an image node to be mapped to. While in SUB, there might be some edges left out even though they are between nodes that are mapped to.  The VF2/VF2++  methods are the same for these 3 problems **except** that the feasibility rules change.  The feasibility rules involve a cutting rule (this candidate pair will not work so we cut this branch of the possible solutions) and a consistency rule (this candidate pair works for the mapping as defined so far). I prefer to think of these as:  cutting is pair_wont_work while consistency is pair_works_so_far.   Rule 1 is for the ISO problem.  size1 != size2 or size1_tilde != size2_tilde Rule 2 is for the subgraph isomorphism problem: size1 < size2 or size1_tilde < size2_tilde.   Rule 3 is for subgraph monomorphism problem: size1 < size2.   
comment
Perhaps we can use better words to describe T1, T2, T1_tilde and T2_tilde. T1 - seen1 (unmapped neighbors of mapped nodes) T1_tilde1 - unseen1 (not a neighbor of any mapped node) T2 - seen2 (neighbor of nodes mapped to that are not mapped to) T2_tilde - unseen2 (not a neighbor of any node mapped to)
comment
@kpetridis24, you mentioned four lines that could replace the long for-loop that computes the argmax of used_degree with ties. And you asked if it looked right. And I wasn't able to absorb that code over zoom.  I'll make comments here.  TLDR: the bottom line is: Looking back at your code, I think it would work if the first two lines added a check for nbrs in order: ```python         max_conn = max(len([v for v in G1[u] if v in order]) for u in dlevel_nodes)         max_conn_nodes = [u for u in dlevel_nodes if len([v for v in G1[u] if v in order]) == max_conn] ```  Details: The code in the ordering branch is: ```python         max_conn = max(len([v for v in G1[u]]) for u in dlevel_nodes)         max_conn_nodes = [v for v in dlevel_nodes if len([k for k in G1[v]]) == max_conn]          max_deg = max(G1.degree[u] for u in max_conn_nodes)         max_nodes = [n for n in max_conn_nodes if G1.degree[n] == max_deg] ``` The first two lines compute the same thing as the second two lines. That is: `len([v for v in G1[u]])` is `G1.degree[u]`. So, `max_conn` is the same as `max_deg` and then `max_nodes` is the same as `max_conn_nodes`.  I think the trouble is the notation from the paper. The inner function is `\argmax_{Conn_M}(V_d)` and I think your first line computes `\argmax_{len(v for v in G1[u])}(V_d)`.  $Conn_M(u)$ is different from `len(G1[u])` in that $Conn_M(u)$ is `len(G1[u].intersection(m))` by Notation 4.0.1.  So, in words, $Conn_M(u)$ is the number of neighbors of u that are already in the mapping.  Said backwards, it is the degree of u in G1 minus the number of neighbors that are NOT in the ordering.  This is precisely what my dict `used_degree` is supposed to compute. `used_degree`, in words, is the number of neighbors that already appear in the order. (There has **got** to be a better variable name than `used_degree`.)  `used_degree` starts with all nodes's values set to zero. And each time we add a node `u` to the order, we add 1 to the `used_degree` value for each neighbor of `u`.  So, `used_degree` counts how many neighbors of the node are already in the ordering.  That is `Conn_M((u)`.  The approach is to update this value as we add to the order and store than in a dict rather than compute the value from scratch each time through the process.  Sorry that I wasn't able to communicate that well over zoom.  Hopefully this is better.  Finally, to address the design of the argmax computation: The two-line version iterates across the list twice -- once to find the max value and a second time to find the nodes which give that maximal value. The longer code does that in one iteration over the nodes -- keeping track of the "max-so-far" nodes.  The longer code is slightly faster and perhaps easier to read though it is longer. So readability is in the eye of the beholder. It takes only one pass over the nodes rather than two.  See also [this SO answer](https://stackoverflow.com/a/9853789/2177056) for suggestions about how to compute argmax while including ties.
comment
Hi @kpetridis24,  Can you move the commits you are working with into the VF2++ branch and push them up to your repo? That will update the PR and we can make comments/suggestions on the code via github. I can see your commits now by going to your github repos and looking at the commits, but I can't make comments or suggestions on e.g. your master branch.  Some first thoughts -- Do you need to remove new_node1 from T1 and new_node2 from T2? Are there any other nodes you need to remove from them?   Maybe not... :}
comment
To get the pytest system to pick up the tests in your test files the module names should have the word `test` in them. So, I think renaming `tests/VP2++/feasibility.py` to `tests/VP2++/test_feasibility.py` should work to get those tests tested by pytest.  You can check it locally by running `pytest -v tests` from the `isomorphism` directory/Folder. It should list each test function that was run.
comment
I thought the rarity of a label is a term we used to describe how rare a label is.  The rarest label is the one with the fewest number of nodes that have that label.   Algorithm 2 uses notation 4.1.1 where F_M(l) is defined as the number of nodes in V2 with label l that are still available to be mapped to.  I think finding the min of F_M(l) is finding the rarest label.  That is, I think getting the minimum F_M(l) is "finding the rarest label".  I haven't gone back and looked at what `current_labels` contains. The labels of the nodes don't change so I'm not sure what the term "current" refers to. Is this tracking the labels of the nodes that have not been mapped to? 
comment
Got it!  You referred to the least rare nodes as the most common nodes earlier.  Now I understand better... And I think I see a potential problem with how I suggested that you set this up...  The article literally says to find the argmin of this F_M(l) function composed with the label mapping.  That means we need the nodes whose label minimizes the F_M(l) function.  That function is written in 4.1.1 as the length of one set minus the length of another.  The first set is the nodes in V2 with label l, and the second set is the nodes in the ordering-so-far with label l. Now comes the part that I may have mixed up...  I thought that since we must have already checked that the number of nodes with each label is the same for the two graphs, that the first set has the same length as the number of nodes in V1 with label l and the second set is the nodes in V1 that are already in the order with label l. So the difference in size of the sets is the length of the set of nodes in V1 with label l that are not already in the order.  The part I am now questioning is 1) whether this argument is true, and 2) whether `min(nodes_of_current_labels.values(), key=len)` finds the nodes with the label that has the fewest remaining unordered nodes.    In thinking about whether it should be the most rare or the least rare, here's my intuition:  We want to put the hardest nodes to map first in the order. The large degree nodes are going to be tricky to make work in the map. So they should go first.  Similarly the nodes with a rare label will be hard to map because there aren't many options to make it work. So I'm pretty sure we want the most rare labels and the largest degrees to be placed first in the ordering. But think it through based on the article...  That is the ground truth -- my impressions are just impressions. :}   I'll have time to look through the ordering code in the next day or so. hopefully I will learn more about how this works. :}
comment
I've looked at the code some more, and I think the min of the rarity is what we want.  Maybe we should call it something other than rarity. If the number is low the label is rare. But the code agrees with the algorithm in the paper for alg 2.  But I don't think we're doing the max_degree part of Algorithm 3.  We find the max "conn" value (which is called used_degree which we decided before is probably a bad name -- perhaps `conn` is better. But anyway, after we find the max conn nodes, we need to find the max degree nodes within that set of max `conn` nodes. Then we can finish by finding the minimal `f_m_labels` from those `max_degree` nodes.  I think we are currently missing that middle step.  
comment
I think the labels from G2 only needs to be computed once. That is `f_m_labels` can be originally set to be the number of nodes in G2 with each label and then as we add nodes to the ordering we can update f_m_labels to reflect the labels in G1.  This is looking like `f_m_labels` might make good use of the `collections.Counter` object with initialization like: `f_m_labels = Counter(self.G2_labels.values())` but a dict keyed by label to a number of nodes could work too.  The Counter object is easy/fast to initialize and update. After that adjusting it is like a dict: `f_m_labels[new_node] -= 1`.
comment
These changes look good. I have thought a little about testing the node order and with a small simple graph we can brute force the order by hand and then check it. And we can use symmetries to ensure the two orderings just relabel the nodes (this might not be sufficient if there are more than one optimal ordering).  ```python G = nx.path_graph(6) G.nodes[2]["color"] = "blue" G.nodes[3]["color"] = "blue" G.nodes[4]["color"] = "red"  H = nx.relabel_nodes(G, {i: len(G) - i - 1 for i in G}) print(G.nodes.data()) print(G.edges) print(H.nodes.data()) print(H.edges)  G_labels = {n: d.get("color", None) for n, d in G.nodes.data()} H_labels = {n: d.get("color", None) for n, d in H.nodes.data()}  ordG = matching_order(G, H, G_labels, H_labels) print(ordG) ordH = matching_order(H, G, H_labels, G_labels) print(ordH) ``` For the two orderings, the nodes numbers at each for H should be 5-i where i in the value for the ordering of G.  I think 6 nodes with 3 labels (None is a label value along with "red" and "blue") and the path graph structure makes the ordering unique.  Can you work through whether this is the correct ordering for this simple graph?  Does the paper have any simple examples like this?
comment
Lots of tests are failing for easy to fix reasons.    Some of the code isn't meant to be tested. For example, `benchmark.py` fails some testing environments because it calls matplotlib and that isn't installed in some environments.  Using  `pytest.importorskip` (pronounced import-or-skip) can make pytest skip the entire module if matplotlib isn't installed.  The benchmark.py code also fails when matplotlib is installed though because it calls the isomorphic routine with a "node_ordering" input and that makes for too many input variables.  The style test is failing because you haven't run `black` against the code to "fix" the style (it isn't really "fix"ing, it is just standardizing so the code looks the same throughout and we don't have to bother to check spacing issues with this system.)  So, it is not a problem if you run `black` on your code before committing.  There is a pre-commit package to help do that automatically as you try to commit... so you don't have to do it manually.   I'm pretty sure that Mridul and I will not be at the meeting tomorrow because the "Birds of a Feather" talk for Scientific Python is being led by Jarrod and is right across our meeting time slot.  So, Mridul and I will be attending that presentation to help hear what is said and to support the conversation.  Ross should be able to attend. Testing and pre-commit might be topics of interest. I think the node ordering is OK now, but we should get some tests for it. 
comment
Maybe a good goal for the upcoming week is to get the PR CI-tests all passing.
comment
Copied from Issue 22 from @kpetridis24 fork:  > Ordering crashes if G1 and G2 do not have the same labels. Should we just add a precheck to make sure that all labels in G1 are present in G2 or should we tackle this another way?   Good point -- Looking at [how isomorphVF2.py handles this](https://github.com/kpetridis24/networkx/blob/41525dde51040cc00516693fd1c2bc2e6b1defff/networkx/algorithms/isomorphism/isomorphvf2.py#L266) we should probably check some obvious things before ever giving the graphs to the isomorphism solver:  - number of nodes (== for ISO problem, <= for sub_iso and for mono) - sorted degree values agree (== for ISO, might need to count how many of each and <= for sub_iso and mono) - sorted label values agree (== for ISO, again might need to count and <= for sub_iso and mono)  I think the ordering should assume that we have made these checks. The difference between problem types might affect something here -- but I think it doesn't. You/we should think that through at some point. The ordering requires the initial F_M_L be set based on G2 label. We should think about whether those values can become negative as we update F_M_L while creating the ordering. G1 must be <= G2 for the sub_iso and mono problems. So I think we will never get negative values so long as we make sure the counts are <=. But I am not entirely sure -- this is my intuition.  When you say that the ordering crashes, what do you actually see? I assume it "raises" because Python doesn't "crash" very often :) What is the exception that is raised when the labels are not the same? Will these kinds of checks avoid these problems?
comment
I think we should keep in the test example with many solutions. The best result for a case like this one would be to have the algorithm yield all the possible mappings. This would not be difficult... When a mapping is found, instead of returning, we yield the mapping and then pop as if it wasn't the end of the process and move to the next candidate.    And while we are talking about returning vs yielding, I believe we shouldn't need to return True/False in addition to the mapping. If the mapping is yielded, we have shown isomorphism. And if a StopIteration is encountered then we have shown that none exists.  So the main function should yield the mappings and helper functions like `is_isomorphic` can check if there are any isomopphisms using something like: ```python try:     next(nx.isomorphisms(G1, G2))     return True except StopIteration:     return False ```  I'm open to discussion on both these thoughts. There may be better ways to handle it. 
comment
Looks like the style issues include setting an order for the import statements. That is the `isort` part of our linting. It wants builtin libraries to be imported first (alphabetical) then non-builtin, and finally the imports for the project being worked on.  I also realized today that many of the containers that you are inputing and returning from functions don't need to be returned!  The container is updated inside the function and since it is the same container as in the calling function, it updates without reassignment into the calling namespace.  We have used that to great effect in our `shortest_path` set of private functions where we update e.g. the path dictionary only if it is provided as an input, and we don't return it.  Does that allow us to simplify the part of the code where we call the helper functions?
comment
Regarding how to update to make it a generator of mappings instead of returning one mapping:  Right now there are two exit points of the while loop -- both are `break` statements.  The second `break` occurs when `stack` is empty. In this case, we have exhausted all possible solutions.  We are really done.  There are no more mappings possible.  The first break statement occurs when `node_order` is empty. Another way to say that is when the mapping contains all nodes. We test for `node_order` before breaking and then we test for the length of mapping outside the while loop to see what to return.  Notice that that break statement could be written `return True, mapping` without changing the output of the function. We would just return from within the loop instead of after the loop.   So, at the first break statement we have an isomorphism!  We can yield it: `yield mapping`.  That is like returning it, only if the user wants to continue they can -- by iterating on the function (iterating calls the `next` function until nothing is left to be yielded).  When the user causes `next` to be called again, the function starts up exactly where it left off at the yield statement.  Now, let's think about this. We have an isomorphism and that happened when we added `current_node` to the mapping. There is nothing more to add. So we need to remove that node from the mapping and continue along our DFS search of all possible solutions.  That continuation means we need to undo what we did after the mapping was set.  But wait --- lines 70-83 didn't do anything to the mapping -- or to `node_order`.  We can check for an isomorphism **before** updating all the state variables. Then we won't have to undo all the updates. We only have to undo the setting of the last node in the mapping:  `mapping.popitem()`.    If we we avoid updating the state, we don't need to undo that updating. We can simply go to the next candidate for the current node to map to. There will be no "next_node" and no need to find new candidates. We just move to the next candidates. And there is nothing else in the rest of the while loop we would want to execute, so we can just `continue`.  That brings us up the while statement and we continue with our DFS of all possible mappings.  In summary, replace the `if not node_order: break` with code just after the mapping is set:  ```python if len(mapping) == len(G1):     yield mapping     mapping.popitem()     continue ``` Then remove the code after the loop -- the if statements and returns.  The yield statement does the returning for us one output at a time. Also there is also no need to `return False, None` since the absence of a yielded value indicates this result. With those post-loop if statements gone, the second `break` statement takes us to the end of the function.  When a function that `yield`s instead of `return`s reaches the end of its code, python has it raise a `StopIteration` exception to indicate that there are no more results to be yielded.   
comment
Using the namedtuple approach you would move the `if not node_order: break` pair of lines up to the top of the if statement (before even visited gets updated) -- at that point, add code like: ```python if len(mapping) == lan(G1) - 1:     mapping = state.mapping.copy()     mapping[current_node] = candidate     yield mapping ``` And then remove the if/return lines 63-66.
comment
We can/should talk about the other two problems on Thursday.  But I don't think we should start coding them yet.  Before we start coding the other two problems (monomorphism and subgraph-isomorphism) we should use your great work to date -- that ensures that this version is well tested!   Now is the time to rewrite it while ensuring that the tests continue to keep passing. You should go through the code looking for places where you can avoid  1) computing sets from scratch each iteration -- I think you removed those already.   2) popping and pushing the same argument that ends up leaving the list the same (I think node_order is the only place we do that).  We can use an index for the node rather than changing the list. 3) any other potentially costly operations that we have to do over and over?   Then it would be worth learning about and using some profiling tools to see where the code is taking time. This should also help identify parts of the code that are increasing the complexity of our algorithm by mistake.  I like the IPython `%prof` tool, but there may be better ones out there. It would be good to check out the memory usage too -- I haven't used those tools much as they are relatively new. But there is an IPython "magic" command for memory too.   I believe the changes for the subgraph problems are fairly minor. So they will be fairly quick to introduce once the isomorphism code is ready.
comment
1.  Yay for dicts acting like sets.  I never thought of this connection between visited and mapping before either.  It probably isn't much faster and only slightly less memory, but one fewer container to keep track of for readability! :) 2. I am surprised -- and my results are similar... It is actually faster to make the set. Thanks for checking that! 3.  That naming choice makes a lot of sense -- I was thinking of a keyword argument somewhere else that would take a function. And I hadn't thought through where that would be. Passing a string to tell which problem to solve works just fine.
comment
What does the VF2++ paper say should be included in the find_candidates section and in the "consistent" and "cut" section? And do these ever get called without the other?  That is, are they really one operation -- find candidates and check for consistency and impossibility?  I'll try to look at the tests some more.
comment
It sounds like we could combine the "find_candidates" and the "check_feasibility" and maybe save some duplicated checks. The new function would yield the candidates after finding them and checking feasibility. By yielding them we would not need to process all the possible nodes ahead of time. But I haven't worked through the details of what might be involved in combining them -- and whether that would save duplication.  In looking around, I did notice a couple things that could be tweaked: - In find_candidates, we are looking up G1_labels[u] and G2_labels[v] a lot. That helped me recall from the profiling that `__getitem__` was taking more time than I expected. So I had an idea. The labels don't change during this process. So we can compute the `nx.utils.groups` for G2_labels once at the very beginning and get a dict keyed by label to the nodes with that label. Then instead of stepping through the `common_nodes` and looking up the labels, we could use `common_nodes.intersection_update` and pass in the set of nodes that have the same label as u.  As I'm writing this I realize that `degree` is similar. It doesn't change throughout the algorithm.  So we can compute the groups of nodes in G2 by degree to get a dict keyed by degree (a number) to the nodes with that degree. Then a similar `intersection_update` with the nodes whose degree matches the degree of u would avoid a python loop checking labels and degree.  - At one place in `find_candidates` we use `and len([nbr2 for nbr2 in G2[node] if nbr2 in reverse_mapping]) == 0`. That requires checking all the neighbors when an `all()` function would stop at the first node that fails.  Something like: `all(nbr not in reverse_mapping for nbr in G2[node])`  - In the main `isomorph_VF2pp()` function. We use a try/except structure to catch the `StopIteration` exception. But the `try` part is fairly long and might raise a StopIteration that our code would incorrectly treat as the end of the `candidate_nodes`.  Raising a StopIteration is unlikely to happen, but possible, especially when future contributors change the code.  We can avoid the possibility of it happening by putting the except clause immediately after the line with `next(candidate_nodes)`. I have read that this is "good practice" for try/except structures. The `try` part should check only the code where we expect the exception to happen.  What to do with the rest of those lines? You can either 1) put them in an `else` clause, or make the `except` code call `continue` so the code after the try/except is only run if the except doesn't happen.  :)
comment
Let's see, the places we would have to add if statements to avoid the label calculation include: candidates.py:   lines 50 and 60 feasibility.py:   lines 100-105 node_ordering.py:  line 168, 169 and 175 (which can be moved up to right after 169  The most lines are in the precheck and initialize. I think the precheck could just skip any involving labels. The initialize could avoid constructing the dicts -- and maybe make those namedtuple parts be None instead of a dict, so the rest of the code can check `if graph_params.G1_labels is None:` or similar.  I'm not sure what is best here, but I like the idea of avoiding the labels calculations entirely when the labels are not specified.
comment
Yes, it seems like two dicts is a good way to handle looking up degree for directed graphs. One dict for ` in_degree` And one for `out_degree`.
comment
I am looking at the potential of using the new `bfs_levels` function in NetworkX. (see #5879) It was inspired by this PR and so it should simplify `node_ordering.py`.  It yields the layers of the BFS starting at a specified `source_node`.  It looks like it does simplify the `bfs_edges` function.  The code of that function becomes: ```python     for dlevel_nodes in nx.bfs_levels(G1, source_node):         _process_level(             V1_unordered,             G1,             G1_labels,             node_order,             dlevel_nodes,             label_rarity,             used_degree,         ) ``` And there is an impact on `_matching_order` in that lines 54-56 are done inside `_bfs_layers` instead of after the call.  So, they would be removed.  This does put `max_node` first in the ordering instead of last -- but I think it should be first... `max_node` has depth 0 in the BFS tree.  Given the simplicity of that `_bfs_layers` function, we might then consider inlining `_process_level`.  I also noticed in that function that we never use `max_nodes`. So we can skip computing it. The result is: ```python def _bfs_levels(     source_node, G1, G1_labels, V1_unordered, label_rarity, used_degree, node_order ):     """doc_string     """     for dlevel_nodes in nx.bfs_levels(G1, source_node):         max_deg_nodes = []         max_degree = 0         while dlevel_nodes:             # Get the nodes with the max degree of those with the max used_degree             max_used_deg = -1             for node in dlevel_nodes:                 deg = used_degree[node]                 if deg >= max_used_deg:  # most common case: deg < max_deg                     if deg > max_used_deg:                         max_used_deg = deg                         max_degree = G1.degree[node]                         max_deg_nodes = [node]                     else:  # deg == max_used_deg                         deg = G1.degree[node]                         if deg > max_degree:                             max_degree = deg                             max_deg_nodes = [node]                         elif deg == max_degree:                             max_deg_nodes.append(node)              # Get the rarest label node from among the max degree nodes             next_node = min(max_deg_nodes, key=lambda x: label_rarity[G1_labels[x]])             order.append(next_node)              for node in G1.neighbors(next_node):                 used_degree[node] += 1              dlevel_nodes.remove(next_node)             label_rarity[G1_labels[next_node]] -= 1             V1_unordered.discard(next_node) ```  
comment
Looking at the kinda ugly `argmax(argmax(...))` code where ties are accounted for makes my brain hurt. Can we abstract `_rarest_nodes` to be something like a general `_all_argmax(nodes, key_function)`? If we need a `min` instead of `max` we just change the sign of the key function.  I think that could make the bulk of the `_process_level` stuff turn into: ```python         max_used_deg_nodes = _all_argmax(dlevel_nodes, key=lambda x: used_degree[x])         max_deg_nodes = _all_argmax(max_used_deg_nodes, key=lambda x: G1.degree[x])         next_node = min(max_deg_nodes, key=lambda x: label_rarity[G1_labels[x]]) ``` And we would be able to use the same `_all_argmax` in `_matching_order` too in place of `_rarest_nodes`: ```python         rarest = _all_argmax(V1_unordered, key=lambda x: -label_rarity[G1_labels[x]])         max_node = max(rarest, key=G1.degree) ``` Would that help readability?
comment
Right -- using `max` will only include a single node. We should "abstract" `_rarest_nodes` to make it into a new function `_all_argmax`...  And have it use a input function `key` rather than an input `label_rarity`. That way you can reuse that function for any key.  You are correct that it has to return **all** the nodes that have that maximal value.    But also notice that in both examples the final argmax/min only produces a single node so it ***can** use `max()`. The preliminary computations are the ones that need out private `_all_argmax` function.
comment
Yes!  Thanks @kpetridis24 !!
comment
I liked the paragraph that you included in the doc_string. It looks like `black` didnb't like the blank line -- probably because it had a whitespace character in it. If you could add back that paragraph change, but make sure the blank line has no characters in it, I think it should work. 
comment
I we make a new PR to move these helper files into a single module, we should consider putting them all into the `vf2pp.py` module. I don't know how long that will make that file, but we should at least consider it.
comment
I think I will go ahead and merge this since other packages are waiting on it... and the merge is what gives the definitive test :}
comment
Can you plot (in the same workflow) a simple matplotlib figure that isn't networkx? That would indicate whether the problem is matplotlib or not.  I suspect your difficulties are with matplotlib rather than networkx.try something like: `matplotlib.pyplot.plot([1, 2, 3], [4, 5, 4])` inside the same environment that you are using to try to draw the graph. You might need `import matplotlib.pyplot` if those imports aren't somewhere else. And to see the plot you might need `matplotlib.pyplot.show()`.
comment
Thanks @ddelange 
comment
I am +1 for just replacing the existing functions with these naive versions.  I haven't looked closely at the function signatures to see how direct this replacement is. But I think they were created to match, so it shouldn't be a problem. And I'd prefer to get this switched before any release so no deprecations are needed.  It could even be in v2.8.5 depending on the timing.
comment
Thanks for this -- it is currently working on my installation. I am using Scipy 1.8.1.  What version of Scipy are you using.
comment
For the slowdown in `G.copy` the benchmark copies an empty graph. The slowdown goes away if the graph has some edges.  I think this is due to the changed lines: `if hasattr(self, "adj"): del self["adj"]`. And since the new graph being created doesn't have an "adj" attribute in its `__dict__`, we are slowing down graph creation by one `hasattr`.  But I'd like to test that more to make sure. If we remove the property for adj, we will remove these issues, and speed up just as the cached property was supposed to. But it will slow down graph creation in favor of faster edge lookup/reporting. I'll try to test the speed differences between those.  For the `graph_create` test, the DiGraph was slowed down, but not the other classes. I don't see how that could be happening. Did something strange happen with just the DiGraph test?  It is true that DiGraph has to check 3 attributes (`adj`, `pred`, `succ`) but so does `MultiDiGraph`...   Similarly for `to_directed` and `to_undirected`.  Some of the graph classes show slowdown and others don't. Are these results repeatable?  Finally, a minor nit, but could the horizontal axis of these graphs be changed from date/time of the commit? Replacing it with a commit index would make the data equally spaced horizontally and perhaps make the slopes a better indicator of impact of each commit. 
comment
Would it be possible to rerun the benchmarks on PR #5836 ? I'm not sure how involved that is. I ran some timing tests locally and it seemed to fix the problem for the methods I tested.  If that PR does resolve the speed regressions we could leave the cached_property in place.
comment
Is the performance regression in the core classes still there?
comment
This looks nice!  Can you add an example (perhaps with a couple lines adding to a previous example) where you use one Graph to set the attributes of another?  That's something that may not be obvious to users who haven't delved into the graph classes.  ```python H=nx.path_graph([1, 2, 3]) nx.set_edge_attributes(H, G.edges) ```
comment
If you use `G.edges` as the second argument, e.g. `nx.set_edge_attributes(H, G.edges)` then the attributes of the edges in H will be updated to the attributes of the edges in G.  That is because the G.edges object acts like a dict keyed by the edge-tuple with value being the dict of edge attributes.  `G.edges` is a view -- which is essentially a read-only dict. So it acts like a dict and thus works with as a `set_edge_attributes` parameter. Also, if G includes edges that are not in H, they are safely ignored.  Similarly, `nx.set_node_attributes(H, G.nodes)` sets the node attributes of H to match those of G.  This is perhaps not fully appreciated or understood. But an example showing this might help users understand.  
comment
You do not need to add a changelog to `doc/release/release_dev.rst`.  But you are welcome to! :}
comment
The viz problem was solved by a merged PR in the last 12 hours or so. Can you merge with main again? Or you can just leave it and we should be able to review this PR without concern for that failed test.
comment
I also like the `method` kwarg approach.   @GuyAglionby do you have a sense for whether and why we should include multiple methods? For example, why not just implement the Wu method?  The default should to be Kou for backward compatibility -- at least for now.  And perhaps the default should be `None` with some code to give a warning that the default will be changing in version 3.? and if they want to avoid that change, they should specify the method explicitly. Along with the warning the method could get set to 'Kou" for now. and then switch it later.  Thanks very much for this!!
comment
I was thinking more simple than the circle and hub layout obtained from an empty graph (which is nice for testing repulsion but not for attraction).  I was thinking more like 3 nodes, 1 edge.   Or even 2 nodes, 1 edge. I'm guessing one could construct a steady state of the equations in this kind of special case by hand. Then using those positions as initial conditions, you could test that the results are indeed a solution (though not the only one).   But I'm leaning toward worrying about testing of all the spring-based layouts in another PR. This introduces new layout methods to the same level of testing that we are providing for the other layouts.   It looks like the only test that is failing now is the `isort` linter which wants `numpy` to be imported after `warnings` (because it is not a builtin library) and with a blank line between the builtins and non-builtins as well as after the non-builtins. This is all based on the "Details" link of the failing style test.
comment
This looks like a nice addition. It allows the user to take advantage of the generator nature of this function to mix and match with other coloring techniques.  This is a very silly request, but this function has about 5 places where the term `vertex/vertices/vert___` arise and mix with the term `node/nodes`.  Would you be willing to change those places to `node/nodes` to make it consistent with the rest? If you don't have time, I can do it later.  Thanks for getting the formatting issues solved. :}.  
comment
Looks like in [8669c4636ce](https://github.com/matplotlib/matplotlib/commit/8669c4636ce3b6ac6f4905c365ab41685186da56) the AxesStack class has been rewritten and is no longer callable. The [AxesStack methods gca and new_axes](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.SubFigure.gca) docs give a hint that to check whether an axes exists, check whether figure.axes is empty.  That seems to work -- at least in our tests (which do test all branches of the if logic here).
comment
Yes, we need to change line 45 to `plt.colorbar(pc, ax=ax)` (which is what the deprecation notice warns us will be needed.) I'm guessing that the deprecation occurs for matplotlib 3.6.  So, again it is not a bug in matplotlib. But now this means that the example is broken in NetworkX 2.8.6 when using Matplotlib 3.6.   rats...  I'll open a PR with the change.
comment
The deprecation warning shows when using a prior version of matplotlib. And the warning says: ``` Starting from Matplotlib 3.6, colorbar() will steal space from the mappable's axes, rather than from the current axes, to place the colorbar.  To silence this warning, explicitly pass the 'ax' argument to colorbar(). ``` So, I interpret that as:  NetworkX was working with the example nd we didn't see or ignored the warning. But when we try to use Matplotlib 3.6, colorbar is stealing space from the mappable's axes. So the example breaks with mpl 3.6.  Does this make sense?
comment
I replaced the placeholder text with the sentence from our Readme: `NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.`  And I added (up to 8... so I did 8) member profiles.  There is also an option to have "Featured Work: Select public repositories to showcase on your GitHub Sponsors profile".   And we can opt-in to have our page be featured on the Github-sponsors page.  @juanis2112 the main text is Github's markdown with a 5000 character limit. I guess you can paste your work here and then when you are ready I'll paste it into the "Update profile" page. 
comment
OK... now I'm embarrassed.  I really need to get my discord notifications set up in a useful way for me... Thanks for getting that all together!!  I've copied it to the github sponsors page, and I selected 8 members of the "core developers" team for networkx. We have 13 members but only 8 can show on the github sponsors page.  Let me know if there is anything I should change.
comment
It looks one of my suggestions had white space at the end of two lines. So black is removing it.  You can run black locally on `dag.py` and it should fix the program. If you feel more comfortable knowing what it will change before it changes it, you can run  `black --diff dag.py` and it shows you the changes it will make.  To automatically run black and isort (our import sorting style checker) before every commit you make, you can use pre-commit.  To install on a `pip` installing environment: ``` pip install pre-commit ``` To install in a conda environment: ``` conda install -c conda-forge pre-commit ``` Then once it is installed, within your environment, install it in the local repository you are working on.  cd to somewhere in the repo, then ```pre-commit install```.  If that is too much trouble and too automatic, you can run `black` locally, installing with pip or conda and then  `black --diff dag.py` followed by `black dag.py`.  
comment
Thinking about it more, I think it would be easiest just to put it into this PR. They are so related, it doesn't really require a separate issue. :}
comment
I think it should return whatever it returned before for backward compatibility.
comment
I guess that also means this should be added to the  `networkx/tests/test_all_random_functions.py` with a line like: ```python t(nx.random_triad, G, seed=seed) ``` 
comment
I like this idea of making the arguments keyword only. We have talked about moving to that treatment of input arguments where we make a distinction between required arguments (usually positional) and optional arguments (usually keyword only). It helps avoid errors where the arguments are provided in the wrong order. I think we decided that we would move toward keyword-only for new functions (after we no longer supported older versions of Python that didn't allow the keyword only syntax).  This seems like a good place to start that process.  I'm also fine with shifting the deprecation notification from v3.1 to v3.2 and that could be done in this PR or a separate PR.  Thanks @rossbar 
comment
I went through this code and it looks like we expect `mapping` to provide: `__getitem__, keys, items, values, get, __contains__`.  So the only methods from `abc.Mapping` we are not using are `__iter__, __eq__, __ne__`.    I have refrained from using the abc classes when we don't use most of the methods required. This case seems like a pretty good match to `Mapping`.   I worry a little for people who have homemade mappings but have not connected them to the abc.Mapping class. That's a potential backward incompatibility issue.    Overall this looks good to me! :} 
comment
That seems good, too.  Technically it does change the current logic if a previous user inputs a class that is both a Mapping and a Callable.  They get the callable behavior instead of the mapping behavior.  Which do we want to "win" when an object can do both?  More rabbit-hole digging: A homemade callable object can register with `Callable` to appear as a subclass of Callable using `Callable.register(obj)`.  I guess we could add to the doc_string that a user can register an object as Callable that way.  But as we convert the `hasattr` logic to `isinstance` throughout the library we will likely need to add such language in many places.  I'm fine with this change. The change in behavior for objects that provide both calls and mappings should probably be noted in the release notes.
comment
Thank you very much for reporting this!!  This is a very strange an ugly bug.  It stems from the function yielding a list-of-sets without copying those sets. When the next partition is requested, the sets are updated while computing the next partition. So the originally yielded partition gets changed while computing the next partition. This doesn't show up if you process each partition before iterating to the next partition. Here's a test that fails due to this bug.  ```python def test_partition_iterator():     G = nx.path_graph(15)     parts_iter = nx.community.louvain_partitions(G, seed=42)     first_part = next(parts_iter)     first_copy = [s.copy() for s in first_part]      # gh-5901 reports sets changing after next partition is yielded     assert first_copy[0] == first_part[0]     second_part = next(parts_iter)     assert first_copy[0] == first_part[0] ```
comment
The [deprecation policy](https://networkx.org/documentation/stable/developer/deprecations.html) (I found it! yay) says  > Usually, our policy is to put in place a deprecation cycle over two minor releases (e.g., if a deprecation warning appears in 2.3, then the functionality should be removed in 2.5). For major releases we usually require that all deprecations have at least a 1-release deprecation cycle (e.g., if 3.0 occurs after 2.5, then all removed functionality in 3.0 should be deprecated in 2.5).  >Note that these 1- and 2-release deprecation cycles for major and minor releases is not a strict rule and in some cases, the developers can agree on a different procedure upon justification (like when we can’t detect the change, or it involves moving or deleting an entire function for example).  In this case, we would deprecate this in 2.8.6...  so it is not deprecated in 2.8.  So I think it really should be removed in v3.1.  We could argue for a special case, but this is easier and less dramatic for users.
comment
Sorry @rossbar for pulling the trigger before you had a chance to look at this!    I'm fine with shifting the deprecation date to the 3.2 release. The rules for deprecation are not clear to me, especially once we started making the patch releases more substantial, (which I suspect is being done to make sure 2.8 has all the recent bug fixes so that people who don't want to shift to 3.0 can stick with 2.8).  But, just to make sure I understand correctly: the stuff we deprecated before v2.8 could be removed in v3.0 because that is a major release. The stuff we deprecate now -- before v3.0 and patched into 2.8.6 -- would be removed in v3.2 because we need 2 minor releases or one major release between announcement and removal.
comment
Yes, IMO these functions should be changed to have 5 keyword arguments instead of a single dict containing the five values.
comment
I agree with you that this second example includes a spurious dict that doesn’t actually do anything. I think is should be removed.  In fact, it would be more clear (in my opinion) to change these examples so they show the results instead of just capturing the results in a variable. Also, using the top level `nx namespace would be preferred: `nx.node_link_data` Something like: ``` >>> G = nx.Graph([(“A”, “B”)]) >>> data = nx.node_link_data(G) >>> data {'directed': False, 'multigraph': False, 'graph': {}, 'nodes': [{'id': 'A'}, {'id': 'B'}], 'links': [{'source': 'A', 'target': 'B'}]}  >>> H = nx.Graph([(0, 1)]) >>> data2 = nx.node_link_data(H, link=‘edges’, source=‘from', target='to') >>> data2 {'directed': True, 'multigraph': False, 'graph': {}, 'nodes': [{'id': 0}, {'id': 1}], 'edges': [{'from': 1, 'to': 0}]}  To serialize with json  >>> import json >>> json.dumps(data) ‘ {'directed': False, 'multigraph': False, 'graph': {}, 'nodes': [{'id': 'A'}, {'id': 'B'}], 'links': [{'source': 'A', 'target': 'B'}]}’  >>> json.dumps(data2) ‘ {'directed': True, 'multigraph': False, 'graph': {}, 'nodes': [{'id': 0}, {'id': 1}], 'edges': [{'from': 1, 'to': 0}]}’  ``` Does this look better to you?  Please improve these examples as you think makes them most helpful.
comment
I was not able to reproduce using:  ```python G = nx.path_graph(6) node_size = np.array([n**2 for n in G]) pos=nx.spring_layout(G)  nx.draw(G, pos=pos, with_labels=False, node_size=node_size) ``` Can you see what I'm not doing?
comment
What's your use case for this?  I know what you are saying we should do and why.  But what are you trying to do that makes this helpful for you?
comment
I'm quite sure we don't want to try to anticipate all the possible mixin init functions that people could possibly want to use. We would need to give up our current ability to tell the user that they spelled `incoming_graph_data` incorrectly. It'd be unlikely they we'd anticipate every case anyway, and it would encourage inheritance in precisely the cases (orthogonal base classes) where using composition instead of inheritance is generally considered better anyway.  I checked other packages in the Scientific Python ecosystem and none of them use `super().__init__` on any classes that inherit from `object`. Many of the packages have multiple inheritance and some even have diamond inheritance. But they only use `super` in objects that subclass something other than object.
comment
Mixins should be changing the methods of the class, not the data.  The example Mixin doesn't have an `__init__` for this reason. And there is no problem in this example with Graph not calling `super().__init__`. In this example, problems would arise if Graph had a method called `_contents`. Then Graph would interfere in your attempts to call `SerializationMixin._contents` using `super()._contents`.  But we're not going to put `super().method()` in every `method` in Graph.  I'm not sure what programming language background you are coming from, but this is not a good way to use Python to provide serialization classes. A cleaner interface in Python for `SerializableGraph` would contain the graph as an attribute -- not subclass the Graph class.   We are spending way too much time on these debates. The idea that any of us know "the right way" to organize the classes/methods/types is counter-productive and disrupts the community. Ultimately, it is the community that decides which way this package is going to choose to organize its classes/method/types. There is no "right way".  Similarly, we are looking at and thinking about typing, just as we have looked at and thought about other ideas like `super()`. Please try to participate in the discussion in a way that respects the work that has gone into this project and the ideas and opinions of others in this community.
comment
Thanks for your comments here @NeilGirdhar. I think I responded when I was tired after a long week. Sorry about that. I apologize. I usually try to not do that.  I don't feel we are protecting code from change. We change lots of things. It is the style of the discussion that I was responding to -- not the particular proposal. It seemed like we were heading into a point/counter-point debate rather than a discussion. My desire is to keep the community enthusiastically showing each other cool ways to get things done, while trying to avoid the negative side effects of those cool ways (which occur with every cool way to get things done). The ultimate goal is to have fun and create useful beauty. 
comment
This is a good point.  We want to identify if something has attribute `__getitem__` (which could be a dict/list/etc) or if they have attribute `__call__` (any function, class, many other objects). But what do we choose to do when they can do both?  In terms of abc classes they are a Callable Sequence or Mapping.  Our code prioritizes mappings specified via lookup over callables. So if something can do both, we use it for lookup rather than calling.  Your suggestion would prioritize calling over lookup.  I'm not sure I can argue for which is better. lookup-first preserves backward compatibility.  But your example shows that it is even more complicated than the story above because classes have method-attributes even though they are not bound methods. In your example `str.__getitem__` exists, but is a class method requiring an instance before lookup is possible. It is not bound to an instance. This suggests to me that our check for `__getitem__` could be improved. The idea is to check for an attribute `__getitem__` first, but to also make sure it is a bound method. Bound methods are indicated by the `__self__` attribute.  So we could change the check for `__getitem__` to a double-hasattr check: ```python # check for bound method __getitem__ if not (hasattr(mapping, "__getitem__") and hasattr(mapping.__getitem__, "__self__")) ```
comment
Yes... the file is at: https://raw.githubusercontent.com/networkx/networkx/master/examples/drawing/knuth_miles.txt.gz  A link should be added to the docstring of the example. Thanks!!
comment
Thank you!  This is indeed a bug. We will need to make a similar fix to the ones we made for G.adj/G.succ/G.pred That is, add a data descriptor for G._node which resets the cache when G._node is set.
comment
The workaround until a fix is ready is: ``` subgraph = G.subgraph(subG_nodes) del subgraph.nodes ```
comment
These are all in the module: `networkx/algorithms/lowest_common_ancestors.py`
comment
Thanks for separating these out from the types PR.  It is **much** easier to review these separately, where the idea for the change is clear.  
comment
I don't think we should move the attribute assignment parts of `__init__` out of the `__init__` function. It's only a few lines of very readable code so repeating it actually makes it much easier to read -- and maintain than a separate function. I know that repeating code is usually a Bad Thing, but in this case (especially now that we are removing the factory functions from the instances) it is actually more readable and easier to maintain.
comment
I'm not sure that there is a preferred order to the subset layers. Why should they be sorted by subset key?  So long as they are all on the same layer, it is a valid multi-partitite layout.  I'm -1 on this suggestion because sorting the layers by subset key makes it so that the user can't control the order of the layers. Perhaps this fix could be added as an example in the doc_string of how to control the order by sorting it(?).
comment
> This used to be the default behavior  Just to be clear, sorting by subset was never the default behavior. It used to use the (always numeric) subset key as the x-coordinate of the group. Now the x-coord is an integer representing the order of the subsets in the dict. Maybe "order" isn't the issue so much as setting what the x-coords ought to be.  The fix to allow non-numeric subset keys lost us the ability to set the x-coords of the subsets. Is there a nice way to allow the subset keys to indicate either the x-coord **or** a meaningless perhaps non-numerical object?
comment
Here's a suggested fix:  - create an optional keyword argument that is a dict from subset key to a value that is the x-coord of the resulting position for nodes in that subset.   - If this is provided, use it to obtain x-coords. If not, check that the subset keys are `numbers.Number` and if so, use that as the x-coord. If not, then use the current method of selecting the x-coord based on position within the dict of subset keys.
comment
With #5705 merged I'm going to close this issue. We can reopen this or open a new issue if needed.
comment
You can't get from the source to the target by going through those two nodes **and not revisiting node 40120e**.  Revisiting a node means the path is no longer simple (it contains a cycle -- thus crossing itself).  That's why no paths through those two nodes are generated.
comment
That sounds specialized enough that a general function probably won't work. I suggest that you take the code for one of the path finding routines and morph it to handle the ideas you need.  One function to work from is [single_source_shortest_path](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.unweighted.single_source_shortest_path.html)  click on the "source" link near the top and scroll down to the helper function below the main function. It is not too long and a straightforward set of loops that traverse the graph. You will need to change it to keep going beyond just finding the shortest path. And still making sure that it doesn't keep going too long.    Another function you could work from is the [all_simple_paths](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.simple_paths.all_simple_paths.html) itself. Again, click on the "source" link and then scroll down to the helper function below the main function.  That one is a little more complicated logic based on whether there are ties in length so it keeps track of all the possibilities.
comment
The tricky part of these formulas is the symbol `m` which represents the number of edges. That value `m` is in some sense twice as big for the directed case as for the (at least one way of making an) analogous undirected  case. The safest way to check the Delta Q formulas is to derive it from the Q formula.   Are you getting a result from an example that you think is wrong? Or are you simply comparing the formulas in the code wit those in the paper.  If the formulas differ we should make sure that the definitions of the symbols are the same. In other words we should check the formulas. :}     The the code is correct but the formulas look different than a comment in the code should be added to help the reader understand those differences. 
comment
Try starting with a small graph and increasing the size to see how the time varies. It might be that you graph is so big it will just take a long time.
comment
Is there a reason you are using pydot instead of pygraphviz?
comment
Yes, it looks like the faster method is to construct the path without the last node.   Something like: ```python         path = []         root = self.parents[object]         while root != object:             path.append(object)             object = root             root = self.parents[object]         # Note that root is not added to the path ```
comment
Thank you for this PR.   The Corona product is a helpful feature to add. And this implementation looks good to me.  Can you add `corona_product` to the list of functions to link to in the documentation? `doc/reference/algorithms/operators.rst`  That will link this doc_string to the reference section of the docs.  Are there performance issues we should think about?  I think CPU usage is not an issue. What about memory?  If you have a really big H (or G) will there be a bottleneck in new_edges getting huge? If so, you could loop over `list(GH)` and add the nodes/edges to GH as you go.  Anything else like that?  Thanks!
comment
oops...  my suggestion had quotes around the output string when it shouldn't have.  I think taking out those quotes gets rid of the test failures.
comment
Take the single quotes out of line 508 where the output of the example appears as `'Graph with 12 nodes and 16 edges'` when it should be `Graph with 12 nodes and 16 edges`
comment
This looks like a nice improvement to me!  Could you add some words to the paragraph at line 16 (it is inside the doc_string)? That line makes it clear that we only return one path when there are more than one. That seems like a good place to describe how we break ties. Your description above is quite good so use something like that.  Thanks!
comment
Can you give an example of something that you would change?  Or even just  a pointer to what you mean by "consistent with heuristic search conventions". What conventions? By whose interpretation?  We don't have a lot of time to review changes to variable names, and prefer PRs like this one where it is pretty easy to see what is happening.  OTOH if the changes are going to make the code significantly more readable and easier to figure out what it is actually doing, then that would definitely be worthwhile. The problem is that there is a lot of grey area between those extremes. :}
comment
Yes, this is a known behavior in graphviz (as stated in the link you provide). Is there anything we should know or do about this? Or is this comment to help people who come to this closed issue looking for a reason why it isn't deterministic?
comment
I haven't looked at this in detail yet, but it seems to be assuming that nodes can be ordered, which the NetworkX package doesn't generally require. Does this need to be a minimum, or is the minimum a way to get a consistent node from a collection? I'd like to finesse the minimum using a different process if possible.  Isomorphism shouldn't depend on orderable nodes.
comment
Yes, we're interested in coordinating with BGL -- though the timing is not going to allow (from me) much more than talking about it until the new year.  But if we can talk about what needs to be done maybe we can open issues and others will pitch in.  What old tickets are you referring to?  Any others interested in chiming in with how we could make BGL and NetworkX work together?
comment
This look good.  To avoid constructing these node sets `old_labels` and `new_labels` can you change the start of this function to: ``` if len(mapping.keys() & mapping.values()) > 0: ``` It was originally written before we could take advantage of the set methods of `keys` and `values`. Thanks!
comment
> A bit confused why this doc test is failing:  It looks like the original edge had key 0.  Then the new edges got keys 1, 2, 3 and the original edge is then removed. I haven't worked through all the logic of the new code for this case, but that's what is happening.
comment
Yes -- and adding tests will make sure we don't change that partial behavior unintentionally. Would it make sense to have a test for a case that the order is not preserved so we know if that changes? I'm not sure how easy that would be.
comment
Thanks!
comment
This looks like it is going to be very hard to review.  You've got Type Hinting which we don't use in the package itself. You are subclassing the base classes with multiple subclasses or subclasses when all it seems like you are doing is adding data structures and methods to to find the source and target of edges. My first quick look suggests that all of this can be handled using NetworkX's paradigm of edges as 2-tuples `(source, target)`.  No complex class structure is needed. The type hinting suggests that nodes and edges are going to be strings, which makes it hard to incorporate into the NetworkX environment where nodes are any non-None hashable and edges are 2-tuples (or 3-tuples for MultiEdges). It would also be helpful if you didn't put what are essentially doc_strings in the middle of code as if they are comments. Put the docstrings just after the viewable function or class signature so they get processed by our autodoc sphinx tools into the reference manual for the package. docstrings on `__init__` are ok... but will never get seen, so you should put most everything into the class doc_string, especially the parameters that the user should provide. It might also help to have a general overview of the whole module in the docstring at the top of the module so someone reading the code (or looking at this module to try to figure out whether it is what they want) can get a handle on the almost 1700 lines of code.  There seems to be a lot of unnecessary code in this module. Subclasses that only provide an `__init__` method that only calls the `__init__` from the base class aren't needed. Also, use Python and NetworkX data structures where possible -- like 2-tuples for edges.  I guess a summary of all these comments is a request that you go through the code again with the goal of refactoring it into a minimalist, sleak set of functions and data structures.   The code clearly is a lot of work and it passes the tests and provides a nice feature. Thank you for this!
comment
Yes -- this is a result of multiple components and too many iterations. If you reduce the number of iterations to 50, it looks much better.  Basically what is happening is that the components are drifting apart as the iterations increase. When the distance apart swamps the distance between node in the component, then all nodes in the component appear to be at the same position.  I think the short term fix is to reduce the number of iterations. The longer term fix involves changing the spring layout to somehow provide spring-like action between components.  Might be best to find a published algorithm that handles this before diving into it.
comment
Yes -- it's probably better to draw the components separately. You might take a look at [nxviz](https://github.com/ericmjl/nxviz) to see if it has better treatment for multiple components. It is a visualization package for NetworkX.
comment
In the documentation for ```network_simplex``` (about line 95 in the file) there is a Notes section:      Notes     -----     This algorithm is not guaranteed to work if edge weights or demands     are floating point numbers (overflows and roundoff errors can     cause problems). As a workaround you can use integer numbers by     multiplying the relevant edge attributes by a convenient     constant factor (eg 100).  This is a better fix than using some sort of tolerance for rounding. Any floating point computational problem will have round-off errors introduced. Flow feasibility does not tolerate round-off. Even rounding at 10 digits will cause inaccurate results for large graphs. So, don't use floating point values.
comment
```all_shortest_paths``` only reports ties. You are looking for non-shortest paths with short length. That is, the 3 shortest paths... I'm not sure that ```all_shortest_paths``` would take much more time. Have you tried it?  You could take the shortest_path algorithm (in ```_dijkdtra_multisource``` within ```networkx/algorithms/shortest_paths/weighted.py```) and try to change the stopping criteria to allow the 3 shortest paths.  It is not a feature we have tried to include up to now.  I don't understand your second question. It sounds like you want to minimize both time and walking distance. That's a 2-D optimization. So, the naive approaches I think of require you to give the relevant tradeoff between walking distance and time.  If you find a way to get a few short paths and want to measure the walking distance for each and pick the one with smallest walking distance, maybe that could work.
comment
I think you can simply add the 16 nodes to all the graphs.  They won't have any edges, but the nodes will be there and will appear in pandas if I understand correctly.
comment
I agree that this should be fixed in `sparse.kron`.  It is legal and desirable in some strange cases to have a zero weight edge.  If you need a workaround, you can remove the edges after-the-fact easily with:  ```python G.remove_edges_from((u, v) for u, v, wt in G.edges.data(weight) if wt == 0) ```
comment
The Watts-Strogatz model rewires so the total number of edges remains the same. That means you are removing an edge whenever you add one.  The `newman_watts_strogatz_graph` adds edges and might be what you are intending. Check out [these documents.](https://networkx.org/documentation/stable/reference/generators.html#module-networkx.generators.random_graphs)
comment
You are using `iso.numerical_multiedge_match` which matches multiedges.  But your graphs are not multigraphs (they don't use multiedges).  So you should either switch to `iso.numerical_edge_match` or switch to `nx.MultiGraph`.
comment
From [the Python docs for the `xml` library](https://docs.python.org/3/library/xml.html): So long as you use expat 2.4.1 or later, the `xml` standard library is Safe except for the `xmlrpc` module which we do not use. But the version of expt shipped with Python is 2.2.8 for Python 3.8 AFAICT  What would allow your Enterprise restrictions to allow NetworkX?  - How are they allowing you to use Python when it ships with the `xml` library? - Do the Enterprise restriction people understand that so long as you don't use graphml or gexf formats, networkx doesn't use the `xml` library?   - If they worry about using a package like networkx that **might** use `xml`, then they shouldn't allow you to use Python at all. - Would a reduced feature version that removed graphml and gexf features (the only ones that use `xml`) work for you?  Honestly, you should be putting pressure first on your Enterprise restrictions, and second on Python itself via `xml` and `expat`. We are so downstream from `expat` that we have no expertise in these matters. The best approach I believe is for your Enterprise to require all programmers to use a sufficiently recent Python version that it ships with `expat 2.4.1` and thus the xml known vulnerabilities are not relevant.
comment
I think having duplicate code as a private function and as an example is OK. The codes can diverge over time -- but it probably is a good thing to let them diverge if needed to serve their different goals.
comment
This behavior was changed in in `remove_edge` back in [#284](https://github.com/networkx/networkx/issues/284) and it looks like the docs in `remove_edges_from` never got updated to reflect this change.  It is not clear that the intent was to change `remove_edges_from`.... just the `remove_edge` method.  A workaround way to remove all edges from a MultiDiGraph between nodes `u` and `v` is: ```python DiGraph.remove_edge(self, u, v) ```  The longer term fix is to 1) change the documentation to match what remove_edges_from actually does for multigraphs. 2) decide whether to change the current behavior (backward incompatible), or to create a new method (polluting the namespace). Either way, make a way to remove all edges between two nodes. 3) Implement this in NetworkX.  To change the current function's behavior, we would need a special case to handle removing edges from ebunch that are 2-tuples. In that case call `super`'s `remove_edge` method where the super is sufficiently far up that it is *not* MultiGraph anymore.  I always have to think hard about `super`, and I haven't, but I think this should work: ```python super(MultiGraph, self).remove_edge(u, v) ```  So, we need a PR for 1) and a separate PR for 3).  Maybe we can decide 2) in the discussion here.
comment
To address 2) above: I think I favor the "same namespace" approach hinted at in 3) above:     The way to remove all the edges between u and v in a multi(di)graph is:  `G.remove_edges_from([(u, v)])`  The current behavior for this idiom is to remove a single arbitrary edge between u and v. This makes `remove_edges_from` behave as the doc_string says it should. So, this behavior is backward incompatible.  So we'd need to make a Big Fuss in the release notes.  But it is unlikely to affect many users or we would have heard about the documentation error at some point in the last 12 years   :}   If you want to remove an arbitrary multiedge between u and v, you can use `G.remove_edge(u, v)`. So the old behavior is not hard to obtain.
comment
This is definitely a defect!!   The returned result is *not* a minimum edge cover... A quick [look at the code](https://github.com/networkx/networkx/blob/bf69de52440a68202f9251e6a402a91fb14eaefe/networkx/algorithms/covering.py#L84) makes me suspect that the trouble is that we add both directions of an edge when building the min_cover.  Why??  We should only add one direction of the edge -- this is not implemented for directed graphs.  It looks like the tests only check for graphs that can just use a maximal_matching result...  Perhaps this shows in the CodeCov report... but not sure.   It should be easy to fix -- and it definitely needs tests. 
comment
As suspected, the problem is not due to un-covered code by the tests, but rather that the code was only tested for the special case when a bipartite version of `matching_algorithm` was used. The deeper issue is that the API for bipartite 'matching_algorithm's returns a dict of "mates", while the non-bipartite functions return a set of edge 2-tuples.  When the non-bipartite matching API switched to sets (#2774), the covering code was changed to handle that case, but the tests didn't reveal the resulting discrepancy between returning both 2-tuples `(u, v)` and `(v, u)` for an edge in the case of bipartite `matching_algorithm`s, and non-bipartite `matching_algorithm`s. So over the last 4 years this function returned incorrect results for the case of non-partite matching algorithms (which is the default) and some nodes not being covered by the max_weight_matching algorithm. Thanks @rossbar for spotting this!!  I think the better fix would be to change the output API for bipartite matching to be a set of 2-tuples with one 2-tuple representing each edge.  This would make it the same output style as for non-bipartite matching functions. It is currently returning dicts keyed by node to its "mates".  Note that there is a function to convert from dict representations to sets: `matching_dict_to_set` which is unpublished, but not named as private in `networkx/algorithms/matching.py`.  Changing that API will certainly require some notice/deprecation.  I've got a PR #5549 that fixes this defect while maintaining the current discrepancy between `matching` APIs for bipartite.  That means that `min_edge_covering` also still returns two types of sets -- if a bipartite matching algorithm is used, it returns a set with both 2-tuples for each edge; while if a non-bipartite matching algorithm is used, it returns a set with one 2-tuple for each edge.  The PR also updates the documentation which e.g. currently says that this function only works for bipartite graphs.  Remaining questions:  [ ]  Change the `min_edge_cover` function to always return a set of one 2-tuple for each edge?  [ ]  Change the bipartite matching algorithms to always return a set of edges instead of a dict of mates?    [ ]  Possibly change the bipartite matching algorithms to allow return value of either a set of edges or a dict of mates?
comment
Can you give us the error that you've got?
comment
It looks like you have stored attributes with values that are lists. That won't work with the gexf writer. Is there a reason you are using GEXF? Perhaps there is a better format for storing your data? Or, perhaps you could store the data attributes as dicts or numpy arrays?
comment
Even if you do subclass the graph class, a subclass should support the existing arguments to the base class. It can add new arguments. But if it removes possible arguments then you will need to overwrite any methods like subgraph that use these arguments.  I like @MridulS suggested approach.  But if you really need the subclass approach, then try ```python class Child(nx.Graph):     def __init__(self, data):         super().__init__(self, data)         self.data = data ``` Or if you are envisioning a string as the data argument, use a different keyword argument: ```python class Child(nx.Graph):     def __init__(self, data, newdata):         super().__init__(self, data)         self.data = newdata ``` 
comment
The thread of imports makes it look like Python's `inspect` library is importing `ast`. But your naming conventions for the koala project have a module named `ast`.  So, inspect is importing the koala_project `ast` instead of the intended Python library `ast`.  In the end the `inspect` module is not fully loading because your `ast` module imports networkx which dependson `inspect`. Perhaps you could rename your koala_project module `ast` so it doesn't conflict with Python's `ast` module.
comment
See @mjschwenne 's fix for this in #5822  In short, the lack of a solution is precisely what the algorithm is looking for and being able to check `OptimiztionResult.success == False` is very helpful.  Thanks very much for the report of this issue!  And Thanks Matt for the quick fix!
comment
It looks like you are looking at the "Faction" column of the original paper instead of the "Club After Fission" column. The original paper has 17 members in each of the two clubs after the split.
comment
I understand your comment ab out not needing the "See Also" section. Sounds good.  For the multiplicative vs additive, I was thinking about whether taking exponentials of the edge weights would allow you to use multiplicative methods and then take the log at the end. But it looks like that doesn't work because the Theorem says we are **summing the products of each spanning tree's edges**.  Once we sum the products, we can't take the log to get back to the correct answer. So, this more expensive method is the way to go.  It all looks good to me.   Thanks!
comment
I think the suggested sentence in the doc_string on the `distance` parameter makes really good sense.  Perhaps we should also think about providing some tools to easily check the validity of edge attributes. These could live beside `set_node_attributes/get_node_attributes`.  Perhaps the checking functions could store the attribute keys in a G.graph attribute for easy checking of many attributes -- though it would potentially need to be updated if the user adds/removes a new attribute name. 
comment
Thanks for this!  Merging your code to implement the algorithm pointed to in an old issue is sooo much better than having a link in an old issue.  This will help many people I suspect.  - I like the name `directed_edge_swap` rather than `triple_edge_swap`. - The formatting of the citation is not standardized, but should include all the information needed to find the main published page for the article. It'd great to have a link that is permanent (like the doi) too. - list this as an improvement in the release_dev file because it isn't changing an existing function. - the handling of multigraph is something I will have to review to restore my memory of how that is handled.  Hopefully soon... 
comment
I would suggest looping over all the layouts, calling each one and sending that into a draw command.  You'll need to use Matplotlib functions to create a new figure.  ```python layouts = [...] for figno, layout in enumerate(layouts):     pos = layout(G)     plt.figure(fig)     nx.draw(G, pos=pos) ``` Lots more fancy stuff can be added to this of course... :}
comment
I would say it should raise the exception. Putting a bunch of nodes at (0,0) would be potentially hard to notice. Does raising the exception mean we should go through every node to check if it has a 2-tuple value on a `pos` attribute? Or is it better to catch the current error and provide a better message for it? 
comment
OK... now my approval stands. All tests passed. :}  Thanks @MridulS and @rossbar 
comment
Don't worry about the MacOS tests failing. That started recently and is probably due to changes in the testing environment. We're working on those.  But...  These parentheses should not be needed. Without the parens it is a class object and with the parens it is a class instance. But both should work with this function.  The examples work on our testing platform. And on my local machine.
comment
I'm going to close this PR. Let's continue the discussion in #5750  Or leave followup comments here and we can reopen if needed.
comment
Thanks!
comment
This is great!!   Thanks very much.  And such a simple idea too.  Instead of looking at the first element of the list, just iterate to  the first element of the list. :}  The trouble with MacOS tests is due to a change in the environment of the testing platform and not related to this PR at all. 
comment
The docstring states that       The closeness of a node is the distance to all other nodes in the      graph or in the case that the graph is not connected to all other nodes     in the connected component containing that node.  So, I think we don't intend to report nodes that are not connected to the other nodes. Isn't that the intent of this part of the code?  If I am missing something (and even if not) can you provide a small example where the resulting dict is not what you would expect?  That is the first step toward getting a test to show this difficulty. (and thus the first step toward a fix. :}
comment
Ahh... yes, I understand ---  it looks like a typo, that should be `node` instead of `n` in two places. And you said that in the original post. I was too focused on it being the bipartite and not the regular closeness centrality that I missed that.    Yes, that code appears to simply avoid dividing by zero. And instead it assigns a value to a nonexisting node. Thanks!
comment
It turns out the tests were actually testing this case:  Incorrectly!! So, I've fixed the tests and the code in #5800. Thanks for bringing this to our attention!
comment
Is the advantage of this that you can choose the node representation of each block? The relabel argument conveniently renames the nodes as integers. What does this new feature offer that the relabel argument doesn't?  And just to be sure I understand, this would allow the following in one call: ```python Q = nx.quotient_graph(G, nrel, edge_relation=brel) nx.relabel_nodes(Q, block_relabelling, copy=False) ```
comment
How about we abuse the ```relabel``` argument? If it is True, relabel as it currently does. If False, use frozensets. If not True or False, it should be a dict with frozensets as keys pointing to values that are a node object for that set.   Are there potential difficulties or confusions with this approach?
comment
I like what you've got here (haven't looked at the code yet).  I'm a little concerned about backward compatibility. If we change too much, people who have ```relabel=True``` in their code will have their code break when they upgrade.   The argument being a function seems a little clumsy -- we are asking the user to provide the same function twice. Does it need to be the same function? If not, fair enough. If so, perhaps we can use that fact along with the ```relabel=True``` option to signal to use that function. 
comment
Thanks!  This is very helpful!
comment
This issue got closed by #4161 but that PR only fixed some of these functions. I don't think that PR by itself fixed this issue. I'm reopening this, and hoping  that there aren't too many function left that appear in the public API and yet don't appear in the online documentation.
comment
I went through the list of functions shown by this script and made the changes in #5289. I think we should leave this issue open though and just move it to the next version so we do this check for each version. 
comment
Yes, please go ahead and start on this issue. It is much appreciated! :} 
comment
Looks like the tests now take over 8 hours. If we need to keep those in we can flag them as taking too long to run on the normal tests. But can you check if that's intended.  It looks like it is when numpy is installed. I haven't tried running them locally yet.
comment
Note that #5021 might be needed to make the class documentation work correctly.
comment
I am hoping that #5022 will take care of the "extra" files.  I used your repo to get the state of this PR, comparing with your `main` branch I was able to  1) construct a new branch to work from:  `git ch -b style_changes` 2) list files that differ from the main branch:   `git diff main --name-only` 3) figure out which of these actually have only style changes (turns out it is your list above :) so I didn't have to do this 4) revert all the files except those that have only style changes: `git checkout main -- <filename>`  5) pushed to a new branch on my repo and make the PR #5022.  But somehow it got mixed up. There were files that have been removed from the repo this summer.  6) I figured out that "pulling" to your mai branch has not deleted some files that are deleted on the networkx repo. So I repeated 3), 4), 5) with a new version of main freshly pulled from upstream: ```bash git branch -m main funky-main git checkout -b main upstream/main git diff main --name-only ``` Followed by many `git rm <filename>` for the files that weren't supposed to be in the `style_changes` branch.  Committed and pushed those files to my repo's style_changes branch which updated the style PR. If those changes look good, I'll merge that PR and the changes should no longer appear as difference with your branch.  ============= I think you should restart your `main` branch. Even pulling from upstream didn't seem to get all the files correct. Something like: ```bash git branch -m main funky_main git checkout -b main upstream/main # check that things seem to have worked... maybe run a `git diff funky_main --name-only` git branch -d funky_main ``` Then on github.com, look at "branches". delete `main` and then push main from your local repo.  Kelly has (at some point) said that it is worth thinking about not even having a `main` branch on your own repo. I think the reasoning is that it is too easy to start thinking that *your* `main` branch is the same as the upstream main. It is safer to just compare with the upstream main branch.  Personally, I still keep my main branch around to compare to, but every so often I make sure it is the same as upstream main using: `git diff upstream/main --name-only`
comment
Thanks @mjschwenne !! It's merged! :}
comment
This has a lot of commits that I don't think you intended. Can you try again (perhaps easiest to start with a new branch and PR). Go to your main branch, pull from the upstream networkx repo to ensure you are starting from the current version. Just change the one file for contributors and the commit, push, create the PR.
comment
The "files changed" tab still lists 15 files as being changed.  Maybe try this:  ``` git branch move main original_main git ch upstream/main   # will warn about being in a headless state and tell you to do something like the following git ch -b main #  now you can fix you github repo's main branch if you want... but you don't have to. You probably never use it. #  then make the changes in a new branch off this new main git ch -b update-contributors # make the change, commit, push to your repo, create a new PR, delete this PR. ```
comment
Sorry for the `ch` -> `checkout`    I made `ch` an alias in my git config file so I don't have to type checkout so often.  Similarly for `br` -> branch, but I don't think I left any `br` in the examples I typed above. :}  Anyway, if you are interested and want to explore git more, check out their aliases... :} 
comment
Can you give a simple example (<=5 nodes) that doesn't work and describe what the results should be? Thanks!
comment
This is not a fix, but a workaround. If you install graphviz and pygraphviz you can use: ``` nx.nx_agraph.write_dot(nx_graph, "testdot.dot") ```
comment
The DataView can also specify which data attribute (instead of `data=True`): ```python >>> G.add_edge(0,1, weight=7)  # Add a new edge for illustration >>> edge_wts = [wt for _, _, wt in G.edges(data="weight")] ``` You can specify a `default` value for edges that don't have that attribute (the default edge weight default is `None`).  For `MultiGraph` You can also request that the edge keys be returned -- then you get a 4-tuple. ```python >>> edge_wts = [wt for _, _, wt in G.edges(data="weight", default=1)] ```  But the bottom line is that the docs for Multi(Di)Graph need to be updated...   :}
comment
This looks great!  Especially nice test.   Can you add a similar test but change one of the nodes to be an integer instead of a string? I mean that the graph would have some nodes that are string and some that are int so they can’t be sorted?
comment
Excellent…  I guess I should have delayed my approval — but I’ll just double down now. I approve this PR. ;)
comment
In PR #5153 we allowed non-numeric layer values. But that change also impacts the order of the layers in the multipartite layout. The order of the layers is no longer determined by the layer's numeric value.  Notice that we never intended the layer's value to be required to be a number nor to reflect the position/order of the layers. But I can see why it would be surprising to suddenly have that change.  At the moment, the order of the layers is determined by the order that the nodes are reported by `G.nodes`.  That is, the layers are ordered based on the first time in `G.nodes` that a node has that layer value.  So, with your example, the nodes are ordered alphabetically, so the layer for 'a' is the first layer even though it's layer is `2`. Layer `1` is not positioned until node `c` is processed.    Technically this is not a bug because we never guaranteed the order of the layers, but our implementation did provide an order and we changed that order. So we broke backward compatibility and should provide a way to indicate the order of the layers.  I'm going to change the title of this issue to reflect a request for the ability to choose the order of the layers in a multipartitie layout.  A solution will need a way to allow the user to indicate the order of the layers while still allowing non-numeric layer values.  Suggestions?
comment
Perhaps we could do something like this:  ```python try:     layers = sorted((lyr, i, nlist) for i, (lyr, nlist) in enumerate(layers.items())) except TypeError:     layers = [(lyr, i, nlist) for i, (lyr, nlist) in enumerate(layers.items())] for lyr, i, nlist in layers: ```  This would handle the case when the layer identifiers are unsortable, but it will also work for ordering the layers according to a sort of the layer identifier when all layer identifiers are sortable.  It does **not** allow the layer identifier to provide the numerical value for the position of that layer.  Is that important to allow? That is how it used to work, so maybe good to have it available...     
comment
If I understand correctly, your example creates a class where every instance shares the same graph attribute dict. In that case, when you make a copy of that graph, it will still be an instance of that class, and thus it will share the same graph attribute dict.  Are you asking that the copy method create a Graph object instance of your subclass?
comment
Another possibly useful pattern for how this would be used is, e.g. if you wanted a defaultdict instead of a dict for your subclass. Maybe you want your subclass to assign a default attribute value of "not assigned" for any attributes that are not assigned.  Then you could create the new class like this:  ```python import collections class DefaultGraphAttrClass(nx.Graph):     graph_attr_dict_factory = lambda self: collections.defaultdict(lambda :"not assigned")  G = DefaultGraphAttrClass([(0,1)]) G.graph["date"]="today" H = G.copy() assert H.graph is not G.graph assert H.graph == G.graph ```
comment
It looks like GomoryHuTree accounts for many of the long tests. And the example graphs used in the testing are not strategically chosen to test aspects of that functionality. So there could be some low hanging fruit to revisit that test module. Yay!
comment
This has clear implications for the doc_string of `remove_edge` as well.  This `remove_edges_from` doc_string states clearly that it is the most recently added multiedge that is removed. While the `remove_edge` doc_string says in more than one place that it is an "arbitrary" edge that is removed. I think the difference is due to dict's now being ordered.  But since we're improving this doc_string is it in scope to change it for the `remove_edge` doc_string as well?  Also, we should check the `MultiDiGraph` doc_string for `remove_edges_from` to make sure it is updated to match this doc_string.  In this case, MultiDiGraph **inherits** this method. So the doc_string is identical.  But that class' `remove_edge` doc_string will need to be updated from "arbitrary" to "most recently added".
comment
I can verify this color issue. After playing with it a little, it looks to me like the edges are drawn first with the correct colors and then the selfloops are drawn again using the colors in the order specified, but only applying them to the selfloops.  So we get `[(0,0): red, (0,1): green, (1,1): orange, (2,1): blue, (2,2): pink]`    but then we redraw the self-loops:   `[(0,0): red, (1,1): green, (2,2): orange]`  Sorry for the cryptic notation -- hope it is clear.   We do draw the self-loops after all edges are drawn. So, it is possible that we need to adjust the handling of color there. I haven't dived into that code.
comment
Very helpful Issue @mgbckr !  Thanks @rossbar for explaining how it works and noticing that every kwarg representing edge properties probably is affected.  Would it be better to construct dicts instead of sequences where the dicts are keyed by edge tuple? The Matplotlib API seems to rely on arrays (or sequences) quite a lot. Presumably that makes a performance difference.  We are breaking that pattern though when we process some edges separately from others.  I suppose we could put the burden for separating onto the user. But maybe we could preprocess the self-loops and separating the self-loop info from the regular edge info.  But that might get messy...      It's not clear to me what is the best path forward.
comment
`create_using` is a strange kind of beast (see NXEP3).  I thought we never used it as a starting graph from which to add nodes and edges. Though apparently `scale_free_graph` does that -- (see below). Learn something every day. :}  The standard treatment is that `create_using`, if it is a NetworkX Graph, is cleared before using it to create the graph.  If it is a graph constructor (like `nx.Graph`) then it is called to create the starting graph.  So...  That argument -- `create_using` is always modified when it is an instance.  Maybe NXEP3 can end up fixing that behavior too.
comment
The feature of having a graph to start building a new graph also appears in #4659 and we should probably be consistent in our choice of names.  Should the argument be named `initial_graph` or `seed_graph`?
comment
Wow --- This looks really nice. I've been focusing on releasing v2.4 and will get more time soon. Thanks!
comment
Thanks for the gentle nudge!    No updates except -- wow why haven't I gotten to this yet?  
comment
I removed the conflicts with the master branch and reformatted docstrings and switched names from vertices/vertex to nodes/node.  Also format changes to match PEP8, and removed numpy requirements for the tests, as well as making the test class into a test function. I also made some cosmetic code changes.  Questions/concerns/suggestions: - Should we make a public function like ```write_latex(G, path)``` to ease use?   - There are other aspects of the public-facing API that raise questions... Does the basic code need to be inside a figure environment?  Should it use subfigures at all (that's for putting together many graphs... which presumably should be done by another program using this one to generate the latex code to draw just the one figure). - Why does the code cut the x and y layout sizes in half? - Could we remove the placeholder files and replace them with text strings in nx_latex.py? The files are small.  Is there another functionality you aim to include by allowing placeholder files? - Could the test compare the strings generated rather than writing them to files and comparing those?  I think  this means, including the expected.tex file as a string and comparing that string to the string generated by ```str(A)```.   - Will this code work for DiGraph and MultiGraph? - Can we make some examples in the doc_string? Or in the examples gallery (directory examples/drawing)?  Thanks! 
comment
Looks like the Appveyor tests were having trouble due to matching files with vs without ending linefeeds.  It might have also been differences in how files end their lines: with \n or \n\r. But I can't tell for sure (and I don't have access to a windows machine for local testing).   The current test I have constructed writes a file and then reads and compares each line of the written test file and an expected output file. This might be better done using a string rather than writing/reading to files. But it seems to work OK and alleviates the difficulties with trailing end-of-line issues.  Any thoughts on better ways to test this?
comment
I can't get LaTeX to compile using the ```expected.tex``` file. It gives an error about too many "," tokens from xparse.    Do I need to install anything, or use a particular flavor of pdflatex/latex/etc to get this to work?
comment
@LucaCappelletti94 can you verify that the ```expected.tex``` file that matches the output of ```A.save``` is working latex code?  I'm trying to use ```pdflatex example.tex``` and it gives a syntax error about extra commas.
comment
Thanks very much for this. It looks like my version of adigraph is 1.5.1 and the latest is 1.7.1.  That may be the cause of the xparse error complaining about extra commas. Does this make sense?  Is there a version of adigraph that we should require for this to work?
comment
This PR was closed since Feb apparently due to the original fork being deleted. I reopened it by creating PR #5639 with these changes.  The adigraph package is still available on CTAN (written by the author of this PR) so it should all still work.
comment
No, it transferred to the new PR just fine.  Looks like it'll need updating/rebasing to get the CI to pass. but I think that will be straightforward.  Thanks!
comment
The dict you want is best returned using:  `out_degree_dict = dict(G1.out_degree(G2))`  The `nbunch_iter` behavior is one of those long existing NetworkX warts where we decided long ago that the possible confusion was worth the better interface. We should consider improving that wart, of course. But the problem is very unlikely to come up in practice.  The strange "extra node" occurs only when both 1) `G1.out_degree` is being called separately on each node instead of on the desired set of nodes, and 2) G1 has strings of length 1 as nodes while G2 has nodes with multi-character strings that contain the single character used as a node in G1. Both of those conditions must hold while at the same time the user is trying to look up the `G1.out_degree` of nodes from `G2` -- which is pretty wacky in itself. :}  All-in-all it should be unusual that this arises. (I guess you win the prize for being lucky enough to have run into it!)   One straightforward fix could be to check each iterator and rule out the strings. That disallows strings to be nbunch containers of single character nodes. It also would require checking for a string every time someone uses a container as an nbunch.  Said another way, two plus sides to the current implementation are that we don't have to check for strings every time a container of nodes is used as an nbunch, and strings can be used as containers of single character nodes.
comment
For MultiGraph, [the `remove_edge` docs](https://networkx.org/documentation/stable/reference/classes/generated/networkx.MultiGraph.remove_edge.html#networkx.MultiGraph.remove_edge) say that you should use the "edge key" to identify specific edges.  It is likely that is the role your edge "class" is playing for your code. To be sure, ask whether there can ever be two edges between a pair of nodes with the same class.  If not, then use the "class" value as the edge key.  How do you do that? Creating the edge key has to happen when you create the edge. By default the edge keys are integers starting at 0 and incrementing for each edge between that pair of nodes.  `MG.add_edge(u, v, edge_key)` is a reasonable approach: `MG.add_edge(u, v, class)` in your case. `MG.add_edges_from([(u1, v1, edge_key1), (u2, v2, edge_key2), ...])` is another approach.  If you don't know the class for the edge when you construct the multigraph, you can convert one MultiGraph with edge attribute "class" to a MultiGraph with both edge attribute class, and identical edge key as follows:  ```python MH = nx.MultiGraph((u, v, c, {"class": c, "old_key": k}) for u, v, k, c in MG.edges(keys=True, data="class")) ``` (actually, you don't need the `keys=True` or `old_key` parts if you don't need the old keys.)   If your "class" attribute is not unique within a pair of nodes, you can't use that value as an edge key. Then the best way to remove the edge is something like: ```python MG.remove_edges_from((u, v, k) for u, v, k, c in MG.edges(keys=True, data="class") if c == 12) ``` That might remove more than one edge -- or no edges depending on which have attribute class==12.
comment
Probably because we didn't get to it, or it wasn't highest priority. Do you know enough about either of these to think of why it SHOULD be included.  If so, give a justification for why it should be there (and maybe even a pull request) .    :)
comment
It seems to me that most geometric networks would use euclidean distance and this change shouldn't mess up any backward compatibility.  Sounds good to me. :}
comment
It looks like one additional line of code will transfer all the edge attributes of the original graph to the line graph as node attributes.  ```python G = nx.path_graph(4) G.add_edges_from((u, v, {'krill': u+v}) for u, v in G.edges) H = nx.line_graph(G) H.add_nodes_from((node, G.edges[node]) for node in H) ```
comment
It's a file that gets created if you run the tests.  It must have been included by mistake at some point. 
comment
Take a look at how matplotlib handles shapes in scatterplots. It might be possible to make this happen.
comment
It looks like matplotlib allows each scatter plot to have a single marker style. So to implement this you'd need to sort the nodes by marker style and draw each style using a separate command. 
comment
Use the ```nodelist``` argument to draw only some of the nodes. Use a separate drawing command for each marker style. If each node has its own shape, then draw one node at a time.  Color can be specified by a sequence of colors in the same order as the nodes in nodelist.
comment
Yup -- I see what happened... The nx.draw() command is too coarse for what you are trying to do. See [the drawing docs](https://networkx.github.io/documentation/stable/reference/drawing.html) for more info. I think you will need to draw the nodes using ```nx.draw_networkx_nodes``` multiple times and separately draw the edges with ```nx.draw_networkx_edges```.  When you do it yourself like this then you don't use the ```nx.draw``` command (which just calls these other functions anyway).
comment
Once this is working for you, maybe we can think about how to put in some code to automagically do this when nodeshape is a sequence of shapes... :)
comment
I think we should be looking at Cython and Numba and perhaps other things for ways to improve performance.  This is a start -- but shows that more is needed for any significant progress.   Thanks!
comment
The blame indicates that this was added to try to speed up the subgraph views.  #3017 Most of the time we are using induced subgraphs. And most of the time there are far fewer nodes in the subgraph than in the graph. So we can speed up the process by iterating over the nodes of the filter and checking if they are in the graph, rather than iterating over the graph and checking if the nodes are in the filter.  So, I'm not surprised that no tests complained when you remove that code. It is supposed to speed thing up. Perhaps a better comment would make that clear.  
comment
Perhaps this wasn't as clear as I intended. The testing for a `nodes` attribute is not to check if a NetworkX Graph `H` is used to specify the nodes in the subgraph. Any `show_nodes` filter has an attribute `nodes`. So, e.g.  ```python SG = G.subgraph([2, 3]) assert SG._NODE_OK.nodes == [2, 3] ``` It's only the `show_nodes` filter, as far as I can tell, that benefits from this, but that is a common filter to use.
comment
Perhaps a test could be something like (untested): ```python pytest.mark.parametrize("graph", [nx.Graph, nx.DiGraph, nx.MultiGraph, nx.MultiDiGraph]) def test_slots_in_views(graph):     G = graph()     (G.adj, G.degree, G.edges)     if G.is_directed():         (G.pred, G.succ, G.in_edges, G.out_edges, G.in_degree, G.out_degree)     # raises error if slots not working for setstate and getstate     deepcopy(G)  ```
comment
Here's another minimal example: ```python G = nx.MultiDiGraph() edges = G.in_edges() deepcopy(G) ```
comment
This is a feature/question for many of the `is_*` functions (both in Networkx and other packages/languages too). It even gets deep into the python language where `1 == "A"` returns False while `1 < "A"` raises an exception.  So I guess the question here is which type of user we want to help out. 1) The person who is scanning through a bunch of graphs to check for tournaments where some are directed and others are undirected. (currently they have to try/except, or check `G.is_directed()`.)  Or 2) the person that (mistakenly) has the graph in a directed/multigraph form, gets the result from `is_tournament` as `False` and can't figure out why the graph isn't a tournament graph when the edges indicate that it really is a tournament graph.  I'm leaning toward leaving it the way it is (based on least surprise). The person scanning many graphs will get the exception and it will be clear to add a pre-check to rule out directed graphs. The person with the wrong type of graph will get the exception and know that it is the base class that is causing trouble -- not the edges satisfying conditions for a tournament.  But I may be missing a use case or type of user we are trying to serve with this function.
comment
The betweenness_centrality normalization is already adjusted for directed edges. ```python G = nx.path_graph(5) nx.betweenness_centrality(G) nx.betweenness_centrality(G.to_directed())   # normalized to be the same values.  # Note: DiGraph with single direction for each edge gives value cut in half nx.betweenness_centrality(nx.DiGraph(G.edges())   # BC is half of value for G ``` I'm going to close this, but you can comment here and we will see it. We can reopen if needed.
comment
I made a pull request  to your pull request with some format changes and file placement changes. https://github.com/jac2130/networkx/pull/1  Just for completeness the text of that pull request comment is:  Hi JC, I'm reviewing your pull request to NetworkX with Hamming distance functions.  I've moved the file into the algorithms folder and changed the formatting of the doc_string for your first function.  Could you update the doc_strings into a similar format for the other functions?  Also I added some lines at the top--see if you are OK with the more verbose names I proposed there.  Its the networkx style not to abbreviate in function names.  When the doc_strings are updated you can push back to your master branch and it will update the pull request too.  Let me know if git causes trouble.  Finally, we will need some simple tests before merging.  If you can come up with some simple graphs and compute correct values for them that's a good start--and maybe all the testing we need at first.  You can create a test_hamming.py file in the tests directory inside algorithms (use test_cycles.py as an example if you want).  Or if that's too much, just put the code for the tests in your hamming.py file after a line like:   if **name** == "**main**":     #test_code here  Thanks! 
comment
This PR is already part of #5497.  Comments and suggestions should be handled in that PR thread. So, I'm going to close this. We can still comment/reopen if needed.    > noob question: >  > ```python >     if u is None: >         nodes = G.nodes >     else: >         nodes = [u] > ``` >  > can be rewritten as `nodes = [u] if u is None else G.nodes`?  Actually, you've got it switched.  Which is one reason this "new" syntax is not always the best (and sometimes is). The indented if/else structure is easier for humans to parse even though it takes more lines.
comment
It seems like sparse matrices are the way to go for most network analysis, but I can also imagine users wanting to choose... Still, it is easy enough to get a full array from a sparse representation.  I don’t recall any discussion of this implementation difference. I’m pretty sure it is not intentional.
comment
I agree that we should provide sane defaults...    And indeed sparse matrices seem like the best choice.   I'd prefer to let the user switch to dense matrices using ```.to_dense()``` rather than crowd our function signature. but as you say-- we can see if users request a kwarg option.
comment
non_edges is an iterator.... So, if you process each non_edge and don't store it you can solve your memory issues.
comment
FWIW:   `degree_assortativity_coefficient` also returns `nan` when the graph is of uniform degree. Also `numeric_assortativity_coefficient` does too.  So, they are all consistent... If there is no variation in the value, assortativity is not defined.
comment
Just to add some more churn :)   the `to_scipy_sparse_matrix` doesn't actually say whether the resulting matrix is an "adjacency matrix" or an "edgelist matrix" or an "incidence matrix".  So the name `adjacency_matrix` is probably better anyway. The `pandas` functions made that clear -- but only years after the scipy functions had been around. 
comment
Good point!   We have both many matrices and also many ways to store a matrix.  Too many combinations... :}
comment
I think that is a good idea.  It could be simply:  install pre-commit using pip or conda so you don't have to get into too much detail. but I guess detail is OK if it is still readable.
comment
My understanding is that the graph_edit_distance can depend on the order of the inputs if the match function causes it to. For example, g1->g2 has cost 3 with steps: add node "e", remove edge ("d","d"), add edge ("d","e"). While g2->g1 has cost 2 with steps: rewire edge ("d","e") to ("d","d"), remove node "e".  In other words with this match function, rewiring ("d","d") to ("d","e") takes two steps, while ("d","e") to ("d","d") takes one step.  I believe this is because of the self-loop attribute representation of a set of one value -- rather than a tuple of two identical values, but I'm not actually sure.  Removing the match function certainly makes it work symmetrically. using a match that is always True also removes this difference.  Does this help any?
comment
Seems like node order does matter for this code.  It was written before dict's maintained insertion order, so this could be a subtle bug that got created during that shift. It only seems to matter for self-loops making it even harder to identify.  Thank you for tracking it down to such simple input graphs!  Now we just need to find where in the code we treat self-loops.  Perhaps @aparamon will see this and come up with a brilliant quick fix. :}  Following the discussion in #2361 I uncommented all the print_debug statements and ran the resulting code with: ```python print("G2 is edge (a,b) and G3 is edge (a,a)") print("but node order for G2 is (a,b) while for G3 it is (b,a)")  a, b = "A", "B"  G2 = nx.Graph() G2.add_nodes_from((a, b))  G2.add_edges_from([(a, b)]) G3 = nx.Graph() G3.add_nodes_from((b, a))  G3.add_edges_from([(a, a)]) for G in (G2, G3):     for n in G:         G.nodes[n]["attr"] = n      for e in G.edges:         G.edges[e]["attr"] = e  match = lambda x, y: x == y  print("Starting G2 to G3 GED calculation") assert graph_edit_distance(G2, G3, node_match=match, edge_match=match) == 2  print("Starting G3 to G2 GED calculation") assert graph_edit_distance(G3, G2, node_match=match, edge_match=match) == 2 ``` This is the output ``` G2 is edge (a,b) and G3 is edge (a,a) but node order for G2 is (a,b) while for G3 it is (b,a) Starting G2 to G3 GED calculation Cv: 2 x 2 [[1. 0. 1. 7.]  [0. 1. 7. 1.]  [1. 7. 0. 0.]  [7. 1. 0. 0.]] Ce: 1 x 1 [[1. 1.]  [1. 0.]]  matched-uv: [] matched-gh: [] matched-cost: 0 pending-u: ['A', 'B'] pending-v: ['B', 'A'] [[1. 0. 1. 7.]  [0. 1. 7. 1.]  [1. 7. 0. 0.]  [7. 1. 0. 0.]] pending-g: [('A', 'B')] pending-h: [('A', 'A')] [[1. 1.]  [1. 0.]]  matched-uv: [('A', 'A')] matched-gh: [(None, ('A', 'A'))] matched-cost: 1.0 pending-u: ['B'] pending-v: ['B'] [[0. 1.]  [1. 0.]] pending-g: [('A', 'B')] pending-h: [] [[1.]]  matched-uv: [('A', 'A'), ('B', 'B')] matched-gh: [(None, ('A', 'A')), (('A', 'B'), None)] matched-cost: 2.0 pending-u: [] pending-v: [] [] pending-g: [] pending-h: [] []  Starting G3 to G2 GED calculation Cv: 2 x 2 [[1. 0. 1. 7.]  [0. 1. 7. 1.]  [1. 7. 0. 0.]  [7. 1. 0. 0.]] Ce: 1 x 1 [[1. 1.]  [1. 0.]]  matched-uv: [] matched-gh: [] matched-cost: 0 pending-u: ['B', 'A'] pending-v: ['A', 'B'] [[1. 0. 1. 7.]  [0. 1. 7. 1.]  [1. 7. 0. 0.]  [7. 1. 0. 0.]] pending-g: [('A', 'A')] pending-h: [('A', 'B')] [[1. 1.]  [1. 0.]]  matched-uv: [('B', 'B')] matched-gh: [] matched-cost: 0.0 pending-u: ['A'] pending-v: ['A'] [[0. 1.]  [1. 0.]] pending-g: [('A', 'A')] pending-h: [('A', 'B')] [[1. 1.]  [1. 0.]]  matched-uv: [('B', 'B'), ('A', 'A')] matched-gh: [(('A', 'A'), ('A', 'B'))] matched-cost: 1.0 pending-u: [] pending-v: [] [] pending-g: [] pending-h: [] []  AssertionError:  1.0 <> 2 ```
comment
I agree that those two lines look for selfloops.  But it's not clear to me whether the next line might also get triggered sometimes when there is a selfloop (that is not simply u-u or v-v).  Then there is tracking down what `ind_h` and `_ind_g` get used for.  Presumably they flag those edges to be changed -- but it is in the logic which tries to change them that we will find whether an edge substitution or an edge delete is chosen.  Thanks for this --
comment
The default is to draw arrows with single arrowhead.  But one can set the `arrowstyle` to "<->" if that is desired. I guess you are saying that a more reasonable arrowstyle default  for undirected graphs would be "<->". 
comment
I can verify this defect.  Thanks very much. It looks like checking `isinstance(m, int)` doesn't work for `np.int64`. We may want to add a conversion to `int` in the `nodes_or_number` decorator that processes `m`. But is there a standard way to check numpy values for whether they are integers or not?
comment
Thanks @rossbar!  I think in this case, the `isinstance` is being used to determine whether the input successfully executed `list(range(n))` used inside the `nodes_or_number` decorator.  Time for an audit of the use of `nodes_or_number`   :}  Potential problems: - The functions that actually use the 1st value returned by `nodes_or_number` are:    - `star_graph`, `wheel_graph`, `lollipop_graph`, `complete_bipartite_graph` - `wheel_graph("")` and `star_graph("")` raise an `IndexError`. Same for any empty sequence input. -  `lollipop_graph` raises if (m, n) are type (int, seq) e.g. `lollipop_graph(3, "abc")` - `complete_bipartite_graph` raises if (n1, n2) are type (seq, int) e.g. `complete_bipartite_graph("abc", 3)`  Potential fixes: - `star_graph` and `lollipop_graph` and `complete_bipartite_graph` use the original input value to check if it was used to construct `list(range(n))`.  These should use `isinstance(n, numbers.Integral)` and the last one already does. - `wheel_graph` should use `if len(nodes) == 0:` instead of `if n_name == 0:` - The following shouldn't use `nodes_or_number` because they just call `empty_graph` anyway:      - `complete_graph`    - `cycle_graph` - unused first values of `nodes_or_number` should unpack it as `_, nodes = n` using `_` as variable name. - `lollipop_graph` should rename `n_nodes` when `n` is Integral, not `m` as it currently does.  - `complete_bipartite_graph` should check both n1 and n2 for Integral, should use `len(top)` instead of n1 and should maybe raise a ValueError if top and bottom are not distinct. - Add tests for the last 3 potential problems listed above. Add tests for non-int Integral inputs for `star_graph`, `lollipop_graph` and `complete_bipartite_graph`. - Adjust doc_string of `complete_bipartite_graph` to explain the case when n1, n2 are not integers.
comment
Can you explain what you mean by that?  References?
comment
We tried to implement much of your idea when moving to 2.x.  We called them "views" and they are closely related to "shallow copies". I think you could do what you are thinking about with ```generic_graph_view``` which is used for ```subgraph```. But you could add or remove edges that way.  It creates a View front end that uses the underlying graph for most of the graph information, but checks for changed values before checking the underlying graph.  It would probably take a bit of coding to make it work as you propose. Or at least as I understand what you propose--- that is, you make a copy and add something to the copy. The copy actually just keeps the old graph and doesn't copy it. But it adds a layer of code in front that checks for the changes before looking it up in the old graph.
comment
the example is correct. For each type of triad, it shows how many triads are of that type. With only 1 node in the nodelist there are no triads. So there are no triads of any of the types. 
comment
~A single node in the nodelist contains no triads -- there is only one node!! Perhaps you want to include only the node and its neighbors: ```nx.triad_census(G, nodelist=['b']+list(G['b']))```~  Sorry -- I was wrong.... It seems that you already *have* the triad type and quantity for e.g. node "b". The only triad is a-b-c and it has type '021D': 1 When you specify node 'c' it has no triads so all types have value 0.
comment
I see what you mean, but clearly this attempt to implement triad_census for a nodelist does not count the same way you suggest.  The PR that implemented it is #4361. It is possible that there is a bug in that change.  I haven't looked carefully, but I think that the condition `m[u] < m[v]`, which is supposed to eliminate double counting of triads, does not work correctly when only a subset of the actual nodes are looped over.   So I am thinking this may have a bug.  Can you tell me what you expect the census to be for each node in 'a<-b->c'? and can you tell me what you expect the census to be for each node of the more complicated 'a->b->c->d'? If we can work through those two cases I think we should be able to check for bugs and identify the source.
comment
For the `"a->b->c->d"` case we should have 3 triads for each node -- so if we sum the individual node counts, we should get 12 counts.  I only get 4 counts total:  2 for "021D" and 2 for "021C". And there are no triads counted for node "d".  I think we made a serious error when we added the `nodelist` parameter to this code. It would work fine if we didn't have all that code to avoid considering the same triad multiple times. But it is a fundamentally different way of counting. I think it should probably be a different function than `triad_census`.  Perhaps `triad_census_by_node`.  I'm going to mark this as a defect.  Let's see if there is a good algorithm for getting the census by node...
comment
The `triad_census` code without a `nodelist` parameter has been around for a long time and seems to be solidly correct. (though I thought the nodelist part of it was correct, so maybe there is something subtle that we've missed). The `nodelist` feature is relatively new and I can see where the bug is coming from there, and that bug has no impact on the correctness of the calculation without `nodelist`.  So, you are safe to use the function without the `nodelist` parameter.  
comment
I looked at the "blame" history of this file (available from a button at the top right of the "view file" screen which is available at the upper right of each file on the PRs diff page). From the beginning, we have defined eulerian and the eulerian_circuit code to rule out isolated nodes. That is the eulerian circuit must visit all nodes and cross all edges.  In PR #3399 we added functions for `eulerian_path` where the doc_string says the non-isolated nodes must form a connected component, but the code itself only checked for a connected graph -- meaning that the isolated nodes were ruled out.  Then in PR #4246 we made some changes to allow a keyword argument `source` and the code with that change added a line to remove the isolated nodes from G in the directed case only -- probably to make that part of the code agree with the doc_strings.  I agree with your assessment. The code in these functions is inconsistent between directed and undirected and also does not agree with the doc_strings.  It appears that the eulerian_circuit code is consistent, but does not agree with the how the eulerian+path (sometimes) handles isolated nodes.  We need to decide whether we want to rule out isolated nodes from our definitions of eulerian...  Most definitions seem to allow isolated nodes. Our original docs and code ruled out isolated nodes. And most of our functions continue to work with way. The doc_string for eulerian_path does not, but the code does for undirected graphs. The only code that doesn't is for directed graphs.  Why would you want to rule out isolated nodes? It means you don't have to check for and remove isolated nodes for every call to one of these eulerian functions. And since isolated nodes are not really relevant for the concept of eulerian, users can safely remove them before investigating the eulerian nature of the graph. So, if we include a note in the doc_strings alerting the user, it should work fine.  And in our case, backward compatibility for most of our routines calls for handling isolated nodes as *not* eulerian. But this is probably not a highly used scenario -- so the backward compatibility is likely to not be an issue.  Why would you not want to rule out isolated nodes? The concept of eulerian is about edges, not nodes. So it is cleaner from a theoretical perspective to allow isolated nodes so you can ignore whether they are there or not. This argument fails when you realize that you want to rule out graphs that have two components each of which has an eulerian circuit. Then you need the graph to have a single connected component (for all nodes of nonzero degree).  So, in fact you can't ignore nodes completely.  Thoughts?  
comment
After re-reading this with a couple days for perspective, I think we should stick with excluding isolated nodes from our definition of eulerian circuit and eulerian path. This should be well documented in each function, with an example in some main function(s) of how to remove isolated nodes for people who want the other definition.  The desire to allow isolated nodes is driven by a mistaken notion that we can then avoid considering nodes — only edges. That view is mistaken because we need to ensure that the non-isolated nodes form a single component. So you do have to consider the nodes when you exclude them from the single component part of the definition.  I feel it is cleaner to use the definition that disallows isolated nodes, and provide info on how to pre-process a graph if users want to allow isolated nodes.  This choice also provides better backward compatibility than other options to make our code consistent.  So, what needs to be done: - change the path docstring(s) to make it clear that we don’t consider graphs with isolated nodes to have an eulerian path. - Provide an example in key functions for how to remove isolated nodes if desired `G.remove_nodes_from(list(nx.isolates(G))`   - Change the directed code within eulerian_path to no longer remove isolated nodes. - Add a release note that this behavior has changed (in 1st doc/release/release_dev.rst`)  Will this make the module consistent in its handling of isolated nodes? Am I missing anything?
comment
```cutoff``` for weighted paths is the weighted length of the path -- not the number of "hops". Your edges all have weight more than 3 so cutoff of 3 doesn't capture any edges.
comment
How about:  `Only paths of (weighted) length <= cutoff are returned.`
comment
Wait -- which version of the docs are you looking at?  I see [this page](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.weighted.single_source_dijkstra_path.html) which has a better description:  `Length (sum of edge weights) at which the search is stopped. If cutoff is provided, only return paths with summed weight <= cutoff.`
comment
Can you explain what you mean by "size and dimension (-1,2)."? When I generate `G` and then try `G.edges` I get things like:  `((0,0), (0,1))` which seems right because the nodes are 2-tuples, so an edge should be a pair of 2-tuples. Also `G.size()` reports 2939.
comment
To relabel (that's the technical term we use) the nodes see [the relabel docs](https://networkx.org/documentation/stable/reference/relabel.html)
comment
Thanks very much!!
comment
Yes, this is a documentation error. The parameter should not be a list.  It should be a node from which to start the search for a negative cycle.  The documentation is wrong.   Thank you for bringing this to our attention.
comment
The documentation from the "show all checks"/"details"/"document artifact" link shows [what the new doc_string](https://output.circle-artifacts.com/output/job/6ead1d27-9815-439b-9123-34eaa52f40ad/artifacts/0/doc/build/html/reference/algorithms/tree.html?highlight=branching#module-networkx.algorithms.tree.branchings) appears like.  (The document artifact is usually near the bottom of the list of checks).  At the module level, the whole doc_string appears in an unformatted way as the "one line description" of the function. You need a blank line after the first line of the docstring.  The [docstring for the actual function itself](https://output.circle-artifacts.com/output/job/6ead1d27-9815-439b-9123-34eaa52f40ad/artifacts/0/doc/build/html/reference/algorithms/generated/networkx.algorithms.tree.branchings.branching_weight.html#networkx.algorithms.tree.branchings.branching_weight) also doesn't get converted completely. More blank lines are needed between the sections of the docstring.  Perhaps mimic the blank lines in a function that does format correctly.  Thanks!!
comment
I agree that this error is unrelated. @rossbar found that MyPy put out a new version that made one of our tests fail. A fix has now been merged. So, I rebased your branch on that fix and hopefully the tests will now pass.
comment
After fussing with this for a while I believe the cached_property is interfering with the current view handling of `__slots__`. It's a nice rabbit hole to fall into, but I'm currently wondering if the need for `__slots__` goes away once we have `cached_property`.  The motivation for `__slots__` is speed and memory usage for objects that are created many times. If we are caching the reportviews like `G.edges`, that motivation is significantly less.  My experience with slots is that they work fine, but you have to be really careful with copy and deepcopy and pickle. So, it would reduce our maintenance of the Views significantly if we remove the slots and just use a dictionary worth of space for each view created. The cached properties that should be on the same order as the number of graphs rather than the number of time the user calls G.edges.  I'd love some thoughts from people who have used `__slots__` before. And also from those who have had difficulty maintaining the Views because of `__slot__` considerations.  I'm confident we can eventually implement both cached properties and `__slots__`. But would it be better to just get rid of the `__slots__`?
comment
Are you saying that it should be named `networkx-2.8-py38.py39.py310-none-any.whl`?
comment
I’m going to close this since the names are chosen automatically, and since the error message is quite clear about what the problem is. Please comment more about suggestions for better error messages or other solutions. We will still see them and can re-open if needed.
comment
There is no edge `(2,2,4)` so the set_edge_attributes didn't set those attributes.
comment
This doesn't follow our usual doc_string tense.  Moving it to the "is returned" form does not change the meaning and it doesn't match the rest. I'm going to close this, but you can still leave comments and we'll open it again if needed.
comment
Yes, this string of code would only work if you have done: `from networkx import nx_pydot.read_dot as read_dot` or `from networkx.nx_pydot import *` or similar.  I like your suggestion of changing the string of code to: ```G = nx.Graph(nx.nx_pydot.read_dot(path))``` This is in the repo at: `networkx/drawing/nx_pydot.py` Should be an easy fix. Thank You!
comment
I'm -1 on removing all the information about what gets returned under what conditions.  If this is just for satisfying a type checker, then let's find a way to do that without removing the human oriented information. (I'm fine with getting the typing correct -- just don't remove the information that is there.)
comment
This question is probably a bigger issue than just this PR.  But since this is a first step to updating docs in other classes to improve typing in the docs, it is probably a good place to talk about it.  The 4 graph classes have overlapping and intentionally repetitive doc_strings across the classes. Whenever we change one, we need to change -- or at least be aware of -- the others. If we improve one, we should make that change to the others. If we put special comments about one graph class into one doc_string, we should try to keep that comment easily identified so that readers can recognize the rest as being the same.  For example, adding a "Note" to describe the differences rather than just changing one or two words in a paragraph in the doc_string.  There is a tension between keeping the doc_strings in all graph classes close to being the same, and adding the tweaks to make them specialized for each class.  When the docs are the same for a method that belongs to e.g. Graph, DiGraph, MultiGraph and MultiDiGraph, there is a unified presentation. Maintainers have fewer doc_strings to parse (but more to remember to update), and users can more easily recognize the docs when they look at the same method in another class and understand that the methods do the same thing.  When the docs differ, the presentation diverges over time but can be tailored to each class. Maintainers don't have to remember to go and change all 4 copies of the doc_string when one is changed (though maybe they should because it is probably better for all the classes). And users get docs that are specialized for that class. But they lose the ability to see what is the same across the classes.  You can see our attempts to keep the doc_strings the same in the Examples section just below the changes in this PR.  ```>>> G = nx.Graph()  # or nx.DiGraph() or nx.MuiltiGraph() or nx.MultiDiGraph()```  appears in all 4 graph classes. This way the user can understand that they all would give this same output after just reading one doc_string.  We could move toward making each graph class use the examples with *that* graph class. But I would claim that the doc_string would be ever so slightly less useful.  Perhaps we should change the return type to ```MultiDegreeView or DegreeView or OutDegreeView or MultiOutDegreeView or int```.  But that certainly doesn't seem clear. And it might mess up typing. We could try to add another line to the doc_string stating that the return type varies from one class to another but that they are basically the same.  If we choose to make the doc_strings differ by graph class, then we should maybe make the examples differ by graph class as well. That would test all the graph classes for the example in this doc_string (which we currently don't do). But someone will have to keep track of the doc_strings in the 4 classes and make sure that improvements in one get updated in the others.  What is the better philosophy about the methods for doc_strings of similar subclasses? I am reminded of the 7 subclasses in  scipy.sparse for the different types of sparse representations.  More relevant for this PR:  should we update the other class doc_strings here so we make sure they move together?
comment
@britapiiro can you tell me what impact this has on PyCharm? Does it fix the typing errors you were getting? Also, can you try changing them all to `DegreeView or DiDegreeView or MultiDegreeView or MultiDiDegreeView or int` and report back whether that also fixes the typing errors you were getting? Thanks!
comment
I guess the current workaround is something like the following. ```python GcomposeH = nx.compose(G, H) node_data = {n: G.nodes[n].get('fun') + H.nodes[n].get('fun') for n in G.nodes & H.nodes} nx.set_node_attributes(GcomposeH, node_data, 'fun') edge_data = {e: G.edges[e].get('weight') + H.edges[e].get('weight') for e in G.edges & H.edges}  nx.set_edge_attributes(GcomposeH, edge_data, 'weight') ``` It seems like it would be hard to generalize the combining of attribute dictionaries. For example, do we sum attribute 'fun' but multiply attribute 'goodfun' and ignore another? Perhaps an example like this would help people construct their own though.
comment
Hello @dtekinoglu , I think what is needed here is an example in the doc_string similar to the "workaround" above that shows users ho to update their edge attributes after the `compose` operation. Thanks!!
comment
I was worried that the new doc_strings have such wide lines of code that we can't read them easily. Then I realized that you restrict the `node_data` to only nodes in both `G` and `H`. So, you don't need the "get" function for this example.  Every node and edge has the attribute.   To reduce the width of the example code, we could: - use integer nodes - remove the `get` functions and replace with e.g. `G.nodes[n]['color']` or `G.edges[e]['weight']` - remove the edge ('x', 'z') since you don't use it (only a minor change in the graph creation -- and that line isn't all that wide)  These are not a big deal, so change what you like and don't change the rest. You can see the doc strings as they get formatted in HTML by clicking on the "Show all checks" link, scrolling to the bottom and clicking on the "details" link for circle-ci's "document-artifact". Of course, screen size matters for this kind of style issue, so that's why we don't get too picky about it. 
comment
Issue #5520 has now been resolved. So I think we can review this PR.
comment
It seems that not all tests pass... but my head is not on very straight today, so I can't be sure. Are the CI tests here producing a different result from what you get locally?
comment
Ahh...   I've found the trouble.  The other tests in this test module `importorskip` numpy at the start of the test class.  This new test is outside the test class, so you'll need to call `importorskip` yourself, or make the test a method inside the class.  ```python         pytest.importorskip("numpy")         pytest.importorskip("scipy") ``` (I dont think you need the scipy... but I wasn't sure and too lazy to check..:) If you put that line in the test function, it will try to import numpy and if that fails it will skip the test.
comment
It seems that now the decorator is not eager for generators using `yield from G` but still is eager for generators using ```for v in G: yield v```.  Something subtle going on....??  
comment
I would like us to handle disconnected graphs for the user (and not making them split the graph into connected components first). We have to figure out what the new `cutoff` and `best_n` parameters mean in that case though... Maybe we will have to ensure that `cutoff` is at least the number of connected components. Or maybe we can simply use a try-except on a StopIteration for the `next` call.  We should try to mimic the previous behavior of this function as much as we can. The intent was to leave the results the same for people who don't use the `cutoff` or `best_n` parameters.   Handling graphs with zero edges is definitely something we should do.  I'm pretty sure that used to fail with the older code too. It's easy to treat that corner case specially and we should do that.
comment
I believe this is now fixed in #5537 so I'll close it. Comment here or reopen if there is more to do.
comment
Once all the tests have passed, you can see the documentation as it will be published when the PR is merged into NetworkX. To do that, click on "Show all checks" on the right side of the "All checks have passed" box. A long list of checks appears. Go to the bottom where one of them says it is a "circle-ci" test with an "artifact" and then a URL.  Click on the "Details" link on the right side of that test.  It will bring you to the full documentation for the project as if this branch was merged. You can then navigate to the function you have changed:  Reference->Numpy (under converting to and from other formats) -> to numpy_array.  Check that your documentation looks good.
comment
To handle the absence of some packages follow the boilerplate code found in e.g. ```eigenvector.py``` at the bottom (basically you move the import into the function and put a ```setup_module``` function at the bottom of your module so the tests can skip when scipy is not installed.  It looks like the ```touch``` package is not getting loaded on appveyor. Can you tell me anything about it? Are there any other libraries you are importing that will be increasing the required packages to run networkx?  We may have to handle them in a similar way to how we handle scipy.  I should warn you before you put too much more effort into this that we have not settled on a platform/library for parallel algorithms yet. Search through previous issues for "parallel"   to see previous discussions about parallel libraries. It may be different now in 2019, but in the past it wasn't clear which libraries to use -- so much seems to depend on the hardware setup, etc.  Do you have a perspective that python parallel libraries have become uniform, or that one library is becoming the de facto parallel interface?  
comment
I guess you need to list the new required packages in the file ```requirements/extras.txt`` file along with version numbers. At first you can just put in the version numbers you used. No need to try finding oldest version that works -- too many rabbit holes already. Other rabbit holes include looking at ```travis.yml``` and ```appveyor.yml``` to see how the requirements files are used when testing. The goal for now is to get the imports to work in the tests. :)  Take a look at #3440 as well as #3270 and #3439 
comment
Maybe you need to change the tests module using ```test_eigenvector_centrality.py``` as a model for optional dependencies... ???
comment
Sorry to ask you to repackage again, but could you make this an example (by putting it in /examples/advanced ).  There is already one parallel example: plot_parallel_betweenness.py  in there and this could go nicely with it.  
comment
I believe the bridges code was recently made to work with multigraph.  #5397  So this PR is probably no longer needed...    Comment or reopen if needed...
comment
We should check whether the 4th item from #5192 helps performance. The initialization of `r` does call `G.degree` twice, but that call only creates a view of the graph data structure, so it shouldn't matter much. View construction doesn't create a new data structure.  Still, it does create two copies of the view rather than one. But removing that trouble may cause other slowdowns if we're not careful. So, it'd be good to run some quick speed tests.  For example, constructing the list as suggested here is almost certainly slower (and much RAM intensive for sure). It's probably better just to create the view once and then run the existing code using the view in two places. 
comment
We should make the documentation clear enough that they can figure out which method to use. I'm dubious about trying to automagically figure out which method to use, but whichever way that decision goes, it would be really helpful to include information about when to use which method.
comment
`spiral_layout` takes as input a Graph and it returns a dictionary keyed by node to that node's (x, y) position value. Then to draw the Graph with those positions, you need to call `nx.draw_networkx` with both the Graph and also the position dict.  Usually something like:  ```python G = nx.path_graph(15) pos = nx.spiral_layout(G) nx.draw(G, pos=pos) ```  
comment
Thanks!   You are correct.    That is a faulty way of thinking about turning a min problem into a max problem. Maximizing over sums of reciprocal weights does not minimizing correctly.  Other options include negating (if the problem doesn't have trouble with negative weights -- which min_weight_matching does not... but shortest_path does). Or perhaps a more robust approach would be to subtract the weights from a little bigger than the maximum of all weights. 
comment
Looking into this bug, I'm pretty sure the keyword argument `maxcardinality` should be deprecated and set to True inside the function. What would `min_weight_matching` mean for `maxcardinality=False`?  It would mean the empty matching or maybe the matching that is just the lowest weight edge.   I propose that we deprecate `maxcardinality`, and inside the function where we call `max_weight_matching` on the newly weighted edges, we set `maxcardinality=True`.  Luckily this code was only merged Summer 2021 so not too many people will be inconvenienced by this deprecation.
comment
I agree that this makes sense as a private function in the `json_graph` code. And we can deprecate the public version and move the code to the new place.
comment
Can you upgrade NetworkX?  This function was updated last summer and you've got a version older than that.    My results show: ```    >>>  G = nx.DiGraph([(0, 3), (1, 3), (2, 4), (3, 5), (3, 6), (4, 6), (6, 5), (4, 3)])    ...: print(f'in-in: {nx.average_neighbor_degree(G, source="in", target= "in")}')    ...: print(f'in-out: {nx.average_neighbor_degree(G, source="in", target= "out")}')    ...: print(f'out-in: {nx.average_neighbor_degree(G, source="out", target= "in")}')    ...: print(f'out-out: {nx.average_neighbor_degree(G, source="out", target= "out")}')    ...:  in-in: {0: 0.0, 3: 1.3333333333333333, 1: 0.0, 2: 0.0, 4: 5.0, 5: 0.0, 6: 1.0} in-out: {0: 0.0, 3: 0.3333333333333333, 1: 0.0, 2: 0.0, 4: 3.0, 5: 0.0, 6: 0.0} out-in: {0: 3.0, 3: 2.0, 1: 3.0, 2: 1.0, 4: 2.5, 5: 0.0, 6: 2.0} out-out: {0: 2.0, 3: 0.5, 1: 2.0, 2: 2.0, 4: 1.5, 5: 0.0, 6: 0.0} ```
comment
It looks like the question is: "What is a neighborhood?"  You are thinking (reasonably in my view) that when `source_degree` is "in", that the neighbors should be the incoming neighbors. That is any nodes which connect to the source node. But in fact, that is not what is done in the code. And it seems to be mixed up from what I'd expect.  In the code, the sum is always over the neighbors on the outgoing edges from the source node.  But the size of the neighborhood is computed based on the keyword `source_degree` as "in" or "out". So that when "in" is indicated, we sum over outgoing neighbors but divide by the number of incoming neighbors.   In your example and considering node 3, it is currently computing (in_degree_5 + in_degree_6) / 3 = (2+2)/3 = 1.333.  This is certainly one way to extend the formula to the directed case, but it seems wonky.  Does the reference talk about directed graphs and how to handle them.  Or is there another reference that does?
comment
Yes -- let's leave this issue open so that we will address the documentation at the least and (hopefully) improve the method so it is actually taking an average of degrees instead of summing over one neighborhod and dividing by the size of another neighborhood.  I'd love to have a paper that talks about "nearest neighbor average degree" (or "average neighbor degree") in the context of directed graphs.
comment
I recommend that the user add the node properties themselves.  In fact it is sometimes better to look up the node properties in the original graph rather than duplicating that storage.   Some people want node attributes and others don't. In any case, it's much better for us to keep the API simple and it is easy to add them yourself. And the extra time and memory of copying them means we don't want that as the default.  Thanks for the suggestion though... :}
comment
Thanks @ChrisKeefe ! 
comment
I think this looks good. The docstring lines and comment lines are sometimes longer than the 88 char enforced by black and so could be shortened. I think some of the comment lines aren't needed because the code says what the comment says in a more compact way.  But these are style issues.    What I'm waiting for before I approve this is a concensus on whether we can just change this and put a note in the release notes that the values of `average_neighbor_degree` have changed for DiGraphs, or whether we need to deprecate the old version in some way.  I would argue that the old values for DiGraph didn't make sense, so we should change it (correct it?) now... not waiting for a deprecation cycle.  @rossbar do you have some insight on deprecation protocol here?
comment
Great!  Thanks @rossbar! @yusuf-csdev can you come up with a simple example (2-5 lines?) that shows how this changes the computed values? We should put that into `doc/release/release_dev.rst`.  I can do this if you prefer <especially since I will likely edit it anyway>... :} Thoughts? 
comment
I went ahead and added the release notes.  They are probably kind of long, but can be edited down when the release is made if desired.  I also tried to shrink the docstring line-length and comments. See what you think @yusuf-csdev  Because I pushed to your fork (the PR), you'll have to pull from your fork before making further changes to this branch.  I did change the code in minor ways -- I moved `arg = {}` to just before the loop where it is computed. And I realized that S_n and P_n were only used once, so I just replaced them with `G_S[n]` and `G_P[n]`. Then for spacing reasons I renamed `tgt_deg` to `t_deg`.  Feel free to change anything you like back to the way it was or to something even better.  
comment
I'm pretty convinced that `extrema_bounding` should be a private function. Looking at the history, I don't think we/I looked very carefully at the naming convention for private/public functions at this time. If it wasn't in `__all__` I thought of it as private. It would be good to get the naming and deprecation changes done before v2.8, but we do also needs tests. Not sure if that is needed before v2.8, but if they are easy and quick, yes.  See also #5409
comment
One approach to look into would be to make self.adj a custom dict-like class that looks at both self.succ and self.pred.  I would love to find a unified class for directed and undirected graphs and maybe an explicit approach supporting mixed-graphs would lead to that.  But even if it doesn't, a class that supports mixed-graphs sounds useful.  
comment
@MridulS  asked about interest from the community for a unified class for all types of graphs. I can give some history on that question. But I don't speak for the whole community. From my perspective, an ideal world would have provided definitions that worked nicely for all types of graphs, and then we would have a single graph class. But my experience suggests that it just isn't effective to try to combine all the different ideas about graph structure people have invented into a single class. The swiss-army-knife graph class leads to more confusion than problem solution. On the other hand having 20 or 30 different graph classes to handle each type of graph doesn't work well either (mostly from a maintenance perspective). The optimum is in the middle somewhere and we try to stumble toward it through incremental improvements. The MixedGraph structure discussed in this issue might be reasonable. It would be nice to have an algorithm or application in mind to guide its development. 
comment
This makes me consider again a generator interface. There are so many possibilities (including sorted and not sorted) that the flexibility of a generator function could serve all these cases.  We decided not to use a generator in the previous PR for backwards compatibility.  But this suggestion makes me think of the following approach:  make a new function that iterates through the communities.  Have `modularity_max` retain its original signature and use the iterator to construct its return value.  That way we get the iterator functionality through the generator interface but we keep the old interface for ease of use and backward compatibility.  thoughts?
comment
What I intended is to yield *all* the currently existing clusters. This behaves like a dendogram with each node in its own cluster at the beginning, and then with each iteration you see the clusters after the latest merge. :}
comment
We have a deprecation question for this PR we wish to replace the keyword `n_communities` with one named `cutoff`.  I believe we need to keep the old keyword, and check its value to see if it is set. If so, raise a deprecation warning, set `cutoff` to the value from `n_communities` and run the function.  If both `cutoff` and `n_communities` are not their default values, then raise an exception.   Would it be better to not deprecate, and instead change the keyword argument name? Then an unknown keyword argument error is raised upon calling the function with `n_communities` set. The release notes can say which keyword argument to use instead, or more likely the doc_string of the function would make clear that cutoff is the keyword argument to be used.  Which is the better way to proceed?
comment
Could we make the change in the code that checks for the multiedge? I feel like this change will be very difficult to figure out when someone looks at it 3 years from now.  Even the check itself `(u, v), d = next(iter(labels.items()))` could use a comment. I'm not very with-it today, but I had to go back to the previous discussion just to figure out why this might be a check for a multiedge. And it's only been 18 days since I approved that.  I'd really like to contain the code that flags a request for multiedge labels to a small part of this function. Perhaps:  ```python         # Informative exception for multiedges         try:             (u, v) = next(iter(labels))  # ensures no edge key provided         except ValueError as err:             raise nx.NetworkXError(                 "draw_networkx_edge_labels does not support multiedges."             ) from err         except StopIteration as err:             pass ```
comment
The geometric graph you construct connects every node to every other node. That makes it hard to see the geometry.   (It also takes a few lines of code that aren't part of the example really -- and NetworkX has a function to do this.) By setting a threshold for edges to exist, you can get a graph that looks like map that people would use to navigate the domain.  (Using a "seed" makes the random process provide the same nodes every time, so pick one that gives a nice picture for the example. :) ```python G=nx.random_geometric_graph(20,.4, seed=1) ``` The long edges do not appear! That makes it easier for the eye to "see" the image as a map of the cities rather than a jumbled mess of connections.  That is what I meant by "graph structure".. better words would have been "geometric structure".  If you are only going to show one method of TSP, you should go ahead and use the "best method" which is christofides (as determined by theoretical limits on the approximation of the optimal tour). If you want to compare many methods that would be fine too, but I think it is beyond what this example is aiming for.  That might be better as part of the [nx-guides](https://github.com/networkx/nx-guides) repository that came online recently and would be appropriate for a more in depth look at the TSP algorithm -- similar to the blog you have mentioned before -- only in a repository of guidebooks for NetworkX.  :) 
comment
Nice!!   Yes, this is what I was thinking of.  You can push to the same branch on your repo and the PR will automatically update. Thanks!
comment
Hmmm... christofides is being tested and doesn't have that trouble.  When you say the "latest release" do you mean 2.5.1?  Because for sure, you need to have the github repo (or 2.6rc1) to make christofides work.
comment
Might it be better to yield each set of communities obtained during the process and then the user can either 1) wait until the end, or 2) stop when they find something they like?  It is sort of like returning the whole dendogram of communities instead of just the final one.
comment
The point of the generator is to *avoid* creating a list.  If you create the list, you might as well return it.  My feeling is that if you need them to be sorted, a generator approach won't make sense unless you can construct them in the sorted order. Looks like a new parameter for when to stop might make the most sense.
comment
As part of this issue we should also add a paragraph to the docstring to describe what a spiral layout is and how it is affected by the `equidistant` option. 
comment
You are correct, that is a documentastion error. The "Latest" documentation (for version 2.7 due to be released any day now) has the documentation for `nbunch` fixed to be:  "The view will only report edges from these nodes." Thanks for the report!
comment
Maybe we should rewrite our docs then to suggest the hybrid approach instead of having an example line of code that doesn't actually work.  I typically just go in and change the requirements file locally so it works for `conda`.  Does `pip` do things like install GraphViz when you install pygraphviz? Maybe pip is smarter now than when I last looked at it closely. :}  Anyway, we should fix the docs to provide a line of code that works for conda -- or to remove the description of how to make it work with conda.  I guess one option is to have requirements files for pip and other requirements files for conda.  I'm not sure we can make a single requirements file work for both.
comment
Thanks for the gentle nudge. If you think of ```transitive_reduction``` as just returning the graph relationships, the current approach makes sense. You can convert them to any graph type and copy over the edge/node attributes. But the algorithm doesn't do this (to save space and copying time).   The following copies all the attributes and sets the type of the graph class.  ```python H = nx.OrderedDiGraph(nx.transitive_reduction(G)) H.add_nodes_from(G.nodes.items()) H.add_edges_from((u, v, G[u][v]) for (u, v) in H.edges()) H.graph.update(G.graph) ``` 
comment
I can not [Edit] reproduce the error -- with the nodes and edges you provide. And I also added a node "short stature" and that worked fine too.  It looks like `dot` is creating the error. Perhaps you should save the graph to a dot file with: ```python import pygraphviz pyG = pygraphviz.ADiGraph() pyG.add_edges_from(G.edges()) pyG.add_nodes.from(G) pyG.write("tst.dot") !dot tst.dot    # or exit python and run dot tst.dot from your shell. ```  
comment
I apologize for my misleading comment earlier. I can *not* reproduce this issue.  Though I haven't tried using the version of Graphviz 2.50.0.    Perhaps you should try using an earlier Graphviz version to see if it makes a difference.  My point with suggesting that you write to a dot file and use command lines is so you can debug whether it is a GraphViz problem or a pygraphviz problem.  I don't suggest you use that for your main workflow.   Both @rossbar and I have found that your requested workflow works for our versions of pygraphviz and graphviz without errors. So it is starting to look like an issue with this new version of Graphviz.  
comment
We should remove the heading of "Maybes" in the release notes (as least as I understand the meaning of that heading as suggestions for what might be put in, but asking for input on whether they should be included.  I would say we should include the first bullet in this section (talking about adding support multigraph) and not include the second (which as Ross points out might imply that we have a plan going forward for including types).
comment
I don't have much experience with structured arrays. But from your code it looks like a dtype with multiple attributes produces an array with retrieval syntax is `A[attr][i, j]` while a dtype without names produces an array with retrieval syntax `A[i, j]`.   Note that a dtype with a single attribute would still be `A[attr][i, j]`.    So, I think you have to expect separate branches. And it is still readable code. You could make the place where the plit occurs more obviou with a comment line:  `# Handle dtypes with attribute names (structured arrays)` or similar. But I think the proposed code is readable.
comment
The problem seems to be in your edge filter. You only check one direction of your edge.  It should work if you check `(u,v) in I or (v, u) in I`. Your edge filter would work with a DiGraph.   Once you use the `.copy()` feature you copy two edges to the new graph, and it is a Graph so while the edge is added in one direction, it is an undirected edge and suddenly node 4 has degree 2.
comment
Maybe before leaving this I should ask how you got to the subgraph view code you tried. There is a networkx filter named `nx.show_edges` which would have done what you wanted -- but perhaps is not documented well enough to find. I would have expect you to use:    `H = nx.subgraph_view(G, filter_edge = nx.show_edges(I))`  What parts of the docs did you look at to figure out the method you used?  We need to put links in there to help people build useful filters.  Thanks!!  
comment
Great!  Thanks that is helpful for us to improve those docs. I think it would not be hard to rewrite `show_edges` so that it handles I as a dynamic set. We chose not to do it that way to avoid surprise changes in the subgraph when the set is later changed. But I can see the appeal both ways.  That might be a good example actually.   Thanks! 
comment
Thanks @rossbar   Question:   I don't see any artifact for the circleci build of the docs on a couple of the recently push/ci-tested PRs. Like this one.  Is there a new way to get to the docs that a PR would create? Or is something wrong with circleci or am I just missing something?  :}
comment
Can you summarize what you have changed and how that fixes the issue? It looks like the substantive change is to compute the maximal clustering instead of the average clustering. But it would help a lot to provide more description of what you did and why. Thanks!!
comment
Thanks for the summary above. That is helpful.  I have a couple of questions: - Why did you replace the transitivity of G with the average clustering of G? -  Why did you replace the `Cl` value with the maximum of average clustering of the `nrand` lattices instead of the average of them?  If these are from something in the article can you say where in the article it says to do that? Thanks!
comment
OK... I think everything I ask about in my previous post is answered [in your comment](https://github.com/networkx/networkx/issues/5064#issuecomment-975284052) from #5064  I have no more questions at the moment -- I will play with it a little before officially approving it. Other reviewers should consider this OK'd from my perspective. I don't expect any major issues.
comment
It appears that omega is **not** bounded between [-1, 1]. Both the old version and the new version are consistently above 1 for the graph shown below. Similar code produces graphs that give omega less than -1.  It may be that the number of nodes is too small and in some limit the results get close to 0. But if that is true, then it shows that the constraint on omega is not a hard one.  I'm not sure which of these versions is more correct. But since the definition is fairly vague, maybe it doesn't matter too much which definition we use. I think we should change the doc_string to indicate that omega returns a real value, but not in [-1, 1] as sometimes suggested. Thoughts?   Here's the test code: ```python In [64]: all_o = []     ...: N=14     ...: p=.2     ...: G=nx.binomial_graph(N, p, seed=22)     ...: assert nx.is_connected(G)     ...: for s in range(21,30):     ...:   try:     ...:     omega=nx.omega(G, seed=s)     ...:     print(f"{s}: {omega}")     ...:     all_o.append(omega)     ...:   except:     ...:     pass     ...: print(max(*all_o), min(*all_o)) ``` The old code gives: ``` 21: 1.1536697247706422 22: 1.1302752293577982 23: 1.1302752293577982 24: 1.1211009174311928 25: 1.1385321100917432 26: 1.0871559633027525 27: 1.1564220183486238 28: 1.1183486238532112 29: 1.1481651376146789 1.1564220183486238 1.0871559633027525 ``` With the new code: ``` 21: 1.1788990825688075 22: 1.1082568807339452 23: 1.1440366972477065 24: 1.1412844036697247 25: 1.1385321100917432 26: 1.1224770642201836 27: 1.1674311926605505 28: 1.1481651376146789 29: 1.1403669724770642 1.1788990825688075 1.1082568807339452 ```  
comment
That sounds good to me.  Thanks!! 
comment
Would the dtype error be avoided using: ``` a = np.array([1, 2, 3], dtype=int) / 2 ```
comment
I **like** this approach!   The title and the sidebar are easier to read and easier for google search engine to find and easier for people to learn about a function without having to learn about the whole package. I believe both the title and sidebar were this way until about 2-3 years ago.  To find the path to the source file, you have to click on the `source` link and scroll to the top of that page -- but you can also search.  Anyway, I am very pleased with this solution. Perhaps others are not happy about losing the full title... But it looks good to me.
comment
What would you expect the communities to be for this example? The example doesn't show that anything is incorrect (as far as I can tell). Remember, that the way to use a max heap is to use a min heap with negated values. Are you reading the code to see something is wrong? Or running an example where you know what the answer should be?
comment
Here's a fix.  The idea is that you need to remove the node attribute that has a value which is a dict. That's a limitation of the GraphML format. If you need these attributes, you can try a different graph format.  ```python  import networkx as nx G=nx.Graph() G.add_edge("A","B") G=nx.contracted_edge(G,["A","B"]) for node, datadict in G.nodes.data():   # these two lines do the node attribute removal     del datadict["contraction"] nx.write_graphml(G,"test.graphml") ```
comment
This look good to me.  It's a little strange to have ```G.nodes[4]``` return an attribute dict while ```G.nodes[4:6]``` returns a list of nodes. Can we explain that in a succinct way to users?
comment
```G.nodes.head(n)``` gives the first n nodes.  This separates the dict-like lookup from the list-like slicing.  Also, "tail" could give the last n nodes.  But, maybe we should call it ```slice``` to allow more generality: ```python G.nodes.slice(10) G.nodes.slice(1:2:20) G.nodes.slice(-10:) ```
comment
I occurs to me that users probably don't want to process "the first 10 edges". They probably want to see them.  We could provide for that use case by adding a `__str__` method that produced something **like** the `__repr__` only stopping after 5 edges (or some other number).  I think this fixes the primary motivating use-case -- are there other compelling use-cases?
comment
Nice!! It makes sense that its better to pause a single iterator over neighbors (while we handle the descendants) than to restart the iterator over the neighbors. I also reran the timing using IPython's `%timeit` feature and got a 10% speedup at both 100 and 1000 nodes with p=0.01 in the rabdom graph. Pretty good for a simple two-line change.  ```python import networkx as nx scc = nx.strongly_connected_components scc2 = nx.strongly_connected_components2  G = nx.binomial_graph(100, 0.01, directed=1, seed=5) list(scc(G)) == list(scc2(G))     # True %timeit list(scc2(G))     #  249 µs ± 3.88 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)     %timeit list(scc(G))      #  273 µs ± 5.93 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) ```  Thanks very much!
comment
Thanks for this @rossbar  I wonder about changing the nodes to start at 0 (pythonic) instead of leaving them as starting at 1 (matching the Graph Theory literature).  Will that impact people from a backward compatibility perspective?  What is the gain from switching to 0-based node values?
comment
Nice!  I did not remember the conversion to 0-based graphs. That was probably one of those debates we had about what to do with the small graph library long long ago.    The node order was not guaranteed (and not enforced until python 3.6) so I'm fine with what is here.   Thanks!
comment
I would just merge this, but I don't know if it should close #5266  I added a github "link" to that ticket, but then realized that maybe it doesn't close that issue completely because this PR says it is "Related to" instead of "fixes".   Sorry if removing the link is troublesome. Hope it is easy to merge.  ;}
comment
The pagerank walk (as I understand it based not on NetworkX, but from teaching) is a random walk over the webpage network where at each step with probability alpha, you take a normal random step chosen uniformly from all possible links, but with probability (1-alpha) you jump to a webpage randomly with equal probability for all webpages.  The **idea** is that webpage users follow links approximately randomly but every once in a while get bored and just pick a webpage at random.  Mathematically, this removes all "absorbing states" from the process...and theP-F theorem holds with a unique steady state vector of probabilities for each webpage. In terms of "pagerank power", no webpage collects pagerank power just because random walkers don't have a way to leave.    Let Q be a matrix with all entries 1 (nxn matrix same as the transition matrix). Let T be the transition matrix for randomly walking on the webpages.  Then the pagerank transition matrix will be:  alpha*T + (1-alpha)Q/n  A Lazy walk is a random walk with a probability of not taking a step being just as likely as any of the links... So the transition matrix gets 1/degree(v) added to the diagonal for row corresponding to node v.  A random walk should follow some edge (with equal chance) out of the node at each timestep.  Notice that the pagerank transition matrix is NOT a sparse matrix. It has a non-zero entry in every position. (The chance of going from any webpage to any other webpage is nonzero.)  So, maybe it **shouldn't** be stored as a scipy sparse matrix. It isn't sparse.  Hope this helps....   
comment
The stackoverflow screenshot of the graph characteristics suggests that the degree of most if not all nodes is zero. Perhaps the code divides by the number of edges somewhere causing an overflow?  Anyway, we need more information before we can help you.    Try making a very small(and similar) DiGraph and see if it does the same thing -- copy/paste the error message (much easier than screen shot -- *and* let's use use the text/find/cut&paste).  The error message should give useful information. 
comment
Have you tried changing parts of this to narrow down the trouble? What jumps out to me is that you are setting the demand of every node to be 1, but the total demand of the network must be zero, or the flow raises a `NetworkXUnfeasible` error. Somehow, you're not giving us a reproducible description.  Can you try it with a small simple graph? Does it give the same warning?
comment
Wow -- there is a lot in here.... :}  LGTM... Thanks @rossbar!!
comment
That doesn't provide the nice headers that sphinx can make use of. I would suggest something more like the function [fullrary](https://github.com/networkx/networkx/blob/766becc132eda01c9d0d458de2d75ab763f83493/networkx/generators/classic.py#L67) in `generators/classic.py`  Thanks!
comment
Our documentation creation tool, sphinx, uses the first part (up to a blank line) as a short description with each link to this function in the docs.  So, it renders much better if the first line is short and sweet, followed by a blank line and  then a paragraph description of what the function is/does.   Also, we usually describe what the function does as `Returns` rather than `creates`.  So maybe the first bit of this should be: ```python     Returns the Bull Graph      The Bull Graph has 5 nodes and 5 edges. It is a planar undirected      graph in the form of a triangle with two disjoint pendant edges [1]     The name comes from the triangle and pendant edges representing      respectively the body and legs of a bull. ```
comment
Use a MultiGraph to start with:  `G = nx.MultiGraph(e)` Or convert to a multigraph as you input it to `contracted_edge`: `H = nx.contracted_edge(nx.MultiGraph(G), (4, 3))`  
comment
A subgraph must be an "induced subgraph".  So your example is not a subgraph isomorphism. What you want is a "subgraph monomorphism" and there is a method to iterate over those as well. 
comment
That makes good sense.  It is faster (even for the cases where counts are weighted) to use an iterator rather than a loop over `update` calls, or even a single update over a constructed dict (or list).  We want to both reduce the calls to `update` and reduce creating dicts or lists which just get sent to `update`.  How about this: ```python if weight is None:     edge_info = iter(G[node]) else:     edge_info = ((v, wt) for _, v, wt in G.edges(node, data=weight, default=1)) label_freq = Counter() label_freq.update(edge_info) ``` I get this as much faster for both weighted and unweighted cases.  Do you get it faster for the unweighted case? And more importantly, did I do it right? Are we still getting the same answers?
comment
Thanks for this!  I agree that it looks like a bug. We should not be creating a `numpy.array` with the default `dtype` and then changing its `dtype` later.  Would it work to create `A` with the `dtype` provided as an input? ```python A = np.full((nlen, nlen), np.nan, order=order, dtype=dtype) ```   We should also add a test to make sure that complex values work in this case. The tests are in `tests/test_convert_numpy.py`. If you have questions about that, or don't have time to do it, let us know.
comment
Yet another possibility is to let the user choose a dtype and a sentinal/`fill_value` instead of just a dtype. ```python arr = np.full((2, 2), fill_value=fill_value, dtype=object) ``` Then we don't even have to convert to a new dtype at the end. If people want to use integer dtype, they can choose a `fill_value` of `0` or `-1` as they see fit. The default dtype can be float with a `fill_value` of `np.nan` to maintain compatibility.
comment
Should these same changes (docs for edges method) also be bumped to the undirected graph class?  How about to the multigraph classes?   It is accurate for all of them and should probably be improved in the same way.
comment
I would recommend using the exact same wording as for the directed case in the undirected case. Then the docs are the same for both classes and still accurate in both classes. Briefly looking at the two multigraph classes, I think the same wording can be used there too. I guess I am just saying it is just universally better wording. :}
comment
Yay!   Approval works!  Now does merging work? We've got 2 approvals Mridul, want to merge this one?
comment
It seems like `math.isclose` is the way to go -- probably available after this was written. And I agree that using a locally defined `allclose` is better than including `numpy` just for `numpy.allclose`.  How many functions library-wide are not included in `all` and yet don't have an underscore  prefix.  I imagine there are many because we only started the private function convention a few years ago. I'm in favor of moving them to underscore prefix names. But that might be much bigger than this PR. Of course, I haven't looked, so maybe there aren't that many.  I'm +1 for simply removing functions that are not listed in `__all__` rather than deprecating, but I'm fine with any way it is handled. Maybe it is better to be complete on all deprecations. But it feels like a lot of trouble for functions that are probably not used outside the function.
comment
I agree! ;)
comment
The milestone date for v2.6 is May 31. We are aiming to be close to that.
comment
Nice! -- and it gives good flexibility...  I think we should try to make a way for users to choose linecollection vs fancyarrows beyond the default system (which seems like fine defaults).  We've got the machinery, so it'd be good to let people use it even in nonstandard use-cases.  Could we enable arrows to be True/False/None... where None gives the default, False forces LineCollection and True forces FancyArrowPatch?  Something like: ```     arrows : bool or None, optional (default=None)         If None, directed graphs draw arrowheads returning FancyArrowPatches,         while undirected graphs draw lines returning LineCollections for speed.         If True, draw arrowheads using FancyArrowPatches (bendable and stylish).         If False, draw lines using LineCollection (straight and fast).  Note: Arrowheads will be the same color as edges. ``` As for code, I think this means: ```python     # Use LineCollection to draw edges by default, for performance reasons     use_linecollection = G.is_directed()     if arrows is True:         use_linecollection = False     if arrows is False:         use_linecollection = True ```
comment
Ack -- I meant to create a PR from my repo to your branch for this PR. But I confused `origin` and pushed it to your  PR instead.    Please take a look at the suggestions -- and remove or rewrite anything that shouldn't be there.   Nice job on the summary @rossbar !!
comment
Thanks Jarrod! Sorry for making you find/fix my mistakes! 
comment
It looks to me like the line in the main `__init__.py` that imports utils also creates the name networkx. ```python import networkx.utils networkx  output  <module 'networkx' from '.........networkx/__init__.py'> ``` This is a useful probing tool/hack: ```python def no_networkx():     try:         networkx     except NameError:         return True     return False  assert no_networkx(), "fresh" import networkx.utils assert no_networkx(), "utils or some other indicator" ```
comment
@rossbar Does this do what we want with removing networkx from the namespace?
comment
Hmmm...   Deprecating for a release cycle seems like it might be a good idea -- since we are deprecating other changes that break downstream code.... this probably should count. I just assumed no one was using this...
comment
Nice!    Should it be titled "Matrix Market Exchange" rather than "Matrix Market Graph Format"? The other formats are titled:  GML or GEXF or "Adjacency List", so the "Format" seems out of place. And the NIST site uses "Matrix Market Exchange" without any discussion of Graph at all.  I'm guessing "Market Exchange" is a qualifier NIST added to "Matrix" when constructing a universal matrix storage standard.  The name "Matrix Market" doesn't have the same meaning somehow.  Anyway, I'm not sure what the right heading is here.
comment
OK. I looked at [the Scipy.io page](https://docs.scipy.org/doc/scipy/reference/io.html) and [the NIST page](https://math.nist.gov/MatrixMarket/) more closely.  Matrix Market is exactly the term they use.  So let’s go with that.  I think that means my only suggestion is that the first heading be shortened to “Matrix Market” instead of having “Graph Format” at the end.  But I’m also open to other ideas.
comment
Nice approach! -- I like it.  Would it help backward compatibility if we could tell people (who have adapted the public `xml_types` for their problem say to a dict `new_xml_types`) that they can replace the new public `xml_types` function using `new_xml_types.__getitem__`?  Does that actually make it work like the old code?  I'm not actually sure we want to advertise this way of doing things, but have I understood the backward compatibility issue?  Are there other things people may have used the public `xml_types` for? I guess it is hard to know....    I'm definitely good for leaving it as you have proposed. I'm not sure a deprecation is even needed. Maybe a release note and a discussion of the extent of backward incompatibility here in this PR will be sufficient....??.. 
comment
> As a side note - there is no indication anywhere in the docs, code, or tests what the expected output of the spiral layout is. I'm certain that the `equidistant=True` option is broken (I'll open a new issue to track that), but even the default behavior is not really clearly specified in the docstring and there are no references or gallery examples to give an idea of what the spiral layout is _supposed_ to look like.  The best description I have seen is the paragraph written as a description for PR #3534  Perhaps adding that paragraph at the top of the docstring would improve the description of what the layout does.  I have assumed that "equidistant" should make the arc-length along the spiral between nodes be the same between each pair of nodes adjacent along the spiral (not necessarily neighbors in the graph). And when equidistant is False (the default), the nodes are separated along the spiral by a constant angle made from the center of the spiral. 
comment
Looks like instead of `incident to these nodes.` the docstring should read something like: ```The view will only report edges from these nodes.``` or maybe stronger with: ```The view will only report edges from these nodes (outgoing if directed).```
comment
Thanks for this!    Perhaps it should go into the gallery examples ```examples/algorithms``` seems like a good location. Name the module ```plot_<something>``` and tweak the module to match other examples sphinx stuff. You can check how it looks using the Circle-CI artifact that shows the documentation.   We're trying to include more text describing the examples than our current examples...   :} 
comment
Congratulations on your first PR!   Could you also move the line `n = M.number_of_nodes()` outside of the loop? Or maybe just delete the one that is in the loop. :}  Also, the style-checker found that your editor removed the end-of-line character from the end of the file. Could you put that back in? (or run "black" on this file...   `pip install black` should work if you want to install it.)
comment
Whoops.... my bad...  The value of n changes with each iteration of the loop.  I missed the fact that we add nodes to M inside the loop so n keeps increasing each time.  I guess this is fate's way of having you really learn about updating your PRs.  Could you change it back again to having n computed inside the loop?  Thanks!!
comment
Thank you very much for this!!  - The `edge_betweenness_centrality` function should definitely use `not_implemented_for`. - Using the `_weight_function` to process the keydict correctly is precisely the way I'd suggest to handle it. - Converting the edge 2-tuples to edge 3-tuples with form (node, nbr, key) should be able to be done by finding the minimum weight edge -- just as you do in the `_weight_function` only returning the edge key instead of the weight.  It might actually be better to just convert the multigraph to a graph so you only compute the shortest edge between two nodes once.  But that presumably uses slightly more memory to construct this smaller graph.  Thoughts?  And, of course, it would be good to have tests to make sure the MultiGraph case isn't ignoring weights. But let's decide which way to present this example first (building _weight_function and something like _find_edge vs. converting G to a Graph with only the shortest edges between each pair). 
comment
Yes, Could you make the `get_keys` lambda expression into a named function? Thanks!  
comment
Can you add some simple test? Also for line 429, the edge keys are not necessarily integers. so just call them the edge keys. Thanks!
comment
I think this was closed by mistake. I'm opening it again. But maybe I don't understand something so correct me if I'm missing it.
comment
It looks like the pydot tests are failing on Python 3.10. Those are certainly not part of this PR.   @jarrodmillman Is there a way to get the "extra" tests to run on other python versions even if the python 3.10 tests fail?   [Edit: Nothing has changed in pydot since July 2021, so it isn't a new version of pydot that is the culprit.]
comment
I would assume that `all_simple_path` would *not* check for a path first, but rather be designed so that if no paths were found it would end the computation. Strange that it "hangs". According to its docs, it uses a version of DFS. I would hope that is just as fast for the first path found as the code behind `has_path`. It works very quickly for the non-connected graphs I just tried.  Can you give a small example?
comment
I can verify that this example takes longer than my patience allowed to compute simple paths even when they are valid nodes. And that was after I had eliminated the filter and only looked at the single component with 1114 nodes.  So, it is not surprising that the case when the target node is not in the connected component would take a long time (I added such a node and tried that too).   So, here is the issue as I see it (please correct or embellish what I miss): The `all_simple_paths` function is slow. If it is going to result in an error because the target is not connected to the source, we could avoid the slow calculation by checking for any path before finding all paths. That comes at the cost of checking for a path for valid source/target pairs.   Often users know whether the source and target have a path.  Should we automatically test for a path to save time for those who don't know? Or should be make the user check it before calling this function?  My (very weak) preference is to add a paragraph to the docs explaining that this function is slow and it might be very helpful to make sure there is a path between source and target before calling the function.    Thoughts?
comment
Thanks for this idea and for the code!   :}  Could you make the if/else logic match that for `width` which is just before your code for `linestyle`? Also, could you add tests for ensure that it is working?  
comment
Your description isn't correct. The current function `rescale_layout_dict` does not return a `np.ndarray`. It returns a 2-tuple for each node.  This is intentional. There are two rescale functions, one returns a single `np.ndarray` representing the entire graph's positions without any node identifier. The other returns a dict keyed by node to a 2-tuple value representing position.  Can you explain what you are trying to do and how the current functionality doesn't allow that?  What is the problem you are trying to solve?
comment
You probably wanted the other "rescale" function where it returns a `np.ndarray` that you can then turn into a dict (see any of the layout functions you reference near the end) using something like: ```python newpos = nx.rescale_layout(pos, scale=scale) + center newposdict = dict(zip(G, newpos)) ```  I'm forgetting the reasons why `rescale_layout_dict` returns 2-tuples as values. I know it was added later than the rest. I'll try to track that down..  Perhaps it should just be the two lines of code above -- but I suspect something deeper was driving the need for the dict version of `rescale_layout`. 
comment
OK... I looked up the history, and as far as I can tell there was no need for the value to be a tuple. It just needs to have length 2. Given this information, I agree that we should change the function to return a numpy array.  Can you change this branch to use `return dict(zip(pos, pos_v))`. The two tests will need to be changed to test for arrays instead of 2-tuples also. If that's too much, just say so and we'll do it.  Thanks very much for this! 
comment
Looks like you will need to adjust two of the tests to work with `np.array`s.  
comment
I think this is ready.    @rossbar, should changing the type of a return value from "always 2-tuple of numbers" to "always np.array of length 2 of numbers" be marked as an API change (I think yes)?  And does it require a deprecation (I think no... but I'm never sure)? 
comment
It is extremely unlikely that NetworkX affects the time needed to write files to disk -- or internet storage. The reason is that we don't actually do the writing... other parts of python do. We call them. If you are writing something as a csv file, networkx is not writing the csv file.  I would suggest trying writing it to a local disk to make sure that is fast -- and then writing a different dataframe (not NX related) of similar size to blob storage and see whether that is slow.
comment
Thanks for this -- How did you conclude that the existing code is quadratic in the number of graphs?
comment
It seems that changing `==` to `<=` in the `return` line allows self-loops to be handled too.   But, it appears to me that the algorithm is implemented inefficiently for python. We are subtracting the cost to indicate membership in a set.  We then subtract 0 from future lookups involving that node.  Better to add the minimal cost node to the set right away and replace the future subtractions with a check for membership in the cover.  In my simple tests, this takes less than half the time and, of course, fixes the self-loop issue as well.    ```python     cost = dict(G.nodes(data="weight", default=1))     cover = set()     for u, v in G.edges:         if u in cover or v in cover:             continue         if cost[u] <= cost[v]:             cover.add(u)             cost[v] -= cost[u]         else:             cover.add(v)             cost[u] -= cost[v]     return cover ```  Either way, we need some tests to make sure the successful treatment of self-loops is not broken by later changes. Can you add a test to `tests/test_vertex_cover.py`?
comment
The shortest_path function ignores edge weights unless you specify e.g. `weight="weight"`. The `all_pairs_dijkstra`-like routines use a default edge attribute of "weight".  So, the way you used the functions, `shortest_path` returns the path with fewest hops, while `path_all` holds shortest weighted paths as sums of the edge attribute "weight". 
comment
The latest decorator version is now listed as 5.0.1 on PyPi so I don't think you will have to worry about 5.0.0 for very long. Just hold on for a couple of hours and it should all go away. :} 
comment
It looks like this change in behavior will not be fixed by a future release -- that is, it is a change in the API for `decorator`. We are working on a fix. In the meantime pin decorator to <5. 
comment
Decorator v5.0.5 has been fixed.  It now provides the v4 behavior by default and allows the new behavior with an optional flag argument. We should be able to use v5 now.
comment
I can verify that decorator is broken again in 5.0.6
comment
This is now working with decorator 5.0.7  It doesn't work with 5.0.0-5.0.4 or 5.0.6 It does work with <=v4 and with >=5.0.7. I'm not sure about 5.0.5 because I'm not sure why it was passing tests when it didn't have the fix put in for 5.0.7. 
comment
Try deleting both decorator and networkx and then reinstall it to get the latest version. You should probably also be using the anaconda channel "conda-forge" (seems to behave better).
comment
The random_state decorator error does not look like the same error as this Issue. That is, I do not believe it is due to having a specific version of the decorator library.  Have you determined that it *can* be fixed by changing the version of the decorator library?  If the decorator library version has not been determined to fix this error, can you open a new issue with more details about what versions of networkx and Python you are using, and maybe a short example of code that reproduces the error?  
comment
Ahh.. so it is repeatable --- but what is it that they were doing? :}
comment
The function is available in NetworkX 2.6.2.  I haven't tracked down which version it became available. The calling name is just as you expected:  `nx.generators.geometric.geometric_edges`  But I think it makes sense to publish this function to the public interface so that `nx.geometric_edges` would work.
comment
Data attributes that have list values are not part of the GraphML spec. That is, you can't store graphs with list or dict attribute values using the GraphML standard. See #485 #3663 for similar discussions.  I would suggest that you save the graph in a different format.  OR you could clear out the offending data attributes before converting to GraphML.
comment
Yes -- thank you @rossbar for your restatement and sorry that my "checklist" is not clear or correct.... :{  There is currently a check in `_multisource_djikstra` two lines that check if the source is in the graph.  Because that is now done in the calling functions it should not be done in the called function.  So those two lines should be removed from `_multicourse_djikstra`.  At least I think that is correct...  Is this more clear?
comment
> Also @dschult I took the liberty of "dismissing" your initial review since there have been a number of additional changes since the original approval. I just wanted to make sure you had a chance to get eyes on the new changes, I hope you don't mind!  I don't mind at all -- I have wanted to dismiss my own approval in the past and not found how to do it -- so thanks for this! :}
comment
It should raise a networkx.NodeNotFound exception. The error message should probably say which node was requested and that it isn't found in the graph.   something like:  f"source {source} was not found in the graph" Similar for the other cases.
comment
We need a second "approval" (someone else to look at the PR) and that may be slower than usual due to lots of people traveling during August and there are the "August finish-up the summer projects" distractions as well. I would expect someone to take it up at some point in the next 2 weeks. Maybe today! (due to this exchange) we'll see... Thanks for the gentle nudge!
comment
Very Nice! Thanks!
comment
So, can we make this issue into a request to   - fix the link to the GML standard - add the literal_stringizer to the "See Also" section of the doc string. - maybe include `literal_stringizer` in the main namespace.    Anything else?
comment
I agree!  Thanks @rossbar  Adding `literal_stringizer` to the See Also section should be sufficient.  No need to add it to the main namespace.
comment
`None` is a useful way to check for whether someone set a value or if some default value should be assigned.  For weight this usually is used as a switch to turn off a weighted method. That seems to be the case here. `None` is a token to mean that no edge weight is used.  But later in the file here, `weight` is used on line 215 in an if clause `if not weight:` which means it cannot take any values that resolve to False when converted to bool.  For example, `weight` could be the value 1 but not 0.  Anyway, line 215 should be `if weight is not None:`.  It'd be even better if `weight` used the paradigm that `weight` uses for shortest _path, where it can be a string, or a function of the data dict for that edge or None.  See #4114 
comment
An unweighted graph can still provide a useful geometric spanner.  If you want the spanner without weights, it is more efficient to ignore weights than to set them all to e.g. 1. I don't know how much difference it makes, but for other routines it is significant.  We often provide an unweighted version of algorithms alongside the weighted version. (Also, spanners are not only used with geometric networks... So "useful" might mean "any" in other settings.)
comment
What is it you are looking for?  That is what would "reasonable results" look like?
comment
My understanding is that this algorithm does what you want. Why do you say the results are not reasonable?
comment
Reopen this issue if needed. 
comment
Now you are running into another bug identified in #5000 and #4992 We have an idea how to fix that bug, but it is not yet implemented in the codebase.   Ack....
comment
We are thinking of making a 2.6.3 release once we get the bugs sorted out.
comment
It will not be within a week. I am hoping for a fix within two weeks and a release within a month.  In the meantime, as a solution a short term fix. The floating point round-off issue (that results in a KeyError) should be fixed by scaling all edge weights in the graph by the total weight of all edges in the graph.  The floating point round-off error comes from dividing (in the formula) by (2*m) where m is the total weight of all edges in the graph.  If you can multiply all your edge weights by m (or even 2*m just to be safe, but division by 2 should not introduce much round-off error), then the values in the numerator should cancel those in the denominator. I'm not sure that this will always work because there may be some round-off before the cancelation. But I think it should work.  Your edge weights are all integers, m will be an integer. Definitely worth a try.  Can you test that with the case that gave you this most recent error?  If that doesn't work, I'll try to get a code fix posted in this issue that could replace that function.
comment
You could also download/copy/paste the solution in #5000 where @tristanic rounds the values to ensure the keys are the same when they are close.  If that function works for you please let me know. 
comment
Unfortunately, GraphML doesn't support dicts or lists as attribute values.  You can convert them to strings, but then you have to convert them back to dicts when you read them.  The reason the error is a KeyError is that the code is looking up the type of the value (a dict) in the dictionary of types that our writer knows how to handle.   The poor error warning should be fixed by a recently submitted pull request #5058 
comment
Could we avoid the floating point trouble by using a term other than `q0` in our computation of `dq`?  Said another way, can we sort based not on dq but based on a scaled version of dq such that q0 does not appear in dq?  Multiplying the modularity expression by `m**2` removes `q0` from that expression and gives the line: ```python     dq_dict = {         i: {             j: 4 * m * G.get_edge_data(i, j).get(weight, 1.0) - 2 * resolution * k[i] * k[j]             for j in [node_for_label[u] for u in G.neighbors(label_for_node[i])]             if j != i         }         for i in range(N)     } ``` I'd want to check for other impacts of scaling the leading value in the 3-tuple used in the MappedQueue.  But this should get rid of the problem of a KeyError...
comment
Wow -- those dq values are **really** huge (~1e+16).  floating point is not good enough to hold integer values that large without roundoff causing trouble even for integers.  Perhaps we need to convert to integers to hold these really large integer numbers.  Thanks very much for this.  Can you give me a small example that breaks?  Or just tell me what kind of size your edge weights are and how many edges for the network?  Thanks!! 
comment
Here's a smaller example: ```python G=nx.Graph() G.add_weighted_edges_from([(0, 1, 12), (1, 4, 71), (2, 3, 15), (2, 4, 10), (3, 6, 13)], weight="weight") nx.community.greedy_modularity_communities(G, "weight", resolution=0.99) ``` Most "simple" values of resolution work here without an error, but if the resolution causes roundoff error -- as 0.99 does -- then we still get that KeyError.  So we can easily construct examples that break it. 
comment
OK... I've looked at this some this evening and I think I have a way forward.  The trouble is coming from using floating point values as keys in the MappedQueue class. This MappedQueue class uses a python heapq but adds to it the ability to remove objects from the queue. And it does this by storing a lookup dict.  The dict uses the value stored on the heapq as a key to look up the index in the heapq.  But -- our values are 3-tuples with a floating point value as the leading entry and the 2 nodes which form the edge fill out the 3-tuple.  We need the floating point value to provide the lowest valued edge.  But (in our usage!) the edge that fills out the 3-tuple is unique to that 3-tuple in the queue. Note that if we expand this to MultiGraphs, we will likely need a 4-tuple to store the edge key as well as the two nodes and the floating point value.  We are currently getting a KeyError due to floating point problems with the key in the lookup dict. But we can rewrite MappedQueue to use the 3-tuple for priority, and all but the first value in the tuple as the key in the lookup dict.  This makes MappedQueue much less general.  It should probably be removed from `nx.utils` if we do this.  But this is the only function in NX that uses MappedQueue. So that's probably OK.  My strategy...   thoughts appreciated... 1) review and merge #5007 to make all this work with DiGraphs and MultiGraphs and MultiDiGraphs. 2) In the same PR: - rewrite MappedQueue to use a full tuple element for priority and all but the first entry for element removal. - move MappedQueue out of nx.utils and probably rename it.  This will likely need a deprecation (removed in v3.0). - rewrite `greedy_modularity_communities` to use the new data structure.  Thoughts anyone?
comment
Niiice...  :}     Other speedups include `def __iter__(self):` instead of (or in addition to `__getitem__`) and in `__init__`, define `self.data` as well as `self.priority, *self.edge = self.data` and then order and has the priority and edge. There may be speedups in `MappedQueue` that help too, but these are pretty straightforward ones. Thanks for this!  I'm going to rebase this on the main because `n_communities` was added recently.
comment
I took a look at the MappedQueue data structure and also the `utils/heaps.py` module.  It seems that both were created as one-off use cases and put in utils so other parts of the code could use them. But there is a lot of overlap and subtle yet poorly documented differences between them. I'd like to see either a single heap with lookup/remove/replace, or a few classes that are clearly documented about their differences and similarities. But that may be a long term solution to a short term problem.  I suspect the best thing for now is to fix MappedQueue and get a new release out. Then we can refactor the utils heap data structures another time.  My understanding is that the fix for MappedQueue (as you have said) is to separate the priority from the object when inserting -- using the priority with the heapq heap while storing positions using a dict keyed by the object without the priority.  This is similar to your linked-to code but that code requires the object be of a specific form which I don't think we need to do here. It is also similar to the `_ProxyHeap` idea in this PR, but it effectively puts the split between priority and object into the Data Structure so the user doesn't have to construct an object just to put it on the queue.    I'm still planning to work on this sometime soon, but don't seem to be getting to it. I would welcome a PR that fixes MappedQueue and makes it more readable. I will get to it "soon" but don't know when exactly. :}
comment
Here's an example taken from the test I added in #5065   ```python G = nx.Graph() G.add_weighted_edges_from([(0, 1, 12), (1, 4, 71), (2, 3, 15), (2, 4, 10), (3, 6, 13)])     expected = [{0, 1, 4}, {2, 3, 6}]     assert greedy_modularity_communities(G, weight="weight") == expected     assert (greedy_modularity_communities(G, weight="weight", resolution=0.99) == expected) ``` The second assert statement leads to a KeyError in the current code.
comment
I'm going to merge this.  I has the 2 approvals so that makes sense, but I wanted to think about whether it was easier to merge before or after the fixing of the floating point keys in the mapped_queue dict.  I understand that problem better now and have a solution -- but more relevant to this, I think it will be easier to merge this first... then work to put the other fix into the code alreay working with DiGraph and MultiGraph.  Thanks for this code @ThanasisMattas and for the help with the other modularity issues! 
comment
See #5000 and connected issues.  The problem is that if the `dq` values are floating point numbers, round-off error causes KeyError or faulty lookups. The dict in `MappedQueue` needs to use the element for dict lookup and the `dq` value for comparison. ​See #5065 for the fix I am looking for reviews on.  It is slightly slower, but works with floating point edge weights and non-unity resolution values.
comment
@elplatt is this designed to work with directed graphs?  If not, we should put the ```@not_implemented_for('directed')``` decorator on the function. Also helpful to know if it is supposed to work for multigraphs.   
comment
Take a look at the weighted graph code for dijkstra (in ```algorithms/shortest_paths/weighted.py```) for a standard treatment of inputing a weight function or weight attribute name. You can probably use the same function to process your input.   While you are in there, take a look at the doc_string formatting. For sphinx to make it look nice it is supposed to have a one-line short description followed by a blank line and then a short paragraph (I think this function has a short paragraph).  Then sections for paramaters, returns, Examples, References and/or Notes and See Also...  none of these are mandatory, but they might make the documentation look better after sphinx processes it.  Thanks very much for looking into expanding it for directed and multigraph! :)
comment
I don't understand.... [1,2,3,4] is not a cycle in this network.
comment
The result is a perfectly good basis for the cycles. To obtain [1,2,4,3] just take [2, 4, 6, 5, 7, 8, 9, 1] - [3,4,6,5] - [3,5,7,8] - [3,8,9,1] The edges 1-2 and 2-4 appear in the 1st term, 4-3 comes from subtracting the second, 3-1 from subtracting the fourth. Other edges cancel out: 9-1 and 8-9 in 1st minus 4th, 8-3 in 3rd and 4th, 8-7 and 5-7 in 1st and 3rd, 5-3 in 2nd and 3rd, 5-6 and 6-4 in 1st minus 2nd.  Just like coordinate axes are not unique, cycle bases are not unique. This one is perfectly fine. What don't you like about it? Is there a reason you want [1,2,4,3] instead of the big cycle?
comment
By design, nodes can be anything, so you can make a subgraph be a node and create edges to that subgraph. That will not easily construct the picture you show. But it can store the information in that picture.  ```python G = nx.Graph([("A", "B1"), ("A", "B2"), ("E", "B1"), ("E", "B2")]) H = G.subgraph(["A", "B1", "B2"]) G.add_edge("D", H) nx.set_node_attributes(G, {(n if not isinstance(H, nx.Graph) else "C") for n in G}, "node_label") ```
comment
The subgraph is a view on the graph. But you can make a copy of it and then it is a read/write graph just like G is. ```python H = G.subgraph(["a", "B1", "B2"]).copy() ```  As for labels, the default labels in the matplotlib drawing package just use `str(node)` to get the labels. In order to use the `node_labels` node attribute as the labels, you'll have to use the `nx.draw_networkx_labels` and maybe [other features of the drawing subpackage](https://networkx.org/documentation/stable/reference/drawing.html).
comment
I think you are missing the distinction between adding a graph as a node and adding all the nodes and edges of a graph.  `G.add_node(G1)` only adds one node to `G`. There are no "nodes of G1" in the graph `G`.  If you want them in G, you have to add them too: `G.update(G1)` -- but in G the nodes "A" and "B1" are still not known to be part of G1. That info is stored in G1, not G.    You say you want `G.add_edge(G1["B1"], "E")`.  But `G1["B1"]` is an adjacency of the neighbors of "B1". I think you want `G.add_edge("B1", "E")`  Right?
comment
So, you have to decide what it means to add an edge from a node of G1 to a node of G. A Graph is a set of nodes and a set of pairs of those nodes called edges.   If you `G.add_edge("B1","E")`, then "B1" will be a node in both G and in G1. The edge between "B1" and "E" will be in G, but it will not be in G1. Indeed there will not be a node "E" in G1.   If you draw G you will see G1 as a node in G. You will also see "B1" and "E" as nodes in G. The graph G will not know that "B1" is also in G1. G1 will know that "B1" is a node in G1, but it will not know that "B1" is also a node in G.
comment
Think about what you are saying/wanting a little more.  Every edge must live in a graph. That means the two nodes must be in that graph.    Your syntax `G.add_edge(G1["B1"], "E")` doesn't make sense because G1["B1"] is an adjacency dictionary with keys that are the neighbors of "B1" in the graph G1.  The node you want is "B1", not G1["B1"].  Said another way, I think you are viewing the nodes as being attached to a graph. The nodes exist as python objects independent of a graph. So "B1" is a python object that can be in both G and in G1 (or not...). Edges are **NOT** python objects independent of the graph.  The edge only exists within a graph and it represents a relationship between nodes in *that* graph.  I think what you want is to add all the nodes from G1 to G. Then you cn connect them as in your picture. The object G1 can also be a node in G. But G will not make use of any relationship between G1 (what the drawing calls Node C) and the nodes.    What are you actually trying to do in the long run/big picture?
comment
`G.add_edge("B1", "E")` does what you want.  
comment
I'm a little surprised by the idea that these don't apply to undirected graphs.  It seems to me that they are well defined and work just fine for undirected graphs. Is there a reason to keep people from using them that way? I generally prefer not to restrict application of a function unless using it in that context will do harm or cause confusion.  The changes look appropriate and add some nice cleanup and even speed-up to the code. I wonder whether switching from `shortest_path_length` to a more generic BFS or DFS function would be faster, but it probably isn't. :}  At the very least it should be changed to `single_source_shortest_path_length` or similar. but maybe to `{child for parent, child in nx.generic_bfs_edges(G, source)}`.  If there isn't much difference, stick with shortest_path.
comment
Yes -- I agree that they were made for DiGraphs and were implemented first in the special case of DAGs.  But they work for any DiGraph and in fact work for any Graph. (I can't come up with a name that provides the undirected version of this -- maybe `connected_component`?) In any case, if it works correctly and is well-defined for the Graph case and doesn't cause any confusion by doing so, I think we should let people use it this way. Otherwise we either direct them to the right function name, or automatically call that other function...  but might as well just run this function. :}
comment
Thanks @ericmjl for this summary. I agree that it would be helpful to have a sentence or two in the documentation about how an undirected graph will be handled by allowing both directions for each edge. That is quite pervasive in this library. It even goes as deep as the choice to use 2-tuples to represent undirected edges instead of frozen-sets.  It's quite possible we made a mistake in that choice, but that is another discussion. I think most people can fairly quickly understand this idea if it is explained.  An undirected edge is treated as allowing connections in either direction. (or similar).  Of the two options, I prefer option 1) where we add some to the doc_string and move these functions out of `dag.py`.  The moving out of `dag.py` is much less important to me. I think we should all be using the `nx.` namespace. People only started using the full path to the code when our documentation started using the full path to the function names and we said we would look for a way to avoid that -- but it hasn't happened yet. Perhaps we need to start forcing people to use the main namespace, but that seems a little strict too. :}  If we move it, perhaps we can provide an alias for it in the old location (not advertised in the docs, but available for backward compatibility).  @rossbar is there any precedence for doing something like that?
comment
The current release of NetworkX **does** lazy load numpy -- in the sense that it only imports numpy if you are using a function that requires numpy.  Pull Requests like #4909 discuss hopefully better ways to lazy-load. But the tried and true -- don't import until the function that needs numpy is called -- should work.  If you see that importing networkx causes numpy to be imported, please report that as a new issue for NetworkX. That means some part of networkx is actually importing numpy before it needs to.  That should not be happening. 
comment
See my [comment](https://github.com/networkx/networkx/issues/4956#issuecomment-888668233) in #4956 where I suggest that the proper "distance" between nodes from different components is infinity.  Then the portion of the score due to distance is zero (and common neighbors is also zero because they can't have common neighbors either).   I think the right fix is to change the shortest_path dictionary to hold path lengths and then use `inf = float('inf')` and ```python     spl = nx.shortest_path_length(G)      def predict(u, v):         return alpha * sum(1 for _ in nx.common_neighbors(G, u, v)) + (1 - alpha) * (len(G) / (spl[u].get(v, inf) - 1) ```
comment
Did you mean to change the denominator to path_len instead of path_len - 1? Also, you should define float(“inf”) outside the function call so it only has to be defined once (the second argument to get is computed before calling… one of the downsides of the get method).
comment
The function `common_neighbor_centrality` is calling `nx.shortest_path` when all it uses is the length of that path.  It should call `nx.shortest_path_length`.  That will cause the same problem with disconnected graphs, but it will be easier to fix. The two options given by @rossbar above should be augmented by a third option to have the length be infinity.  I think this makes the most sense in this case because an infinite length means that term of the formula becomes 0 so there is no contribution to the probability from being "close" to the other node.  I prefer it to option 1 (raise an exception) because presumably we want to be able to predict link formation between the components.  Option 2 (computing for each component separately) is a good approach for algorithms that focus on connections. But this question of link prediction focuses on **missing edges**, not connections. So treating each component as their own separate graph is not as compelling to me.    I've looked through the Ahmad paper, and it doesn't talk directly about the case of disconnected graphs, but they do talk about having a distance metric between nodes. This is what made me think of using a metric that includes infinity as a possible distance. (not even sure it that violates the definition of metric, but my intuition is that we want a very large distance in this case --> zero contribution toward the probability of an edge forming.  Thoughts?
comment
I think instead of copying the graph at the beginning, we can make a new graph with only the weight attribute. Then we don't have to treat each level differently.  Something like this: ```python graph = G.__class__() graph.add_weighted_edges(G.edges(data=weight, default=1)) graph.add_nodes_from(G) ``` This should work for multigraphs too, but we should include a test to make sure that is maintained.
comment
Some comments on the tests: - we should set a seed for any random aspects of the tests. That way they don't create intermittent failures that are hard to track down. So, if you are going to use `randint` to generate random weights, you should also import `random.seed` and set a seed for that test. - The test for `weight=none` should also test that the weights change the partition. Otherwise we can't tell whether using "None" is any different from using "weight". Perhaps it would also be good to also test a name for weight other than "weight".  This would require calling louvain twice -- once with weight="foo" and once with `weight=None`. Then assert that "foo" makes it not match `part` and `None` makes it match `part`. - The `nx.set_edge_attributes` call doesn't need to be inside a loop. The loop can be put inside the input parameter.  ```python random.seed(1234) nx.set_edge_attributes(graph, {edge: randint(1, 20) for edge in graph.edges}, name="foo")  ```
comment
The Petersen Graph constructor (generator?) works from an adjacency list data structure that contains both directions for each edge. When create_using is MultiGraph, this will double each edge. When create_using is a DiGraph, it puts edges with both directions. With Graph is creates a single edge -- though it actually does this twice.  Constructing the graph using a nx.Graph puts a single edge. So `nx.MultiGraph(nx.petersen_graph())` has only single edges. While `nx.petersen_graph(create_using=nx.MultiGraph)` has double edges.  Fun fact:  `nx.DiGraph(nx.path_graph(5))` makes an edge going each direction, while `nx.DiGraph(nx.path_graph(5).edges)` makes a single directed edge for each step along the path. 
comment
There is another issue #3112 that asks for community detection algorithms for directed graphs. The [pdf linked to from that issue ](https://github.com/elplatt/Paper-CNM-Modularity/blob/master/paper.pdf) is not a refereed paper, but gives a reasonable overview with definitions of modularity and directed graph treatments.  Before jumping into that too far, it might be reasonable to think about whether to separate that feature as a separate PR, or just build it in to this one.
comment
It might be worth unrolling the `while` loop slightly, use`while improvement:` and use `return` instead of `break` inside the loop.   I also notice that you currently don't yield the last partition computed if the change in modularity is below threshold. It might be good to go ahead and yield that last partition since it is already computed.   What do you think of something like this? (untested) ```python     partition, inner_partition, improvement = _one_level(         graph, m, partition, resolution, is_directed, seed     )     yield partition     while improvement:         new_mod = modularity(             graph, inner_partition, resolution=resolution, weight="weight"         )         if new_mod - mod <= threshold:             return         graph = _gen_graph(graph, inner_partition)         partition, inner_partition, improvement = _one_level(             graph, m, partition, resolution, is_directed, seed         )         yield partition ```
comment
Yup!   You are right.... :}  Many ways to get it right I suspect...
comment
This looks good!  Thanks @z3y50n !!
comment
It looks like `descendants_at_distance` is [called by ](https://github.com/networkx/networkx/blob/e8914bb5681b6fad8a6764406d3c7a78ebc582ae/networkx/algorithms/dag.py#L624)`transitive_closure_dag` in the `dag.py` module.   But that just means even more -- we should include tests for it. :}
comment
Should we merge this and then work on a PR for fixing it elsewhere?  Or should other classes be included in this PR?
comment
Great!   That clarification really helps...  So we don't even need to worry about the other classes -- adding autoclass or anything like that.  Nice! :}
comment
Why do you say unexpected?  When you change the p_dist form, you are going to have to adjust the threshold (unless you adjust it implicitly in your p_dist function).  I wouldn't know what to expect unless I thought it through.  In this case, 1/r**3 is going to be much bigger than 1/r**2 because r is the metric on nodes placed within the unit square. So I would expect many more edges unless theta is similarly divided by some sort of average value for r.  Similarly, if you rescale the metric to use cm instead of m, you would have to rescale theta to use the same units if you want to have the same number of edges. I'm guessing an average value of 1/r is about 2, so either use 0.5/r**3 or set theta to 200.  That's a guess though -- more complete examination of average 1/r would tell you more. 
comment
Those imports work for me... but you should be able to do: ```python import networkx as nx nx.lowest_common_ancestors(G, u, v) ```  I notice that you are using Python 3.6.7 and NetworkX 2.6.2 doesn't officially support Python 3.6.  But I don't think that should cause this error.
comment
NetworkX supports Python 3.7-3.9 but should work for most tasks with 3.6-3.10. I would get 3.9 as that's the latest (3.10 is prerelease still) but again, it seems unlikely that is the trouble. You might also see whether you can just import networkx. Perhaps you are running a python from your operating system, but installing networkx on a different installation of python?  That's a common issue.  Check which python you are actually running and also which has networkx installed.
comment
If you can import networkx then it isn't a problem with it trying to find the wrong installation.  Most of networkx is available in a flat namespace from the main module. So: `networkx.descendants`, `networkx.ancestors`, `networkx.lowest_common_ancestors`. So you shouldn't need this feature.  But.... I don't understand why it isn't working... unless there is a typo somewhere... It works for me as expected.
comment
It looks to me like `**kwds` should be removed as you suggest. Thank you! 
comment
Looks like this builds on #4690   Thanks for this!
comment
How are you counting the cycles to say that it only has 100-200 cycles? Generally, constructing a graph with cycles will generate far more cycles than you might think. The cycles you know are there can "add" together to create other possible cycles. 
comment
That code is very close to the NetworkX function [random_geometric_graph](https://networkx.org/documentation/stable/reference/generated/networkx.generators.geometric.random_geometric_graph.html) though it does choose the radius where edges are formed in a way to make the expected number of edges be a controlled value. Our function allows you to select the radius instead, but they can be mapped from one to the other.  This Issue is not about creating random geometric graphs. It is about creating "planar graphs", which are defined as graphs for which the nodes can be placed in the plane so that no edges cross other edges.  Thanks for the offer of a new function. But I think we already have that functionality. If I'm missing something please open a new issue.
comment
I haven't looked at the code much yet. I can make a comment on the name though... SNAP is a big project with many aspects to it. It refers to much more than summarization.  Can we find a better name for what this algorithm does? Even `snap_summarization` is  more communicative than `snap`. Even better would be a word that describes what it does. Maybe `snap_aggregation`?
comment
Great!  Thanks for that overview.   For step 2) does "have edges to nodes in the same groups" mean *all* nodes in the same groups?  Or is it OK an edge to *at least one* node of the other group?   Groups {a,b} and {c,d} with edges a-c and b-d. Would the splitting stop at this point?  **How to start?** One Group {a, b, c, d}  any edges you like between those 4 nodes...  This looks like it satisfies stopping step 2).  Thanks! 
comment
OK!   Thanks for that!   We've merged your fix.  I guess that means you need to pull/rebase from networkx:master to avoid the trouble in this branch.
comment
This looks pretty good.  Thanks for the addition!!  You've got a test to look at the supernodes. Can you add some tests to make sure the entire code is working as you'd expect? Maybe a full (but small) example where you know what it should be and can compare to that. (Or maybe the examples file already does this.... I didn't look at that.)
comment
Thanks for the nudge. :}    I don't understand the term "relationship type" or "relationship key".  Is there a more revealing name? Does relationship have something to do with edges? And "key" is confusing especially since the returned variable is named "value". The terms "key" and "value" have specific python meanings for dicts. Is there a better term for either of these?  Miscellaneous comments follow:  - It looks like `snap_relationship_keys` is a one liner:  `{snap_relationship_key(G, edge, attrs) for edge in G.edges}` Is it worth making a function for that?  - Similarly, what would `snap_relationship_key` be used for? Is it:  `tuple(v for k, v in G.edges[edge] if k in relationship_attributes)` And is there a reason for changing the empty result to `None` as is done in the code at present?  There may be reasons for these choices that I don't understand   - The documentation is not getting linked to the reference section of the docs. Can you add to `/doc/reference/algorithms/summarization.rst`?    - Can you go through the doc_strings and check that each starts with a single line short description followed by a blank line followed by a paragraph saying what the function does and how it does it.   - Can you remove the `logger` stuff -- or explain why we need to add that as a requirement for the library?  - In your examples you are calling the function using the whole path to the function instead of `nx.snap_aggregation`.  The shorter form is better for examples and allows us to move code more easily.  - The karate_club example in the doc_string doesn't actually show anything about what is output. It is useful to show possible input values, but is there an example where you look at the supernodes or the output more generally that would help explain to a new user what the function does and/or why it might be useful? 
comment
These errors are due to an error in a new scipy release. I'll get that fixed. I'll make time to go through what you improved too. I hope sometime this week, but maybe on the weekend.  Thanks!
comment
Looks good -- can you remove `edge_attributes` from the inputs to the functions that don't use it?  It looks like I forgot about having `edge_types` store both directions of the edge. That makes the creation step slightly uglier, but saves the failed lookups in the inner loop. To correct that we have to replace `for edge, attrs in G.edges.items()` which only returns one direction of an undirected edge with `for node, nbrdict in G.adjacency() for nbr, attrs in nbrdict.items()` and then in place of `edge` put `(node, nbr)`.  Come to think of it, is this algorithm suppose to run with DiGraphs?  What about MultiGraphs? To make it work with those, we could try a different approach -- after creating `edge_types` we can add the other direction like this: ```python edge_types.update(((v, u), edge_type) for (u, v), edge_type in edge_types.items()) ``` Then of course remove the `if` code from the inner loops that switch the node order in the edge.  I think you can remove the function that constructs the groups also by creating `groups` and `group_lookup` using the nice `collections.defaultdict` structure: ```python group_lookup = {node: tuple(attrs[attr] for attr in node_attributes) for node, attrs in G.nodes.items()} groups = collections.defaultdict(set) for node, node_type in node_types.items():     groups[node_type].add(node) ```  `snap_is_eligible` also seems like it has to compute the same thing each time it is called and it might be made faster and easier to read. I think with the recent changes that you've got `neighbor_groups[node][other_group_id]` to be a Counter. If so, you could avoid the sum of the values by just checking the length of the Counter.  Counts aren't stored unless they are more than 0 so if any of them exist the sum will be positive and `group_edge_count` can be incremented. ```python # Are there any connections to the other group? if neighbor_groups[node][other_group_id]:     group_edge_count += 1 ```  I have a question about a loop in that function too... It loops over all `other_group_id`s. But doesn't `current_group` come up one of the times?  How do we keep it from returning True in that case? Or maybe we want it to.... I get confused.
comment
OK.... now looking at `snap_split` and `snap_update_groups`.  The fancier data structures allow us to simplify the updates to the point where we should just in-line that code and remove one function from the API.   `snap_update_groups` can avoid using len/sum/range/zip by assigning `new_group_id` to be `len(groups)`...  it works out the same because `groups` grows at just the right rate to make it what the current code makes it.  ```python # keep the first group in it's current group and group_id for new_group in new_groups[1:]:     new_group_id = len(groups)     groups[new_group_id] = new_group     groups[old_group_id] -= new_group     for node in new_group:         group_lookup[node] = new_group_id ```  That's short enough to just include in the function that calls it (`snap_split`). The `snap_split` can save a fair amount because of the `defaultdict` and `Counter` data structures. The dict `new_group_mappings` should be a defaultdict(set) so you can avoid the `setdefault`.  The next change I'm suggesting removes the `tuple(sorted(...))` portion. Instead of sorting, it'd be nice to use `set`s which compare irrespective of the order. That allows the `edge_types` to be non-sortable objects, and should speed things up too. But we can't use sets as key's in the `new_group_mappings` dict. So, use `frozenset` instead. Finally, we can take advantage of the Counter in `neighbors_groups[node][grouping]` to avoid storing all edge_types with a boolean for `count>0`.  With a Counter, the only keys that iterate are the keys with count>0.  So the `grouping_signature` is not longer needed: ```python # Construct new groups based on edge_types to the other groups new_group_mappings = defaultdict(set) for node in groups[old_group_id]:     signature = tuple(frozenset(edge_types) for edge_types in neighbor_groups[node].values())     new_group_mappings[signature].add(node) new_groups = list(new_group_mappings.values()) ```  When combining the two functions you could think about whether to sort `new_groups` by length to use the largest group as the "old" group_id. If sorting is likely to take longer than saving moving a few nodes, don't bother changing. If you think it might help, use code like:  ```python new_groups = sorted(new_group_mappings.values(), key=len) for new_group in new_groups[:-1]: ```  I also noticed that these functions return `groups` but they don't actually need to because the original `groups` was input and was updated in place.  But the API may be nicer to avoid thinking about inputs getting updated. Up to you....  Thanks!   And if you prefer I can save these kind of things up until I've looked at all the code and just make one comment...
comment
When you construct the graph, it looks like you add the same edge multiple times with different edge attributes. ```python for edge_type in neighbor_groups[node][other_group]:     edge_attributes = dict(zip(edge_attributes, edge_type))     output.add_edge(*summary_graph_edge, **edge_attributes) ``` Two problems with how this is set up...  1) edge_attributes is turned into a dict instead of a list on the first time through the loop. Luckily this doesn't impact the rest of the loop because iterating over a dict that has the same keys as elements in a list gives the same result.  But still, did you intend to name this something different? 2) Adding the same edge with different values for the same attributes overwrites the previous edge.  So only the last set of edge attributes will actually be present in the `output` Graph. ```python G.add_edge(1, 2, color='red') G.add_edge(1, 2, color='blue') print(G.edges[1, 2]) ``` gives output `{"color": "blue"}`.   What do you want here?  Should there only be one edge_type? That's what I am figuring. If so, maybe remove the loop and use: ```python (edge_type,) = neighbor_groups[node][other_group] edge_attr = dict(zip(edge_attributes, edge_type)) output.add_edge(summary_source_node, summary_target_node, **edge_attr) ``` But I might be missing something.  [EDIT:  [EDIT-of-the-edit:  As pointed out below, if multiedges are possible we need to loop over the edge types. So this suggestion is not relevant.] It seems that some `other_group`s have no edge types. So maybe something like: ```python for other_group, etypes in neighbor_groups[node].items():     if etypes:         (edge_type,) = etypes         edge_attrs = dict_zip(edge_attributes, edge_type))         output.add_edge(summary_source_node, node_label_lookup[other_group], **edge_attrs) ```  ]
comment
One more question on graph construction: The `deterministic_labels` option requires that the original nodes be sortable... More precisely, it requires that the first elements of `supernode_lookup[k]` are sortable, which if I parse this right are the first node in each group_set. Those are original nodes, so the original nodes must be sortable. NetworkX doesn't require sortable nodes (think of a Graph whose nodes are Graphs).  Furthermore this doesn't enable the supernodes to be the same if the same graph is summarized twice. Python sets are not ordered, so the first element chosen from a `group_set` may not be the same between two summarizations. So the representative node from a group might be different and they would sort into a different order.  Does this make sense?
comment
Here's a speed up for `snap_is_eligible`. We can push the inner loop into CPython for speed by using Counter.update: Also, we don't need to check for size 0 because the Counter will simply not report those edge types. ```python     current_group = groups[group_id]     group_size = len(current_group)     for other_group_id in groups:         edge_counts = Counter()         for node in current_group:             edge_counts.update(neighbor_groups[node][other_group_id])         if not all(count == group_size for count in edge_counts.values()):             return True     return False ```
comment
Excellent -- thanks for this helpful description.   I didn't take into account that the original graph being a MultiGraph would make the summary graph a MultiGraph as well!  So adding the same edge more than once with different edge_type will work just fine.  No need to store a list of edge types because we have the MultiGraph structure to do just what we want anyway. :} I'm not sure if others will fall into the same trap I did, but could you leave the loop that adds edges and add a comment above it reminding the reader that this handles multiedges.    I made a suggestion in an Edit [above](https://github.com/networkx/networkx/pull/4463#issuecomment-803510418) that also assumes we aren't dealing with multiedges. So we can safely ignore that suggestion as well. I'll add an edit that readers should ignore that edit. :}  For the `deterministic_labels`, can we include that code in the test suite? For the tests we control that there are sortable nodes in our tests so relabeling to check the answer should be OK.  I haven't looked into whether this is a reasonable suggestion or not...  Also, we should get some simple multigraph (and digraph) examples in the tests.
comment
We could also work on getting this merged with a `not_implemented_for` restriction to only use Graph classes and then make another PR to update it for DiGraph and MultiGraph.  I'm fine with either approach.
comment
Multiedges are 3-tuples: (u, v, edge_key) where edge_key is usually an integer, but is allowed to be any hashable. You can get just the node 2-tuple using G.edges() when G is a MultiGraph.  You can get the 3-tuples by using G.edges  To make the line you ask about work don't split apart the edge: `edge_type[e] for e in list(edge_types)`
comment
Sorry -- I missed the context and the edge order swap...  You can use `G.is_multigraph()` to check if it is a multigraph. It is often best to do this outside the loop.   ```python if not G.is_directed():     # swap node order to get both directions of the undirected edge     if G.is_multigraph():         edge_types.update(((e[1], e[0], e[2]), edge_types[e]) for e in list(edge_types))     else:         edge_types.update((e[1], e[0]), edge_types[e]) for e in list(edge_types)) ```  For the code in `snap_calculate_groups`:  `G.edges(keys=True)` gives 3-tuples for multigraphs.  ```python edges = G.edges(node, keys=True) if G.is_multigraph() else G.edges(node) for edge in edges:     edge_type = edge_types_lookup[edge]     .... ```
comment
Yes, I think it makes more sense that if you start with a MultiGraph (or MultiDiGraph) you get back a MultiGraph with one edge for each edge type between the two supernodes. :)
comment
ooo.. nice work figuring out that `.keys()` is needed. That's a feature of `Counter` I hadn't thought about. I knew that `Counter.update` worked on an iterator and counted how many times each element occurs. But if that iterator is a `Counter` then it doesn't just treat it like an iterator. It adds the counts (which are like a dict's values) for the two Counter objects.  So, yes you need the `.keys()`.  That's good to realize... :}  
comment
```python     neighbor_info = {node: {gid: Counter() for gid in groups} for node in group_lookup}     for edge, edge_type in edge_types.items():         node, nbr = edge[:2]         neighbor_info[node][group_lookup[nbr]][edge_type] += 1 ``` I tested that this does the same thing as `snap_calculate_neighbor_info`.   Does it make sense that it should? Am I missing any special cases? It handles multigraph OK.  I'm still trying to see if `snap_is_eligible` could be combined with this to shortcut the process once it finds an eligible group for splitting. The code here (if correct) shows that we are looping over the edges once. I think we have to do this before we can start counting edges between groups of each edgetype. 
comment
Nice!!   This definitely cuts the computation -- it does one group's nodes at a time instead of all nodes into neighbor_info and then shifting to checking if eligible.  I especially like the part that the neighbor_info for just the one group is needed for snap_split.  I think it is easier (at least for me) to read when the eligible check is right there with the neighbor_info collection code. :}
comment
I had a thought today (while doing something else)...   Do we need to recompute the first groups in neighbor_info? If all nodes in them are pointing with a single edge type to each other group, then even if that other group gets split, the first groups won't need to be split. If we skip them, then neighbor_info for those nodes wouldn't have the right group_ids because of splitting of later groups. So I guess we'd have to go through once at the end and rebuild neighbor_info. Would it save time to only look at groups that haven't been examined before?
comment
Nice description!  That means we will definitely have to recompute the `neighbor_info` after splitting. There might even be a way to keep track of which groups must be recomputed. That would reduce the time even more, but I think that could be done in a separate PR after this is merged.   We're getting close...  I think we just have the build_graph code basically. Question:  - It's a little confusing with all the many different kinds of nodes here. Can we find names to make it more clear which does what?  We have `node`, `supernode`, `original_node`, `summary_source_node` and `summary_target_node`. My question is really about what these terms refer to.  `supernode` sounds like the node that will represent the group in the summary.  But then `node` is the word used to hold that node object.  Am I understanding correctly? - Then `node_label_lookup` seems like a dict to lookup the supernode labels.  But so does `supernode_lookup`, which holds the groups for each supernode.  Generically, dicts are described by the `values` sometimes with a phrase like "keyed by ...".   So, for example, `groups` is a dict of groups keyed by group ID. I suppose it would be more complete to call that dict `groups_by_id`. and `group_lookup` as `node_type_by_node`.  But that can get long and is often not needed. Are there better names for `node_label_lookup` and `supernode_lookup` that would help me keep straight which type of keys and values these dicts hold? - In the second loop it seems that `node` refers to what `original_node` used to be. And `summary_source_node` and `target_summary_node` are supernodes.    - The `if` logical construct for multigraph and for non-multigraph are confusingly opposite. One has a long construction leading to adding an edge and the other seems like the opposite logical expansion leading to a continue that doesn't add the edge. It's hard to tell  if the logic is the same or different for the two cases: multigraph and graph. - Near the end, a call to `nx.set_node_attributes` seems to set the supernode attributes, but I think that is done in the first loop. But I might be missing something.  - Finally, `snap_aggregation` consists of calling two functions that don't ever get called in any other way by any other functions. Would it be better to transfer the code of each of those functions into `snap_aggregation` so a reader doesn't have to track it down?  I guess some parts of the doc_strings should be merged as well. What do you think?  Let's not do it yet -- until the build_graph code is pretty much done. But think about it. :)
comment
If you are thinking that you will use `build_snap_graph` from another PR then leave it separate. But you could "inline" the `snap_find_groups`.  The logic in `build_snap_graph` for the multigraph case is still hard to read. And a couple of the if clauses can be done outside the loop. What do you think of something like: ```python                 has_edge = output.has_edge(*summary_graph_edge)                                    if output.is_multigraph():                                                             if not has_edge:                                                                       for edge_type in edge_types:                                                           output.add_edge(*summary_graph_edge, **edge_type)                          elif not output.is_directed():                                                         existing_edge_data = output.get_edge_data(*summary_graph_edge)                                              for edge_type in edge_types:                             if edge_type not in existing_edge_data.values():                                                                             output.add_edge(*summary_graph_edge, **edge_type)                      # This should not be reached... right?   If so, you could make the previous `elif:` into an `else:`                     else:                         raise NetworkXError("We should not find a duplicate edge for directed graph in summary graph") ```  Then -- on to the tests!  The current test class structure organizes the graph type cases nicely. But it is hard to see if the text code itself is the same, or if that differs. Certainly the graphs being tests change from one class to another.  To make that more clear and also to avoid duplicating identical code, you can put the duplicate code into the preliminary class `TestSNAP` that you already use for `node_attributes` and `deterministic_labels`. But if you add the method `test_summary_graph` to that class you'll need to change it's name so that pytest doesn't look for tests in it until it is subclassed.  Perhaps change the name from `TestSNAP` to `CheckSNAP`. Then add the `test_summary_graph` to it and remove from all the subclasses.   Finally, all the `test_empty` tests don't use the class structure so it'd be better to use `@pytest.mark.parametrize` to loop over the 4 graph types, testing each in turn. You'll need to `import pytest` at the top to make this work.  Something like:  ```python graph_types = [nx.Graph, nx.DiGraph, nx.MultiGraph, nx.MultiDiGraph]  @pytest.mark.parametrize("graph_type", graph_types) def test_summarization_empty(graph_type):     G = graph_type()     summary_graph = nx.snap_aggregation(G,  node_attributes=("color",))     assert nx.is_isomorphic(summary_graph, G) is True ```  Lookin good! :}
comment
Can someone take a look at this PR?  I approved it, but it needs another and it'd be good to get in before 2.6 is released.
comment
The original graph nodes are stored in a node attribute of the summary graph.  You can see them with ```list(summary_graph.nodes(data="group"))```, or create a dict from summary nodes to original nodes using ```dict(summary_graph.nodes(data="group"))```
comment
#4435 has been merged. Should this have been closed with that PR, or is there more to finish from this Issue?  I'm going to close this. If it should be reopened, post to it even though it is closed I will see it and reopn.
comment
I am +1 on removing all dependencies for NetworkX 2.6.  The default and extras options should be used when possible. But it sounds like pypi doesn't have a good way to set that up.   I don't know much about the magic that these words mean, but it seems like we need a networkx[minimal] a networkx[default] (or maybe recommended?) and a networkx[extras]. And we need pypi to allow options like this is standard `requirements.txt` files (and maybe conda use of requirements files too). It seems like a wart that pypi "requirements" files use only one set of dependencies -- or maybe I'm just missing how e.g. Pythran could require a minimal set of requirements.  Also, we should write down somewhere what the advantages of including these requirements are. We've talked about it before, but it seems that they aren't at the tip of our tongues in important moments like this.  Perhaps part of getting v2.7 released with these dependencies is to write an NXEP (or maybe a SPEC or maybe both) about how to handle dependencies. Strong buy-in for something like that could influence pypi, if needed.  Thanks for everyone thinking about this -- it is helpful!  And Thanks to Jarrod and Ross for working through so many details.
comment
We are making use of the new features of the Python language. So our new releases won't work with the old versions of Python that don't support these new features. It seems reasonable that keeping an environment that works with an older version of Python will need to use an older version of NetworkX. But those users with a newer version of Python will be able to use newer versions of NetworkX. Can you make that work in your environment?  Is there a way you can choose a version of networkx based on the version of python being used?  It sounds like that is what you need here.
comment
That's an interesting point...  NetworkX on pypy just needs decorator. So, even a one liner: ``` pip install decorator networkx --nodeps ``` should work. I'm +1 for removing the environment markers and 3.10 stuff from the requirements file.  And +1 for taking out the shell tools from the conda (and pip, which you already did) install line and hardcoding the requirement files: ``` conda install -c conda-forge --file requirements.txt --file requirements/developer.txt ```  As for your first question about `pip install -e . --no-deps`, I don't think there is a way to install from `.` in conda. I always just set `PYTHONPATH`... though that doesn't check the installation process.  It looks like you could make a `tar` or `bz2` of the packages and then install from that, but I haven't tried it.  The `pip install -e . --no-deps` works just fine within conda and should install the result into the correct `site-packages` directory within the current conda environment. So I think that is OK.  
comment
I was able to get the recipe for conda to work with adjustments. But I can't figure out the wheels stuff within Github actions. The Pypy run on MacOS is not able to load the wheels for numpy and scipy...  And I think, now that we've removed the pypy stuff from default.txt, that we need to call pip to load decorator and requirements/test.txt.  But I can't find where we tell the CI to pull in the wheels so the tests ch=rash on loading the wheels for the requirements we aren't going to use. :}
comment
I'm  not sure why this was so hard to find. The release notes for v2.6 of NetworkX explain what is going on, and the discussion here is searchable.  Maybe we need more discussion about dependencies to make it more easily searchable, but it's hard to enforce that in any quantifiable way.  PyPy does work with NetworkX -- well, with the parts of NetworkX that don't require scipy, etc. To install networkx that way you need to impose the --no-deps option (or we need to remove all requirements from our `requirements.txt` file -- which is being strongly considered).  Best would be to allow other packages to specify something like `networkx[minimal]` as a requirement rather than `networkx` -- but my understanding is that pypi doesn't have a way to do that yet. 
comment
We have pulled the 2.6.0 and 2.6.1 releases from pypi and are reconsidering the dependency structure. We test the package on PyPy with our CI and we plan to continue that. We just want a good way to make the default installation also include a few bigger dependencies so more of our algorithms are available by default.
comment
If I understand correctly, this would make  ```python @py_random_state(2) def random_stuff(G, alpha, seed=None): ``` become: ```python @argmap(nx.py_random_state, "seed") def random_stuff(G, alpha, seed=None): ```  It might be easier to get `open_file` to work if you relax the generality to make the usage: ```python @py_random_state("seed") def random_stuff(G, alpha, seed=None): ``` and `py_random_seed` is come code decorated by `@argmap` to supply all but the crucial code that transforms inputs and handles outputs.  I don't know that `open_file` will work this way, but relaxing the generality of `argmap` might help (and make the ending interface easier to read).
comment
The import time for decorator is faster between 4.4.x and 5.0.7.   Here's the last line of output of `python -X importtime -c "import networkx"` which each installed. Note: I have to run it multiple times and take the min to get systematic results. probably multitasking in background.  ``` decorator==4.4.2 import time:      1709 |     273803 | networkx   decorator==5.0.7 import time:      1067 |     155814 | networkx ``` So, it seems that while decorator does impact import time, the new version is much faster than the old. I also cloned #4739 to see if replacing decorator did better.  Though maybe if it was updated to include the newer wrap part of decorator it might be faster still. ``` #4739 armap branch import time:      1440 |     186956 | networkx ```
comment
This provides about 10% faster imports than the new decorator. I don't have a good test yet for run-time differences.  We should remove/deprecate the `preserve_random_state` function.  It looks like it used to be defined in `test_algebraic_connectivity` and in the same PR it got moved to utils and also removed from use. :}  Classic!    #2681 Should I make a separate PR for that?
comment
The use of `inspect.signature.bind` in `decorator` is due to my request for being able to handle default arguments. `bind` is the way `inspect` provides access to default arguments. I'm glad to hear you've made progress on a better way for default arguments.   My simple checks suggest that #4739 is already faster at import time than decorator 5.0.7.  But it sounds like you are working to beat both its import time and run time. Which (import or run time) is difficult to beat with 5.0.7?  Why is it fast yet doesn't do the fancy compiling and `__code__` monkey patching?  Or maybe I'm misunderstanding your results.  In any case, it's fun to watch and keep tabs on what you're developing. :}
comment
What do people think about changing nx 2.5.1 to depend on decorator <5,>=5.0.7?
comment
This looks good!  Thanks @guyroznb ! Two other centrality modules have functions using a `weight` input parameter.   In `load.py` the function `newman_betweenness_centrality` (which is aliased to `load_centrality`) has a weight meaning distance.  In `percolation.py` the function `percolation_centrality` has `weight` meaning distance.  Others use an input argument named "distance" or "capacity" so they probably don't need this. It makes me wonder whether we should change all `weight` parameters to `distance` or `capacity`. But that would cause all sorts of backward compatibility issues and `weight` is pretty standard terminology even if it isn't entirely clear always.
comment
Thanks very much for this!!  It looks like the calling order of `threshold_accepting_tsp` is not being respected in the code or in the docs. This should be fixed and tests added.  Code is near line 285 and docs in that function as well as the init_cycle argument to `threshold_accepting_tsp`.
comment
This might be a good time to talk about the calling signature of `simulated_annealing_tsp` and `threshold_accepting_tsp`.  They each have an argument `init_cycle` which can be any cycle of the nodes of `G` (including the starting node at the end).  We require that as the 2nd positional argument instead of making it an optional keyword argument.  We decided on that in order to make the user think about whether to use the `greedy_tsp` algorithm or not.  Otherwise we figured many people would use a fairly slow algorithm to get a first cycle. We make it easier for them to use this common idiom by allowing `init_cycle` to be the string "greedy".   My question is: should we make a default for this?  Other common initial cycles might be `list(G) + [next(iter(G))]` which is not as simple as you might like with `list(G)`.  And of course, you can use the output of one method as the starting cycle for another method.  Also, should we provide another special word like "nodes" or "G_nodes" or "Graph_order" to make it easier to construct/use/read the case `list(G) + [next(iter(G))]`
comment
I don't think `mapping = {x: x for x in range(m + 1)}` affects or is affected by the attributes of nodes. This constructs a mapping of nodes to integer indexes in the matrix. In this case it is mapping from the indexes created earlier to the indexes used here -- so it is the identity mapping.  The numeric attributes can range with both positive and negative values.  Also, your example gives coefficient -1, which seems to indicate that it is not ignoring the negative values of attribute `a`. What value would you expect?  Can you clarify your description? 
comment
Sorry -- my mistake -- I didn't understand the `mixing _dict` data structure.  So, would a fix be to replace `range(m + 1)` with `range(min(s), max(s)+1)`?  Also, what value would you expect for your example instead of -1?
comment
Did you mean #1480 or was the title supposed to reference #4275 
comment
Thanks!  This is helpful!!
comment
Thanks for this!!  What do you think of the comment in the "Notes" section of the doc_string that:  ``` Without specifying a `stringizer`/`destringizer`, the code is capable of handling `int`/`float`/`str`/`dict`/`list` data as required by the GML specification. For other data types, you need to explicitly supply a `stringizer`/`destringizer`. ```  It seems like this isn't quite correct if we need to supply `int` to convert to ints. Shall we change it to something like: ``` Without specifying a `stringizer`/`destringizer`, the code is capable of writing `int`/`float`/`str`/`dict`/`list` data as required by the GML  specification.  For writing other data types, and for reading data other than `str` you need to explicitly supply a `stringizer`/`destringizer`. ```  If people approve, this paragraph appears in 4 places in the file and could be changed in each place. I can do that if its too much trouble for you. 
comment
I think we should change the doc_string to match the code -- not change the code to match the doc_string. And reading to strings by default is reasonable for a string based format like GML.  What do you think of the new wording I proposed? Any suggestions? Go ahead and make the change to what you think it should be. Once the two of us agree we can request another set of eyes take a look.  Thanks!
comment
Yes.... an error occur in this case. Shouldn't it? What is your idea? Your recent two issue descriptions aren't complete thoughts and could be more helpful when restated as such. 
comment
Thanks for this -- and for your careful reading and the PR to fix things. I'm still not sure what this issue is reporting though. Are you saying that the current setup does not produce the result -1? What does happen?  Can you give a short example code that demonstrates your example above? (maybe it is in the tests you added in your PR. I haven't looked closely yet.
comment
Closed by #4851
comment
Closed by #4851
comment
Closed by #4851
comment
Thanks for this!! It is clear that the original API was built heavily dependent on the degree assortativity case and thus could only handle nonnegative edge weights and created matrices with a large number of zero rows and columns.  Your PR not only makes that restriction very clear, it also uses a simple fix to allow a huge variety of natural use-cases.  I like it. :}  It is a fairly large change in API -- even though the code is largely the same. In particular, the reduction of rows and columns of the output. Is there a way to make this easy for people who use the older version to update their code for this version?  Even if there isn't an easy way, we should think about how to explain the difference. I can worry about that, but if it is quick for you to come up with something I'd love a headstart from the one who redesigned the interface. :}  My other initial concern/question before actually jumping in further is about floating point edge attributes. If we construct a row and column for each edge attribute value, it is quite possible that round-off error will cause two edges to be incorrectly found to have different attribute values when they should be the same.  We could end up with a row and column for each edge simply because of round-off error.  Is my concern valid? How might we handle that?  (in other cases, I've told people to convert the floating point values to integers by e.g. multiplying by 1e6 and rounding.  So this problem is not new -- but the impact here could change the size of the resulting matrix.)  Thanks!
comment
Yes!  I did indeed mean "node attribute" and not "edge attribute". Sorry for the mistake and nice job figuring out what I meant anyway. :}  Also, I agree completely that the issue with rounding floating point values is a statistical issue, not a software development issue. And your comment relating it to the choice of bins for a histogram is very useful and insightful.  Perhaps we can include a short paragraph describing this issue (similar to what you wrote above) into the docstring.  And I like your examples for how to create mappings, though it might be good to have code that could work when the number of cases is not so small that you create each entry in the mapping manually.  For example adjusting what you put could lead to: ```python # mapping: degree values to row/column index mapping = {v: i for i, v in enumerate(set(G.degree.values()))} mix_mat = nx.degree_mixing_matrix(G, mapping=mapping) # then to find the mixing value from degree 2 to degree 1 mix_mat[mapping[2], mapping[1]] ```  I'll take another look at the other mixing functions and see what might work for your proposal to reduce it to 2: degree-based and attribute-based.   Thanks!
comment
The code looks good. The new tests are nice!  And they verify the changes made to the expected return values.  I think you pointed this out before, but should we remove `numeric_mixing_matrix` and `numeric_mixing_dict` now that `numeric_mixing_*` does the same thing as `attribute_mixing_*`?  If, so we can put in a deprecation warning and actually remove it later. Indeed, both can be done in a separate PR, or in this PR by a core maintainer, unless you want to go through those steps.    Also, could you add short examples that show: - that the rows and columns correspond to the attribute values.  (maybe this should go in `attribute_mixing_matrix`) - that the user can obtain a row for each possible degree up to the maximum (the old behavior) by building an identity mapping. Is this only needed for the `degree_mixing_dict` function?   You've got some examples a couple of comments back in this thread that would be fine.  I'm just looking for examples that would help users figure out what to expect (and for developers like me to remember what the function actually does) :}
comment
Thanks @rossbar ! I agree that the `mapping` parameter should go at the end for backward compatibility.  The change in behavior is fortunately (or unfortunately) **not** due to an incorrect original definition. The papers don't bother to explain how to handle values that do not appear in the graph.  For example, if no nodes are degree 3, but some are 1, 2, 4, 5.  Then we must have rows for 1, 2, 4, 5. But rows 0, 3, 6, 7, ... will not have any nonzero entries, so the formulas in the paper won't change value if we include or exclude them.  Let's treat this as a bug-fix, so we need to put a note in the release notes that the dimensions of these matrices will change due to removal of any rows with all zero entries. I'll do that soon.    
comment
OK... I think tis is now ready.  Did I miss anything?
comment
Actually, this is a bug and the fix is to replace `Graph` with ` graph_class` in lines 1181 and 1183.  similar to line 1595 or so in graph.py   Thanks for finding this!!
comment
That’s one of the changes.  The other one that is needed is two lines prior where Graph should be replaced by graph_class. `generic_graph_view(self, Graph)`   ->   `generic_graph_view(self, graph_class)`  ii wonder if there is a test we could add that would check this… 
comment
I moved your test to the `test_graph.py` module so it could be applied to graph, digraph, multigraph and multidigraph. I also added a little to it and made a second test of the `to_directed_class` feature (which does not seem to be tested anywhere).  Then I made undirected versions of directed tests and vice versa.  See what you think...  Anything else you'd like to include in this PR?
comment
In reading through this, I saw that one of the nice additions this allows is using a string to indicate which argument to map.  I know that our current codebase is all written with an index (number) to indicate which argument to map, but it "feels right" to use the name instead.  In fact, if it simplifies the interface I would be in favor of requiring people to use the name rather than the index of the argument.  @boothby, what do you think about restricting in the interface to using a string instead of "string or index"?  I would be willing to put in some cycles into this. Based on what looks like a 1-2 week delay in 2.6, maybe we could make a push to get this in before 2.6 so users could be encouraged to "just use `pip install networkx--no-deps`" when they want to avoid numpy. 
comment
Some questions have come upon while enhancing docstrings:  1) The `n_positional` and `mutable_args` variables are effective only for very strange cases. For example, `@argmap(func, ("A", ("B", "C")))`. Or for using an index that only works when the var_positional argument is provided and long enough to provide that size index. Do we need this? Or is it just for completeness?  Could we simplify significantly by removing that feature and only providing at most a tuple of arguments (I think that is all that is needed for `G, partition`)?  See what you think of my attempt to describe the generality of argument specifying syntax. (line 485 or so -- search for "overflow").  2) The `argmap.try_finally` method seems like it is only used to construct decorators in utils/misc.py.  If I understand correctly it is actually only used in the `open_file` decorator.  Would removing it simplify the interface to make it easier for future developer to understand?  We would replace:  `return argmap.try_finally(_openfile, path_arg)` with ```python     result = argmap(_open_file, path_arg)     result._finally=True     return result ``` I'm thinking that makes the description of this feature much simpler:  to wrap the decorated function inside a try/finally block, set the private attribute argmap._finally to True.   We can then remove a lot of description about an alternate way to instantiate an argmap object.  [Edit: Most of the examples and tests use `@argmap(...)` as a decorator. But most (maybe all?) use cases in our codebase use `return argmap(...)`. Should we pull back from @argmap in the docs? What about in the tests?]
comment
I push some doc clarifications and marked comments as resolved. There are some comments that I left unresolved --  @boothby can you take a look at those (and any of the others that I made more confusing too)
comment
OK.. I removed decorator from github workflows and install instructions. :)
comment
Thanks @boothby !!
comment
The current code idiom when you have a dictionary is: ```python G.add_nodes_from(nodes_with_attrs.items()) ``` But it would be possible to add some logic to slurp in the attributes when you have the dict-of-dict structure. But of course in that case `nx.set_node_attributes` works well.  Are there cases where you’d like to use the keys of a dict as the nodes to add? That would be a reason **not** to implement this.  your example with edges is **not** analogous— there you are using the `(node, nbr, data)` structure that works for `G.add_edges_from` but doesn’t work for `nx.set_edge_attributes`.  The analogous example would be `{(“A”, “B”): {‘color’: “blue”}, (“B”, “C”): {‘color’: “red”}}` which doesn’t work for G.add_edges_from but does work for set_edge_attributes. In this case adding `.items()` doesn’t work for G.add_edges_from. You just need to use the correct set_edge_attributes function.  For that reason, I’m inclined to leave the api as is.  If you have a dict keyed by nodes to an attribute dict, then use set_node_attributes.  Same with edges. If you have a dict keyed by edge-tuple to an attribute dict, use set_edge_attributes.
comment
I think you mean `nx.graph_clique_number(G)`.    :}
comment
Sorry this took so long to get to...  This is well formatted and applied using git. It should also have some tests and the doc_strings should restrict their line lengths to about 80 chars.   But I have trouble with the algorithm --  If you want to explore many different outcomes, perhaps it is better to construct a method that explores the different results systematically rather than randomly.  On the other hand, if you are introducing a random aspect to the result, shouldn't you be able to tell what the distribution of the possible outcomes are? Maybe that is too much to ask, but you should at least think about it. The implemented randomness may sample the possible outcomes in a very non-uniform manner.  Do you have a feel for what distribution the outcomes have?  Finally, your desired outcome might be better achieved by shuffling the node order of the graph. By that, I mean you could reorder the nodes so that they are stored in a different order on the data structure and run the resultant graph through the deterministic algorithm.  Something like: ```python nodes, edges = list(G.nodes), list(G.edges) random.shuffle(nodes) random.shuffle(edges)  newG = G.__class__() newG.add_nodes_from(nodes) newG.add_edges_from(edges) ```  Thoughts?
comment
I think opening an algorithm discussion might be in order.  In regard to your specific question about implementing top-down controller set (TDCS) and bottom-up.  As far as I can tell, these methods are only mentioned in the last few paragraphs of that paper from 2011 -- not the heart of the study.  And with no further description or links that I can find, it is hard to justify including them.  That said, it might help **you** in your studies to go ahead and implement these algorithms and play with them to see if you get useful results. Who knows, you might find out something! :}  But -- I would suggest that you read more broadly before choosing which algorithms to implement. Perhaps these **are** of interest and I just wasn't successful in finding that literature. Or perhaps there is a more interesting algorithm for you to implement. 
comment
Thanks for the PR!  I don't understand the concern about global uniqueness. If the user is able to select the edge attribute used for the edge_id, they can make it unique on whatever scale is needed.  If someone prefers to use `(rsc, dst, id)`, then they can create an edge attribute with that string and use it for the `edge_id`.  ```python MG = nx.MultiGraph([(1, 2), (2, 3), (1, 2)]) nx.set_edge_attributes(MG, {e: str(e) for e in MG.edges}, "eid") nx.write_graphml(MG, path, graphml_edge_id_attribute="eid") ``` 
comment
The new input argument should be made the last argument to avoid messing up the calling order with legacy code.  It looks like you only use this functionality for MultiGraph inputs. Can you extend it to work for the other types: Graph, ..., ...  What does the `named_key_ids` argument do and how is it different from this argument?
comment
The doc_string for `read_graphml`  says explicitly that for yEd files, the graphics information is discarded.   The comes from the 11 year old commit https://github.com/networkx/networkx/commit/d2673781f826a97c25d0c388fe5e32039f0bfaa3  Is your solution generalizable to get all of the graphics information?  Would that be worth doing?
comment
That would definitely be helpful as we don't have much yEd experience. We could do PRs piecemeal referring to this issue if you like -- or put it all into one PR.  Whichever is easier for you.
comment
We'll need more information to track this down. Obviously this doesn't happen on most installs or we would have seen it and/or other issues about it would have appeared here. So, can you tell us how you installed it, what kind of system you are running on.  From just that error message it looks like networkx didn't get installed completely or else correctly.
comment
I'm going to close this in favr of a larger-scale solution being discussed in #4836
comment
I'm closing this PR in favor of #4889
comment
Let's just use the same branch for all the commits you make. When you push to that branch github automatically updates the PR and runs the tests.  I'll close the other PR and we can use this one as "the branch" for this addition.
comment
Yay -- the PR is created! If you run black from the `networkx/algorithms/bipartite` directory it should find and check only the files that have been changed.  If you want to be more specific (so it doesn't check everything) you can specify the filenames relative to the current directory.   Typically I would do: ``` black extendability.py tests/test_extendability.py  ``` That will make the style changes for you. If you just want to see what the changes will be, use `black --diff filenames`.
comment
Then commit those changes and then push to the same branch! Each time you push to the branch on your repo it will automatically update the PR and run the tests.  
comment
I think just removing the conda example in the docs is insufficient. You are making the user either use venv or have to figure out what is required for conda. Many users are introduced to networkx via conda.  When they start to become contributtors we should help them -- not make them switch environments.  If our requirements file syntax no longer works, let's fix the requirements file or fix the docs -- not get rid of them. Then each person who tries to use the requirements files with conda will have to figure out the syntax error issue themselves.  Maybe Mridul or Ross already had to do that for the Sprint last week???
comment
I think we can make the (already crazy long) conda command slightly longer and it works. The idea is to remove anything after and including the first semi-colon from the requirements as we concatenate them to list the packages desired.  ```bash conda install -c conda-forge `for i in {default,developer,test}.txt; do for n in $(sed 's/;.*//' $i); do echo -n " $n "; done; done` ```  If someone knows sed better, there is probably a way to substitute the newlines with spaces and avoid the inner loop and echo command.  Anyway, this change to the documents is sufficient to make them work with the current default.txt file.  We could also create a parallel default.txt file called `conda_default.txt` that removes the conditions on pypy. But this seemed simpler to me. 
comment
Good point!   Mac has sed. But I doubt Windows does. Somehow I missed the idea that these instructions were supposed to work on Windows too. Does Windows have any of these command line tools?  I haven't been able to get students to obtain a nice working setup with windows. They either end up installing linux or using one of the macs in the dept. Is there a place in the Scientific Python community where we help new users get set up with a working IDE, installation and set of tools like git? Perhaps anaconda is the best attempt at that that I know of... but I haven't looked enough to know, really...  Also -- I'm confused... how does `pip install -r requirements.txt` work -- I don't see any requirements.txt file anymore. How do we get requirements.txt to match default and tests?
comment
I guess that we should also put all the requirements files on a single `pip install` or `conda install` command: ``` pip install -r default.txt -r developer.txt -r test.txt ``` or ``` conda install --file conda_default.txt --file developer.txt --file test.txt  ``` where `conda_default.txt` removes the pypy checking syntax from `default.txt`
comment
We should add code to send the default values through the `decode_data_elements` method -- similar to what we do with the non-default properties of the nodes.
comment
You can create a Pull Request of this work by clicking on "Open a Pull Request" under the "contributing" label of the home page of your repository. Give that pull request a title similar to this issue and your code will be provided to NetworkX in the form of a Pull Request.  You can continue to make changes and push them to your repository -- they will automatically update the pull request, so be careful only to put changes that relate to this pull request.  (that's why people usually create a separate branch -- so they can do more than one set of changes while the NX folks review the code in the first PR.)  Since you haven't created the PR yet, you can go ahead and make a new branch with what you've got, and create the pull request from that branch -- leaving your "main" branch available.
comment
How are you installing? What tools are you using to install? It looks like the error is in the `flask_cors` code.  Can you try to install them separately?
comment
It may be more effective to find bugs by running pytest locally on your machine rather than having to wait for the github testing machinery to finish.   To install, something like: `pip install pytest`, or `conda install -c conda-forge pytest` should work. But it may already be installed depending on your system.   Then to run it use: `pytest --doctest-modules laplacianmatrix.py tests/test_laplacianmatrix.py`.  You can check syntax by just running `python laplacianmatrix.py` locally.
comment
It is convenient to assume that the additional nodes should be ignored, but it is perhaps worse -- surprise! -- if they think the nodes should be in the graph and be fixed in position, but they aren't and no error is provided. Which is worse, an unreported node-not-in-graph error or a inconvenience of having to use `fixed={n:p for n,p in pos.items if n in G}`?  Which is better?
comment
I'm not invested in either side of this...  I want to know what is the better API choice. Certainly the current error message is not a good option.  Thanks for this!    Would you be willing to expand on your opinion? What use case do you have with fixed nodes not in the graph? 
comment
Could we summarize the reasons for making these required in this PR? It'd be nice to have a link to an answer for questions like  "why did you add requirements to NX", or "why can't I just load the part that doesn't need xxx".  The changes seem good.  I wonder why lxml is in this list of required packages though. We only use it in one place. Is it that lxml is easy to get so requiring it isn't an issue? How are we choosing what gets required and what is not?
comment
I think moving read_yaml and write_yaml to an example is a good idea. especially if we aim for 3.0
comment
Well... this runs now... but it still gives errors because the type of the return value is different I suspect. I'll leave that to someone closer to the problem than i am. :}
comment
This is actually slightly complicated than we thought -- `simrank_similarity_numpy` doesn't work with non-integer nodes. So, not only does the output need to be turned into a dict-of-dict, but the input `source` and `target` do too. And both directions need to map nodes/indexes.  We should set this up so we only have to remove the `simrank_similarity_numpy` function when we deprecate. So, I created a private `_simrank_similarity_numpy` function called by both the new simrank_similarity function and by the deprecated `simrank_similarity_numpy` function. I set up tests to work appropriately for this structure.  @jarrodmillman could you take another look? I'm not sure if I'm following the protocol for deprecation correctly. In particular, with deprecation, aren't we supposed to leave the function that changed in tact until the deprecation?  In this PR we are changing `simrank_similarity` which breaks backward compatibility. Do we need to delay that change?  What counts as a deprecation?
comment
Yes, you understand the difference between Numpy and Python versions.  The numerical difference for graphs used in the tests is between 1e-2 and 1e-4.  That is, the same graphs are used and if I set the tolerance to 1e-2 they all pass, but if I set tolerance to 1e-4 some don't pass.  I can add a test to show this level of difference.  It seems like a judgement call as to whether that is a deprecation or not, so I'll leave that to you to make the call.  About adding the new private function:  I think I understand now --> you were thinking we would fold the old code into `simrank_similarity` when we deprecate. Somehow I had the impression we were supposed to rewrite when we put the warning in. That way the deprecation is only deleting supposedly unused code so fewer bugs induced.  But in this case rewriting now *without* the new private function would cause us to do double work in the deprecated function (converting to dict and back again).   I will pull out that new function with the idea of folding the `_numpy` version into the main function when we deprecate.  
comment
The deprecation warning for `simrank_similarity_numpy` shows up when we run `simrank_similarity`. So people will get warnings even if they do the right thing. I think the private `_numpy` function takes care of that problem.  Is it OK to have the extra private function `_simrank_similarity_numpy` so we don't have warnings for people who call `simrank_similarity`?
comment
================ Start of rant Ack -- I don't like this whole implementation... :{ Both versions stop after max_iterations without any warning the the method didn't converge. And Both version don't compute the tolerances correctly.   The `_is_close` function doesn't treat tolerance correctly. So the Python version is more accurate than the numpy version simply because it effectively set the tolerance to 0.  I fixed `_is_close` but it is still maxing out the max_iterations without notification on most of the tests.  The numpy version only uses the absolute tolerance feature of `np.allclose`, so it doesn't get any better than the relative tolerance setting allows.  ================ End of rant Should I put in a feature to raise an exception when it doesn't converge? I guess so.... That might trip up users, but perhaps for the better... they'll know their previous results weren't at any set tolerance level. 
comment
I'm going to remove `_is_close`. It is a private function, used only once, and implemented in a bad way. I'll put a more correct version in the function.  Do I need to deprecate it ?   It is a private function.
comment
It probably should be....  === rabbit hole alert    But that does lose the transparency of the name indicating that it is an adjacency matrix stored as a numpy array.  danger of a rabbit hole here as I start revising the name "adjacency_matrix" to mimic the pandas version: "to_pandas_adjacency".  We want to be able to make an incidence matrix, adjacency matrix and an edgelist. and store each of them as numpy array, scipy sparse matrix, or pandas dataframe (and maybe an Xarray?).  I think we should stick with our current convention that names that don't say otherwise return numpy arrays.  And then they should all have `nodelist` parameters to indicate node order, etc.  ============= back out of rabbit hole Anyway... I'll make that change.
comment
Rats -- that changes it to a sparse matrix instead of a numpy array. We'd have to change the code some...    e.g.  sparse matrices are all 2dim.   use `identity` instead of `eye`, etc. 
comment
See #4823 
comment
@timebarn, It might help us decide what API to use for this case of dict-of-dict-of-dict-of-dict input if we knew more about your use case.  Where are these data structures coming from? Would it be tricky or limiting if we made you provide an input flag to state whether the input represents a muiltigraph as opposed to a set of edges attributes with the values being dicts.  The short term workaround is: ```python MG = nx.to_networkx_graph(dodod, create_using=nx.MultiGraph, multigraph_input=True) ``` as the code currently stands, you can't do it with the `nx.MultiGraph(dodod)` syntax. #4823 would require  ```python MG = nx.MultiGraph(dodod, multigraph_input=True) ```
comment
I think a d-o-d passed into the constructor should consist of only one type of data -- if you want to add edges that are different types, use `G.add_edges_from`...  It is already too complicated to figure out what `incoming_graph_data` is. :}  But there **are** cases where we want to input a d-o-d-o-d to a `nx.MultiGraph` constructor class. That can currently be done using `nx.to_networkx_graph(dodod, create_using=nx.MultiGraph)`, or `nx.MultiGraph(dodod)`.  The proposed "fix" in #4823 will hopefully allow for this same syntax to work as before...  BUT:  we also want to allow dododod inputs that have in the 3rd layer **either** edge_keys **or** not...  (This is slightly different from what @MridulS is promoting above.)  I think we agree that a d-o-d-o-d-o-d with edge_key in the 3rd layer should be accepted by the constructor and treated like the data structure of a MultiGraph.  Here is the other case where the d-o-d-o-d-o-d doesn't use an edge_key in the 3rd layer.  ```python dod = {'a': {'b': {'graphics': {'color': 'blue', 'shape': 'box'}, 'traits': {'budget': 300, 'revenue': 10}}}}  H1 = nx.MultiGraph(dod) print(list(H1.edges)) H2 = nx.MultiGraph(dod, multigraph_input=False) print(list(H2.edges))  output  [('a', 'b', 'graphics'), ('a', 'b', 'traits)] [('a', 'b', 0)] ``` Note that for `H2`, the edge data has values that are themselves dicts... So the H2 data structure could be thoughts of as a d-o-d-o-d-o-d-o-d    (which is hard to type!)
comment
I believe this issue is now fixed by #4823 
comment
I haven't looked carefully yet, but this looks like a defect and your idea that it could be fixed by passing along the `multigraph_input` parameter seems like a good fix. Thanks!!
comment
Wow --- this bug has been there for 9 years. :)   Looks like the ```_rescale``` function should be moved inline with the function and it's really just a way to avoid dividing by 0 for graphs with 2 or fewer nodes. It sets the 1/((N-1)*(N-2)) term to 1 if <=2 nodes. That's not really a "normalization"... And we can tell that no one wanted the "unnormalized version" because it leads to an exception because of this bug.  I suggest we deprecate the ```normalized``` parameter and replace the function with a scaling that occurs in the function only ```if G.order() > 2:```. 
comment
This Issue requires a deprecation to fix 
comment
Do you mean the [minimum cut functionality](https://networkx.github.io/documentation/stable/reference/algorithms/flow.html)?
comment
For the shortest_path routines, we allow the input argument ```weight``` to be a function which takes in the edge datadict and computes the numerical value used. Your request suggests that this same feature would be useful here too.  Let's make the input argument ```capacity``` be either a string (the name of the edge attribute) or a function (inputs: node1, node2, edge_dict and output: number). Then the function allows all kinds of special requests about edge capacities including setting a minimum value.   As a temporary workaround, you could traverse all edges, creating a new edge attribute (or replacing the one there) that sets the minimum value where needed.  
comment
We'll have to think about what the capacity function should look like. The code currently ignores edge keys completely. The capacity is examined for each edge (whether a multiedge or not).  In ```shortest_path``` we called the function using the edge's node-pair and attribute dict: ```weight(u, v, edgedata)```.  The function is supposed to make sure to handle multiedges if they might arise. Note that users could create weight functions that don't work with multiedges if they don't ever expect to run into that case.  Suppose we make capacity work similarly: ```capacity(u, v, edgedata)``` but for multiedges we assume ```edgedata``` is the attribute dict (not the keydict!).  Notice that we are not allowing the capacity function to know what the edge key is. If there are multiple edges between u and v, capacity will be called with a different ```edgedata``` dict. But otherwise won't know the key of the edge being evaluated.  This code reveals our multiedge wart where we can't use ```G.edges(keys=True)``` for DiGraph. Maybe we can fix that on our way to NX v3.0.  But in meantime, we could restrict the capacity function by not providing the edge key itself to the function.  Actually, all this is unnecessary for the original poster's Issue. It is more efficient to create a new edge attribute for each edge and then run the existing code than it is to have a function compute the capacity each time the edge is observed.  Maybe we should leave it like it is.  Thoughts?
comment
```python def set_min_capacity(G, min_level, capacity='capacity', adjusted_capacity='adjusted_capacity'):     for u,v,dd in G.edges(data=True):         dd[adjusted_capacity] = max(min_level, dd[capacity]) ``` 
comment
I don't understand what you mean.  The adjusted capacity is the usual capacity, unless that capacity is below the min_capacity in which case it is t he min_capacity.  What did you want it to be?
comment
Wow -- I still don't understand. I think either I'm not getting something or you're not. After you adjust the capacity, how come it is not being used?  Do you need to rename the edge attribute "capacity"?  Don't you get to specify which edge attribute holds the capacity value?   What do you mean by "the correct implementation"? And are you saying that NetworkX has an incorrect implementation?  Maybe it's best to start using a small (2 edge?) example to show each other what we mean.  ```python G=nx.Graph() G.add_edge(1,2,capacity=5, weight=2) G.add_edge(2,3,capacity=3, weight=4) G.node[1]['demand']=-4 G.node[2]['demand']=0 G.node[3]['demand']=4  min_level=4 for u,v,dd in G.edges(data=True):         dd[adjusted_capacity] = max(min_level, dd[capacity])  flow_cost = min_cost_flow_cost(G, 'demand', 'adjusted_capacity', 'weight')  ``` flow_cost is 24.
comment
Ahh... then it is not a minimum capacity.  I think you mean you want there to be a minimum flow along that edge.  In this example, what answer do you want it to give? The demand is only 1. How can the flow be 2?
comment
That should probably be called "minimal flow" and "capacity" then....  not capacity min. Since capacity means the maximum flow the edge is capable of.  So, my example would have not a solution then because the demands are less than the minimal flow. This sounds like a different problem. But maybe there is a way to convert it to a problem the current code can work on.   Does anyone know about flow problems with a minimal flow constraint?  Maybe you run through the minimal flows, changing the demands as needed. Then compute the flow on the resulting network and finally add the minimal flows to the computed flows. For example: ```python import networkx as nx G=nx.DiGraph() G.add_edge(1,2,capacity=7, weight=2, min_flow=2) G.add_edge(2,3,capacity=5, weight=4, min_flow=1) G.nodes[1]['demand']=-4 G.nodes[2]['demand']=0 G.nodes[3]['demand']=4  for n, attrdict in G.nodes.data():     attrdict['adjusted_demand'] = attrdict['demand'] for u, v, dd in G.edges(data=True):     dd["adjusted_capacity"] = dd["capacity"] - dd["min_flow"]     G.nodes[u]['adjusted_demand'] += min_flow     G.nodes[v]['adjusted_demand'] -= min_flow  print(G.node.data()) print(G.edges.data()) _, flows = nx.network_simplex(G, 'adjusted_demand', 'adjusted_capacity', 'weight') full_flows = {u: {v: flows[u][v] + G[u][v]['min_flow'] for v in flows[u]} for u in flows} print(full_flows) ``` prints ``` [(1, {'demand': -4, 'adjusted_demand': -2}), (2, {'demand': 0, 'adjusted_demand': -1}), (3, {'demand': 4, 'adjusted_demand': 3})] [(1, 2, {'capacity': 7, 'weight': 2, 'min_flow': 2, 'adjusted_capacity': 5}), (2, 3, {'capacity': 5, 'weight': 4, 'min_flow': 1, 'adjusted_capacity': 4})] {1: {2: 4}, 2: {3: 4}, 3: {}} ```
comment
Yes -- changing the network can/does make the flow infeasible.  Especially if you require a minimum flow level to a node with no outputs and no demand.  That's infeasible without considering any other part of the graph.  The code still works  -- your request doesn't have a solution.
comment
I don't think there is any feature to add. The code works as is and does what it says it does. The result from some examples is raising the unfeasible exception and that is what it should do when there is no solution.    The only suggestion i see coming out of this thread is perhaps adding documentation saying that:  If you want minimum flows across specific edges, then you should  1) change the problem to reduce the capacities of those edges by the minimum flow desired,  2) solve the resulting flow problem 3) add the minimum flows to the edge flows obtained in your solution. But I'm not sure this even makes sense to put into the docstring... There are so many other similar variants of problems that can be converted into the desired form. We don't really want to get into listing all possible problems that can be converted into this problem...
comment
I'm going to close this as the code works as is, and the documentation for how to convert this problem into one that our function solves is very problem specific and would lead to many long variants of the problem described in the docs.  If people are interested, post to this issue and we'll see it, or open a New Issue.
comment
It's not at all clear what is shown here. 
comment
Please label your axes. It is hard to determine what the scatterplots mean without some context.
comment
I definitely like the `pytest.approx` !!   The `assert_edges_equal -> edges_equal` makes sense if the assert is done in the testing file: ```assert edges_equal(edges1, edges2)```. But if the assert is done inside the function  the name should include the assert prefix.  I'm fine with switching to having the function return True/False and using the `assert` outside the function.  It does match the pytest pattern better. And it would allow those functions to be used in nontesting situations where you want to compare edges, but not raise an exception based on the result.  So +1 from on both options.
comment
I'm good with going back to different return types depending on whether arrows are requested or not.  BUT...  there are two issues that switching to FancyArrows fixed for us that it would be good to  revisit before reverting.    #3337 points out that LineCollections can't be iterated over.  #3398 points out that LineCollections don't allow curved edges.  Those are both nice features of FancyArrows.  Should we make two draw_edges functions -- or an argument to select?  That way a return type and capabilities aren't restricted to a certain graph type.  Default choices could be of course. 
comment
We've got a lot of keywords for edge style already:  `style`, `arrowstyle`, `arrowsize`, `arrows`, `connectionstyle`.  Do we already have a way to control the output using these keywords? Or is the logic tree too complex without a single keyword for Line vs Arrow?  Looking at the code, I might have answered this (at least from my perspective): Can we co-opt the `arrows` keyword?  I don't think this loses any functionality.  Here's the current behavior: ``` arrows : bool, optional (default=True)             For directed graphs, if True, set default to drawing arrowheads.             Otherwise set default to no arrowheads. Ignored if `arrowstyle` is set. ``` If we are going to allow FancyArrowPatch for undirected networks then one approach is: - remove the "For directed graphs" part - remove the "Ignored if arrowstyle is set" logic  So something like:  ``` arrows : bool, optional (default=True)             If True, use FancyArrowPatch for the edges. Not that, if desired, `arrowstyle` can turn     off arrowheads while still allowing the other features of FancyArrowPatch.     If False, use LineCollection for the edges, which does not allow arrows or bending edges     including self-loops, but is much faster when you have a lot of edges. ``` I suppose we could change the default to something fancy like:  `(default= (G.size() < 1000))`
comment
Right!  If the user wants to show self-loops they need to choose FancyArrowPatch.  We could add a line stating that `arrows` must be True to show self-loops.   I snuck that info in my example, but not in a very obvious way. :} ``` arrows : bool, optional (default=True)             If True, use FancyArrowPatch for the edges.          Note that: if desired, `arrowstyle` can turn off arrowheads while still allowing                           the other features of FancyArrowPatch.     If False, use LineCollection for the edges.         This does not allow arrows or curved edges but is much faster for lots of edges.     Note: you must use FancyArrowPatch to be able to see self-loops. ``` There may be another better way...  but It would be good if we were able to use the existing keywords. 
comment
This looks good to me too!  As far as placement, it looks like these are now the only functions in `testing/utils.py`.  So moving them to `utils.misc.py` would allow you to reduce the `testing` directory to a single module called `test.py` and no `tests` directory.  Furthermore, the distinction between `utils` and `testing/utils` has always been a little strange to me.  If/when we move them, we need to make the doc_strings quite clear about the fact that these are checking equality in a python object sense, and not in a graph theoretic sense (which is complicated). But we have had a recent request for such a feature. So I think they should eventually become public -- though not advertised heavily perhaps.  The `utils` subpackage does that by requiring the user to import is specifically (at least until lazy loading gets more common). As for timing -- I am fine if you want to do the move now or in a separate PR.  I suspect this will get pretty hairy and touch a lot of code. So, whatever makes that process better for you is good with me. :} 
comment
Yes, I think that we can remove the `run` functionality in favor of using pytest.    I don't think I fully understand the implications of that change. But I have the impression it is moving from an older install ecosystem where testing was done within the python interpreter instead of via a standalone program. And it seems the pip install features and impact (it is also a standalone program) has grown to the point where the `run` function is unlikely to ever be used. Let's make that break and remove some code. :}
comment
Wow!  This is Great!!    Thanks!!!  I haven't had a chance to go through it yet, but I think (if you want) you could remove the leading underscore from the method names since the class has an underscore to indicate private.  But that's up to you. I look forward to reading it more carefully. :}
comment
Would it be better to leave ```strict``` as an argument, and only compute its value if it is not specified?      if strict is not None:         strict = ...  I don't know which is better... would a user want to override this computation? Thanks...
comment
Thanks very much -- this is helpful!
comment
It looks like ```k_crust``` removes the k-core and so is the complement of k_core(G,k). I think the documentation says:  ```The k-crust is the graph G with the k-core removed.``` Also ```Note: This definition of k-crust is different than the definition in [1]. The k-crust in [1] is equivalent to the k+1 crust of this algorithm.```  Are you saying the function doesn't do what it's documentation says? Or that it agrees with the documentation but should be defined differently?
comment
Ahhh... I think I see the confusion now: The docs say the k-crust is G with the k-core removed.  But it's not clear there whether we remove the nodes or the edges of the k-core.  We remove the edges as well as any isolated nodes after those edges are gone. This seems reasonable, but I can believe there are reasons to return all nodes, and only the edges not in the k_core.  But we should not return only the nodes not in the k-core. That removes too much. I do believe that the k_core and the k_crust must have some nodes in common. It's the edges that are the focus of this way of looking at graph structure. 
comment
That code was put into NetworkX a long time ago. The question is what is the definition of the intersection of two graphs. The NetworkX function should return that.  From what I'm finding with a quick search, the resulting graph should have nodes that are the intersection of the two node sets and edges that are the intersection of the two edge sets.
comment
Thanks @guyroznb !
comment
I think Ross' idea might be correct.  If so, the tests here should stay, most of the code changes can be removed and the doc_string should add an example where a subgraph is created and used.   Thanks for this idea!
comment
Perhaps we need new descriptions of what this subset handling does then. And better variable names than `nbunch` and `nbunch_v`.  Currently the docs say: ```If nbunch_v is given as an arguemnt, the harmonic centrality for u in     nbunch is calculated considering only nodes v in nbunch_v. ```  That does not align with taking paths through nodes not in `nbunch_v`. Perhaps `nbunch_v` should be called `sources` or `source_nodes` with  documentation like:   ```If `sources` is a container of nodes, the harmonic centrality is computed by only summing over reciprocal distances from these source nodes. Otherwise the sum is over all nodes.```  Do you have a citation for this measure?  It doesn't seem to measure centrality so much as distance from a set of nodes. Perhaps some more explanation, description or citation is needed to make this concept clear. 
comment
Maybe this should be a separate function then -- that is, maybe called `partial_harmonic_centrality` or something better.  What do you need to use it for? Maybe there is a literature of papers that have talked about it?
comment
Thanks for that reminder. I recall the discussion now. Somehow I thought it would be less of a change in the routine. It's not even clear to me that calling it with `nbunch_v=G.nodes` will give the same results as `nbunch_v=None` the way the code is written.  Certainly calling the function to find the shortest path between each pair (u, v) is very likely to be slower than getting all_pairs and eliminating those not in nbunch_v.  Could this change be done (and what's the impact on speed) with ```python if nbunch_v is None:     return {             u: sum(1 / d for v, d in spl(source=u).items() if d > 0)             for u in g.nbunch_iter(nbunch)         } nbunch_v = set(G.nbunch_iter(nbunch_v)) return {             u: sum(1 / d for v, d in spl(source=u).items() if v in nbunch_v if d > 0)             for u in g.nbunch_iter(nbunch)         } ```
comment
Sorry for the delay -- I've finally made time to check on this again.   I think this proposed function need more documentation about what the method is. Harmonic centrality sums `1/dist` over all pairs of nodes `v != u`. This subset version only looks at pairs of nodes where `u` is in `nbunch` and `v` is in `nbunch_v`.  Changing the math formula might be enough, but some words would help too.  So, this measure describes something related to the centrality of `u` to a subset of nodes `nbunch_v` rather than to the whole graph.   My next question is whether the algorithm is efficient. It computes the shortest path between each pair of nodes, starting from scratch to find the path to each `v` from `u`. Wouldn't it be better to compute shortest_paths from `u` to all the nodes `v` using a single BFS? It seems a shame to explore the early parts of a BFS over and over again.   The full `harmonic_centrality` function computes the full BFS because it wants distances to all nodes. I would guess that is not much more costly than the somewhat shorter computation from u to v. But you only have to do it for node `u`. Whereas  this subset version has to recompute the BFS for each node `u` for each node `v`. If nbunch_v has 10 nodes, you compute 10 BFSs for each `u`. Certainly there is a tradeoff in size where it gets faster to compute all distances from `u` and only use the ones to `v`.   Does anyone know that tradeoff?   @ChristopherReinartz how large are the subsets `nbunch_v` that you are using? and how many nodes in your graphs? 
comment
For efficiency I think it is best to switch the loops so `v` is the outer loop. ```python #  Don't reverse G anymore centrality = {u: 0 for u in nbunch} for v in sources:     dist = spl(v)     for u in nbunch:         d = dist[u]         if d == 0:  # handle u == v and edges with 0 weight             continue         centrality[u] += 1 / d ``` (untested) or similar...  It looks like that is about 10 times faster on binomial graphs with 100-300 nodes. But maybe I am leaving something out -- so check for yourself too. :} 
comment
I was waisting time thinking about `for u in G.nodes & dist.keys():` which does work  vs `nbunch = set(G.nodes)`. It's fine as it is.  This is ready to merge as far as I am concerned.   👍 
comment
Why does it appear a lot in our codebase? ------------------------------------------  We often want to get a node and we don't know what the nodes are. We just need one node to start the algorithm.  I assumed this was a common need for programs that use `dict`, but that does not seem to be the case. Usually you know at least one of the keys in a dict.  How do numpy and skimage get a key from a dict?  Or maybe they never do that. The same need occurs for sets as well as dicts.  Why the word arbitrary? ------------------------  The word _arbitrary_ implies that it is *not random* -- it comes from the description of dict order (close to: the order of the keys in a dict is arbitrary and platform dependent). The distinction between arbitrary and random is important. Arbitrary means there is a deterministic process that creates it, so it is not random, but the process to create it is not easy to describe -- and perhaps different for different versions.  Which way is "best"? ---------------------  The old discussion @jarrodmillman points to at #1534 has all more of the story, but short summary:   ```next(iter(iterable))``` was used a lot in the codebase. #1534 was proposed and then changed to a faster-at-least-at-the-time ``` for x in iterable:      break ``` but then changed back because ```next(iter(iterable))``` is clearer. Aric was convinced to merge it when he saw Dave Eppstein's recipe called ```arbitrary_item```.  The feature of raising a ValueError when `iterable` is an iterator ensures that the iterator is not changed by the function.   My preference --------------  I prefer `next(iter(thing))` over the other options. I'm fine with removing `arbitrary_element`  and I'm surprised `dict` hasn't been given a method to produce a key... like a non-mutating `.pop` method. I guess `next(iter(d))` is clear, but it [still appears on stackoverflow](https://stackoverflow.com/questions/61729309/idiomatic-way-to-get-key-from-a-one-item-python-dict-not-knowing-this-key-but-k)  
comment
I think  the `arbitrary_element` PR is ready to merge. Anything else?
comment
I'm +1 to leave the question of changing/removing `arbitrary_element` to a subsequent PR.  If people feel the `arbitrary_element` is likely to be replaced then maybe we should keep it out of the reference docs. Renaming it shouldn't affect whether we put it in the docs or not IMO.   My personal opinion is that we should replace it with `next(iter(...))` but I occasionally see code like `list(G.nodes())[0]` in NetworkX (found one today while working on `inverse_line_graph`). So maybe it would be better to have an advertised way to get one.   In fact, maybe that should be a new method of Graph.  Another idiom that could be made into a utility but hasn't been so far is finding a way to get an unused node for a graph. `generate_unique_node` is one way, but `generate_unused_node` could be better for most purposes.  Again, that could be a new method of Graph if we want to go that route.  Sorry for the distraction -- I'm +1 for putting off discussion of changing/removing arbitrary_element and also for including it in the reference section of the docs.
comment
This looks good.  I've verified that the old code fails this test and that the new code works. Thank you very much!
comment
This looks pretty good.  You can click the "Resolve Conversation" button for any comments that your changes have addressed. Leave it open if it wasn't fully addressed and add a "reply" comment. Thanks for this!
comment
Copying the node and edge attributes is not really part of the transitive reduction algorithm. You've already got that information stored somewhere else, so the transitive_reduction function doesn't copy it. You can copy it yourself of course. But not everyone would want to do that, and while it's not much for a small graph, it's not so good for a big graph.  Your suggested code is precisely the workaround I would suggest.  ```python TR = nx.transitive_reduction(G) TR.add_nodes_from(G.nodes(data=True)) TR.add_edges_from((u, v, G.edges[u, v]) for u, v in TR.edges) ``` Perhaps we should put this kind of example into the doc_string so people know what to expect? But personally, I would just use G to hold the attributes and use TR to hold the reduction info.
comment
Thanks for looking into this! The syntax you have works. I wonder if we should implement a more "universal" weight system currently prototyped in `shortest_paths/weighted.py` and used in `shortest_paths/astar.py`.  The idea is that instead of using a string  for the input keyword `weight`, we accept the input as either a string or a function.  To process that we need to import with code like: ```python from networkx.algorithms.shortest_paths.weighted import _weight_function ``` Then early in the function, use `weight = _weight_function(G, weight)`.  This turns `weight` into a function that takes input like `weight(u, v, edge_data)` and returns the edge weight for that edge.  If the original `weight` is a string the function is basically `edge_data.get(weight, 1)`.  So, that function is defined in `shortest_paths/weighted.py` and for an example of calling it from another module, look at  `shortest_paths/astar.py`.  Also, my understanding is that in Python 3 we don't need to use decimals like 10.0 or 1.0.  10 and 1 are good now.
comment
You are right.  At this point, the weight function approach is limited within the NetworkX package. Issue #4085 is the issue tagged to take on this conversion.  And #4114 is a start on this change.  Is `G.degree` needed for this PR? I thought this would not intersect with the other issue. If it will work to install the weight function approach here it would be nice and avoid any need to change it when addressing #4085 (I hope). 
comment
No -- it is better to get it right when we do it.  I see now that the changes you made aren't affected by allowing `weight` to be a function, but at another point it does require `G.degree`.  Thanks for thinking about this.  This looks good then!
comment
After some thought I agree with @Rossbar.  Let's keep it simple by adding a new optional argument `margins` that (if specified) gets passed to `ax.margins`. :) 
comment
Yes... that would be helpful. Thanks!
comment
Is this another case where we should be testing against prerelease code? Or is that even possible with sphinx_build? :{ If it is OK with you @rossbar maybe we should merge despite a failing test.
comment
Agreed.  Let's replace ```numpy.matrix``` 
comment
Since numpy is just coming out with v1.15 and that documentation (see the link in the original post above) only states,  ```Given the above, we intend to deprecate matrix eventually.``` . I can't find any other indication of the impending deprecation.  Thus, I don't think there is any hurry here... And I think some people will still be using numpy.matrix for a long time. It looks like we have already fixed up most of the code in NetworkX that used ```numpy.matrix``` by replacing it with arrays. If it really is only the ```to_numpy_matrix``` and ```from_numpy_matrix``` functions that we are changing, then I'm much less worried about the whole thing.  Probably the right approach for those two functions is to insert a warning in the docstring that using numpy.matrix is discouraged and will eventually be deprecated. Also include a suggestion to use ```to_numpy_array``` instead.  That should nudge people the right direction.  But the primary question I think is:  **Are there other places in NetworkX that uses numpy.matrix?**
comment
This all sounds good!   Thanks...
comment
How are we doing with removing `numpy.matrix` from networkx?   I only know of #4089 still open.  I think we still haven't deprecated or changed `attr_matrix` (checkbox above). And we **have** deprecated `to_numpy_matrix`.  Are there other issues/prs we need to finish before closing this issue?
comment
@neilgirdhar the problem is that scipy.sparse uses np.matrix and we don't want to stop using scipy.sparse.  
comment
Here are the nice preliminary code that could be urned into tests from the Networkx-discuss email list message:  So there are two parts to this issue: 1. Outputting special floats (nan, inf, -inf) 2. Reading those values and converting them to the proper Python objects  Current behavior: ``` >>> import networkx as nx >>> nx.__version__ '2.6rc1.dev_20201228174925' >>> import numpy as np >>> np.__version__ '1.19.4' >>> >>> special_floats = [float('nan'), float('+inf'), float('-inf'),  np.nan, np.inf, np.inf * -1] >>> >>> G = nx.cycle_graph(6) >>> >>> # Assign special floats to attributes of G >>> for i, e in enumerate(G.nodes): ...     G.nodes[i]["ndefloat"] = special_floats[i] ...     G.edges[e]["edgfloat"] = special_floats[i] ... >>> # Show the resulting assignments >>> G.nodes(data=True) NodeDataView({0: {'ndefloat': nan}, 1: {'ndefloat': inf}, 2: {'ndefloat': -inf}, 3: {'ndefloat': nan}, 4: {'ndefloat': inf}, 5: {'ndefloat': -inf}}) >>> G.edges(data=True) EdgeDataView([(0, 1, {'edgfloat': nan}), (0, 5, {'edgfloat': inf}), (1, 2, {'edgfloat': -inf}), (2, 3, {'edgfloat': nan}), (3, 4, {'edgfloat': inf}), (4, 5, {'edgfloat': -inf})]) >>> >>> # Write gml file >>> nx.write_gml(G, "special_floats.as.attributes.gml") >>> >>> H = nx.read_gml("special_floats.as.attributes.gml") ### ERROR ### ```  Contents of special_floats.as.attributes.gml: ``` graph [   node [     id 0     label "0"     ndefloat NAN   ]   node [     id 1     label "1"     ndefloat INF   ]   node [     id 2     label "2"     ndefloat -INF   ]   node [     id 3     label "3"     ndefloat NAN   ]   node [     id 4     label "4"     ndefloat INF   ]   node [     id 5     label "5"     ndefloat -INF   ]   edge [     source 0     target 1     edgfloat NAN   ]   edge [     source 0     target 5     edgfloat INF   ]   edge [     source 1     target 2     edgfloat -INF   ]   edge [     source 2     target 3     edgfloat NAN   ]   edge [     source 3     target 4     edgfloat INF   ]   edge [     source 4     target 5     edgfloat -INF   ] ] ```  After this pull request:  Contents of special_floats.as.attributes.gml: ``` graph [   node [     id 0     label "0"     ndefloat _NAN   ]   node [     id 1     label "1"     ndefloat +INF   ]   node [     id 2     label "2"     ndefloat -INF   ]   node [     id 3     label "3"     ndefloat _NAN   ]   node [     id 4     label "4"     ndefloat +INF   ]   node [     id 5     label "5"     ndefloat -INF   ]   edge [     source 0     target 1   ]   edge [     source 0     target 5   ]   edge [     source 1     target 2   ]   edge [     source 2     target 3   ]   edge [     source 3     target 4   ]   edge [     source 4     target 5   ] ] ```  GML file can be read successfully: ``` >>> H = nx.read_gml("special_floats.as.attributes.gml") >>> H.nodes(data=True) NodeDataView({'0': {'ndefloat': nan}, '1': {'ndefloat': inf}, '2': {'ndefloat': -inf}, '3': {'ndefloat': nan}, '4': {'ndefloat': inf}, '5': {'ndefloat': -inf}}) >>> ```  
comment
@NeilGirdhar The NEPs are in the github Numpy repository under doc/NEPs  We are definitely part of the scientific python community. The strict adherence to the schedule of deprecations may inconvenience some people, and we will try hard not to inconvenience them too much.  For example, we could officially stop supporting python 3.6 but still try to keep the code working with 3.6 wherever we can (which would require testing with CI to at least know where it broke). Or maybe we don't test it, but we don't intentionally introduce code that breaks 3.6.  I'm not sure what the final decision should be. But it seems like we are interested in staying "on schedule" to the extent that it doesn't inconvenience too many users.  
comment
I wonder if we should shorten the name `Developer Guide` to the single word `Developer`. In the same way that `install` is a good representation of what is linked to in a very terse, grammatically incorrect, but never-the-less clear and concise communication, I think that `Developer` does what we want and reduces confusion.    I am a little concerned about `API Changes` as well since that is also 2 words (and it's not clear whether each word is on its own, or if we're supposed to read them together).  But the `API` part is short so it seems to hold together OK.  `NX Guides` is better than the longer words suggested.  But to further allow people to understand what it refers to, it might be better to locate it near the tutorial and gallery links.  I would put it between `Gallery` and `Reference`.  But I'm not sure that is possible with sphinx.  I suppose a word like `Library` might be good too.  Is there a word for a collection of guidebooks?  I actually keep coming back to `NX Guides`... :}  It looks nice! 
comment
I like the switch to one word labels -- `Developer` and `Releases` fit well.  And now I am thinking that the `NX` in `Nx Guides` is redundant and could become just `Guides`.    Jarrod, you mention that putting external links at the end makes more sense than switching between them. Can you explain that more? I was thinking documentation stuff first and then developer stuff later (releases and developer being the developer stuff).  I guess taking that to an extreme would put `Install - Tutorial - Gallery - Guides - Reference` in the order of sophistication of the user with our tools (though I always seem to go straight to reference for other libraries).  Then probably `Developer - Releases`.  But that is content based, not structure based.  And it is very left-to-right based...   Anyway... this second example verifies to me that I like one word links.  :} 
comment
That makes sense, Thanks... The distinction between internal and external is whether the content is created together or separately. We're trying to mimic cooperatively created content while keeping the actual files separate. So it's kind of a boundary case. The tipping point for me is that we'd need to either 1) hard code the link using @rossbar rst file, or 2) commit ourselves to maintaining a template file. I'd prefer to use a stock template. And option 1) would mean a different hard link for each version, etc and would likely become a headache.  So, let's put the external link on the right... How about putting Gallery next to Guides -- like you demonstrated but without the NX? That put the tutorial/reference docs together and the more in-depth gallery and guides together. 
comment
I'll try to get this to pass the tests again....  I think the version of black was increased so we can prepare for the v2.6 release and that is causing this error. I'll go ahead and change to the `yield from generation` idiom too.  If you can indicate somehow that you are OK with this @ArtinSarraf then we can get @rossbar to look one more time before merging. 
comment
Thanks for the `Returns` -> `Yields`  I couldn't remember that option... :}  And +1 for getting rid of the `" "` that black leaves in the strings.
comment
See also #3281 which discusses one example choice of initial graph (the complete graph) 
comment
These functions haven't been touched (except for style) since put in place in 2008. I suspect they were included as self-documenting pedagogical functions for people learning about authority and hub.  It is true that they are used in one function, but they are called one after the other which duplicates converting G to a numpy matrix, so even that code would be more efficient without these functions.  They can/should probably be turned into docstring examples, (or a gallery example?? if so, should probs be more than just creating the matrices). That means deprecating and removing.  Not just from public API, but from the internals as well.  For a Gallery example, what could we do with the matrices other than compute their spectrum to yield the HITS values?
comment
Does the example you point to work? Did you check, e.g. `root in G`, or `print(G.nodes)`? There is no reason to expect that your computed value of `root` should be a node in `G`. Check it to make sure you know what is what.
comment
I agree that the changes that should be merged here are the `callable` ones. But we should really change the `json_graph.tree_graph` code API so that instead of inputting a dict of attributes including the keys 'id' and 'children', we just use arguments `id` and `children`.  This is a change in API so it should be marked as a deprecation for v3.0.  Perhaps that should be in a separate PR.
comment
That looks like a namespace or installation problem. networkx certainly has an attribute selfloop_edges.  The import statement seems to be able to find `girvan_newman` so perhaps the installation is OK. Do you have a variable, file or folder named networkx that is causing a networkx namespace collision?  Try: `networkx.__version__` Try:  `networkx.selfloop_edges` as a command all by itself.  And maybe `dir(networkx)` Something is set up strangely. 
comment
It's hard to tell without a short simple reproducible example. I can say that reading in the pandas dataframe gives the same resulting graph only with edge attribute "weights" instead of "value".  I suspect the problem is in how you call the community function. If you don't specify an edge attribute to use, it may default to having every edge have weight 1.  Here what I mean by a short simple example to show the problem or solution: ```python import pandas as pd import networkx as nx df=pd.DataFrame({'source': [1, 1, 1], 'target': [2, 3, 4], 'weights': [.85, .88, .87]}) print(df) G=nx.from_pandas_edgelist(df, edge_attr="weights") print(G.edges.data()) df=pd.DataFrame({'source': [1, 1, 1], 'target': [2, 3, 4], 'value': [.85, .88, .87]}) print(df) H=nx.from_pandas_edgelist(df, edge_attr=True) print(H.edges.data()) ```  Output ```    source  target  weights 0       1       2     0.85 1       1       3     0.88 2       1       4     0.87 [(1, 2, {'weights': 0.85}), (1, 3, {'weights': 0.88}), (1, 4, {'weights': 0.87})]    source  target  value 0       1       2   0.85 1       1       3   0.88 2       1       4   0.87 [(1, 2, {'value': 0.85}), (1, 3, {'value': 0.88}), (1, 4, {'value': 0.87})] ``` Can you show your commands to `best_partition` in a similar small example and print what is happening?
comment
If you don't specify a weight in `best_partition` the edge weights are all considered to be `1.0`. That is the difference between the case when you get the error (using edge weights which look negative!?) and the case when you don't (edge weights are considered to be 1).
comment
This could be a problem in the R-calling-python aspect of your code. Functions that don't ever look at the `pred` attribute of a graph will work fine if `pred` is somehow missing from the Graph object.  So 1st try a simple version in python. 2nd try the same simple version from R. 3rd find a way to list all attributes of a networkx graph object from your R session.  1st: ``` G=nx.path_graph(3) G.pred ```  2nd: ``` G<-nx[["path_graph"]](3) G$pred ```  3rd ``` G<-nx[["path_graph"]](3) ??? ``` In python, your would use `dir(G)` to list all attributes. There must be a way to do that in R. 
comment
See also #4548  We are removing this code in NetworkX v3.0...  Maybe not soon enough. :)
comment
Just to make sure I understand your needs...  Would a new release of NetworkX (coming soon) correct your issues? Or will you need the 2.5.1 version to pass these security scans even after NetworkX moves to v2.6?
comment
Do the tabs across the top allow center alignment within each tab?   Minor nit-pick but it is not clear that the words at the top are tabs you can use for navigating.  Maybe if they had lines or a graphic to show they are tabs. The left side bar of the page also seems quite wide to me for nothing other than a search dialog box. Maybe make search be one of the tabs... or make the sidebars slightly gray background...  But these are style issues and I'm not very good at style choices. :}   Also if my comments are too little too late, I perfectly understand.   Thanks for taking on a new documentation theme!
comment
Well, changing Cytoscape is probably better discussed somewhere else.  NetworkX has a `with_labels` option for drawing which controls which attribute is used for the text describing a node. Similar feature exists for text describing an edge. The GML storage of a graph may not be your best choice. But in any case, drawing the graphs in NetworkX doesn't require any specific attribute name.  You can use whatever attribute you want, and even more that one if you draw the node labels twice (not recommended).  ;)
comment
My understanding of that text in the networkx documentation is that GML uses the node attribute 'id' and 'label', so be careful if you want to use node attributes with key "id" or "label". They may be overwritten by the 'id' and 'label' used in the gml file.  ```python >>> G = nx.path_graph(3) >>> G.nodes[1]['label'] = "hello" >>> nx.write_gml(G, "G.gml") ``` The file `G.gml` looks like: ``` graph [   node [     id 0     label "0"   ]   node [     id 1     label "1"   ]   node [     id 2     label "2"   ]   edge [     source 0     target 1   ]   edge [     source 1     target 2   ] ] ```  So the "label" attribute was overwritten by the string representation of the node itself. Your nice examples change the node, not the node attributes. The documentation is referring to node attributes 'id' and 'label'. 
comment
Closing this, but reopen or make a new issue if you need to.
comment
Thanks for this!  I'm not sure I understand the difference. I understand that with your example code the nodes are "too big" and so they get clipped at the edge of the figure. What is the picture supposed to look like with the improved call to ax? In my version it seems like the nodes are still clipped.  Also, why do you need ```set_facecolor('w')```?  Isn't that limiting if someone wants a different color?  Does setting the axis off affect the rest of this or just to make the example clearer?
comment
The order of the nodes in `pos` and `node_sizes should be the same. So, perhaps something like this can parse the position and size together (untested). ```python for npos, nsize in zip(pos, node_size):     ... ``` 
comment
Maybe you could turn your branch with that solution into a Pull Request. Then we can see how the tests work with it and maybe play and add more tests to it, to make sure it is DRY before we merge. :} We'll have to keep track of processing time too. I don't think your example code would add too much. The Patch Collections might be faster or slower and its hard for me to predict.
comment
Closed by #4769
comment
It looks like your version of matplotlib doesn't match your version of networkx.  You can try to use a more recent networkx version, or an older version of matplotlib. I would recommend upgrading to a newer networkx:  `pip install networkx==2.5`
comment
I don't think you successfully upgraded networkx. This was fixed in #3179 back in 2018. Perhaps you upgraded NX on one version of python but are running it on another?  Downgrading matplotlib will also work, but is not a long term solution (or in my opinion not a good short term solution).
comment
@rossbar, is there a syntax for requirements.txt that would allow <5 or >=5.0.7 (and of course, we could merge #4739 and the problem goes away too)
comment
Thanks very much for this -- I would have had a hard time identifying that there even was a problem here... :}  Two quick questions while it is fresh in your head and before I dive into it.   1) You say that `sorted()` can fix the problem -- Did you sort the results of the `query_pair()` method, or later on, and would that work if the nodes (and thus the node-pairs) are not sortable?  2) Do you have suggestions for how I can check that this is working or not? How different do you "two machines" have to be?  [Edit:  What version(s) of scipy are you using on the two machines?]
comment
Unfortunately sorting the edges isn't always possible because nodes (and thus node-pairs) are not generally sortable -- only hashable.  If you provide a number of nodes to the function all nodes are integers, but if you provide a list of nodes, the nodes are not necessarily sortable.  I also find that scipy 1.5.1 and 1.6.0 give different results. I'll try to track it down from there.  It might be that to get the same results you have to use the same version of scipy.
comment
Sorted can work here...  The return value of sp.spatial.cKDTree(coords) is a **set** of 2-tuple edges.  So order is arbitrary and potentially different in different versions of Python.  But it is not a set of 2-tuples of **nodes**, it is a set of 2-tuples of **node_indexes**.  The index is the index within the list `coords` used as input.  So, there are integers and can be sorted before what is now the next line where they get mapped to nodes. All we need is to add `sorted` inside the list comprehension where we loop over `edge_indexes` (line 55).  Is this where you had it?  Maybe I should just say:  Your suggestion looks like it will work fine!!  I'd love it if you could make a PR, or indicate if this fix matches what you were suggesting.     Thanks!
comment
Hmmm...   We changed the branch name to `main` so there shouldn't be a `master` branch anymore. If you make a fork into your set of repositories you can name the branch whatever you want and then create a pull request from that branch to networkx/main I can do it pretty easily so if your setup isn't working smoothly just say so and I'll make it and ask you to check it. 
comment
It worked! Thanks very much for spotting this -- ferreting out what was going on and suggesting the fix. You are now a "contributor"! :}
comment
Thanks for spotting this @rossbar  I don't think the CI error is related to this PR 
comment
We've decided to not copy all attributes to the new graph in `spanner` and every similar function (I hope) in the library. You'll need to copy the attributes if you want them: ```python H = nx.spanner(G  10, weight='weight') H.add_nodes_from([(node, G.nodes[node]) for node in H]) H.add_edges_from([(u, v, G[u][v] for u, v in H.edges]) ```
comment
We've decided to not copy all attributes to the new graph in `spanner` and every similar function (I hope) in the library. You'll need to copy the attributes if you want them: ```python H = nx.spanner(G  10, weight='weight') H.add_nodes_from([(node, G.nodes[node]) for node in H]) H.add_edges_from([(u, v, G[u][v] for u, v in H.edges]) ```
comment
Yes, very easy:  the next line should be `H.add_nodes_from(G.nodes.items())`
comment
We've decided to not copy all attributes to the new graph in `spanner` and every similar function (I hope) in the library. You'll need to copy the attributes if you want them: ```python H = nx.spanner(G  10, weight='weight') H.add_nodes_from([(node, G.nodes[node]) for node in H]) H.add_edges_from([(u, v, G[u][v] for u, v in H.edges]) ```
comment
The interface **is** consistent...  Though maybe what you mean is that the names of the class view objects are incomplete in their description.  The `*EdgeView` family of objects handle requests for all edges without any data.  The `*EdgeDataView` family of objects handle special requests:  either restricting to source nodes via `nbunch` or asking for edge data via `data`.  So, if you can wrap your head around the word `DataView` meaning anything other than a plain unrestricted view, I think it gives expected results.    Perhaps we should change the name to something like:  `*EdgeDataOrNbunchView`   (<-- ugh)  With MultiGraph's it gets even funkier because `MG.edges()` restricts the output by NOT reporting edge keys. While `MG.edges(keys=True)` does not restrict the output.    ```python >>> G = nx.MultiGraph([(1,2,0, {"color": "blue"})]) >>> G.edges() MultiEdgeDataView([(1, 2, 0)]) >>> G.edges(keys=True) MultiEdgeView([(1, 2, 0)]) ``` While we're on the topic of strange `EdgeView` warts:  I think `G.edges()` and `G.edges` could be confusing, especially for `MultiGraphs`.   `G.edges` reports edge tuples.  If they are multiedges, then the edge tuples are 3-tuples: `(n, nbr, edgekey)`. But if they are not multiedges they are 2-tuples: `(n, nbr)`.  But `G.edges()` returns 2-tuples whether they are multiedges or not. ```python >>> G = nx.MultiGraph([(1,2,0, {"color": "blue"})]) >>> G.edges() MultiEdgeDataView([(1, 2, 0)]) >>> list(G.edges()) [(1, 2)] >>> G.edges MultiEdgeView([(1, 2, 0)]) >>> list(G.edges) [(1, 2, 0)] ```  To be more precise, I think both of those functionalities should be available. We need a way to ask for 2-tuple edges and also for 3/2-tuples depending on graph type.  But the similarity of `G.edges` and `G.edges()` could cause confusion.   As for chaining, I assume you mean something like:   `G.edges(nbunch=[2,3]).data("weight")` I would encourage users not to do that because the way to do that should be:   `G.edges(nbunch=[2,3], data="weight")`   
comment
I'm closing this with the issue #4765 taking the lesson learned and focusing on improving the docs.
comment
Just to verify:  this feature is for v2.6 and won't work with v2.5. Are you able to reproduce the problem with v2.6?  (v2.6 is not released, so this means you need to install the development version from github)
comment
The function `eulerize` returns a MultiGraph with a double edge (3,4,0) and (3,4,1).  Thus to be an eulerian_circuit it must traverse the (4, 3) span twice -- using each of these edges once.  Your drawing of the graph doesn't show both of those edges. Perhaps they are drawn on top of each other.
comment
Very Nice!! Thanks for hunting this down and fixing it! :}
comment
That's nice approach to finding new nodes. But it is fragile in the sense that 1.0 equates to 1 but isn't an integer. So a graph with nodes 0.0 1.0 2.0 3.0 would have `cursor` be 0 even though it should be 4.  We could use `isinstance(node, numbers.Number)` in place of `type(i)==int`.  Or even `type(node) in (int, float, complex)`. But we might have to keep adding types to that list... However we do it, we should make sure that counter is an integer -- perhaps when we add the 1: `counter = int(max(...)) + 1`.
comment
Hmmm....   This is getting annoying and obtuse. :} Perhaps the best way is to implement an easy -- imperfect method for finding new nodes and then let the user override it with parameter e.g. `new_nodes`. We could force `new_nodes` to be a list of `n` nodes. Or we could be fancy and let it be either a starting value for new nodes or a list of new nodes. But the easy/simple interface might look something like:  ```python if new_nodes is None:     ...current code to find max...     cursor = itertools.count(max + 1) else:     try:         assert len(new_nodes) == n:         cursor = iter(new_nodes)     # catch objects with no `len` or `iter` capability     except (TypeError, AssertionError):         raise nx.NetworkXError("new_nodes should be a list of `n` node objects") ``` By making `cursor` an iterator we can use/update it in one step with `next(cursor)` 
comment
The `df` (DataFrame) object has ordered columns which are not necessarily in the same order as the order of the edges stored in the `G_main` (Graph) object. You should add the 'colour' edge attribute to `G_main` so that it keeps the colour info with the edge. This will avoid the mismatch between the order of your two representations.  ```python G_main = nx.from_pandas_edgelist(df, 'from', 'to', edge_attr = ['count', 'colour']) nx.draw_shell(...,  edge_color=G.edges.data('colour').values(), ...) ``` 
comment
Sorry about the incorrect `values` syntax. ```python nx.draw_shell(... edge_color = [c for u,v,c in G_main.edges.data("colour")]) ```
comment
Decorator v5.0.5 fixes this issue so we should no longer need changes to NetworkX.
comment
It is even more complicated that keyword arguments ending up in `kw`. The named positional arguments end up in `args` if specified by the user, but not if the user doesn't (relying on the default value). And they don't appear in `kw` either.  So the length of `args` is not consistent as provided by `@decorator`.  We will have to add logic to check it's length and update args -- or convince `decorator` to implement an option for this behavior.
comment
Decorator v5.0.5 now provides the v4 behavior and also allows the new behavior with a Boolean keyword argument. So no restriction on version should be needed anymore.
comment
The decorator package released a buggy version 5.0.0, but it has already been updated to 5.0.1, so they are obviously on track to fixing it.  Just hold on a few hours and it should all be better.  [Edit:  I'm sorry I didn't read this carefully before responding.  This is a deeper issue than just moving to 5.0.0.  Sorry -- please disregard my comment here...]
comment
It is not the Python 2 requirement that is breaking tests.  The tests are broken in Python 3 as well.  The problem seems to be in the v5 treatment of named positional arguments (optional arguments with default). v4 put the default value into the tuple or args while v5 only includes the arguments explicitly provides.  v5.0.0, 5.0.1, 5.0.2 and 5.0.3 all released within 24 hours....    perhaps other problems fixed, but this problem still remains. [Issue 103](https://github.com/micheles/decorator/issues/104) tracks it on decorator
comment
Backward incompatibility shouldn't mean a loss of functionality though.   As you stated, if named positional parameter's default values are gone from that API they should put somewhere else. We'll see though.  It has been less than a day total and lots done in that day...  I guess next time they will/should make a release candidate and yell loudly for people to try it out. :}
comment
Closed by #3260
comment
We don't want to print this often. For large Graphs it creates too much output.
comment
I thought that the python version of pagerank works with multigraphs.  It uses `stochastic_graph` which is supposed to work with multidigraphs.  Maybe we need to have a test showing whether it works with multiGraphs or not.
comment
My understanding is that the stochastic_graph transformation (that creates W) handles the MultiGraph aspect of the problem. So W is NOT a multigraph. The code has already taken care of the multigraph nature of G by computing the weighted degrees of each nodes as it constructs W. Those weighted degrees are obtained by summing the multiedge attributes.
comment
Thank you for that!  I checked that it was dividing by the right value, but not that it was collapsing the edges (which it isn't).  So W is a MultiDiGraph afterall... Sorry to send you on a goose chase.  I guess the replacement code should be: ```python     for _, nbr, wt in W.edges(n, data=weight):         x[nbr] += alpha * xlast[n] * wt ```
comment
I restarted the build -- and it completed now. Ready for Review! :}
comment
Looks like it can also raise a TypeError with input `G`. The suggested fix is `sorted(G)`.  ``` >>> random.sample(G, k=3) TypeError: Population must be a sequence.  For dicts or sets, use sorted(d). ```
comment
Nice!  Thank you!  I'll leave it up to you to say which is more clear, but you might consider (inside of list lookup) an if/else structure like: ```get_valid_edge = lambda depth: unmatched_edges if (depth % 2) else matched_edges``` Or you could even remove the function call `get_valid_edge` by putting the if/else where the function is used and shifting `depth` to start at 1/0 based on `along_matched`.  The test is very helpful!! Thanks!
comment
Recursion is not a good idea for algorithms in Python. Not only is there recursion depth issues (especially if it is O(n)... ), but there are memory and speed concerns. Recursion pushes the stack and loop programming into function and namespace programming, which might work fine in a compiled language setting, but is not ideal in interpreted languages like Python. We've been trying to remove recursive parts of the codebase.  Besides, most graph algorithms deal with stack and loops, so we're used to it. The recursion is not even that much easier to read. But I might be biased there.   Anyway, if you could return to a stack/loop form instead of recursion I would appreciate it.   As far as tests for `is_connected_by_alternating_path`, what difficulties are you having? Can you set up a small graph that should return True and another that should return False? Even better is to test each possible way that it could return False or True. But I haven't worked through the logic of the problem to figure out what they all are. That might be too much for a simple unit test setting.
comment
It is usual to **revert** and **force push**, but in fact either way is fine because github (by default anyway) squashes all the commits into a single commit when we merge it into the repository.  Writing tests for private functions is a good idea!  And we'd love to have those tests. To access the functions within the tests, use something like: ```python nx.bipartite.matching._is_connected_by_alternating_path(...) ```  Your nice exhaustive test everything approach certainly covers all small cases and can be easily made bigger.  It gives me an error on case ```test_all_cases(3,3)``` and larger cases too. Is the error for 3,3 and bigger a problem with the test or a problem with the function?    Takes a long time to run the test of course too (4,4) gives 50K times through the loop, (5,5) gives 28 million -- gotta love combinatoric explosion. I was envisioning a more selective test based on all logical possibilities in the algorithm -- rather than an exhaustive test everything approach. But this worked for this problem!  We got an error that we didn't expect. 
comment
Interesting that your pytest run takes 7 secs for the 5,5 case and mine takes 12 secs for the 4,4 case and 500 times that long for the 5,5 case (I haven't run it till it stops.)  So, I've been running that function outside of pytest.  More troubling, when I run it in pytest with 4,4 parametrized, it passes in 12 seconds. But when I call the function outside of pytest it fails. And so does the 3,3 case.  Any idea why it would behave differently in pytest than outside of pytest? input: ```python import networkx as nx import itertools as it def test_all_cases(n_row: int, n_col: int):     # make all possible combination of bipartite graph     for rights4left in it.product(*([[rs for i in range(1, n_col + 1) for rs in it.combinations(range(n_col), i)],] * n_row)):         edges = [(i, j) for i, js in enumerate(rights4left) for j in js]          G = nx.Graph()         G.add_edges_from([((i, "L"), (j, "R")) for i, j in edges])          try:             matching = nx.bipartite.maximum_matching(G)         except nx.exception.AmbiguousSolution:             # covered by test cases with smaller `n_row, n_col`             # Don't have to worry about it             continue         vertex_cover = nx.bipartite.to_vertex_cover(G, matching)         for u, v in G.edges():             assert u in vertex_cover or v in vertex_cover test_all_cases(4,4) ``` with output: ```python --------------------------------------------------------------------------- AssertionError                            Traceback (most recent call last) <ipython-input-16-9d81ec4a9a1f> in <module> ----> 1 test_all_cases(4,4)  <ipython-input-5-346799795eb1> in test_all_cases(n_row, n_col)      15         vertex_cover = nx.bipartite.to_vertex_cover(G, matching)      16         for u, v in G.edges(): ---> 17             assert u in vertex_cover or v in vertex_cover      18   AssertionError: ```
comment
I'm not sure what is going on with your output. I am using python 3.6 and networkx 2.3 and the matchings do not switch c to r nodes like your output shows. Also, the output is consistent      matchings M =  {'r6': 'c1', 'r8': 'c2', 'r1': 'c4', 'r4': 'c8', 'r3': 'c5', 'r2': 'c3', 'c3': 'r2', 'c4': 'r1', 'c8': 'r4', 'c5':      'r3', 'c2': 'r8', 'c1': 'r6'}     |M|  =  12     minimum cover K =  {'c4', 'c8', 'r1', 'c5', 'r3', 'c2', 'r2', 'r6'}     |K|  =  8 
comment
I'm assuming you want the longest shortest-path rather than the longest path.... :) The eccentricity and diameter functions have an argument ```sp=``` which allows you to provide all-pairs-shortest-path-lengths as a dictionary.  So if you need weighted paths, first use ```sp=dict(all_pairs_shortest_path_length(G, weight='attrname'))``` then send that to the eccentricity function.   By the way, eccentricity doesn't provide the source and target vertices of that path. Best way to get that is to loop over the ```sp``` dict-of-dict-of-lengths looking for the max and storing that length, source and target as you go.
comment
I think to find the maximal path you have to look at all the paths. :)
comment
Be careful -- one of you means "longest path" and the other means "longest shortest path".
comment
Please feel free to use the basic framework from this PR. It is probably better to start a new PR however. There are other PRs linked to in the project descriptions that work with the Louvain algorithm too. And there is a stand alone package that implements it too. So one of the early steps will be to figure out whether it is better to rewrite one of these implementations, or to implement your own. :)
comment
Great! Can you add your example code to create the error as a test so we don't fall into that again? Thanks!
comment
Good -- pushing to the same branch automatically adds to the PR so you don't need to request a PR again (as I think you found out).  Thanks!
comment
Thanks!
comment
Looks like that spurious link appears twice in that module/file.  Thanks! 
comment
The [What's New in Python 3.6 docs](https://docs.python.org/3.6/whatsnew/3.6.html#new-dict-implementation) documentation has a paragraph about ordered dicts. It states that this feature was pioneered by Pypy. A [post on Pypy documentation](https://morepypy.blogspot.com/2015/01/faster-more-memory-efficient-and-more.html) announces this way back in 2015. So pypy started the whole ordered dict thing and will continue it.
comment
This documentation does not seem to match the code -- as you say. Based on the cited reference, I see no definition of "boundary expansion" as a quantity -- rather it is defined as part of a list of ways in which measures of expansion can be extended. Still, in that definition, it appears that "boundary expansion" refers to nodes rather than edges.  What is your understanding of this term? Can you find a definition published anywhere?  What should this be?  An easy "fix" is to switch the documentation from "edge boundary" to "node boundary". But I think it'd be better to find out whether/how this is actually defined in the literature. And if it isn't then we should say so -- or maybe remove the function...
comment
This concept doesn't seem to appear in the literature anywhere else. And even in this place it is stated as an extension that could be done -- not that has been done. I'm going to switch the docstring from edge boundary to node boundary.
comment
Thanks for this!  If your edges aren't in the graph they shouldn't be part of the matching.  I haven't looked at this code much, can you verify in your understanding that these tools should only apply to simple graphs (not multiedges or selfloops)?  What about directed edges?  (I think yes.)   I should know this, but if I did, I've forgotten and thought maybe you would know immediately.  Thanks!
comment
Thanks @rossbar It looks like the directed case is also fixed by this PR. The input graph can be multigraph and have selfloops, but the candidate matching will return false if it contains selfloops or multiple identical 2-tuples(multiedges represented by 2-tuples).  I was worried that we should have `not_implemented_for` restrictions but it looks like we don't need them. :)
comment
I agree with @rossbar and I've opened an issue to look at the set-representation-of-edges part of this issue. Let's merge this PR in the meantime.
comment
I'm in favor of deprecating and also keeping "show" arguments to a minimum. 
comment
I think this should get merged soon to avoid trouble with the CI. :} So I'm going to merge it.
comment
These are all good changes to the tutorial!  I also really like the circleci artifact where we can see the documentation for this PR easily linked from the checks section.  This will make maintaining docs better. :} 
comment
Where more than one random number generator is used in the same example, is it best practice to use a single RNG with seed set once, or multiple seeds -- one for each part of the code where the RNG is used?  One example is ```plot_directed```.  **IF** it is best practice to use a single RNG for the whole example, (and I don't know that it is), then the examples should probably create an RNG, set it's seed and pass it into each function that uses an RNG.  Here's [a post about RNG in numpy](https://albertcthomas.github.io/good-practices-random-number-generators/).  ```python RNG = np.random.default_rng(14365) G = nx.random_k_out_graph(10, 3, 0.5, seed=RNG) pos = nx.spring_layout(G, seed=RNG) ``` 
comment
A separate comment...  We mostly make our functions available from ```nx.``` and without a long trail of folders to get to the function: ```nx.generators.directed.random_k_out_graph```.  The idea is to relieve the user from having to learn the path structure of the codebase and also to allow us to refactor the folders without messing up people's code.  In the examples, should we remove unnecessary path syntax? Then this example (from plot_directed.py) becomes ```nx.random_k_out_graph```.
comment
We actually did a lot of work to include the numpy 1.17 features alongside the python random features. So, NetworkX currently allows RNGs or integers for every seed argument for every function.  Furthermore, if the code requires/uses Python's random RNG interface, we allow either numpy's RNG or Python random RNG or an integer to be passed in. (we wrap the numpy RNG in an object to allow Python's API to access the numpy RNG.)  If the code requires/uses the numpy RNG interface, we allow either a numpy RNG or an integer and if someone uses a Python random RNG we raise an exception.  So, I think we've already gone through the NXEP work...  It's all there. We just need to use it. And we should use the numpy RNG interface whenever we can.    :} 
comment
Sorry for spam... I should also say that the current NetworkX functionality is provided by code in the utils/misc.py and utils/decorators.py modules and consists of the last three functions/classes in each of these modules. 
comment
As for the first question, `np.asarray` seems to raise a `ValueError` if it cannot convert the value to the correct `dtype`.  Because `np.asarray` can adapt the dtype to Python's `object` dtype, this line of code should not raise a ValueError. But I suspect this was the original source of the catch code. Is it possible that numpy wasn't as good about finding the right dtype in the past?    Also, it looks from a deprecation message that asarray will no longer accept ragged nested sequences in the future. But I'm not sure what exception that will raise when that functionality is removed. That might also create a ValueError.  Would we ever mistakenly have a pos[v] return a tuple of tuples, etc? I don't think so...   I would say remove this code and let numpy raise whatever exception it raises. The traceback will make it pretty clear that the error is the value of `pos[v]`.   :}
comment
What if there is only one node?  Then there's not really a space scale to use to scale the self-loops...  Could we use the size of a node?   And what should we really do even in big graphs? Should the self-loop be a fration of the size of the graph, or is there a better way to choose the scale of the loop?  Perhaps size of the node is a reasonable choice? But I haven't played with any drwaing to tell.  I recall that @tacaswell was not completely satisfied with this solution but we didn't come up with a better choice at the sprint.
comment
Wikipedia has an entry on [graph theory loops](https://en.wikipedia.org/wiki/Loop_(graph_theory)) They say they can also be called self-loops.  I think it is standard to use the hyphen in self-loop except in code where minus signs in names are frowned upon. Hence `nx.selfloop_edges()`  Personally, I think selfloops is more clear than loop which could mean a cycle -- though Graph Theory carefully follows the definitions and loop is a self-loop, Network Science is not so careful... ("silly scientists" say the mathematicians). There are even references to [loop networks](quantamagazine.org/in-natural-networks-strength-in-loops-20130814/). And papers by titans of network science that [refer to cycles as loops](https://www.pnas.org/content/116/47/23398). Biology uses the term loop for [feedforward loop](https://advances.sciencemag.org/content/4/3/eaap9751) and feedback loop which are cycles in the interaction network.  I like self-loop because it (perhaps overly) emphasizes that it does not go through any other nodes. But I spell it differently in code than in prose. :}
comment
Nice! Thanks!
comment
This is a corner case need. The UUID generates effectively unique nodes, but they aren't human readable. I think we should not spread the uuid library throughout the code. But it's not clear to me that we need to use this as much as we do.   This should *not* be named anything with `random` in the name. It does not have a distribution. It is not random. Indeed, the results are not independent because they can't repeat. `unique` is the right meaning -- perhaps there is a better word, but I don't know of one. 
comment
Thanks @stefanv for that list.  I suspect many/most of those don't need a unique node. They probably only need an unused node.    For example, the line graph usage here is only for the `inverse_line_graph` function when the input graph has no edges. We want to return an empty_graph with 1 node. The PR #3538 changed the code to make the 2 node case avoid `generate_unique_node` but punted on the 1 node case.  I think that rather than use `generate_unique_node` we should construct a node ```python a = 0 while a in G:     a = a+1 ``` or fancier would be ```python a = len(G) if 0 in G else 0 while a in G:     a = a + 1 ```  The `uuid` nodes are not very user-friendly.  I'd prefer to avoid it if possible -- and the submitter of #3538 agreed.  [Side note: the `inverse_line_graph` code uses `list(G)[0]` to get an arbitrary node from G. There are other fixes needed in  `line.py` like the `_edge_func` is now `G.edges` as of version 2.0.  I'll open a new issue for that.]  I believe the other uses of `generate_unique_node` are most likely similar and probably should use it.
comment
While looking into #4458 I see that `lowest_common_ancestors` also does not need a unique node, just an unused node.
comment
I like the explicitly named "Graphviz layout" and "Graphviz drawing". Some people get those confused and having them named so clearly will at least help people know that there is a difference.
comment
Looks good. We should change the docstring description of the parameter nodelist too so it doesn't say we will only draw edges incident to nodes in nodelist.  I wonder if we need to check whether nodelist and node_shape are the same length? I guess that is a different PR though...:}
comment
LGTM :)
comment
Fixed in #3098 
comment
I think this PR is ready to merge. It brings some of the previous test and documentation about the `resolution` argument to `modularity` up to date and tested. It also expands the use of the resolution argument to `naive_greedy_modularity_communities` with tests and docs.  I does now have tests that check the generalized modularity formula against calculations by hand. 
comment
The `_naive_greedy...` removal is in the list of deprecations to happen for v3.0.  For 2.5 we renamed it without the underscore -- it had always been a public function with a private name. :{  I agree with you about the documentation of parameter and gamma.  I was uncomfortable with those names -- and I finally found the relatively recent Newman paper that actually used gamma and also called it "resolution". I added the reference, but didn't clean up the docs enojgh. I'll do that. Thanks!  I'll also remove the extra tests of computing modularity. I figured some people might want to see the decimal representation. The integer fractions version is the by-hand computation so should stay there.  I put the decimal values as comments if anybody ever cares about that. :}
comment
Isn't the link to the source explained well in the section above your problematic line?  In the section called "Install the Development Version"?  
comment
On the page you show the link for, there is a section called "Install the Development Version" which describes how to get the source code using git.  I guess you are saying we should include a link to the github webpage instead (or in addition to).  Is that right? 
comment
I don't see any good reason not to use the commented out code.  Perhaps the current code could be left as a code comment in case it is easier to understand what it is intending to compute. 
comment
This memory cost is minimal. The usual guidelines are that additional data structures on the order of the number of nodes are fine. But data structures on the order of the number of edges can be costly and should be avoided if not needed.  However, in either case, if the returned structure can be generated per node, or per edge, that is preferred to returning a collection of all the results.  If it saves time to create the collection, then the collection should be returned.  In this case, however, the number of sets is **smaller** than the number of nodes. So it is quite unlikely to cause a memory crunch. And the speed-up is reportedly quite dramatic.   It looks like the speed trouble is that we used to iterate over all nodes for each label. Now we iterate over all nodes once.  Notice also that we do construct all sets simultaneously, so the number of sets held in memory is the number of labels instead of 1.  The size of those sets is constrained so that the total number of elements across all sets is the number of nodes.   I definitely approve this nice speedup.  Thank you!  Next question -->  this changes the output from a generator to a dict_values object.  Should the output be `clusters`? the values object `clusters.values()`? or an iterator over the sets like `iter(clusters.values())`?  I can see reasons for changing it to be the whole dict. I can see reasons (backward compatibility) for returning an iterator  . We could also deprecate the current behavior by returning `iter(clusters.values())` and say that the change will occur in v3.0.  What is the right choice here?
comment
I guess that's the question.  Is changing an iterator to an iterable a small enough change to declare it acceptable. Historically we have made changes like that fairly often -- but we're trying to deprecate these days and I don't know where the boundary is for what should be deprecated. I'm fine with accepting as is, especially if we open an issue to consider whether iterator -> iterable is an OK API change to avoid deprecation.
comment
I added a release note announcing the api change without deprecation. This should be ready to merge.
comment
I see `iterator` and `generator` as very similar. But `iterator` and `iterable` are different in that one is exhaustible and the other can be reused.  And Yes, we need to change the docs as well.
comment
We have a distance measure [function called resistance_distance](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.distance_measures.resistance_distance.html).   That seems related to what you are looking for, but probably not set up to do exactly what you would like.  In particular it finds the resistance between two nodes in the circuit/network.  That's very local to those two nodes... so you might need to call it more than once to get the info you are looking for.  Indeed, you might need a wrapping function to call "all pairs resistance distance".  If that involves too much recomputation, take a look at the source (green link in upper right of the docs for that function) and see if you could save computation by computing all pairs in one call of a new function: ```all_pairs_resistance_distance()```  If that is much better than just computing it for each pair, we should implement that.   
comment
If you are sticking with a single source and target and looking across subgraphs then I think your approach is good and what we offer...    I don't know of any way to consider Kirchoff's laws in a directed network. There is a kind of symmetry that if our society had collectively defined voltage to be the negative of what it is, then all currents would just get reversed and electrons would be positive charge and protons would have negative charge. But of course, the physical system would give the same results.  There are a fair number of functions in networkx that compute flows on networks (directed or undirected) so you might look at those. But in terms of describing an electric circuit, I don't even know what a directed circuit would mean (other than a diode I guess). 
comment
Looks like this PR #3685 should be combined with #4083 to put the functions into a single module and combine the tests and such.  Should we do that as two PRs with one going first? Or do we merge in a PR and then accept one PR?  Also, this PR has things like tests for the import structure of the package that aren't directly related. Should they get separated out?
comment
This PR is now contained in #4607 
comment
I agree that they should probably be computed together.  How about the naming scheme:  ```python def partition_quality(G, partition):     """Returns the coverage and performance of a partition of G. ```  That way deprecation is also straightfoward. The new function can be put in place with deprecation notices for the others that suggest using the new code.  As for the name, I'm open to other options, but I chose "quality" because it is a general term for what coverage and performance measure -- and it is the name of the module and not used elsewhere in the module/api.
comment
Given my mistakes that require edits of my comments, you should probably rerun at least a subset of the performance tests. I think my suggestions should be faster, but better to check...
comment
The performance results still hold by my checks.  This is ready to merge.
comment
You can also use yaml directly:   ```python import yaml yaml.dump(G, path)  # write to a file GG=yaml.load(path, Loader=yaml.Loader)   # read back in ``` 
comment
This looks like a useful function/algorithm to include in dag.py in NetworkX. It'd be great to have a PR for it.  Some comments/questions:  - Does it work (return the same values) to use:  `A = nx.to_scipy_sparse_matrix(DAG)`? I think you then need later: `B = B@A` instead of `B = B*A` but otherwise everything goes through and we've saved some RAM and presumably computation times as well.   - Can you make the while condition: `while p>0` and avoid the `while True:   later break` idiom.    - It seems like a more graph oriented would be possible where you do a breadth first search starting from the roots (in_degree == 0) and any time you discover a new nodes, yield the edge that got there (or add it to the new_DG).  My guess is this would be faster than the linear algebra approach, but maybe not -- and maybe it has been studied before.  Any insights?  Thanks!  
comment
Thanks -- I'll close this Issue then. :)
comment
Can you explain why you thought the function would allow paths outside of the listed nodes?  The current documentation says `The rows and columns are ordered by the nodes in nodelist.`  Looking further, the `nodelist` is passed to `to_numpy_array` which says in the doc_string: ``` When `nodelist` does not contain every node in `G`, the adjacency matrix is built from the subgraph of `G` that is induced by the nodes in `nodelist`. ``` Perhaps we should copy that documentation to `floyd_warshall_numpy`. 
comment
Unfortunately (or fortunately) this interpretation would preclude taking advantage of the dense matrix data structure that this function takes advantage of. Or, perhaps it could use the full matrix and then manipulate it after-the-fact to remove all computed path lengths except those between nodes in nodelist. I don't think that is a good design for this function.  But the current implementation makes it hard to track errors where users provide a subset of the nodes. The SO original post looks like such a difficulty. Our current implementation hides the fact that an induced subgraph has been created. That induced subgraph could easily be constructed by the user before passing into this function. I see little to be gained by providing an automated subgraph feature and it makes providing a nodelist error prone.   I think the best solution is to raise an error if `nodelist` does not specify all nodes in G and no others.  One possible test of such could be:  ```python if nodelist is None:     nodelist = list(G) elif len(nodelist) == len(G) == len(set(nodelist)):     pass else:     raise nx.NetworkXError("nodelist should provide an ordering of all nodes in G") ```  Does this make sense? Is it s good idea?
comment
The users can subsample the rows and columns themselves...
comment
I think the degree mixing matrix should be a "number of different degrees" by "number of different degrees" matrix.  So if the max degree is 97 for you network then you get a 97x97 matrix.
comment
I agree with @rossbar It looks like a configuration problem. It is quite likely that your jupyter lab is running a different version of networkx than you think it is. The `is_path` function was only added in v2.5, so if the networkx version you are actually using is older this is what would happen.  The command `print(nx.__version__)` should tell you what is installed.
comment
Yes... Reference [1] verifies this.   Thanks very much for this correction!
comment
There can be two paths with the same length. When you try to find the path in the opposite direction, it will not always give the same path (if there are more than one path with the same length).  But you don't need to find the path going the other direction...  You can reverse the order of the nodes.
comment
It sounds like you might have a directed graph.  In that case, the direction of the path will impact the shortest path. You can't go the opposite direction along the path.     If not, then I agree with @jarrodmillman . We need more information. Please share a small graph which shows the problem.
comment
I agree. For both descendants and ancestors the input graph does not need to be acyclic (or even directed).  Also, this code is not taking advantage of python 3 dict.keys capabilities. It should be closer to something like: ```python if source not in G:     raise.... return nx.shortest_path_length(G, source).keys() - {source} ```
comment
The complexity issue should be fine. If a weight is not specified `shortest_path_length` defaults to a breadth first search.  I think simply removing the restriction from the docs of ancestors and descendants is the main issue. Moving `descendants` and `ancestors` to, e.g., the traversal subpackage would also be OK with me.  You can chose a module name, but `relatives.py` could work I suppose... :} There will be a bit of changes needed in docs and `__init__.py` files. If you want to just do the doc_string that would be fine too.
comment
I think the right approach here is to remove the restriction written in the doc_string that these two functions work only for DAGs. I don't think we need to move the functions elsewhere.  Pretty straightforward I hope.
comment
Yes, that seems good!  Thanks!
comment
Thanks for this comment. We need to make sure this is easy to do. What is difficult about bundling code like numpy? It provides wheels for the installation.Can you describe more about your bundling process and where the difficulties arise?
comment
So the zipapp bundles the python code... Then you don't need to follow the requirements file right? You can bundle it without numpy or scipy. Obviously you can't use the functions that call routines from numpy and scipy. More important for you than the change in the requirements file are any new functionality we introduce that takes existing python based code and rewrites it to be faster using numpy or scipy.  Luckily for you we 1) usually keep around the pure python version, and 2) that process of finding out where to use numpy in the library is quite likely to be slow -- so you'll be able to bundle as a zipapp for many years (again, depending on which functions you actually use.)  What kind of application do you have where bundling with zipapp is effective and useful?
comment
For the failing style / format test, you just need to install black ``` pip install black ``` and run it on the files you are changing. You can automate this using a pre-commit hook ``` pip install -r requirements/developer.txt pre-commit install ``` More details about contributing to NX are available here: https://networkx.org/documentation/latest/developer/contribute.html
comment
After installing `pre-commit`, each time you use `git commit` it will run black before accepting the changes. If black finds the style acceptable the commit completes as usual. If black finds style issues, it automatically makes the changes and stops the commit process. You can check (or not) the changes and then commit again.   Personally I like to use `black --diff filename` to see what is being suggested before I rely on the pre-commit hook. But that's my personality more than any specific reason for doing it. I just don't like black changing things and not telling me what it changed. :}
comment
Hmmm...  Something is strange  It looks like you commit still has two lines with spaces and no letters. `black` replaces those with blank lines.  But it should do this as you commit...   It's hard to know what is going on without more info.  So try running black locally:  `black weighted.py` The commit that change (if there is one).
comment
I suggest we leave the line in the Notes section as it is. And that we change the `cutoff` parameter description to:  ``` Length (sum of edge weights) at which the search is stopped.  If given a value, we only process paths shorter than `cutoff`. ```
comment
The line that you changed from `length <= cutoff` is the line I was referring to. So my suggestion was to replace that whole line (which confusingly calls it "depth" and "length" and never says it is the "sum of the edge weights".  I'm proposing replacing that whole line (and making it the two lines above).
comment
How about:  Length (sum of edge weights) at which the search is stopped. If cutoff is provided, only return paths with summed weight <= cutoff.
comment
Your stated goal is not well defined in almost any graph. If there is a cycle with any positive cost, you will get an infinite maximum path.  Most likely you have some other constraint on your path and you'll want the max cost path with some restriction.  Hopefully you can convert whatever it is that you want into a shortest path problem.  For example, maybe adding the maximum weight to the negation of the weights for each edge ---- or using the reciprocal of the edge weights.  Neither of these map directly to the maximum path problem.... but most likely you don't want the maximum cost path... 
comment
I meant ```new_weight_edge_uv = max_weight_over_all_edges - old_weight_edge_uv``` That makes all edge weights positive, but the smaller weights now become the bigger weights. Same with reciprocal ```new_weight = 1/ old_weight```.  But this is NOT max cost path...  Your graph better be directed as well -- right otherwise any edge forms a 2-cycle.  Hopefully you can figure out a way to transform the edge weights so your max cost path can be written as a min cost path.
comment
You should also look at: https://www.techiedelight.com/maximum-cost-path-graph-source-destination/ (make sure it is what you are looking for)  You can probably use edge_bfs() to get the edges to look at.
comment
I don't think this is worth pursuing -- at least not by inventing a problem that we can then use this to solve.  The problems we solve should be useful problems.  I'm going to close this issue. But people can still post to it...
comment
I think that sounds good -- there is a backward compatibility argument for the order ```harmonic_centrality(G, nbunch=None, distance=None, nbunch_v=None)``` But more than that, is there a better name than nbunch_v?  For example, ```starting_nodes```. Or maybe they should be called sources and targets?
comment
The trouble with PRs like this is that they are very hard to review, hard to make sure they don't have alternate impact, and let's face it, they don't actually help the code much.  I am most leery of the list comp changes that sum over True/False to use the bool trait that addition is like the numbers 1 and 0. `sum(val == 5 for val in ttt)` is just not as readable as `sum(1 for val in ttt if val == 5)` or `len([val for val in ttt if val == 5])`.  The last could create a large list, but maybe it is known not to. An automated translate -- and even a large scale manual translate -- doesn't take the context fully into account.  I'm also -1 on removing parens from tuples that are return values -- and many other cases actually.  Let's let Black do the linting so we don't have to.  If black doesn't have a preference, then let's not worry about it -- or else bring up the issue with black.  Even removing python2 class syntax has almost no benefit to offset the noise in github's "blame" feature. But I suppose I could go along with that. Most of these are in tests and maybe we can wait until we convert a bunch of test classes into test functions -- which I see on the horizon... :}
comment
OK... this is ready for review -- much more change than the original... Thanks @amamory for the nudge! :}
comment
I was able to push to that branch.  Very nice intro for me to intersphinx!!    Now I know enough to want to go look up more. Thanks!
comment
Yup!   That is fine!   Thank you!!
comment
It looks like #4378 changed the behavior to exclude edges incident on nodes not in nodelist. But that was **not** the original intent of this parameter. The nodelist parameter is only included to allow lookup of the index of the nodes in node_size. I think we should back out the restriction on edges from #4378. We should keep the documentation parts (except nodelist) and explain the nodelist is only used in `draw_networkx_edges` to index the nodes in node_size.  Then we should remove the new behavior of excluding edges not incident on nodes in nodelist.  Users can exclude edges if they need that with `G.subgraph()`.  Looking back through the discussion in #4374 we talked about removing nodelist altogether, replacing node_size with a structure (like dict) that associated a node with a size. But that would require rewriting functions like `draw_networkx`. So we decided to keep `nodelist` as a parameter at least until we got the drawing API closer to being figured out. Somehow I missed that the PR actually put in the code to remove edges not incident on the nodes in nodelist.  (Actually, changing the API to remove nodelist wouldn't require much change in the upstream `draw_networkx` function. `draw_networkx` uses the same nodelist for drawing nodes as it does for drawing edges. But maybe I'm missing something. Anyway, I'm fine with however we leave it so long as the nodelist parameter does nothing in the edges function except allow lookup of node sizes. :)
comment
Is there any reason to include the argument `nodelist=[ncenter]`? That was put into the code from the very beginning and I don't think it does anything functional. There is no `node_size` argument for this to be used with.  Let's merge this (take out of draft and merge... right?)
comment
This looks nice -- take a look at my comments. They may be off-base, but see what you think.  
comment
Great!   Now I have one more thought -- When we check for a connected or strongly_connected graph do we turn the two_sweep method into 3 sweeps?   If so, maybe we can reduce it back to 2-sweeps by including the check for connected as a step in the two step process.  Is it correct that shortest_path_length with a source specified will return all the nodes in G if-and-only-if the graph is connected (I think so...)?  What about strongly connected for a DiGraph (I think not...)? But we go both directions with the DiGraph, so maybe there is some similar relationship.   A 3-sweep method is still linear in number of nodes. But it'd be good to use 2-sweep if it saves time.
comment
Thanks for the gentle nudge. :}   This looks good to me! Thanks very much for the submission. I approve this pull request.
comment
This looks interesting, and you have chosen the correct module place and name. It would be great if you could put together a PR. Hook into the documentation with an entry at doc/reference/algorithms/approximation.rst. Hook into the namespace with an entry in `__init__.py`. If you have questions, let us know.
comment
It looks like all these functions should be decorated as undirected simple graphs. `ramsey.ramsey_R2()`, `max_clique`, `clique_removal`.  They should all have the decorators to match the function (from the clique module) `large_clique_size`. That is, two decorators: not_implemented_for('directed') and same for `multigraph`.  Thanks for this! 
comment
Yes, those functions are all interrelated. They could probably be in the same module too, though that would require moving the docs to docstrings.
comment
Is it true that your PR implements the Onnela form?   That seems like the one most commonly referred to in other papers. There aren't very many papers that look into this corner case (possibly negatively weighted graph clustering) in a systematic way.  Perhaps we should implement the adapted Onnela and make sure the docs state that this formula works for negative weights too.  If you (or someone else) wants to pursue the Zhang alternative we could add a separate function for that approach.  What do you think?
comment
That sounds like a good plan.   Thanks!
comment
The numpy import has to be handled specially so the package can work if someone doesn't have numpy installed.  1) put the import into each function that uses it (at the start of the code right after the doc_string).  2) In the tests you can skip some tests that will use the numpy import. Use `pytest.importorskip("numpy")`.  I think the easiest way for you to do that here is to create a `setup` method that just calls importorskip in each of the two "weighted"  classes.  I don't think any other tests will run into `np.cbrt`. But if there are others, they would need an `importorskip` too.  I hope this makes sense.
comment
Thanks very much!
comment
It would be nice to have them triage in this way. Do we need to change our email notifications to see the new posting locations? 
comment
The `draw_networkx` is supposed to match (make a distinction from) the `draw_graphviz` function which was removed in 2016. So a renaming makes sense -- though we are looking to a larger refactoring and expansion of this code anyway so I'm not sure what the best timing is.  The module name is currently `nx_pylab.py` which should probably be changed to `nx_mpl.py`  OR we could change the name to `nx_draw` or something better.  I'm -1 on a module named `draw.py` as it is too generic and we can save the name `draw` for a function.  But, I'm not up on best practices for namespaces and modules/subpackages/etc. Any pointers to descriptions of how to do it better would be appreciated.
comment
I've done a comparison with a bunch of possible choices.  networkx.utils.random_weighted_sample np.random.choice with weights and replace=False removing the nodes from the list as proposed by OP.  All are much (5-10x) slower than the implemented method for n=100, m=3 and all scale worse than the implemented method as n and m increase.  I'm going to close this Issue 
comment
Excellent -- Thanks for this!  @rossbar is traveling and might not respond this week, but will likely shortly thereafter. My quick look through suggests that this is a big improvement.  :)  I'm not sure all those need to be static methods, as opposed to [putting code in an appropriate setup function](https://docs.pytest.org/en/stable/xunit_setup.html).  But it works and I haven't looked into the implications of the different approaches.  Thanks! 
comment
I think pytest [now has fixtures parameters](https://docs.pytest.org/en/2.8.7/parametrize.html) Not sure if it is flexible for what we need, but hopefully close...
comment
Closed via #4292  Thanks @AndrewEckart 
comment
Try: [parse_adjlist](https://networkx.org/documentation/stable/reference/readwrite/generated/networkx.readwrite.adjlist.parse_adjlist.html) 
comment
I think we provide this functionality -- `from_numpy_array` creates the graph with all relevant edges present, then `nx.relabel_nodes(G, dict(enumerate(nodelist)))` allows you to change from integer nodes to whatever nodes you want/need.
comment
The short answer is that `G.nodes[node][key] = value` is the direct way to update node attributes. `G.nodes[node].update(attrs)` is another common idiom.  You can also use `add_nodes_from` if you prefer:    `G.add_nodes_from((node, nodeattrdict(node)) for node in G)`   Suppose you have the node attributes in a dict called `nodeinfo` keyed by node to an attribute dict (adapt to fit your data). `G.add_nodes_from(nodeinfo.items())`
comment
I would personally add the edges first and then the node attributes -- only because the edges are coming from a numpy array. We're working on a nice way to add edges later -- at this time the best way to add them after the fact is to go through a separate graph -- something like (untested): ```python  H = nx.from_numpy_array(adj_mat) H.relabel_nodes(H, dict(enumerate(nodeinfo)), in_place=True) G = nx.Graph() G.add_nodes_from(nodeinfo.items()) G.add_weighted_edges_from(H.edges(data=True)) ```
comment
Once we get a nice way to generate edges and nodes (see  [doc/developer/nxeps/nxep-0003.rst](https://github.com/networkx/networkx/blob/master/doc/developer/nxeps/nxep-0003.rst)  and   #4395 ) then you could add nodes and edges in one shot -- but you'd still have the step of relabeling the nodes from the numpy array. 
comment
I have not had good success using timing to calibrate order of magnitude dependence unless I use the same machine under similar conditions. So, it may be that this test is overly optimistic about the testing configuration.   I'm +1 for removing it.
comment
That sounds reasonable (I didn't look at your graph shape much though) --  How many did you expect?
comment
Just as a warning/disclaimer, Networkx v2.5 doesn't support Python 3.9.  Most everything should work, but it wasn't tested on Python 3.9 before being released.  We expect networkx v2.6 to be released fairly soon and it does/will support Python 3.9.  In the meantime you can install the github latest branch to get a version tested on 3.9.
comment
Read the docs for `nx.bfs_predecessors` and `nx.Graph.predecessors`.  They do not intend to return the same thing.  In the example you show the BFS doesn't take any steps and so visits only the source node and there is no predecessor for the source in a BFS.
comment
Yes -- that's correct. The BFS only searches outward along the directed edges of a graph. So you might think of the traversal as being successors only. But the search itself follows a directed tree across the network with each node having a parent node from which it was first "discovered" in the search. So the word "bfs_predecessor" is used to refer to a predecessor/parent node during the BFS process rather than a predecessor in the original graph/network.  We apparently expect the adjective BFS in "BFS predecessor" to signify the context (either original graph or the search tree) within which the predecessor is determined.
comment
That is a confusing part of the docs. I think the confusion is caused by the code where the term `triangles` is used to represent what is actually two times the number of triangles. The code correctly includes the 2, but someone looking at the code might think the 2 is not there. So I suspect the docs incorrectly interpreted the code as not have the factor of 2.   While we're changing the docs for `clustering`, they don't even show the formula used for a weighted directed clustering case.
comment
Just to be clear, the issue is memory. There is not a limit from a software perspective. 200 million weights takes about 1.6GB just for the weights. It's probably 8-10GB for the edge attribute dicts. Double that to overestimate node and adjacency info. Then you want to be able to do something with it... So, do you have 64-128GB of RAM?
comment
Can you give a short example of `relabel_nodes` raising a `KeyError`?   Because it is not supposed to matter whether the mapping includes fewer or more nodes than are contained in G. For example: ```python G=nx.path_graph(9) H=nx.relabel_nodes(G, {1:2, 2:1, 11:13}) ``` works for me.  With copy=False I get a `NetworkXUnfeasible` error.  Also, I don't think a KeyError for a missing key in the dict mapping and a NetworkXError for a node missing from G is inconsistent. A dict can have any key -- not just nodes. While a Graph uses nodes as its keys.  The `NodeNotFound` was an experimental attempt to move toward having many error types within the NetworkX package. It became too much to maintain with little benefit since the error message is present either way.  But we should probably figure out what to do with that.  Care to write a NXEP for how best to organize our error types?
comment
Ahh...  The corner case where the KeyError is raised is: ```python G=nx.path_graph(9) nx.relabel_nodes(G, {1:"A", 2:"B", 11:13}, copy=False) ``` The other trouble with this corner case is that the mapping has been partially applied by the time this error occurs. So it leaves the relabel in a bad state that is not easy to predict.  I'd like to figure out if it would work to `continue` rather than raise any error. Otherwise, I think we should make sure the whole relabel will work before we start doing it. 
comment
It will indeed work to have the code ignore extra keys in the mapping that are not nodes in the graph. Ignoring that entry is already done when copy=True and doesn't cause any issues when copy=False.  Making that change would be a change to the API and could mess up a user who relies on `nx.relabel_nodes(G, mapping, copy=False)` raising a KeyError when a key of mapping is not a node in G.  Would anybody do that?  @jarrodmillman, would this change need to be a deprecation since the code wouldn't raise an error anymore, or can we treat it as adding a feature where we don't need to deprecate?  To be clear, it isn't completely settled that ignoring keys that aren't in G is the right thing to do. But I think it is... 
comment
Thanks @jarrodmillman !  That sounds like a good approach. I approve...  :} 
comment
That's pretty big.  You should try[ the pygraphviz interface to dot files within NetworkX](https://networkx.org/documentation/stable/reference/drawing.html#graphviz-agraph). But that might take a long time too -- no promises.
comment
It sounds like you have a memory issue. NetworkX stores a dictionary to hold the edge attributes for each edge. This can use a lot of memory if you can a lot of edges or many attributes.  Dot files often have memory intensive edge attributes because it is a format for drawing. You could preprocess the dot files to remove some of the edge attributes.   You could also try using the [ThinGraph subclass of Graph](https://networkx.org/documentation/stable/reference/classes/graph.html) and you can use integers instead of strings for your nodes. But I think both of those options are not effective when reading from a dot file.  To address your question about support for large graphs in NetworkX: NetworkX is capable of working with graphs with the number of nodes and edges you are talking about (1.5M edges). But once you start storing information on each edge it uses a lot of memory to store those that information. I think the trouble here is that the dot file is creating edge attributes for each edge. Can you get the graphs in a format other than dot?
comment
If you can use another structure to hold the edge information it might work. Somehow the information is stored in 300 MB in the file. What makes it expand so much? Does the file only store default values while the pydot created graph holds those default values on every edge? Some other reason?  You don't have to store edge attributes on the edges. You can use a separate structure that might be more efficient, for example with default values. It depends what you are trying to do.
comment
I think ```.itertuples``` is supposed to be an improved ```.iterows``` but I haven’t looked closely at this case.
comment
You should add the deprecation warning to ```networkx/conftest.py``` so it gets ignored. Also in that file, you'll need to add code so Appveyor can test on Windows when geopandas is not present. I would suggest using the pandas dependency as a template for your geopandas dependency in that file.
comment
It looks like the [Wikipedia entry on binomial tree](https://en.wikipedia.org/wiki/Binary_tree)indicates it could be directed or not. And other sources I found agree that the direction should be outward from the root -- just as you are implementing it.  The other case your code allows is MultiGraph/MultiDiGraph and I think this works well for that -- no multiedes are created, but I don't think people will expect them to be and this doesn't try to add the same edge twice (relying on it not being MultiGraph).  Still, it would be good to include MultiGraph and MultiDiGraph in your tests if that is easy.  Thanks for this! 
comment
I guess you should add this to the docs in ```doc/reference/algorithms/approximation.rst``` and you can check how it looks on the Details link of the "Circle-CI:  build artifact" portion of "All checks have passed" section of the PR webpage.  If that isn't clear please ask.  For the tests, can you make the tests deterministic (add seeds for the random generators there)?  Thanks!
comment
I've reviewed this and it looks read to merge.  The docstrings should ideally start with a single short one-line description (used for any links to this function from other parts of the docs) followed by a blank line and then a paragraph describing in more detail what the function does. These two functions have two-line starts to the docstrings. So we'd need to reword them to be shorter and then add a paragraph to complete the description of what they do. Would be good to have a very brief description of what a cut value is in that first paragraph.  I can do the docstring changes if you don't have the time now. I'm going to approve the PR as is so we can move toward getting a second approver. At that point I'll put in docstring changes if they're not in yet.   Thanks!!
comment
Looks like something is wrong with the debian ip address.  I'm not sure how to fix that... wait and then restart I guess...
comment
You want `G.nodes[1]['value']` The syntax `G[1]` returns the adjacency dict for the node 1. That is related to edges, not nodes. Check out this [tutorial](https://networkx.org/documentation/stable/tutorial.html).
comment
MultiDiGraph structures are supported as of #1280 (about 6 years ago). But it looks like the doc_string should be updated to reflect this!!  I'm not sure what Dantzig actually refers to in terms of an algorithm, but there are [citations in the docs](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.flow.network_simplex.html) to papers describing the algorithm.
comment
Good!   I'm going to leave this open as a reminder to change the docs for simplex_method (and maybe others in that module) to remove text stating an error is raised for multigraphs when it actually isn't true.
comment
You are assuming that the node is the same as the index number. That is not generally true. The sixth node does not have to be the node 5.
comment
You are still using the node index as if it is the node. When you set up `for node in range(len(G)):` you don't get the nodes in G necessarily. Use instead `for node in G:` or if you need the index and the node use `for idx, node in enumerate(G):` The `nodewise_deg` variable apparently doesn't include all the nodes.  Perhaps do:    ```python for idx, node in enumerate(G):     if idx != node:         print(f"index {idx} is not equal to the node {node} with degree {G.degree[node]}") ```
comment
This is interesting. If I understand right, the idea is that once scipy/numpy/matplotlib make it so importing sub packages happens automagically upon attr lookup, we will remove the commented import lines.  I like the idea that a single import of scipy allows you to be able to use e.g. scipy.sparse without having to import that sub package explicitly.   Could we do the same thing? Instead of having to import approximations separately, let's try to make it so nx.approximations loads the sub package.  If we work the kinks out for that, we might learn enough to try to make it work for scipy, numpy, etc.  But that's another PR. This looks good to me. I haven't searched to make sure all places have been changed. I guess that you have done that.
comment
It looks like github actions might provide cuda support but I haven't looked closely:  - https://github.com/ptheywood/cuda-cmake-github-actions  - https://github.com/FLAMEGPU/FLAMEGPU2_dev/issues/290
comment
Yes!  Thank you for this nudge. They should have been included in the sphinx docs and never were...
comment
See #3341 and #3362 Try updating your networkx version.  This short code is an attempt to implement what you wrote and it seems to work in recent versions. ```python In [12]: import networkx as nx In [13]: DG=nx.DiGraph([(1, 2)], weight=1) In [14]: nx.closeness_centrality(DG, distance='weight') Out[14]: {1: 0.0, 2: 1.0} In [15]: nx.closeness_centrality(DG) Out[15]: {1: 0.0, 2: 1.0} ```
comment
Thanks for this!  The geojson file in question is ~200KB. The URL is likely to cause more trouble simply because we don't know when it will change. If we put the file into our repo then we know when it will change.  That's my reason for wanting it in the repo rather than via https.  I may be missing something here though. I'm guessing the data won't need to be updated often -- or at least if it does, the URL would need to change anyway, so we're not adding to our work to keep in in the repo.  Nice work! Thanks!
comment
What do you suggest? Is there an easy way to test that the shapefiles are 2D as we process?
comment
Before we can give an error message we have to know how to identify that the file is 3d or 2d.  If using geopandas instead of gdal would work, then maybe a separate function for 3d would be better and the user can decide which function to use.
comment
I guess the first thing is to raise an error message when there is 3D data in a shape file. That's the first PR, perhaps. Then it'd be nice to implement a way to handle 3D shape files. Not sure how much code that takes though.
comment
Take a look at: http://networkx.readthedocs.io/en/stable/reference/generated/networkx.readwrite.nx_shp.read_shp.html In particular, there is a "source" link which takes you to the source code for the function. It looks fairly straightforward -- simply using getGeometryType to determine nodes (if 1) or edges (if 2).  Perhaps your file uses a different convention for GeometryType?    Its hard to see what could be wrong without more info. Can you make a small example that doesn't work? 
comment
Thanks for this!  We will need tests and hooks into the docs (```doc/reference/drawing.rst```). I know it's been a long time and life is different now. So if you don't have time let me know and we'll finish it up.
comment
Thank You!!
comment
Thanks!  Hard to believe there were so many of these. :)
comment
Maybe that keyword should be removed -- or replaced with edgelist?  If they want to draw edges that connect nodes listed in `nodelist` then they could draw the induced subgraph instead of the graph. `nx.draw_networkx_edges(G.subgraph(nodelist), ...)`
comment
I think we should leave it in...   At the very least it reminds us that we need to rethink our drawing parameters.  I actually suspect drwaing will have a fairly major rewrite, probably after 3.0  For example, do we really need nodelist to get nodesize? Perhaps nodesize should be a dict so if doesn't separate nodes from data about nodes. Maybe all node data related to drawing should be contained in a single structure.  This sounds like it might be what the fiber-bundle approach being developed for matplotlib may evolve toward.  Perhaps best to leave it alone for now, especially if it requires more than simple removal.
comment
I think this allows us to test what happens when the package isn't available. Wherever we have `try/except ImportError` along with code in the except, this will be helpful.  `convert` is probably the most important place for this.   Don't we just raise an error if `pygraphviz` or `pydot` aren't available?
comment
> I missing something?  No -- I think you've got it... I'm missing something. :}  I thought this was created to test the except code of a `try/except ImportError` clause.  
comment
```n_choose_k``` is nicely expressed in Python 3.8+ as ```math.comb(n, k)```. But we probably need to keep something around until we stop supporting Python 3.7.  There is only a reason to keep a ```numpy``` version and plain python if there is an advantage to each approach. For example, the algorithm used might be different with one using matrix manipulations and one using graph search techniques, or similar.  Thanks for this!  I think there could be demand for Panther++. Let me think about where ```generate_random_paths``` might fit better.    small tweaks -- typo "ictionary" in docs for ```generate_random_paths```.  And could the example do soemthing with random_path to show that it has two outputs and what someone might want to do with them?  Perhaps you could return just the paths and have an optional argument ```index_map``` that could be filled if supplied. ??
comment
Rats -- I didn't notice....  two things:   1) you don't need the ```>>> import networkx as nx``` in the doc_string examples. That is run automagically by the testing software before the example code. 2) you can use ```n_choose_k``` without reference to ```nx.``` because it is in the current module.  So I think the errors go away by removing ```nx.``` from the ```nx.n_choose_k``` and for looks, please take out ```import networkx as nx```  Thanks! 
comment
Quick question:  In the docs, delta is:   The probability that 𝑆 is not an epsilon-approximation to (R, phi)  What are R and phi?  You can answer here-- but if you prefer, you can add more to that doc string.
comment
Good Thanks!     But what is S... ?   I don't think these symbols are defined in the doc_string.... only in the article. The code refers to "S", but it is an array of values, which is hard to see how to be epsilon close to a 2-tuple (R, phi)...    
comment
This seems good -- I like that it is obvious to us that its got mis-styled code. It's too bad that the user still has to figure out 1) that not running black is why it failed, and 2) how to run black so that it will pass. Can we find a way that the CI runs black and makes a commit in the PR? Is there a reason (like black changes we might not want?) we shouldn't do that?  A more helpful message (like: use black) would be nice too. Many may not know the reference to "lint".  Can we make the name of this test: use black for style
comment
Yes -- I see the problem. The contributor would have to pull from their repo even though they didn't make the changes... :{   that would be confusing.   I'm just trying to find a way that "style" isn't a hurdle to contributing or reviewing code. We should be thinking about the code and the API -- not the code style.  Maybe we could forget about style/black for merging PRs, but once a month(?) run black on the whole repo?  Or, maybe forget about style and add running black to the release process?  In this case, we should find a tool to make the doc_strings styled nicely too... run them both before each release.  I'm just brainstorming here... don't feel pressure to implement any of this.  But it'd be nice to not think about style so much.
comment
Yes, I think it is worth trying this system and seeing how it goes.  I approve this PR. 
comment
Thanks for finding this -- I couldn't figure out why bridges was failing to work... :}
comment
I believe current Graphviz is v2.42, but the buggy version (v2.40) is still installed by conda.  Once we get v2.42 the tests should pass.
comment
This looks good -- and a nice solution to the conda graphviz mess. Thanks! 
comment
This could be related to [a bug in graphviz discussed in the pygraphviz email list](https://github.com/pygraphviz/pygraphviz/issues/111) I still have not been able to recreate the error on my machine --  but the symptoms on travis are precisely that multiedges are ignored. 
comment
This problem was skirted around in #2853   When graphviz releases Version 2.41 we can revisit whether to go back to only using homebrew for the osx travis-ci build.
comment
That's great!  Thanks!!
comment
Yes, let's deprecate those two functions. That'll simplify that code some I hope...
comment
Very glad to see this!                    Maybe it fixes #2730
comment
Good idea to split these out into separate PR.  I'll take a look...
comment
I think the TestThinDiGraph setUp was setting up an undirected set of graphs to test instead of directed graphs. I also had trouble with self.K3 not being set up the way I thought it should be.  I'm not sure I need the ```.copy()``` in every place I made it ```G = self.K3.copy()```.  But I figured, if ```G``` is going to be mutated, we might as well act on a copy to make it clear that self.K3 is not getting changed.  The main change is to use the base class ```BaseGraphTester``` instead of ```TestGraph``` as the base class for ```TestThinGraph```.  Similarly for ```TestThinDiGraph```.  That removes all the tests that check attributes. The ThinGraph doesn't update attributes. 
comment
I'm in favor of updating the code to take advantage of py36 features. I think this approach is a good start. Looks like mostly switching to set comprehensions and fstrings. I'll take a closer look soon. Keep it coming! :)
comment
I've looked through it -- very nice. I learned a couple of things too. Nice to clean up the code. I know such a big commit will probably make it slightly harder to trace back the history of some lines of code, but I think it's worth it.  I don't see any potential issues with this. I can merge it you'd like, or you can when you're ready.
comment
Excellent!   Thank you!
comment
The code shows that the edge data is handled by simply adding edges that used to go to the disappearing node to the newly contracted node. That means any edge attributes for edges that used to go to "src1" can be silently overwritten. I'm not sure there is a definition of node contraction that imposes what we do with edge data. So I think we can choose. We can certainly do better than what is currently implemented.  Perhaps we check for each edge attribute of ```('dest', 'src2')``` whether edge ```('dest, 'src1')``` has that attribute. If it doesn't, update with the new attribute. If it does, then collect that in a datadict to store it in a 'contraction' attribute of the attribute ```('dest','src1')```.  Many other possibilities... Ideas?
comment
Yes, that'd be helpful. Thanks!
comment
Take a look at the code for [draw_networkx_edges()](https://github.com/networkx/networkx/blob/33980ffeef6de64fa543692bcc713e2b1fcdbcce/networkx/drawing/nx_pylab.py#L429) to understand more.  The directed graph edges are drawn with a matplotlib ArrowCollection. The connectionstyle keyword argument is only used in this case. The undirected graph edges are drawn with a matplotlib LineCollection. The style collection keyword argument is only used in this case.  See if you can look at the matplotlib docs for these two collections to figure out how to make e.g. a dashed arrow.  NetworkX doesn't do this, so if you figure it out please let us know how (and maybe create a pull request)
comment
The example code you point to doesn't have a list that iterates over the edges. Both stable and latest version of documentation uses the following idiom to set the alpha values:      # set alpha value for each edge     for i in range(M):         edges[i].set_alpha(edge_alphas[i])  Try the code at [this documentation link](https://networkx.github.io/documentation/stable/auto_examples/drawing/plot_directed.html)
comment
Hmmm...  I think 3.0.2 does allow indexing on LineCollection. I have used it with 3.0.3 so I know it works there, and it worked with 2.x.  This is an example session. Figure shows up fine -- no errors.      Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46)      Type 'copyright', 'credits' or 'license' for more information     IPython 6.4.0 -- An enhanced Interactive Python. Type '?' for help.          In [1]: import matplotlib as mpl      In [2]: mpl.__version__     Out[2]: '3.0.3'      In [3]: from  __future__ import division        ...: import matplotlib as mpl        ...: import matplotlib.pyplot as plt        ...: import networkx as nx        ...:         ...: G = nx.generators.directed.random_k_out_graph(10, 3, 0.5)        ...: pos = nx.layout.spring_layout(G)        ...:         ...: node_sizes = [3 + 10 * i for i in range(len(G))]        ...: M = G.number_of_edges()        ...: edge_colors = range(2, M + 2)        ...: edge_alphas = [(5 + i) / (M + 4) for i in range(M)]        ...:         ...: nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='blue')        ...: edges = nx.draw_networkx_edges(G, pos, node_size=node_sizes, arrowstyle='->',        ...:                                arrowsize=10, edge_color=edge_colors,        ...:                                edge_cmap=plt.cm.Blues, width=2)        ...: # set alpha value for each edge        ...: for i in range(M):        ...:     edges[i].set_alpha(edge_alphas[i])        ...:         ...: pc = mpl.collections.PatchCollection(edges, cmap=plt.cm.Blues)        ...: pc.set_array(edge_colors)        ...: plt.colorbar(pc)        ...:         ...: ax = plt.gca()        ...: ax.set_axis_off()        ...: plt.show()        ...:  
comment
That seems like a reasonable addition to the triads.py module. You might be able to leverage ```triads_by_type``` to build a dict keyed by node to a dict keyed by triad type to the count of triads with that type that involve that node.
comment
I would prefer all these code variable names go to `nbr` instead of `neighbour` or `neighbor`. e.g. `nbrs_dict`. That resolves the issue of spelling differences and makes a distinction between a variable name and the method name `neighbors`.   Also I'll agree with @rossbar that this could be folded into #4309
comment
Right -- the function does not copy that data because edge data doesn't exist for the new edges and node data is available in the original graph. It should be easy to copy the node data if you'd like with something like: ```python # G is original and H is complement nx.set_node_attributes(H, G.nodes) ```
comment
Hi!     Can you give a description of your goal with this PR?  Why are you implementing a MinHeap and what do you hope to use it for?  Your [comment elsewhere](https://github.com/networkx/networkx/issues/4224#issuecomment-729089918) suggests to me that you don't quite understand this Heap class yet. For example, the first doc_string in the module (for MinHeap) says that the heap is a collection of key-value pairs sorted by value.  That is not at all the same as sorting by keys. Indeed, in our use cases the keys are not guaranteed to be sortable (they are often nodes which don't have to be sortable).  Do you have other questions about BinaryHeap?  Are you interested in improving the tests for this module? 
comment
The current implementation does that in fact -- only it puts the unsortable object in the keys paired with a value that can be sorted.    :}   Edit:    Said another way, the self._dict **is** the heap.
comment
That's clearly a bug.  I agree that it is likely a missing else statement.  But there is more of a problem here.  The try/catch surrounds the entire `__init__` function, so it is "try"ing to define the function -- not run it.  So, really the `import numpy` part of the code is broken.  The try should be inside the if statement and only around the import numpy statement.  AND there needs to be an else clause...  Thanks very much!!
comment
This feature is added in #4360  Thank you for this nudge to create this feature -- it took a long time but I think we've got it now. We wouldn't have without this. :}
comment
 Nice!  There's a lot here.  We would be interested. There is recent interest in tree isomorphism so that provides a place to put it too.  Note: we already have monomorphisms and embeddings in the isomorphisms folder -- we may have to refactor some day, but today is not that day.  On obvious place to put it would be in ```algorithms/isomorphisms/tree_isomorphism.py```.  That module will need its docstring and ```__all__```variable changed at the top and then add your code at the bottom. Tests would then be added to ```algorithms/isomorphisms/tests/test_tree_isomorphism.py```  Alternatively, if it makes more sense to you or is easier (we can refactor if needed later) you can make a module ```algorithms/isomorphisms/subtree.py```. Then a separate module for tests.  Actually this way may be easier for us for now -- but slightly harder for you adding the hooks below.  Either way, put a hook into ```isomorphisms/__init__.py``` to add your functions to the namespace, and a hook in ```doc/reference/algorithms/isomorphism.rst``` to add your functions to the docs. It would be really cool if you have a use case to create an "example" in ```examples/advanced/XXX.py```. These appear in the NetworkX Gallery part of the docs.  Short example usage is good to have in your function docstring. The "examples" folder is good for slightly longer or more specific examples.  If you have questions, just ask. :)
comment
Recursive code is good for proofs, not so good for performance. All recursive code can be written without recursion -- the idea is to replace the recursive function call with a stack to hold the state and then make a loop that runs the function and moves things on or off the stack when the recursive function would have been called or returned from.  Easy to say, often quite head-bending to implement.  But it does speed things up quite a bit.  You say you've done some profiling...  What are the bottlenecks? I think that cython code will work well in cases where the type of the variables is known and low-level and there isn't much translation between low-level representation and higher level representation.   Finally, run-time asymptotics is 1) valid in a limit so sometimes not accurate for real-world sized "n" and 2) sometimes easy to mix up when coding -- putting a seemingly innocent idiom that bumps up the exponent in the polynomial. To me, the data looks faster than quadratic...  but that's just using my "eye-ball norm"    :}  I haven't looked through the code in detail, but a first pass suggests that you spend a lot of code packing and unpacking tuples of values... In Python calling functions is often a bottleneck (in my experience) and packing and unpacking tuples is similar (the args to a function are packed to make the call and then unpacked to start the function.
comment
This is a really big PR. It's likely to take a long time to review.  We can proceed in a couple of ways. Either split up the PR into many (ugh) or split up review into parts (maybe also ugh).  I'd like to consider the heart of the new algorithm, but it's hard to know where the boundary of that code is. Which parts are interlinked and which could stand separately?   It doesn't have to be split into multiple github PRs, but it might be helpful to know where the interdependent algorithm code is.  My guess is that ```balanced_sequence``` is the base code of this PR.  It looks like ```balanced_embedding``` is needed for ```tree_sequence``` but not the other direction. Same for ```path_embedding```.  Also, path_embedding and tree_sequence are independent...??? The demodata is used in some of the tests, but isn't crucial to make simple tests. The benchmarks are not depended upon elsewhere.  What do you think about putting benchmarks, and maybe demodata into the ```/examples/``` folder?  It wouldn't take much to make them into an example, right? Mostly adding documentation explaining what they do. But I might be missing something -- I didn't look into that closely.  Maybe we can start with ```balanced_sequence```.  There are many similar functions in there...  Are they doing the same thing different ways? Or is it similar things in similar ways?  What's the story there?
comment
I almost wonder if ```forest_str``` should start a module called  ```networkx/drawing/ascii.py```  (or utf8, I guess ascii is technically incorrect, but its the common name for these kinds of drawings.  :)  Also, this PR uses xdoctest for doctests. Can you make it work with   ```pytest --doctest-modules```? 
comment
Thanks for that description of xdoctest.  And I think ```readwrite/text.py``` is a good name.
comment
I'm +1 for including a recursive version as well as the non-recursive version.  Perhaps called ```recursive_restofname``` or similar to easily identify it as the same function with different approach.
comment
What definition do they use? Does Graph-tool cite a paper (or give their definition).  Our doc_string has the formula for harmonic centrality as defined in the paper [Boldi and Vigna 2013](https://arxiv.org/pdf/1308.2140.pdf) cited in our doc_string. The sum is over nodes v and uses the distance from v to the node of interest u.  By distance, the paper defines d(v, u) as the length of a path from v to u.  We reverse the graph because our shortest_path_length routines are convenient for finding distance from one node x to all others y. So, by reversing the graph we can find d(v, u) for all v by using our shortest_path_length routines.
comment
That's correct.  The iterator is exhausted when creating the list and does not restart when being used later.  If you want a list, use `loops = list(ns.selfloop_edges(gg))`.  If you want a tuple/set/etc, use `tuple()/set()` similarly. 
comment
This makes sense to me.  My understanding is that the GraphML spec doesn't say what values the edge ids should be. Gephi requires that ids are unique across the graph. NetworkX requires ids are unique within edges between a pair of nodes.   What do about it?  One fix would be to create the NetworkX with your own edge keys -- instead of relying on the NetworkX default.  You could use `itertools.count` to generate an increasing sequence of edge ids and use them as edge keys.  I don't recall exactly how it works, but the `write_graphml` function has a `named_key_ids` boolean argument which when True uses an edge attribute named `attr.name` as the id when writing the graph. Here you would need to use a unique edge attribute value for each edge of the graph which could be done with something like:  ```python c = itertools.count() edge_name = {e: next(c) for e in G.edges} nx.set_edge_attributes(G, edge_name, "attr.name") ``` This is untested and I may be forgetting some subtleties ...  
comment
See also #3960          It looks like we should make selection of an attribute to be the edge `id` would be helpful.
comment
The test was to check for a memory leak. We'll have to keep an eye on that. I restarted it and it passed, so indeed it is not due to this PR.  I approve this PR 
comment
Good question...  The use-case envisioned was someone who has a large dict with attributes for nodes from many graphs (for example, the graphs are the component subgraphs of an original graph). Our idea is that they could use that same dict in a `set_node_attributes` function call and if it can't find the node it just ignores that node.  We certainly shouldn't add the node to the graph -- for that we have `add_nodes_from`.  But should we silently ignore, or loudly raise an error. I can see arguments both ways. The current code silently ignores.
comment
The edges are ordered by the adjacency order of the underlying data structure. So the 1st node increases in the same order that nodes were added to the graph. Then the 2nd node on the edge appears in the order that edges incident to the  1st node were added.  This order changed when python 3.6 changed how the order in dicts changed. So prior to python 3.6 the order would be different.  
comment
I'll close this, but we'll still see posts here...
comment
This is tangential and in response to the docs claim of the fastest way to do this. ```python         for n in self.adj:             yield (n, set(self.adj) - set(self.adj[n]) - {n}) ``` might be better said: ```python         nodes = set(self.adj)         for n, nbrs in self.adj.items():             yield (n, nodes - set(nbrs) - {n}) ```
comment
We haven't ever found a good place for benchmarking code. It looks like numpy has found a way to do that. But those functions and algorithms have fewer parameters to explore, so are easier to benchmark.    Also, the example case here is only benchmarking 2 lines of code -- How do we store benchmarking code for every set combination?  Anyway, we don't currently have a good way to store benchmarking code.  What *should* we do? 
comment
First of all, the docs are incorrect when they say that ```edge_data``` should be: ```list, optional```.  It can actually be anything. It is the value in the dict-of-dict.  The text talks about making it the value ```1```, so it clearly is not intended to be a list.  When ```edge_data is None``` we put the edge data dictionary into the dict_of_dict structure. That makes this a dict-of-dict-of-dict structure. But the edge_data input allows any **single** value in the dict-of-dict output.  I am NOT saying this is what it should be. Only what it is.   Trying to include a dict or list there is misleading because it will be the SAME DICT FOR EVERY EDGE!  Changing ```dod[u][v]['weight']``` will change the weight for ALL EDGES simultaneously because there is only one dict.  I suspect historically this was created for symmetry with ```from_dict_of_dict```.  And since it wasn't clear what people might want -- except to make a value of ```1```, the interface left a way to do that (and maybe more).  But I suspect that ```to_dict_of_dict``` has not been used very often. @berlincho have you used this function?  This is an artifact that shows our initial neglect of multigraphs and edge attributes. I think v3.0 and a reconsideration of the matrix interface might give us a chance to enrich the treatment of attributes. For example, requiring ```edge_data``` to be fa function of u,v here could be helpful. But would make the code more tricky...  Perhaps the interface from #4217 could be used for this ```to_dict_of_dict``` too.  We could also remove the function -- or at least the ```edge_data``` optional argument. We've never received any comlaints about it despite it being incorrectly documented and not useful the way it is documented (you'd have the same list for every edge in the dod).
comment
After more thought: `to_dict_of_dict` should return a simple dict-of-dict representation of the graph/multigraph. edge_data should be a singleton (usually `True` or `1`) to indicate an edge.  The default can remain that we create a dict-of-dict-of-dict with edge data in the inner dict. But the docs should change.  Let's change the doc_string of `to_dict_of_dict` from: ```   edge_data : list, optional        If provided,  the value of the dictionary will be        set to edge_data for all edges.  This is useful to make        an adjacency matrix type representation with 1 as the edge data.        If edgedata is None, the edgedata in G is used to fill the values.        If G is a multigraph, the edgedata is a dict for each pair (u,v). ```  to a more accurate and help version including a one-liner to build your own: ```   edge_data : singleton, optional       If provided, the value of the dictionary will be set to `edge_data` for all edges.       Usual values could be `1` or `True`.       If edgedata is None, the edgedata dictionary in G is used creating a dict-of-dict-of-dicts.       Note that if G is a multigraph, this will be a dict-of-dict-of-dict-of-dicts.       If you want something more custom made try:       dod = {n: {nbr: custom(n,nbr,dd) for nbr, dd in nbrdict.items()} for n, nbrdict in G.adj.items()} ```  Maybe the one-liner should be used in an example in the doc_string.
comment
Can you elaborate on this a little...  We're talking about small fractions of a second and potentially costing a fair amount of maintainer time to keep the lazy imports working well. What are the benefits to you of this change? I know in an ideal world it'd be nice to have imports take no time and we'd just load the functions we need. But it's not an ideal world. So, can you expose the cost/benefit for us a little?   Thanks!
comment
That was not my question.  Of course, we can pull in exactly the code you want to use and nothing else... but at a large cost of making sure this works for eac usch user. We would obviously not customize it to each user. But have chunks of the package.  But what I'm asking is: what is the benefit to you of us doing that?  How much difference does this make and how costly is that?
comment
Well said @rossbar !   I agree... We'll certainly merge PRs that help with import time if they don't introduce other maintenance or performance issues. 
comment
I think there are other methods (called lazy import?) where you import a stub to a subpackage that, when referred to, does the import of that subpackage and replaces the stub with the imported package. I don't know much about them. :)
comment
Interesting... Do I read correctly that it's the import of scipy.special that is the hold-up? If so, we can easily move that inside a function so it only gets done if that function is called. It is only used in one place in one function.  But maybe I am not read the profiling output correctly.  
comment
Cool -- I don't see where it would import scipy.sparse on the initial import. But both scipy.special and scipy.spatial are imported at the module level when those imports could be moved inside a function. I'll try to put together a branch to try...
comment
How about this branch: https://github.com/dschult/networkx/tree/time_imports I can make it a PR if that's better for you.  I found a number of places where an import could be delayed.  Not sure if lazy-imports would work to reduce this even more, but just with these changes it is faster... Surely not worth the time on its own, but the code is probably better because it got cleaned up a little.
comment
I haven't looked into this much yet, but I can verify that the example shown does produce the stated (incorrect) output.  I've put the label "Defect" on this issue. @manasjoshi14 if you can find a/the bug and can fix it that would be great. Does the "source" keyword even make sense with this algorithm? Maybe that needs to be a separate function?
comment
This looks like a bug I believe we ran into elsewhere in the code before. The set that is yielded is added to the `seen` dictionary AFTER yielding. Therefore any changes to it affect whether the algorithm thinks it saw this set previously.   This is definitely a bug.  And should be easily fixed by updating the "seen" dictionary before yielding the set `c`.  While we do this, we need to check the other `weakly_connected` code and I see it in one place in the `strongly_connected` code as well... so we better look through that as well. 
comment
I think I've got the bug squashed. I added some tests so it doesn't come back. Thank you for reporting it!! 
comment
The VF2 algorithm in NetworkX is capable of isomorphic subgraph matching as well a monomorphisms (subgraphs that aren't induced).  It is very well documented in the sphinx docs. But once you create the GM=GraphMatcher(...) you can set it's `GM.test` attribute to be `graph`, `subgraph`, `mono` (see [the docstring for `symantic_feasibility`](https://github.com/networkx/networkx/blob/master/networkx/algorithms/isomorphism/isomorphvf2.py#L321)  You want the `mono` option. There are also methods like: `GM.subgraph_is_monomorphic` and `GM.subgraph_monomorphisms_iter`.   There is more in there than is obvious at first.  We should open it up -- should also update to VF2++ which I hope to do in January.
comment
Sorry that you had to refork and copy the changes over. And I don't know why I didn't nogtice this before, but you should also add your functions to ```networkx.algorithms.__init__.py``` so that they can be called using e.g. ```nx.dedensify()```  Also, the top comment of the main module says that these function remove edges from a graph. Doesn't densify add edges to the graph? Is there a better way to describe what these functions could be used for?  Thanks!
comment
I like the docs for the module. And it's great to have the example in the gallery. The top docstring for the gallery example could use a paragraph describing what the example is all about... -> what it does and why that might be interesting.  I have a question about the ```expansive``` flag input:  If dedensification is used to reduce the total number of edges, why would someone want to use the ```expansive``` option?  I think this is a good addition and it's close to being ready to merge.
comment
Dijkstra's algorithm doesn't work with negative edge weights. That's why Bellman-Ford was invented (well one reason anyway).
comment
Thanks!
comment
I think the trouble is not the algorithm in this case. The edge data is not fully unpacked in your example. The edge data looks like:  ```python g.edges['1', 'stmt:1-P131']          # =>  {'data': {'weight': 14, 'predicate': 'P131'}} ``` In your graph, each edge has only one attribute called ```data```. The value of that attribute is what would normally be the edge attribute dict. I think this is what you want: ```python g.edges['1', 'stmt:1-P131']          # => {'weight': 14, 'predicate': 'P131'} ```   To make your code load correctly, try changing the adding of the attrubutes: ```python def load_json(infile):     data = json.loads(open(infile).read())     g = nx.MultiDiGraph()     g.add_nodes_from((n, d['data']) for n, d in data['nodes'])     g.add_edges_from((u, v, k, d['data']) for (u, v, k), d in data['edges'])     return g  G=load_json('graph1.json') arb=nx.maximum_spanning_arborescence(G,'weight') sum(wt for u,v, wt in arb.edges.data('weight'))        ``` Then I get 67 for the solution.
comment
Actually, it's good to avoid any floating point numbers as keys for dicts.  Round-off makes floating point values really bad for lookup.  The trouble with NaN though is that "equality" doesn't hold for NaN with itself. Still, it's not good to use any kind of floating point for nodes.   If you really want them you can convert to integers (after scaling up as large as you want). 
comment
On my Appveyor page when I look at the list of all jobs run, there are 3 options in blue at the top right. The right one ("Re-run Incomplete") rebuilds only failed or cancelled jobs.  I selected the middle one ("Re-build PR") and it restarted all of them. *oops*.  Ah well...     At least its better than touching the PR branch to trigger the CI.
comment
Is it better to have ```__str__``` call ```nx.info``` or to have ```nz.info``` call ```__str__```? Two criteria I can think of: which is more efficient (shouldn't matter) and where will people look for the code (they'd probably look for ```__str__``` before looking for ```nx.info```... Are there other reasons/criteria?
comment
Just want to flag @chebee7i on this ```nx.info``` issue :}
comment
It seems to me this is a bug in ```nx.contracted_edge``` which should not assume that ```edge``` is length 2. It should use ```*edge[:2]``` as the argument, or add a line with ```u, v = edge[:2]``` before using ```nx.contracted_nodes(G, u, v, self_loops=self_loops)```  Thanks for the report!
comment
You probably want  ```isomorphism.DiGraphMatcher``` instead of ```isomorphism.GraphMatcher```
comment
Excellent!  Thank you so much...   They aren't pdf links anymore but that's the point I suppose... We should really be using the universal locator DOI links...
comment
Maybe the [relabel_nodes functions](https://networkx.github.io/documentation/stable/reference/relabel.html) will help.  Are these other modules within networkx or in another package? I expect all NetworkX functions to work with nodes that are tuples.
comment
Actually, for Python 3.6 or later the behavior of Graph/DiGraph is also ordered because dict's became ordered with that version.  And...   The order of adjacency reporting... g.adj, g.successors, g.predecessors is indeed the order of edge adding. But the order of g.edges is not always the order of edge adding. The order of g.edges is the order of the adjacencies which includes both the order of the nodes and each node's adjacencies.  ```python g = nx.DiGraph() g.add_edge(2, 1)   # adds the nodes in order 2, 1 g.add_edge(1, 3) g.add_edge(2, 4) g.add_edge(1, 2) assert list(g.successors(2)) == [1, 4] assert list(g.edges) == [(2, 1), (2, 4), (1, 3), (1, 2)] ```
comment
Just to be very clear, you seem to be getting a different Graph class than what you think you are getting.  I think we've narrowed down that you have the correct version of python and networkx.  Now, can you run this short piece of code:  ```python import networkx G=networkx.DiGraph() len(G) ```  Your DiGraph object should indeed have an attribute ```_node```.   Maybe your DiGraph construction process is removing that attribute?
comment
Also, see ```read_edgelist``` in [the readwrite section of the docs](https://networkx.github.io/documentation/stable/reference/readwrite/index.html)
comment
Thanks!
comment
It looks like the new code is creating more children than expected. can you take a simple example and create graphml files with both old and new code to see the difference?  I like the idea of this PR.  Thanks!
comment
The example is the one that is failing in Travis-CI tests. You can see which tests are failing by clicking on "Details" next to the Travis-CI tests (preceded by a red X) near the bottom of the Pull Request page on github. The Details show you all the various configurations we automatically test. Any rows that are red have an error. For now, don't bother with the first row which also builds the documentation. Click on, say, the 2nd row and search for the word "fail". The report shows which test failed and t hen a little later gives the error message for the test which you can then find the line number and look up in your code on your computer. You can run the tests yourself using nosetests (install with pip install nose). Then cd to the readwrite directory and run ```nosetests --with-doctest graphml.py tests/test_graphml.py```  The error reported from the test indicates that the expected result gives 3 children while the new version gives 5 children.
comment
nice -- what was the fix? I can't see any difference... Also, anything else for this PR? It looks ready to me...
comment
Thanks!
comment
Thanks for this!   Can you remove the deepcopy from the tests? It should work without deepcopy right? 
comment
Hello -- sorry for the lack of attention here.  The timing was bad last Spring for us to get back to you and we still haven't responded even after your polite nudge on the email list.  [You can post a comment here to nudge us as well... The developers get emails from github the same way as for the email list. ]  I still haven't had much time to look through the code, but I've skimmed through it. This is a perfectly good algorithm and it seems to be well structured for merging. Thank you! I've added labels which put it in the queue for the next release, which raises its visibility.  Unfortunately I probably won't have time to look at it in detail before mid-November.  The files are in the right place. You should add the functions to a section of the documentation by adding an entry to ```doc/reference/algorithms/index.rst```.    The function signatures are not specific enough -- in particular, can you replace ```**kwargs``` with ```threshold=2``` and ```in_place=False``` in the 2 functions respectively? The docs would have to be updated too.  Thanks!!
comment
Thanks very much!!    It looks like all our tests use integers where ```index[a] == a```. Can you add a test to ```tests/test_kernigan_lin.py``` that checks this bug? It could be one of the first simple tests, but e.g. with strings as nodes instead of integers.  Thanks! 
comment
hmmm...   I see what you mean -- but I don't understand what the advantage of shuffling the labels is. Why do the shuffle?  Also, now I don't understand why our tests don't raise any exceptions.  Can you explain why the tests pass with the current code if this error is a bug?
comment
OK... Thanks!  I understand now.  Could you add a test to ```tests/test_kernigan_lin.py``` that checks for this bug?  It could just copy one of the simple tests, but e.g. with strings as nodes instead of integers, or like you said: integers 1-12 instead of 0-11.  That way, we have a test to make sure this bug doesn't creep back into our code somewhere else.
comment
It sounds to me like we are converging on a scenario where we only report graph name, class, number_of_nodes and number_of_edges. Then we maybe make this output the ```__str__``` output.   Looks like @parth-verma [suggested](https://github.com/networkx/networkx/issues/4139#issuecomment-673532370): ```DiGraph(name:"asdf",nodes:3,edges:2)```  Could also be: ```DiGraph(name="asdf", order=3, size=2)```
comment
I was having trouble with ```nodes: 3, edges: 2``` saying it was nodes and not number_of_nodes...  But how about ```DiGraph named "asdf" with 3 nodes and 2 edges.``` or if G.name is empty, ```DiGraph with 3 nodes and 2 edges```.  Do we want the memory location for any reason?
comment
I think we agreed to push the rest of this Issue to v2.6. Most of it is done except the hadr part of moving from travis.
comment
Thanks for this -- especially the test. :}    The change in behavior was released in NetworkX v2.5 and will be fixed in v2.6.
comment
Thanks!
comment
I think it's probably better to do the conversion to int before you run the algorithm.  Something like: this should work: ```python G.add_edges_from((u, v, {"weight": int(wt)}) for u, v, wt in G.edges.data("weight")) ``` This doesn't actually add new edges because they already exist. It just updates the edge attribute dict. So even if other attributes exist, this doesn't change them.  It'd be nice if ```read_dot``` had conversion capabilities for attributes, but no one has implemented it.
comment
Right ---  sorry about that... Put square brackets around the input to ```G.add_edges_from``` ```python G.add_edges_from([(u, v, {"weight": int(wt)}) for u, v, wt in G.edges.data("weight")]) ```
comment
Yes -- for multigraph you should keep track of keys too... ```G.edges.data("weight", keys=True)``` with appropriate changes elsewhere in this line...
comment
Thanks!
comment
Perhaps your concern is different from the original issue of the poster? The compose function did what that user needed. Apparently it is also what you wanted.  In fact the newer graph method ```update``` might do even better without the potentially confusing name: ```python G.update(H) # or if you still need G around: GG = G.copy() GG.update(H) ```  The ```compose``` function and ```update``` method simply take the union of the node set and also the union of the edge set.  That is NOT the union of the two graphs as commonly defined in the literature. It is also NOT the composition (or lexicographic product) of two graphs as defined in the literature.  Perhaps your issue is that we should improve the documentation of ```union``` and ```compose``` by having them point to each other better.  Would that be sufficient to have helped you?  If so, we'd love a PR that improves documentation, but we will get to it if you aren't able to.   It would help to open a new issue rather than reopen an issue someone already closed themselves.
comment
You'll need to write some code to turn the triangles into 3 edges each. Add those edges.  I'm going to "close" this (not a bug), but you can still post follow-up comments on this issue and we will see it.
comment
If you have a triangle "0 0 1 3" then I don't know for sure, but it seems like this means you need edges ("0","1"), ("1","3") and ("3","0").  Adding those edges will add the triangle. Is this what you want?  If so, then you need to write code to split the triangle into it's nodes and form 3 edges from those nodes. You can use something like this to split the string and add the edges (I am guessing about your file syntax so this may not be correct). ```python number, a, b, c = triangle.split() G.add_edges_from([(a, b), (b,c), (c,a)]) ```
comment
Some history from my sieve-like memory:  NX1.x presented 3 ways to get information about nodes: - ```G.node``` was a dict keyed by node to that node's attribute dict. - ```G.nodes()``` was a function that returned a list of nodes (order given by the underlying dict which could change order) - ```G.nodes_iter()``` was a function returning an iterator over the nodes.  To summarize: - ```G.node``` was a dict;  ```G.nodes()``` was ```G.node.keys()```;  ```G.nodes_iter()``` was ```G.node.iterkeys()```  With Python 3.0 there was a move away from creating lists and toward creating views.  The idea of the view is to create a fast-to-create, read-only interface to the data in the original dict. Some dict-views also had set-like capabilities.  We intended to adapt NetworkX with this same approach in mind. But the items view poses problems for dict-of-dicts because the set operations of the ItemsView don't work with unhashable values.  So, we added a ```data()``` feature/view. In addition, we added set-like features to ```G.nodes``` and we reduced the number of ways to get node information.   Thus NX2.x presents only 1 way (```G.nodes)``` to get information about nodes: - ```G.nodes```  is like a dict keyed by node to that node's attribute-dict with some additional set-like operations.    - Iterating over ```G.nodes``` yields the nodes.    - ```G.nodes.values()``` and ```G.nodes.items()``` and ```G.nodes.keys()``` work like its a dict.     - Subscripts ```G.nodes[n]``` return the value (attribute dict) associated to node n.    - Set-like operations behave like ```G.nodes``` is a set of nodes.   - The ```G.nodes.data()``` view is similar to ```items``` but allows optional parameters for the attribute key and default key. So, e.g. ```G.nodes.data("weight", 1)``` is a view to ```(node, weight attribute of that node with default 1)``` 2-tuple pairs. Set operations act on the 2-tuple pairs just like for ```dict.items()```.   Perhaps confusingly, but with good intentions, we allowed ```G.nodes()``` notation to work -- it is the same as ```G.nodes```.  This leads to lots of extra ```()``` in code that could be removed, but it seemed better for those who like a function-oriented interface.  Thus ```G.nodes()[5]``` could be ```G.nodes[5]```.    For a slicing interface, we could use ```G.nodes[5:10]``` replacing ```list(G.nodes)[5:10]```. I prefer a separate method:  something like: ```G.nodes.slice[5:10]``` 
comment
The instinct to slice may be a documentation issue. If users are thinking of ```G.nodes``` as a list, we need to make it more clear up front that it is like a dict.  That is true whether or not we approve this NXEP.
comment
I added some language and made some suggested changes. I couldn't find a way to make a PR to your repository. Please feel free to change/improve any wording or ideas from what I've pushed here.
comment
    What should be the recommended way of getting node attribute data out of the Graph?     G.nodes(data=True) or G.nodes.data() I guess the latter one? We should probably add more      documentation regarding these recommendations.  Thats a good question.  We've left room for a functional interface ```G.nodes(data="raisins")``` as well as a more attribute oriented ```G.nodes.data("raisins")```.  Duplicate routes to the same info is probably not a good style.  "There should be one and only one obvious way to do something." But flexibility is good too if some people think in function oriented ways.  Recommended:   - G.nodes  over G.nodes() - G.nodes.data() over G.nodes(data=True)              [only because G.nodes() yields nodes while G.nodes(data=True) yields 2-tuples (node, datadict) so its a different kind of beast...]  In the end -- I don't know which is best...  and is it any different for edges?  I guess edges have an ```nbunch``` argument too...  It'd be nice to have G.nodes and G.edges as the objects that provide info about nodes and edges respectively. What's the best way to iterate over the info that's provided? We have keys(), values(), items() and data(). But we also provide a callable interface to get the same info.  Is that helpful? hard to maintain? good? bad?   :} 
comment
I believe the definition of topological sort is:  A topological sort of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering.  So, ```[2, 4, 1, 3, 5]``` is a perfectly good topological sort.   There is no implication that all leaves are listed before going to non-leaf nodes.
comment
I see what you mean.  The word "long" used as an attribute type should be translated as an int. In the code, we should add an entry (int, "long") to the list on line 320. (I think)  Short term workaround it to change all "long" in the graphml file to "int".  Thank You!!
comment
You should also add a line in the docs for the "new" function. ```doc/reference/algorithms/minors.rst```  And then the doc_string itself should be updated to standards with one-line intro, blank line, longer description, sections for Parameters and Returns.
comment
One way to avoid the "equivalent, but formatted differently" error in the doctest is to wort the results: ```python >>> sorted(equivalence_classes(X, mod3)) [frozenset({1, 4, 7}), frozenset({2, 5, 8}), frozenset({0, 3, 6, 9})]  ``` It would be nice not to have to convert to a list just for formatting -- it misleads the user to think that we somehow can't work with the sets. But I don't know a better way to arrange the output across platforms.   I supposed we could create an ```expected``` variable and then print an equality check that says "True". But that looks like a test instead of natural code that someone would write. ```python >>> expected = {frozenset({1, 4, 7}), frozenset({2, 5, 8}), frozenset({0, 3, 6, 9})} >>> equivalence_classes(X, mod3) == expected True  ```  Maybe @rossbar has some idea for which is better (or another way)?
comment
Yes -- put the doc_string back the way it was without sorted and add a comment to skip the test. If we ever drop pypy from tests we might be able to remove the skip. But this should be fine for this PR.
comment
The other test is failing due to an old SWIG version using a deprecated package "imp" instead of the newer "importlib". Fixing it does not require new code, but a newer version of SWIG for pygraphviz. (I believe...)
comment
This looks close to ready.  Can you make your changes consistent with PEP8 style guides. Search for PEP8 for the guidelines. You can install a pep8 checking program using ```pip install pep8``` to check your code.  The other nice features would be to add a test of the rescaling, and to check make the check for ```n==2``` case provide non-zero scaled values when ```endpoints is True```.  Thanks!
comment
The ```is_triad``` explicitly rules out self-loops as not triads. Self-loops mess up the census identification process as none of the census types have self-loops.  You can remove the self-loops before processing with something like: ```G.remove_edges_from(nx.selfloop_edges(G))``` 
comment
Excellent! --- Thanks very much for this.   I'm not surprised that this needs a close look and your help is much appreciated.  I would say the "clip_on" argument probably should be added with a default of True. We do specify ```clip_on=True``` in our call to ```text()``` and I can imagine a user wanting to set it since it is a feature matplotlib provides.  That change makes this PR no longer only about docs, but I think it is worth crossing the boundary when it improves the package. Putting clip_on as the last parameter doesn't change the API so no deprecations are needed.  I approve these changes
comment
This looks good!  I don't know a good way to test many of the matplotlib parameters unless you can find a programmatic way to look at the image. We can run a "smoketest" where we create an image with both options and if there are no complaints then we assume it is working (no smoke, no fire?).  I'm also fine with leaving ```clip_on``` untested.  I believe we don't have black automatically check doc_strings yet.  I feel like getting sphinx to interpret it correctly is more important than the PEP8speaks flags.   Thanks very much!  I approve this PR.
comment
I am absolutely swamped with the pandemic disruption at the moment and am unlikely to get to any meaningful review done before mid May. I've put the 2.5 release tag on this PR to be sure it gets done in time for that.   Thanks for the gentle nudge. :}
comment
I have been looking at it occasionally over the last couple weeks. I am having trouble with the overall API for the function -- especially with the ```partitions``` and ```labels``` inputs. It feels like they are providing the same information in a different form. The examples all seem to construct ```labels``` by reformatting ```partitions```.  I want to think about a different call signature.  The ability to have ```partitions``` be an equivalence relation function should probs be given to the user by making ```equivalence_classes``` a public function. Then partitions could be a dict keyed by label to containers holding the nodes for that partition. Partitions that are containers of sets could easily be converted to dicts using ```partitions = dict(enumerate(partitions))```.  I think a function from node to partition label should also be supported too -- perhaps with examples in the docstring for how to create the partitions input from such a function. Both that construction and equivalence_classes``` could be shown side-by-side. Having that outside the function simplifies the parsing somewhat and it is fairly easy to do:  ```p = {n: f(n) for n in G}``` for a labels function and ```p = equivalence_classes(G)```.  I would say the default behavior should be to use labels of the partitions dict as the new nodes. If labels not provided then we use numbers assigned by ```dict(enumerate(partitions))``` and explain/demonstrate in the docstring how they can obtain that mapping from new nodes to old nodes. We'd also need to explain that using partitions as an equiv. relation function will lead to new nodes that are assigned numbers based on the order constructed from ```equivalence_classes```.  Backward compatibility: We can identify old-style partitions: lists of sets or lists of lists would not have an ```items()``` attribute. Callable ```partitions``` have ```__call__```. They can be handled as described above.  The input ```relabel``` when set to "False" should construct the new nodes to be ```frozenset(p)```. It should probably also show a deprecation warning where we plan to remove this argument in nx v3.0.   ```relabel``` left as default can switch to the new default behavior so long as the docstring has a "Note" that old-style default behavior can be fixed in both old and new versions by explicitly setting ```relabel=False```.  ```relabel=True``` can be ignored because it is the new default.  I'm not completely sure this will all be smooth -- I need to look at ```equivalence_classes``` more to see if it does the job. Before anything more is implemented I'd like some feedback from you --- a user dedicated enough to think about all this. What is the best way to represent these partitions and get the info to the quotient_graph function?  How would you naturally think about a partition? A labeled partition? What is someone from this field going to be looking for?
comment
The idea for the currently used labeling system when ```relabel is True``` is that the labels (0,1,2,3,4,5) are the indexes of each partition in the input ```partitions```. This is useful for looking up the nodes in the original graph from the partition node in the quotient graph:   ```partitions[label]```.  It is potentially confusing to have the new graph nodes be one of the nodes from the original graph. Then you can't tell what 36 is referring to.  In terms of code then, let's refer to a list-of-lists as a dict-of-list via ```enumerate```. Also, I'm using the term "list" here but "set" could be used if lookup is desirable. Also I'm treating tuple like frozenset. I think we have various ways to store partitions: 1) a dict-of-lists keyed by partition label to lists of nodes 2) a dict keyed by tuples of nodes to partition label. 3) a function from node to partition label. 4) an equivalence relation stored as a function rel(node1, node2) that returns True if related.  Hopefully we can accept any of these and convert to a convenient one easily.  I'd like to avoid having 2 forms of the partition info input to the function in the same call.  In fact, since it is useful to have the mapping from new_node to sets of old nodes, I think it is simplest to have the user enter case 1) with the help of the docstring.  My preference is to allow case 1): either a dict-of-lists or a list-of-lists.  Then: - we use the docstring to guide users to the ```equivalence_classes()``` function to convert case 4) to case 1).  - we provide an example of switching keys and values to get from 2) to 1). Something like: ```{label: part for part, label in labels_dict.items()}```. - we have an example with a function where conversion from 3) to 1) happens with code like: ```python from collections import defaultdict partitions = defaultdict(list) for node in G:     partitions[f(node)].append(node) ```  I think this allow code improvement by avoiding the relabeling at the end. I think the existing code remains unchanged otherwise except for changing ```product(b, c)``` to ```product(partitions[b], partitions[c])``` everywhere.  This would definitely involve some deprecation because the API changes -- but the old input of list of partitions would work.   Thoughts?
comment
Nice!  Thanks...  Can you expand a little on why a label has to be a node from the block?  I think that could get quite confusing and misleading. In your example, 'a' refers to the node "a" but also to the block "a". I assume you will continue this block process and get a block of blocks that is then also labeled "a". We can't tell which level of the block decomposition "a" refers to. Wouldn't it be better to introduce new labels for the blocks? For example block 0 => {"a","b"} and block 1 => {"c", "d"}. Then if you want a representative, you can always get one, but the label doesn't get confused with a node in the block.  Your term "representative" has different connotations than "label". If you want a representative of the block then you are suggesting that the nodes in that block are similar to that representative. But still, wouldn't you want a label so you can clearly refer to the block (as opposed to the node in the block) and the representative to represent an example (typical?) node in the block? That distinction seems important to me, but I'm not used to dealing with these objects.  I see what you mean about wanting information about both the set of nodes in the block AND the label for the block. My complaint about your proposed storage for it is that it requires storing the node-tuples multiple times. It seems that a block with 100 nodes would both be big AND get stored 100 times.  For 100, it probably doesn't matter, but for 10K?  But it could be effectively stored as two dicts: a dict keyed by node to a block label and a 2nd dict keyed by block-label to the block-node tuple.  That might save memory because the tuples of nodes for the block only get stored once.  I'm not sure how big the graphs are that you're working with...  The two-dict  structure also has the nice feature that the first is a mapping from node to block and the second is the inverse mapping from block to node.  Finally, we can get from one to the other easily so we really only need one of these as input to the function.   My understanding is that the block->node lookup is what we need to construct the ```quotient_graph```. But we probably want to make it clear to the user how to get that from a node->block lookup as well. So, whatever they start with, we should convert to a block->node representation (or tell the user how to convert). 
comment
Yes -- that would be a good way forward. Put code to do conversions into the doc_string, and put ```equivalence_classes``` in the public namespace.  What is the advantage of having the block label be a node in the original graph?
comment
Thanks for this!  
comment
I believe the ```pip uninstall networkx``` solution works even if you installed from source using ```python setup.py install``` but I haven't tested that.
comment
The [docs for bipartite projection](https://networkx.github.io/documentation/stable/reference/algorithms/bipartite.html#module-networkx.algorithms.bipartite.projection) show that ```projected_graph``` works with multigraph and explains how it handles that case.  
comment
Yes -- you are absolutely right -- the notes in the docs are about a MultiGraph output... not an input. Sorry about that.  Hmm.... what would you want it to do? I'm just repeating @rossbar here but do you have any references for what you should do?  One approach that makes sense is to treat the multigraph like a graph and connect nodes that share a neighbor no matter how many edge connect them.  Thats likely to be what you're looking for unless you are worried about edge attributes in some way.
comment
That should be as straightforward as:  ```bipartite.projected_graph(nx.Graph(G), top_nodes)``` 
comment
Yes, let's use ```np.sqrt```!  I believe this could raise an exception if someone was using negative edge weights that are sufficiently negative to make the weighted degree be negative for some node. But none of the theory with normalized laplacians consider that case. Even the very rare negative edge weights examples I can find make sure that the weighted degree of the nodes is positive. With the more simple ```np.sqrt``` such a user pushing the boundaries of theory will be obviously flagged in a way that they probably should be. Much better than introducing complex values into their spectrum without notifying them. If they are purposely doing that kind of work, they will know to look out for that issue. If not on purpose, they should be stopped from further confusion. :}  Thanks!
comment
I'm not sure how much this matters, but the conda forges don't have pygraphviz 1.6 yet, so they will have to get that working in order to make networkx installs work.  Maybe we should leave pygraphviz requirement as >=1.5 for now... ???
comment
I agree that a generator makes the most sense here.  Code like (from @jarrodmillman in #4004  ```python for C in (G.subgraph(c).copy() for c in connected_components(G)):         if C.number_of_nodes() == 1:             yield frozenset(C.nodes())         else:             unnumbered = set(C.nodes())             v = arbitrary_element(C)             unnumbered.remove(v)             numbered = {v}             clique_wanna_be = {v}             while unnumbered:                 v = _max_cardinality_node(C, unnumbered, numbered)                 unnumbered.remove(v)                 numbered.add(v)                 new_clique_wanna_be = set(C.neighbors(v)) & numbered                 sg = C.subgraph(clique_wanna_be)                 if _is_complete_graph(sg):                     new_clique_wanna_be.add(v)                     if not new_clique_wanna_be >= clique_wanna_be:                         yield frozenset(clique_wanna_be)                     clique_wanna_be = new_clique_wanna_be                 else:                     raise nx.NetworkXError("Input graph is not chordal.")             yield frozenset(clique_wanna_be) and delete _connected_chordal_graph_cliques. ``` I believe this also makes the cliques deterministic. The connected components are ordered and while the nodes within each clique are not ordered, the cliques should be yielded in the same order. Nice!!
comment
This looks like a good approach to me. Can you add a test to make sure we don't mess it up with future changes? Perhaps the example you show above?  Thanks!
comment
This looks like a good suggestion to me!  Can you also add tests to make sure we check what happens with each case? The tests that are failing were checking that it did one thing and now that we have changed it we need to check the new behavior. Thanks!
comment
Thanks for all this -- especially the tests... I think I misunderstood something -- but maybe I'm just missing it at this point. How do we get the old behavior? That is, how do we read a graphml file with edge_ids but no parallel edges and have it create a Graph instead of a MultiGraph?    (I'm ignoring directed in this response for simplicity.)  Old system:     - parallel edges -> MultiGraph - no parallel edges, no edge_ids -> Graph - no parallel edges, edge_ids -> Graph with edge_ids stored as edge attribute "id"  New system:  third case -> MultiGraph  This makes it possible to keep it a multigraph even if there are no parallel edges so long as edge_id doesn't exist. But I think it's not possible to make it create a Graph with edge_ids stored as edge attribute "id".  Perhaps we can include a keyword argument to the reader: self.force_multigraph which makes the output a multigraph no matter what.  To be clear, I'm suggesting using the old system (so third option is possible) but allow an override flag to always make it MultiGraph whatever the input.  What do you think? Why did you need this PR, and would this alternative allow what you needed?
comment
Yes, it seems reasonable to increase the version of pyyaml in the extras file. :) 
comment
I don't see any reason to hold off.  Go ahead and merge!   Thanks very much -- this will help a lot.  Its the first (and I think biggest) hurdle to moving forward with python 3 features. :)
comment
Removing the ```default_opener``` command allows the tests to finish. All ```default_opener``` does on linux is run ```xdg-open```, but that could be opening the file using a variety of programs depending on the xenial setup.  I don't see any reason that ```adg-open``` would hang and cause the tests to not complete.  I'll need some help tracking down how the configuration of xenial will allow ```xdg-open``` to finish and return control.  The osx test on 3.7 is still not working, but it looks like it just doesn't run the tests.  The log says  ```script.sh``` is started, but there is no output after that and the test is marked as failed. 
comment
Be careful with ```sort```ing.  The nodes of the graph are not guaranteed to be sortable. For example, they might be NetworkX Graphs.  So, it's fine to use "sort" in an example but it shouldn't be in the public-facing function. Perhaps better to see if we can get ```chordal_graph_cliques``` to use/return a different data structure.  It's too bad that we are starting to avoid sets. I might have to go back to using dicts as sets (which was fairly standard before the "set" data structure in python.  Python could fix the temptation by using the same under-the-hood ordered mechanism for sets that they do now for dicts. It is certainly performant. I was surprised that they didn't do that in Python 3.7.
comment
Well, my understanding is that we are trying to avoid the non-deterministic nature of the algorithm. It is nondeterministic because the order of iteration over a set can vary from one creation of the set to another. So, the order that the cliques show up in the output is not the same every time. That makes it slightly harder to test whether your function is working.  It's also good practice to make your computations reproducible (sending the code to someone else and having them run it will give the same answer) and that's not the case if the code uses nondeterministic data structures. Am I getting the right gist @jarrodmillman ?  We can still use sets.... but its a little more of a pain than other data structures especially when deterministic behavior is desired. 
comment
Thanks @mbruhns !
comment
That module has a different history than many. It's based on a paper Aric and I wrote about threshold graphs. All those duplicate-sounding functions are there because it is sooo much faster to compute them for threshold graphs than for general graphs.  Restricting yourself to threshold graphs allows very fast computation -- but...   (there's always a "but")  We suspect that not many people use threshold graphs. It is a very restrictive class. Perhaps some sort of decomposition of real networks into nested threshold graphs would be possible, but never been done as far as I know.  Anyway, we didn't want to "pollute" the namespace with these functions that won't get used (and have similar or the same names as other functions). We even had this module out of the documentation completely for a while. But then people discovered it and wondered why we didn't have it hooked in to the docs.  That's part if the trouble with having studied something carefully -- you know its weaknesses.  I'm sure there are other functions we have included in NX that are just as obscure as threshold graphs. But I don't know as much about them.  Anyway, I suspect we either want to keep these in their own namespace, or change the names to make it more clear that they only work for threshold graphs. The two that are public seemed the most useful for someone exploring whether to use these graphs. And the rest are available at ```nx.algorithms.threshold.*``` if needed. But -- really -- we should treat this code like we treat the other code we collect here. In that sense I should recuse myself from the decisions.  But I'll talk your ear off about what the functions do and how they work if you want. :)
comment
I see how that would fail. It looks like the implications of MultiGraph were not looked at closely. Your coded solution would not work in general because it assumes a pattern for the edge keys (they are assumed sequential integers starting from 0).  Your suggestion of ```NotImplementedFor``` will certainly fix the surprise nature of the thrown ```ValueError```.   Another small changes I'd like to see in steiner_tree is a paragraph description of what a steiner tree is in the doc_string. 
comment
That's great!  Thanks...  I'll try to merge the quick-fix before the new release coming up. We can aim for the MultiGraph implementation in the next release.
comment
The currently available ```steiner_tree``` actually computes the correct edges. It just doesn't return the 3-tuple with the edge key.  It returns a 2-tuple and makes you go find which key has the minimum weight for those two pairs of nodes.  This is a similar problem to #3159 where the treatment of MultiGraph was not clear because the edge keys were not returned as part of the edge-tuple.   I'm going to re-allow MultiGraphs for this function. The code works just fine. It just doesn't return as much information as might be useful. To find the 3-tuple edges from the 2-tuple edges you'll need something like: ```python def argmin(keydict, weight):     return min(keydict, key=lambda edgekey: keydict[edgekey][weight])   multiedges = [(u, v, argmin(G[u][v], weight)) for u, v in steiner_tree.edges()] ``` I'll add to the docstring to make that more clear.
comment
In #4160 I updated the steiner_tree function to provide the desired multigraph tree with edge keys. It passes the test set up when multigraph was marked as "Not Implemented"  So this issue is fixed when that PR gets merged.
comment
Hi!  Thanks for this -- very helpful.  I think the logic for finding keys still doesn't work in some funky cases -- in particular, it seems like directed edges for the in-place routine might end up with the same key because it is checking against G so 2 new edges would get the same key.  Perhaps it is just a matter of including a seen_edges set to hold them for the in-place case too.  For the copy-case I think you need to add both directions of each edge to seen_edges when the graph is undirected.  To investigate a little, I tried to extend the tests beyond a structure with one node to many. I added many to one node as well. And I combined the graph type and copy/in-place combinations into a single function with a loop.  What do you think of something like this? ```python     def test_relabel_multidigraph_inout_merge_nodes(self):         for MG in (nx.MultiGraph, nx.MultiDiGraph):             for cc in (True, False):                 G = MG([(0, 4), (1, 4), (4, 2), (4, 3)])                 G[0][4][0]["value"] = "a"                 G[1][4][0]["value"] = "b"                 G[4][2][0]["value"] = "c"                 G[4][3][0]["value"] = "d"                 G.add_edge(0, 4, key="x", value="e")                 G.add_edge(4, 3, key="x", value="f")                 mapping = {0: 9, 1: 9, 2: 9, 3: 9}                 H = nx.relabel_nodes(G, mapping, copy=cc)                 # No ordering on keys enforced                 assert {"value": "a"} in H[9][4].values()                 assert {"value": "b"} in H[9][4].values()                 assert {"value": "c"} in H[4][9].values()                 assert len(H[4][9]) == 3 if G.is_directed() else 6                 assert {"value": "d"} in H[4][9].values()                 assert {"value": "e"} in H[9][4].values()                 assert {"value": "f"} in H[4][9].values()                 assert len(H[9][4]) == 3 if G.is_directed() else 6 ```  Another request (which is admittedly minor) is for you to switch variable names from tail-head to source-target to match the surrounding code. And the "current_keys" variable could be removed since it's used only once.  Thanks!   
comment
This fix assumes that key can be maximized and added to 1. Keys do not have to be numeric. Perhaps something like: ```python newkey = 0 current_keys = G[tail][head] while newkey not in current_keys:     newkey += 1 ``` There may be other better ways to do this.
comment
Actually, upon further reflection -- it might be better to explicitly combine nodes... That is, we might want to impose node_contraction and not let the relabel_nodes function change the graph connections.  What do you think?
comment
Yes, it seems that the convenience and flexibility of ```relabel_nodes``` may offset the disadvantages.  We will need a fairly strong statement in the docstring about how this relabeling affects the nodes when more than one is relabeled to the same object.   This sounds good to me.
comment
This is related to #4152 perhaps?
comment
Please describe what you think is wrong with the way it is handling the weights, or provide a simple example showing the trouble. "isn't properly fetching weights" is too vague to investigate.
comment
Good -- Thanks!   Now I see the problem. It looks like other functions in the same module have a similar problem.  Do you want the minimum distance or the max? Seems easy to get that switched.  We could try to fix the functions for MultiGraphs by using ```g.in_edges``` instead of ```g.pred```. Instead, we could allow users to input a ```weight``` function which takes an edge dict and returns the weight. That would allow MultiGraphs to work if the weight function treated it as an edgekeydict keyed by edge key to the data dicts. But I have a preference for marking the function as "not implemented for" MultiGraphs and having the user convert multiedges to single edges separately from the algorithm. It is more effective and controllable if the user handles the conversion manually.  Perhaps we could provide functions to help with that.  What do you think
comment
The shortest_path functions default (for multigraphs) to using the minimum weighted edge between node-pairs.  Furthermore, all those functions allow weight to be a function ```wt(u,v,datadict)``` which can handle multigraph classes in a very general way. Is this general enough? Would this solve the problem in this issues for longest_path?  Some progress in this realm has been made in the 2 years since this issue was raised. But I'm not sure that it is solved. Take a look at the shortest_path handling of "weight" and MultiGraph. I hope that porting that treatment to this function would be sufficient.
comment
I see the confusion. It isn't advertised well, but the ```(u,v,datadict)``` includes ALL the keys.  That is, in this setting for Multi(Di)Graph, ```datadict``` is the edge key dict:  a dict keyed by edge_key to the edge attribute dict for each edge. So, by default, the weight function for a MultiDiGraph is: ```python lambda u, v, d: min(attr.get(weight, 1) for attr in d.values()) ```  To use the keys you could use: ```python def mymultiweight(u, v, dd):     return min(my_function(u, v, ekey, edatadict) for ekey, edatadict in dd.items()) ``` 
comment
This issue will be complete when #4114 is finished. I'm going to close this issue. But feel free to post more for continued discussion. I'll reopen if need be.
comment
FancyArrowPatch doesn't accept a ```style``` argument.  Any ideas for how to make this work?
comment
FancyArrowPatch doesn't have a ```style``` argument, but it does have a ```linestyle``` argument. Looks like ```linestyle=style``` is reasonable. Does the LineCollection style and the FancyArrowPatch linestyle accept the same values? If not, we had better document the style argument carefully. I guess we should do that in any case. :)  Thanks!
comment
According to the docs for [LineCollection](https://matplotlib.org/3.1.1/api/collections_api.html#matplotlib.collections.LineCollection) and for [FancyArrowPatch](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.patches.FancyArrowPatch.html)  FancyArrowPatch linestyle allows {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}. LineCollection linestyles allows [ 'solid' | 'dashed' | 'dashdot' | 'dotted' ], or a dash-tuple (offset, on-off-seq).  So, we'll have to adjust the docs carefully.
comment
I believe this was closed mistakenly by PR #3554 due to a typo on my part. I apologize. I have reopened the issue and hope it can be fixed for NetworkX 2.5  We need to add "linestyle=style" to the fancyarrowpatch call (similar to the LineCollection call). Then we need to expand the docs to tell people what is available for the linestyle (or refer to matplotlib docs).  Thanks for the nudge. :)
comment
Fixed in #3747
comment
Fixed by #4154
comment
The standard definition uses the data computed in ```_triangles_and_degree_iter```. I am not aware of anyone using the information from ```directed_triangles_and_degree_iter``` to compute transitivity. But there is nothing stopping us from doing so. One issue is that there are so many combinations the data which create such measures, it is hard to know which is useful. Depending on your interests and background, you could explore and publish a rigorous comparison of transitivity measures. As you can see, the code is not difficult. The difficulty is figuring out what question is relevant, what we want to describe and what measure of that feature is most effective.  On the other hand, if you want to compute the transitivity as commonly defined, the current implementation is commonly used for directed graphs. Hopefully it works for what you want to measure.
comment
Try ```G = nx.DiGraph({'1': ['2'], '2': ['3', '4'], '3': [], '4': ['5'], '5': ['6'], '6': []})``` Your data is in "adjacency list" form. It is not a list of edges. That determines how you add the connectivity information.
comment
Graphs with nodes that are 2-tuples (like geometric graphs) are iterables of tuples, and dict-of-dict constructs are also acceptable input but should not be treated as iterables of tuples. I haven't thought carefully about whether sets of tuples should be handled.  Probably -- (the set object became part of standard python after our initial creation and I don't know if we revisited sets of edges after it became more popular..... maybe now is the time. :)
comment
We wrote much of the drawing code before matplotlib had 3d plotting. I don't think it would be too hard to make it work. I haven't looked at mayavi recently, but it used to be a respected and effective 3D modeling/visualizing system. I wouldn't be too concerned about whether users need to say "nx.draw" or "nx.draw_3d" or "nx.draw_mayavi". As you indicate, it would be nice to have some functions for 3d -- whether they are the default tool or not is a separate question.  We've got @ericmjl who is the primary author of nxviz. We've also got some heavy lifters in the matplotlib world (like @tacaswell ) plugged into GraVe. Learning more about those packages may be helpful in this effort too.  
comment
The algorithmic part of 3d network drawing is finding good layout algorithms.  Do you know any for molecules?
comment
If you already have the cartesian coordinates in a dict keyed by node to (x,y,z), then @boothby 's code looks pretty effective. If you don't have experience with 3d layout algorithms you could focus on the drawing part first. Those are currently largely separated for 2d functions in NetworkX.
comment
Thanks @rossbar...   The ```with_labels``` keyword argument did nothing within ```draw_networkx_nodes```.  The input was silently ignored without warning or error.  That ```with_labels``` keyword argument exists for ```nx.draw``` and ```nx.draw_networkx```. But if you are getting to the fine-grained detailed functions like ```draw_networkx_nodes``` then you will have to use ```draw_networkx_labels``` to show the labels (as @rossbar points out).  The changes in #4033 make it clear that ```with_labels``` is not a keyword argument of the function by raising an exception when you use an invalid input to the function. So, running your code on the latest version (which will be v2.5) would give an exception telling you that ```with_labels``` is an invalid input parameter.  Hopefully you will no longer have this trouble now that we have fixed this defect.   I'm going to close this issue because the confusing behavior has been fixed. But please post follow-up questions if this isn't clear.
comment
Suggestions?  Is the rounding done in networkx?  It sounds like the problem is when the file is created, not when it is read...  Reopen if I didn't understand or if there is another suggestion for how to fix this.
comment
There should be a button at the bottom (next to the comment button) that says "reopen issue" or "reopen and comment". If not then just keep commenting and we'll get someone to reopen it.  I don't understand your comment about the format for the edges tuples. Are you saying that instead of converting a floating point number to a string in the default way we should be converting them to a string with a specified format string that could be chosen by the user? I am not an expert  in shapefiles Wkt Wkb etc.   In what way does geopandas help?
comment
Might be an administrator thing. I thought the original poster could reopen... sorry.  I still don't understand the sentences in your description of u,v, first last, Wkt etc. I understand what roundoff error is and that the error will depend on the hardware used and order of operations. 
comment
It looks like you are reading the file and the program converts the values to floating point. When you then try to print the floating point values, round-off error naturally arises and you get lots of decimals. Is this the problem?  How do the many decimals arise? Are you printing the edges?
comment
Yup! That's a bug in #3696 that was fixed in #3698  so it will be in the v2.5 release.  Thanks for the heads up and nice example code!!
comment
Edge keys are only unique between two edges. In fact the default edge keys are all 0 for the first edge between any pair of nodes. You are thinking of a unique edge key over the whole graph. For that NetworkX uses (u, v, k) as the unique edge key over the whole graph.
comment
Please be careful when using the term "unique edge key". Unique on what scale? Do you see that the term is in itself confusing. Certainly if one MultiGraph has an edge with key 'e001', another MultiGRaph might have an edge with the same key. Would you still call them unique? Instead call them unique within each graph.  Similarly, NetworkX MultiGraphs have unique edge keys within each pair of nodes. So, if there already exists an edge between node 1 and node 2 with key 'e001' then you can't add another edge between 1 and 2 with that same key.  But NetworkX chose to let edge keys be re-used for other pairs of nodes. So edge (1,2,0) can exist along with (2,3,0).  NetworkX does NOT allow a graph to have "edge keys only for multi-edges and the 'normal' edges without a key". Each graph must specify whether it has edge keys (MultiGraph) or not (Graph). The mixed graphs you are talking about have been discussed for NetworkX but we haven't found a good way to implement them yet.   Also, NetworkX also prefers consistency over a more general scenario. We choose consistency of the return data structure from methods like G.edges(). The return data structure is always 2-tuples. If you want 3-tuples you should use G.edges(keys=True) and this will only work with MultiGraphs.  You can create a subclass of MultiGraph that enforces unique edge keys across the entire multigraph. We have talked about that too. Minor conveniences like referring to an edge by its edge key instead of by a 3-tuple are outweighed by cost of creation/enforcing/checking/storing the unique keys.  Just think of the network-wide edge key as being (u,v,ekey) and it all works smoothly.
comment
Turn it into a shortest_path problem  (??weights -> 1/weights...   use -> 1/use??  If floating point and ties are important, turn into integers) For multiedges, use the ```weight``` *function* capability:  ```python def myweight(u, v, edgedata):     return min([1 / datadict['use'] for key, datadict in edgedata.items()])  nx.shortest_path(G, source, target, weight=myweight) ```
comment
It sounds like you might be looking for a measure like: ```betweenness_centrality```  If you want to know which nodes appear on the most shortest paths, that is what betweenness centrality measures. That function computes it for all nodes, but then you have to sort the results and select the top "n" of that sorted list.
comment
Thanks very much for this report. It is indeed a bug in the ```__contains__``` method of the ```EdgeDataView``` family of objects. Let's add the example code from the stackoverflow link as a test. Then fix it, which will involve checking whether either (I think) node is in self._nbunch  ```python graph = nx.Graph() graph.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]) end_nodes = [n for n in graph.nodes if nx.degree(graph, n) == 1] end_edges = graph.edges(end_nodes) print(f"end edges: {end_edges}") for edge in graph.edges:     if edge not in end_edges:         print(f"edge {edge} is not an end edge.")     else:         print(f"edge {edge} is an end edge.") ```
comment
That sounds great!  Thanks! The first thing would be to get to know the ```classes/reportviews.py``` objects generally. Probably add your  example that breaks the code as a test in ```classes/tests/test_reportviews.py```.  Then let's nail down the docs on how to handle ```nbunch```.  Right now the EdgeDataView object docstring has very little about what nbunch should do. The last sentence of the docstring is the best we've got. These EdgeDataViews hold a data/default pair of attributes and ```self._nbunch``` which is currently a list of nodes. The object iterates over edges starting from nodes in ```nbunch```.   We should probably change the ```self._nbunch``` object to be a set of nodes rather than a list because almost everything we do with it is a membership check.  For the ```__contains__``` method, I think nbunch should restrict the view to edges in the graph that start from nodes in nbunch. For ```directed``` graphs you can just check the first node of an edge-tuple. For ```undirected``` graphs, you'd need to check if either node in the edge tuple is in nbunch.   Is this your expected behavior?   The code to actually enact the fix is in many places: the ```__contains__``` methods of the family of EdgeDataView classes. Start with ```OutEdgeDataView``` and work your way through the classes that build off of that. I hope it just involves adding a check whether ```nbunch is None``` and if not, checking the candidate edge to see if the appropriate node is in nbunch.  Ask questions of course, and feel free to post half-done code as a PR to get feedback... Thanks.
comment
This seems like a reasonable request. Let's make the return values match the container type that is described in the doc_string. If no edges/nodes are drawn, then return a container with no edges/nodes.
comment
Your network is not very big. It would help if you ran one property at a time so you could tell which property is the slow one.  The problem with ```average_shortest_path_length``` is one of definition. You say that it would make sense to ignore any pair of nodes for which there is no shortest path. That may be an easy way to handle it, but the result no longer measures a quantity that describes what average shortest path is trying to measure.   The correct way to handle this case is usually to find the connected components, form subgraphs for each and compute the average shortest path of each component. But that's not always best. It depends what you are trying to measure...  That is, why are you looking for the average shortest path? The answer to that question will help tell you how to handle the case with multiple components.
comment
The [documentation for optimal_edit_paths](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.similarity.optimal_edit_paths.html) describes the return values for this function as:  ```     Returns     -------     edit_paths : list of tuples (node_edit_path, edge_edit_path)         node_edit_path : list of tuples (u, v)         edge_edit_path : list of tuples ((u1, v1), (u2, v2))     cost : numeric         Optimal edit path cost (graph edit distance).      Examples     --------     >>> G1 = nx.cycle_graph(4)     >>> G2 = nx.wheel_graph(5)     >>> paths, cost = nx.optimal_edit_paths(G1, G2)     >>> len(paths)     40     >>> cost     5.0 ``` The return values are ```paths``` and ```cost```. It sounds like you want the paths.  In particular, you want to know which nodes are isomorphic to other nodes. So...  the output ```paths``` is a list of edit paths. You may want to look at all of them, or you may just want any one of them. Each path is made up of 2 lists. The first is a list of node tuples, the second a list of edge tuples.  It sounds like you want the node tuples list. Suppose it is [(1, 2), (9, 8)]. That means that node 1 gets changed to node 2 and node 9 gets changed to node 8.  Those are two of the changes (substitutions) needed to make the graphs isomorphic. There could also be deletions, e.g.  (3, None) or insertions (None, 4).  See the description of return values for the ```get_edit_paths``` function within the ```optimize_edit_paths``` function:  ``` Returns:             sequence of (vertex_path, edge_path, cost)                 vertex_path: complete vertex edit path                     list of tuples (u, v) of vertex mappings u<->v,                     u=None or v=None for deletion/insertion                 edge_path: complete edge edit path                     list of tuples (g, h) of edge mappings g<->h,                     g=None or h=None for deletion/insertion                 cost: total cost of edit path ```  But, alas, this only tells you the edits that make the two graphs isomorphic. It does not tell you what the resulting isomorphism is. For that you should construct the two now isomorphic graphs and use the isomorphism routines to get the isomorphism.   Hope this helps.
comment
Thanks for this!  I have only scanned through the code quickly at this point. Here are some comments. If you're up for learning a little more about how the package is set up, take a stab at fixing them. Otherwise I'll get to it.  Rename the module to tree_isomorph.py to match the other modules' naming system. (similar for test file)  Add a definition of the variable ```__all__``` just after your imports. It should equal a list of all functions you want added to the networkx package when importing.  Look at some nearby modules for examples.  Hook into the namespace:  Add an entry to ```algorithms/isomorphism/__init_.py``` which imports your module into the isomorphisms module (and thus into the main networkx namespace).  Mimic the other entries there.  Check for yourself that importing networkx from your repository allows access to your functions.  Hook your tests into the documentation system by making an entry in ```doc/reference/algorithms/isomorphism.rst``` that mimics the entries there already. I think a subsection for tree isomorphisms is reasonable and it should include the functions you "make public" in the ```__all__``` variable above.  You should install and run "black" (also known as psf/black) on your two modules -- this is a recent addition to networkx and we hope to unify the style and spacing.  Right now the PEP8speaks tells you what needs fixing. But black ought to fix it for you.  Thanks again!
comment
This is ready to be merged. :}
comment
I can verify that the Circle-CI errors are due to inability of the testing environment to load ruby2.5. Not your problem.  Thanks for this!
comment
The ```if not edgelist``` part of that line is very old when that syntax was subtly different. The code is intended to work for lists, tuples, sets, frozensets and other containers (like numpy arrays). We assume they have a ```__len__``` feature.  I'm actually surprised that numpy changed the behavior of ```if a:``` It used to check whether ```len(a)!=0```.  In any case, this should be changed as per your suggestion!  Thanks!
comment
Yes, that is (I believe) the only change needed.  It might be good to have a test to make sure that a numpy array works.
comment
Your example shows that both nodes and edges order is not maintained. But that's because your example uses ```nx.ordered.Graph``` instead of nx.ordered.OrderedGraph``` (which is more easily accessed as ```nx.OrderedGraph```).  But, your remarks do hold true for OrderedGraph.  The OrderedGraph family of classes only orders the nodes in the order they were entered. The edges have an order, but it is not the order that they were added. (In fact it is the order of the adjacencies.) The docs are careful to state that there are many potential orders and this one might not be what you expect. It is an ordering of the edges. It may not be the order that you expect.  Now that Python 3.6+ makes dict ordered, you probably don't need the OrderedGraph class. The order is determined and consistent across platforms and sessions, but again, it might not be the order you want.  This restriction should not affect the naming convention of nodes -- that is determined by you (or metis??). With the same input graph and the same seed I would definitely expect the same results. What are you changing between runs that are different?
comment
It is true that the adjacency order is the same if you add all nodes before adding edges. This can also be accomplished by using ```H=nx.MultiDiGraph(G)```, or if you prefer: ```python H=nx.MultiDiGraph() H.update(G) ```  In summary, the order of the edges is based on the adjacency dict-of-dict structure. So, in general G.edges will not preserve the order of edges being added. But it does preserve the order of the adjacencies and that can be propagated to another graph by adding nodes from G before adding edges from G. This order preserving nature is coded into the constructor ```nx.Graph``` as well as the update method ```H.update(G)```.  I'm going to close this issue, but post here with questions and I'll reopen if needed.
comment
No need to rebase or squash commits.  github does a lot of that automagically for us. :}
comment
Good question. The underscore functions usually are not imported if you use ```from shortest_path import *``` But so long as shortest_path has been imported, we can still get the "private" functions by explicitly listing them. The file at ```networkx/algorithms/shortest_path/astar.py``` provides as example:  ```python from networkx.algorithms.shortest_paths.weighted import _weight_function ``` It's a little convoluted, but it works. 
comment
I'm not sure why the third Appveyor test is failing to start. Hopefully it will be fixed for your next commit -- don't worry about it.  The loop for ```is_path``` creates a potentially long list. So it might be better to switch to a lower footprint approach, something like (not tested): ```python for node, nbr in nx.utils.pairwise(path):     if nbr not in G[node]:         return False return True ```
comment
Looks like this is running into an import order issue where ```function.py``` is imported before enough of the classes are imported that it is too soon to import stuff from ```algorithms```.  Looks like it might be time to move _weight_function into a different module (perhaps utils?)  Let's just implement this with weight being a string (attribute name).  There is another issue #4085 for working toward using _weight_function wherever possible in the package.  That's the place to change its location and we'll update this code as part of that anyway.
comment
Thanks for reporting this!   That's the result of #4076 which was recently merged after apparently not enough review. :) It needs to be fixed.
comment
This seems like a reasonable addition. Let's make it work for directed/undirected/multiedge  A good place for it could be in classes/functions.py but I'm open to suggestions...
comment
No traversal is needed. The path is already known and provided. You just need to loop over the edges and add up the edge attributes along that path.  ```python     def path_weight(G, path, weight):         for n, nbr in nx.utils.pairwise(path):             sum G[n][nbr][weight] ``` Obviously this pseudocode doesn't work, but something close to it should work. Then make it work for multiedge, etc.
comment
I guess this function will do that -- if the path doesn't exist in the graph, there will be a KeyError we could catch and raise a NoPathFound error.     And yes, you could create a function ```is_path``` to return True/False based on whether a path is found or not.  I'm sure there are many ways to construct these functions. So if another way makes more sense, go with it. :)
comment
Is this Issue completed already by PR #2558 ? Or am I missing something in this suggestion that isn't in the current ```from_pandas_adjacency```?
comment
The trouble comes from your understanding of the term “subgraph isomorphism”.  To be an isomorphism, the *induced* sub Ralph must match. That is, all the edges connected to the nodes in the subset must exist in the subgraph.  In you first example, the candidate isomorphism Maps all three nodes. So g2 must include all edges between the 3 nodes. And it doesn’t. So there is not an isomorphism.  In your second example, the isomorphism maps 2 nodes, so g2 must include all edges between those 2 nodes. There is one such edge and it exists in g2 so there is a subgraph isomorphism.  What you are thinking of is a matching of the nodes and *some* of the edges. This is called a subgraph monomorphism.
comment
I'm going to close this issue, though you can still post here and we'll see it. ```print(list(GM.subgraph_monomorphism_iter()))``` might be what you're looking for.
comment
In NetworkX 2.4 and the latest (github) versions it exists. It was put in during pull request #3435 
comment
I'm sorry, it is named ```GM.subgraph_monomorphisms_iter()``` I missed the "s" at the end of monomorphism.
comment
No, it just got left out of the docs when implemented...  #1729  Thanks very much for reporting it! It is just a one line addition to ```doc/reference/algorithms/community.rst``` in the quality section.  Sorry for your trouble!  Did you come up with any improvements over the existing code?
comment
The cause is hard to guess based on this (tho I didn't download and look at the file). I would focus on nodes first (fewer of them). Is it possible that some are equated even though they aren't officially the same for some reason?  Perhaps take a smaller portion of the file and run that through and see if you can track down specific nodes that are in the file but not in the resulting graph. It shouldn't be hard to write a small python scrip to read in the edgelist and create a set of nodes that you can compare to the generated graph. Sets will also check if any nodes are "equal" (only add one when there are many). You should check the delimiter too -- whitespace is fairly generic andis a common source of misreads.   You can often get quite far with a short script like:  ```python for i,line in enumerate(file.readlines()):     s = line.strip().split()     if len(s) < 2:         print(line, s, "Skipped line!", j)         continue     node1 = s.pop(0)     node2 = s.pop(0)     # add a print statement or many...     nodes.add([node1, node2])     if j>10:         break ```
comment
@realmarcin do you have any updates?    I'm going to close this, but you can simply post anyway and we will see it. We can reopen if you find a bug.
comment
The flow ```1->2->3``` doesn't satisfy the demands and capacities.  Flow problems must satisfy all demands and capacities. Are you looking for an algorithm that takes the specified problem and looks for all possible partial solutions? That's a much more difficult problem.  Any ideas for an algorithm to approach it?
comment
I don't think the problem is well defined.  If pairwise flows is what works for you then great --  I think your subgraph S should include the entire graph...
comment
You might take a look at [the original issue](https://github.com/networkx/networkx/issues/2361) on this GED part of the code, where it gives some description of which attributes slow it down considerably. Follow-ups to that suggest that advances are fairly rapid lately in these types of measures and there might be a better algorithm out there than what we've got here. 
comment
This function uses T in the arguments for ```nx.edge_boundary``` which takes care of the default value for T. I checked with a simple path graph and it works.  Please post again if I've missed something or it doesn't work for you.
comment
A flow is synchronous. There is no inherent order to the flow. All the nodes are receiving and creating flow at the same time and the net flow should meet the demand. Thus if nodes 0 & 1 both are sinks with demand 1, and 2 & 3 are sources with demand -1 and they connect with a node 4 with no demand, then you can't tell whether the flow from 2 goes to 0 or 1. The demands describe the net flow over the whole network. The flow across each edge is the net flow on that edge. But you can't assign an order to the flow in the general formulation of the problem.  I'm going to close this issue, but you can keep posting and we'll see it.
comment
There are also improvements on VF2 (I know of VF2++) that help considerably -- but "Yes"! it should help to consider only one component at a time.  In Fall, I plan to push a version of VF2++ written by my student and that should help some too. I'm going to close this for now. Re-open or open a new issue as needed.
comment
I don't think we've thought carefully about multigraphs, edge keys and uniqueness of edge keys in a pandas data frame. This Issue sounds like a good idea. We should probably also consider what to do when the edge keys match.  Without multiedges we just overwrite existing data when new edges come in. Maybe duplicate edge/edgekey should do that same?  But maybe not...  What are the use cases here? Are there common data frames that represent multigraphs?
comment
Yes -- please investigate this.  I think for starters you can follow the ```G.add_edge``` treatment that if the edge exists, "adding" the edge just updates the data dict. That is the simplest treatment. Thanks! 
comment
This looks pretty good. Almost ready.  Some comments:  Could you inline the 3 helper functions? They are only used once and the namespace is already pretty big in this module.  Could you add documentation of the new feature into the docstring?  Thanks!
comment
The Travis-CI is failing on a new OSX test platform just put in place yesterday. It is not your code so don't worry about getting that test environment to pass. The others are all passing.  This looks good -- here's some last bits to clean up: - is the ```key = -1``` needed? what is its purpose? - can you make the msg on line 386 avoid the white space and newline by concating strings like this" ```python msg = ("first part"                f"second part {with formating}") ``` - change the name of one of the first two tests (same name) so they both run. - can you remove the "as excinfo" in the with statement for pytest.raises? PEP8 complains because excinfo is defined but isn't used.  
comment
I am able to use it on a Graph without any issues. We'll need to drill down to how our configs differ.  ```python import networkx as nx G = nx.path_graph(9) G.add_edge(3,3) list(nx.selfloop_edges(G)) #           [(3, 3)] G.remove_edges_from(nx.selfloop_edges(G)) list(nx.selfloop_edges(G)) #           [] ``` Python 3.7.6,   NetworkX 2.5rc1.dev_20200712223336 
comment
Thanks for this!  With MultiGraphs, the (new with v2.0) iterator nature of ```nx.selfloops_edges``` caused this RuntimeError.  That's because the selfloop_edges iteration goes through the elements of the neighbor dict (e.g. G.adj[n]) and the neighbor dict gets changed. This should cause problems with MultiGraph and MultiDiGraph (I'm not sure why you aren't getting the trouble with MultiDiGraph).  Strangely, this doesn't happen with Graph and DiGraph because, while now the iteration goes through G.adj it does not go through G.adj[n].  So, removing a neighbor changes the value of G.adj but doesn't change the G.adj dict itself.  I would like to keep ```G.remove_edges_from(nx.selfloop_edges(G))``` working though... Luckily, that case can be finessed because instead of iterating over the G.adj[n] dict, we can iterate over ```range(len(G.adj[n]))``` and still get the same number of selfloop edges to remove. Note that ```G.remove_edges_from(nx.selfloop_edges(G, keys=True))``` would not allow that since we actually do need the keys in this case.   Here's some code for testing.  @joelmiller can you check that I haven't missed something with the MultiDiGraph case? ```python # test removing selfloops behavior vis-a-vis altering a dict while iterating G.add_edge(0, 0) G.remove_edges_from(nx.selfloop_edges(G)) if G.is_multigraph():     G.add_edge(0, 0)     pytest.raises(RuntimeError, G.remove_edges_from, nx.selfloop_edges(G, keys=True))     G.add_edge(0, 0)     pytest.raises(TypeError, G.remove_edges_from, nx.selfloop_edges(G, data=True))     G.add_edge(0, 0)     pytest.raises(RuntimeError, G.remove_edges_from, nx.selfloop_edges(G, data=True, keys=True)) else:     G.add_edge(0, 0)     G.remove_edges_from(nx.selfloop_edges(G, keys=True))     G.add_edge(0, 0)     G.remove_edges_from(nx.selfloop_edges(G, data=True))     G.add_edge(0, 0)     G.remove_edges_from(nx.selfloop_edges(G, keys=True, data=True)) ```  
comment
Can you point to a definition of these values for multigraphs? The first thing I think of is to treat the multiedges as redundant connections -- that is, they don't affect clustering nor length of paths between nodes. In that case, you should convert the multigraph to a graph and get sigma and omega for the graph.   What is your use case?
comment
I believe that kind of contact network is quite typical.  It's like a superposition of networks.  You might even want to store each type of contact as a separate network in some cases. So, then the question about small worldness becomes one of "which edges do we want to consider?" If all edges provide a connection then insert ```nx.Graph(Mymultidigraph)``` to make it compute using all edges. If you want to measure just the familial contacts then you can make a separate network or you can use a graph_view with filter to only include edges of that type.  Lots of possibilities.. hard to know how to interpret the results. :)
comment
It looks like your tests are taking too long for the Travis-CI system. I also notice that they are testing random graphs and that the test is an inequality.  Perhaps it would be better to create a small deterministic graph that you know the sigma and omega values for. Then you are testing equality on a known input. It might be a better indicator of future bugs introduced by mistake. 
comment
See #4083
comment
See #4083
comment
I agree that we should have a consistent clear way to handle these aliases.  I'm fine with however we arrange it, so long as its easy to find the docstrings in an IDE, google search, reading the code, etc.   One nice feature of the aliases is that you get the docstring without having to look anything else up. Maybe it has become easier to look things up than I often experience (try Google search for networkx binomial_graph...   but that's a different Issue). The question really is:   If someone looks at the docstring for ```binomial_graph```, isn't it better to give them the docstring for ```gnp_random_graph``` than to tell them to look for the docstring for ```gnp_random_graph```?  If we make the docstrings clear about aliases, we shouldn't need to make new functions that simply call an existing function -- that sounds like a deprecation is in order...  What do other packages do with functions that are commonly referred to in literature by different names?  I notice that scipy has ```comb``` and ```binom```, but I guess there are subtle differences.  Are there any aliases in numpy/scipy?  Maybe we should just be getting rid of ours?
comment
I like that treatment @jarrodmillman And it doesn't even have to be in a "Notes" section. It is currently in the body of the docstring. If it is put in the Notes section, we should keep it in a separate paragraph from the other notes.  :}
comment
I approve this PR.  :)
comment
Nice catch!  Thanks
comment
A PR for this would be welcomed!  Thanks!
comment
How about a new module in ```networkx/algorithms/isomorphism```. Then tests go in the tests subfolder there with something like:  ```tests/test_canonical```.  To connect the docstrings to the documentation make an entry in ```doc/reference/algorithms/isomorphism.rst```. And to make the package find the new functions put an entry into ```networkx/algorithms/isomorphism/__init__.py```  Return values seem to either be dicts keyed by nodes to partition numbers or dicts keyed by partition labels to some collection of nodes. It is fairly easy to convert from one to the other. If you return only lists of integer labels of the members then you'll have to deal with a mapping of integers to nodes. Does the algorithm depend on the labels being integers? Maybe you can make it work with arbitrary nodes. Perhaps the best way is proceed is to put together a PR with what you've got and we'll see how easy it would be to convert the results to other formats.
comment
Wow -- I have never had it take anywhere close to 2 minutes to import networkx... sometimes a few seconds, usually hardly any time at all.  Do you get much shorter times outside of this conda environment, or by just importing networkx without the others?  I can't think of what in the setup could cause this...  Does it improve the time if you switch the order of the timings? (without first then with)  Maybe there is something going on with memory caching or something...
comment
Please add a test that shows the previous break (and passes with the new code). Thanks for this!
comment
Also, can you fix the PEP8 line-too-long complaints?  And get you add a test that ensure the sorting actually works.
comment
Time complexity in terms of what variables? There is no one answer to this question.  I agree that it would be great to know these values -- both from the perspective of algorithms and from actual tests of the time complexity as we have coded them (which hopefully matches the theoretical algorithm).   This seems like a noble goal. But it needs to be specified more specifically.  We will certainly add complexity measures to the documentation wherever it is known and an PR is created.  For example, ```Graph.subgraph(nodes)``` should not depend on the length of nodes or the size of Graph when it is created. But will depend on those sizes when it is used.  This IS documented already, though perhaps not clearly enough.  We are open to PRs in this area.  I am closing this Issue but please comment or reopen as needed.
comment
closed in #4059 
comment
Thanks for this!  I have some comments/suggestions... - it is probably better to raise an exception for an invalid interval than ignoring it. - checking for a tuple restricts the intervals to be tuples. We've found it better to allow any object of length 2 so people can use lists, numpy.arrays, etc.   - to check that length is 2, it is often more effective to use something like ```min1, max1 = i1``` - shouldn't you check that the interval is (min, max), So (3, 2) will raise an exception? ``` for i1 in intervals:     min1, max1 = i1     if min1 >= max1:  # not sure about = case here         raise ValueError("Each interval must be a 2-tuple (min, max). Got {}".format(i1)) ``` - To avoid the sort on the intervals, can you use the criteria:  ``` if max1 >= min2 and max2 >= min1:     G.add_edge(i1, i2) ``` Then a double loop over intervals will suffice (though you have to check ```if i1 == i2: continue``` to skip that case).    - You need to make this function available in the networkx namespace by adding an import of this module to networkx/generators/__init__.py - You might consider whether other Graph classes like DiGraph or MultiGraph can be used. If so, use an argument like ```create_using``` and process similar to other generator functions. If not, then rule then out using the ```@not_implemented_for``` decorator. - You'll need to add this new module to the docs by editing doc/reference/generators.rst - You can add yourself as a contributor to CONTRIBUTORS.rst  - For docstring parameters, put the type of input after the ":" for the parameter. Then on the next line start your description of the input parameter. This allows Sphinx to format the type and description correctly.  Take a look at other code if you want an example. 
comment
The core dump sounds like a python issue...???   But maybe it's running out of memory. It doesn't sound like a "fastest algorithm" problem. It sounds like a "lowest RAM algorithm" issue. The current version is a low RAM improvement of the ```recursive_simple_cycles``` algorithm. The code is not too long -- you might be able to track down where the memory is being used.  Comment again (or make a new PR/issue) if you find what's causing this.
comment
Any more info on how to recreate this problem? Please comment again or reopen the issue.
comment
Could you add some tests to make sure the roots functionality continues to work when other changes come in? Thanks for this!
comment
You also added a timeout feature.  We need to test that too. Nothing in depth, but add test of a short timeout.  And roots should be a 2-tuple (documented as a tuple).   Does this give nice clear error if roots is not a 2-tuple? What if not nodes of G? etc.
comment
No problem with the issues... :}  Could you make the timeout in your examples shorter -- so the tests don't take so long, and so the result doesn't depend on the machine doing the tests. Maybe 0.5 instead of 5?
comment
I was having trouble with the tests on my slow machine and since CI testing is done on a variety of machines I moved testing timeout to the ```tests/test_similarity.py``` file.  I also caught a bug when timeout is 0, that would ignore timeout because of ```if timeout:``` To check if timeout is input we should use:  ```if timeout is not None:``` instead of ```if timeout:```.  I also changed the "assert" to raise a NetworkXError instead of the more generic AssertionError.  I think this is ready to go.
comment
I approve this PR...
comment
This is good stuff!  Thanks!  You should make a file called tests/test_hashing.py (see the others in that directory for templates). Basically each function with name starting with test will be run by pytest. That replaces the ```if __name__ == "__main__": idiom for testing.   In naming, abbreviations are long term headaches for us, so we prefer the full name. e.g. the function ```wl_hash``` should probably be: ```weisfeiler_lehman_graph_hash``` and then you could make a shortcut name like:  ```python graph_hash = weisfeiler_lehman_graph_hash ```  In the docs, it might be good to use the term "graph hash" to keep this separated from the hashing needed to store a key in a dict.  Use blank lines in your docstring to indicate structure: one line description (for hyperlinks in html docs) then blank line, then paragraph describing what it does.  Use other nearby files/functions to see the patterns for docs. Combine the description of the example with the example code itself in a section entitled "Examples". References should be included at the bottom and referred to from above. If you have questions ask.. :} 
comment
Let's go with ```graph_hashing```.   Thanks!
comment
I forgot about hooking this into the doc system.... so there's a couple more steps needed.  - in doc/reference/algorithms/index.rst add (alphabetical): graph_hashing - then create doc/reference/algorithms/graph_hashing.rst to match the others (moral.rst is an example)  You can build the docs locally with "make html" from the doc/ directory. But you should first install stuff in requirements/doc.txt   If you follow the pattern in the rst files you probably don't need to do this.  Also, in the doc_string, put the first doc line on the same line as the """ and leave a blank line after it (to help sphinx identify what is the string to use for short descriptions).  Almost there! :)
comment
I approve this PR...
comment
Thanks for the PR. You've got a reasonable start.  I agree with @jarrodmillman that examples should just import and let the viewer install before trying.  In the codebase we protect the imports.  My concern with how it looks so far is that it might just be wrappers around code from sklearn. If that's the case then I'm not sure it makes sense to include it in the package as functions. Maybe examples would be sufficient? Are there other functions you see adding to this?
comment
@zfrenchee I am fine with a swiss-army knife approach.   It is important to save people from having to spend hours figuring out how to interface two APIs. Let's work to do that. One way is to create new functions that create a new interface to sklearn but hide the algorithmic ideas. Another way would be to create an example that shows people how to connect the API of NetworkX to the API of sklearn. Which is more effective?  In some sense it doesn't matter. Either way, we need a paragraph describing what spectral clustering is. And we need a parameter definition for k, perhaps along with a quick description of how to choose k. Then we need a simple graph G to demonstrate the input and verify the output. (This last sentence would be a docstring test if this is a function, or the heart of the example if this is an example.)  Why don't we keep developing this with a function interface and see if we can build an effective interface that bridges the two.
comment
Hmmm...  This PR points out that we use node-paths. That is, we store and report paths as lists of nodes rather than lists of edges. There is a bijection for simple graphs between node-paths and edge-paths. But for MultiGraphs, node-paths do not uniquely specify edge-paths.  I think mixing the two output structures will lead to a convoluted API.  Sometimes a function will return a list of nodes and other times a list of edges. That's going to be hard for users to debug and for us to maintain.  Does anyone have any comments on paths specified as lists for edges?  How hard would it be to make a new function that returns simple paths as edge-paths, that is as lists of edges instead of lists of nodes. For non-multigraphs it could use the current functions and then convert to edge-paths. For multigraphs it would return what you've put into this PR. What do you think?  
comment
Yes -- that looks good to me. Let's think about name (you get to choose):  paths_edges, path_edges, edge_paths    [with the all_simple in front, but I think the version without all_simple will become a name for the data-structure of specifying a path by making a list of edges. What should we call a list of edges that specify a path.  :)
comment
I saw the PR on trails. Looks like it has some testing errors on travis (but apparently not on appveyor).  Let's see if we can make it clear:  And just so I check what I think I know: A trail is a path that doesn't repeat edges. A simple path is a path that doesn't revisit any nodes. Right?  ```all_simple_edge_paths(G)```   Use first full paragraph of the doc_string to define "simple path" and to say we return edge_paths. Then, even if the name is not 100% clear, the docs are.  ```all_simple_paths(G)``` and ```all_trails(G)```  return paths as lists of nodes.  I suppose we could create something like: ```all_trails_as_edge_paths``` if it comes to that.
comment
Same comment as on the other PR. Perhaps a ```sorted()``` function will help the test results. We need to make the tests pass independent of the order of  dict reporting (and thus order of Graph reporting).
comment
Great -- Thanks for this.  Some things to change:  - First line in doc_string should be on one line (<80 chars). Maybe:        """Generate lists of edges for all simple paths in G from source to target  - In the first example in the doc_string, use integers instead of strings that look like integers. '1' -> 1 - same example, use ```g.add_egdes_from([(1, 2), (2, 4), (1, 3), (3, 4)])``` - you don't need ```import networkx as nx``` in the examples. It is automagically included. - remove blank lines in examples created by ```>>>``` - remove spurious lines added to ```test_simple_paths.py```  - add some tests?
comment
Most of the time, ```G.edges``` is called without ```data``` or ```nbunch```. So there is a separate set of classes for that common, easy to speed up case.  In the ```DataEdgeView``` family, most of the time, ```nbunch is None```. Only ```data``` is specified. So, you can't assume that ```self._nbunch``` is a set.  That is what is causing the error.  You will need to check: ```if self._nbunch is not None and u is not in self._nbunch:``` or similar.
comment
This fixes #3829 and #3819 
comment
It's quite possible that the Ramsey_R2 code didn't take selfloops into account when it was written. As such, I think we can call this a bug. Do you see a good way to either identify the problem and raise an error, or even better, fix the error?
comment
Great!  Thanks --  I have a suggestion to keep it as an iterator:      nbrs = (nbr for nbr in nx.all_neighbors(G, node) if nbr != node)  And we should add a line of doc_string to explain that we ignore self-loops. Thanks again!
comment
Fixed in #4037
comment
Edit distance is computationally hard. Any suggestions for improvements?
comment
Its not a matter of a fix or update. The algorithm is what it is. If someone has developed a new algorithm, then we've got something to work on. Otherwise it's just a hard problem...
comment
What is your definition of subnetwork? We have lots of features "like" what you are talking about, but none named subnetwork.
comment
In our lingo, those are called "Connected Components". You can get from nay node in a connected component to the other in that component and not to nodes outside that connected component. Directed graphs have a more complicated structure. Take a look at these docs <https://networkx.github.io/documentation/stable/reference/algorithms/component.html>  On Thu, Jul 2, 2020 at 3:46 PM Alex Cannan <notifications@github.com> wrote:  > @dschult <https://github.com/dschult> Sorry, by subnetwork I'm referring > to subgraphs that share no edges between one another. So, in the following > picture, there are 7 "subnetworks" visible. I'd like to return a list of > tuples containing the nodes that are in each subnetwork. > [image: image] > <https://user-images.githubusercontent.com/19368028/86403079-faeba280-bc7a-11ea-902a-33f2c719d3e9.png> > > — > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/4040#issuecomment-653190725>, > or unsubscribe > <https://github.com/notifications/unsubscribe-auth/AAG7MXMDUB4KLP6WVUT42RTRZTPYZANCNFSM4OPFWKOA> > . > 
comment
Thanks for this!  The idea of the EdgeView classes is to make a low memory -- fast to create object that does useful things with edge information. We tried not to create large objects inside . the EdgeView itself, instead referring to the original graph data structure when needed. We do of course have to store which nodes in nbunch to restrict the view to. And we store a function on self._nodes_nbrs that gets the nodes and the neighbors from the original graph data structure.  By creating a dict to hold the ```self._adjdict``` information, we would be duplicating the info in the graph data structure. Can we find a way to do it without creating a dict on the EdgeView object?  Duplicating the graph data structure is also why this doesn't work with MultiGraphs. That data structure is different from Graphs so you would need to copy the MultiGraph data structure instead of the Graph data structure.  I suggest trying the following approach instead. Currently nbunch is stored as a list on the EdgeDataView object.  1) Change that to a set (since we really only use it for node lookup, this is faster).  2) Add a check to the ```__contains__``` method that checks whether the edge should be reported based on nbunch.  Something like (for OutEdgeDataView): ```python         try:             u, v = e[:2]             if u not in self._nbunch:                 return False  # this edge doesn't start in nbunch             ddict = self._adjdict[u][v] ``` Each of the other EdgeDataView objects would need a different condition. For directed out edge views, check that ```u``` is in nbunch. For undirected edge views, either u or v could be in nbunch. For directed in-edge views, check that ```v``` is in nbunch.  Does this make sense?  Comments on the test code: - you start with ```graph = self.G``` but  go back to ```self.G``` later. Maybe just stick with ```self.G```? - to construct ```end_nodes```, use the ```self.G.degree()``` feature of iterating over nodes, degree tuples:  ```[n for n, deg in self.G.degree() if deg == 1]``` - the ```inclusion``` dict simply copies ```end_edges```.  Instead just use ```end_edges``` in your assertions. So, ```assert inclusion[(0, 1)]``` becomes ```assert (0,1) in end_edges``` - instead of checking the type of the graph, use the ```G.is_multigraph()``` method. It is similar to ```G.is_directed()```.  And the way it is written, you don't actually need to check ```is_multigraph``` because the tests are the same either way...  Just check for ```is_directed```  Ask again if questions.
comment
Let's close this in favor of #3845 
comment
We need to check that "_" is allowed by the GML standard. We can read GML with _ in it, but we should be writing standard GML
comment
Can either of you check that the gml standard does or doesn't allow underscores in the strings?  Perhaps we should provide a "strict=True" argument? Making that False could allow things like this.
comment
The original GML standard has been removed from the creator's webpage. It seems that this markup language is now bing supported and evolves with the network analysis community, especially drawing and analysis packages. I'm going to merge this change which allows WRITING a GML file that includes underscores in the keys of the key-value pairs. 
comment
Fixed in #3952
comment
Yikes!   That's pretty bad...   and looks like might have been caused by our inconsistent ordering of those arguments in this module.   Thanks very much!
comment
Fixed in #3952
comment
See #3861. And #3871 of course...   Thank you very much!
comment
I'm going to reopen this....   Is there anything wrong with merging it? Any further work still on the way, or is this enough to fix the problem. Thanks!
comment
Fixed in #3952 
comment
This may seem a little bit picky, but the original question was being a little picky too... :}  The old code passes your new tests.  That is ```list(path) == []``` is True whether path is an empty generator or an empty list. If we're going to bother making a generator object that just returns an empty iterator, then shouldn't we either test for that or forget the whole thing?  I looked into how to test for that, and it doesn't appear to be easy. So, I'm thinking we shouldn't spend time on this and spend time on algorithms instead.    I'm going to merge this to avoid spending more time on it. :}
comment
I checked the "Blame" on those lines of code. They were put in place 14 years ago with the ironic commit message: ```put in **kwds arguments until we handle them more carefully``` So... I guess it's probably time to figure out how to include them...    :}  The issues at the time were that we wanted people to be able to include any keywords for ```draw_networkx_nodes``` and ```draw_networkx_edges``` without having to call them explicitly -- so ```kwds``` might have keywords for both of those functions and that'd be OK.  We also didn't want to have to spend effort maintaining either the keywords that are valid for Matplotlib or a list of the keywords valid for each drawing function.    What do you think is best here? 
comment
It feels like the interfaces have converged, but there may be some matplotlib arguments that have changed without my knowledge. It seems like making a list would likely be helpful and could avoid bugs from people misspelling keywords, etc. That sounds like a  good solution.  Thanks!
comment
Sounds good!
comment
Thanks very much @jeremiastraub !
comment
@rossbar  Thanks very much for pointing this out. I can imagine cases where this would just add to a list of potentially confusing exceptions being reported and we already have long chains for exceptions caught inside decorators that make it slightly harder to trace what happened.   I haven't been able to think of any cases for ```from None``` in NetworkX, but I will check the PRs for this eventuality and I guess @cool-RR should too. :}
comment
Could ```G.clear_edges()``` be done with ```H = nx.Graph(G)```?  I guess it doesn't copy the graph attributes so if that matters you would need to add ```H.graph.update(G.graph)```.   Can you briefly describe a use-case for ```G.clear_edges()```? I'm reluctant to add another method to the base class without due diligence in demonstrating usefulness. Thanks! 
comment
Interesting...  That's a generated doc page so presumably a feature of sphinx... Is there a sphinx option to affect this change?
comment
Even with niter=1 you still need to find edges that can swap. How long that takes depends a lot on what ```giant``` looks like. :}
comment
Not enough info...
comment
Yes -- that is correct. The code of ```uniform_random_intersection_graph``` consists of:  ```python     G = bipartite.random_graph(n, m, p, seed)     return nx.projected_graph(G, range(n)) ```
comment
Hmmm... that doesn't happen with my setup. I don't know of any version where the graph classes has a positional argument 'x'.    What version of NetworkX are you using? Or has it been played with after being installed?  Maybe try printing  ```g.__class__.__name__```
comment
Try to find a way to make the positional argument 'x' have a default value (even if using the default is never actually done outside of these "views").  The views create a shell object that is an empty version of the class of the graph itself. The data structures of that graph are then replaced with read-only pointers to the original graph.  So, any methods your custom subclass defines are available on the subgraph view too.   Of course, if you want the subgraph to access the original graph's value of 'x' then you should rewrite the subgraph method on your custom class.  Something like: ```python def subgraph(self, nodes):     SG = DiGraph.subgraph(self, nodes)     SG.x = self.x     return SG ```  I'd like to make it as easy as possible to construct subclasses of the main classes. So please follow up with questions/suggestions on how to make this work better for you. I'm going to reopen this as a reminder to look at how to ease subclasses that require arguments to their ```__init__``` method.
comment
That's a recursion of 75 frames... Seems pretty big -- this algorithm does that. It's recursive. But it doesn't seem like so many recursions to mess something up.  It's really hard (for me) to see what is going on from that error report. It is clearly looking for the node 1. I believe that, despite the error message, node 1 is in the "self._atlas" view because the call to ```getitem``` is from a loop like:  ```python for key in self._mapping:     yield (key, self._mapping[key]) ``` So 1 should be in ```self._mapping```.   I'm guessing that means that self.NODE_OK(1) returns False for some reason.   Any way to narrow this down a little?
comment
That looks like an artifact of your drawing program, not of NetworkX. You can check by seeing if the  NetworkX graph has any nodes with position (0,0,0).   This is just a guess, but perhaps your drawing program starts lines at the origin by default unless you specify that the first line should start somewhere else. I have not used paraview.
comment
Why do you say that the order should be the other way? For an undirected graph, order of the edge nodes doesn't matter. Are you trying to do something with the edges that requires an order on the nodes?
comment
If order of the nodes in a 2-tuple of an edge is important then maybe you should be using DiGraph???? 
comment
The "working forever" symptom is almost certainly due to numerical error issues. You can try increasing the tolerance. But I suspect it will work it you simply switch methods to e.g.       ac = nx.algebraic_connectivity(g, method='tracemin_lu')
comment
These might help: https://networkx.github.io/documentation/latest/reference/readwrite/generated/networkx.readwrite.nx_shp.write_shp.html https://stackoverflow.com/questions/34885465/write-shapefile-in-networkx-using-write-shp
comment
NetworkX follows the standards written in the literature. In general edge weights may designate connection strength (bigger is more connected) or connection resistance (bigger is less connected). Different algorithms and papers in the literature treat the weights differently. We have tried to maintain consistency with the literature for each algorithm so people reading that literature can easily implement code to run the algorithm.  It's definitely something you have to keep in mind when crossing the literature from one area to another. Similarly, you have to keep it in mind when crossing our code from one algorithm to another.
comment
Yes -- Let's get some PRs that try to make the documentation clearer about the use of edge weights!
comment
The operation "reverse" is not one of the operations allowed when computing edit distance. The definitions I have seen usually stick to insertion, deletion and substitution of nodes. (They sometimes talk about substitution of edges, but can always related that back to substitutions of nodes because: "the matching of the edge structure is uniquely defined via operations which are actually carried out on the nodes" (pg 30 of [this book](https://www.springer.com/cda/content/document/cda_downloaddocument/9783319272511-c2.pdf?SGWID=0-0-45-1545097-p177820399)).    Why people choose to define it that way I can't say. I believe some people have considered other more exotic edit operations. But I don't know what they were. I think there is a reference to them in the above referenced pdf file.
comment
Yes -- ```draw_networkx_edges``` is intended to be called after other drawing calls. Probably good to make it able to stand alone, though not overwrite what already has been done.  What's the best way to do that?
comment
Before any code refactoring, I should probably mention that, long term, we plan to move stuff for graphing functionality with matplotlib to the [GraVe software package](https://github.com/networkx/grave.git).  But it is true that the development there is pretty slow. 
comment
@ericmjl    Yes -- and your package has been much more active too!  :) Everyone interested in graphics with NetworkX should look at (and use) ```nxviz``` https://github.com/ericmjl/nxviz
comment
We wrote ```rescale_layout``` with those inputs to reduce the number of times we have to convert back and forth between arrays and dicts.  How about implementing your suggestion as a new function rather than replacing the current one:      def rescale_layout_dict(pos, scale=1)         pos_array = np.array([[px, py] for px, py in pos.values()])         new_pos_array = rescale_layout(pos_array, scale=scale)         return {v: tuple(p) for v, p in zip(pos, new_pos_array)}
comment
Big O of what? edges, nodes, neighbors, creating an iterator, creating a list from an iterator, iterating over an iterator... There are so many choices of ways to report this that I don't even know where to start.  Perhaps a Wiki page (design spec? as suggested by @hagberg) could include a section on various choices for reporting things like neighbors and maybe even evidence for which performs best. But I don't think that is reporting complexity.... is it?  @robertour you mention that you previously thought nodes() and neighbors() were linear. What do you think they are now? I'm not sure I understand what you desire here...
comment
My understanding is that whenever there are multiple methods they have the same complexity. For example, ```G.neighbors(n)```, ```G.neighbors_iter(n)``` and ```G.adj[n]``` are all linear in the number of neighbors. But the constants are different. Of course, creating an iterator doesn't take much time at all -- it is using the iterator that takes time. Creating a list that holds the neighbors is the same complexity as creating an iterator over the neighbors.  so maybe complexity is not what we need...  perhaps a description of the relative speed of similar methods. That makes me realize that memory usage is important too. Again, complexity is probably not what we need, but a comparison of the memory footprint of the various methods.  Am I on the right track? 
comment
Hmmm...   "The case A would take twice as much as the others"  means that it is still O(n).  Twice O(n) is still O(n).  This is what I mean by the complexity is not the right measure. It's the constant in front of the "n" that you are asking about -- not the big-O.  The big-O is the same for all of them.  Also, I don't think ```G.neighbors_iter(n)``` is O(1) in this usage.  I believe "creating" the iterator is O(1). But you still have to iterate through it to make it useful and that takes O(n). I don't think you can process all the neighbors without having an algorithm that is at least O(number of neighbors).
comment
I've started a part of the [Design Specification Wiki page](https://github.com/networkx/networkx/wiki/Design-Specification) to discuss relative performance of Graph methods. Is this the kind of thing you are looking for?
comment
It would be reasonable to add a graphviz converter module the ```networkx/readwrite``` folder on NetworkX. To build such a conversion system you need functions to add nodes, add edges, get nodes and get edges from the graphviz structure. Having it work with the node and edge attributes would be important too.  If you can get that part of it done (maybe with some tests) and submitted as a PR I can help getting the rest of the structure and docs and so forth.  It's Ok if your tests are not in the right style for networkx. I'll change them. The important thing is to get the functaions that add/report nodes and edges.
comment
That seems like a nice practice problem: take the code for bfs_edges, read it and understand it sufficiently to change the order of the nodes when added to the queue.  If you can make it easily understandable code and general enough to be useful for others and not detrimental to the time needed for the standard case, please create a PR for adding it to NetworkX. :} 
comment
Actually, could you remove the numpy requirement?   The ```is_k_regular(G)``` code can be: ```python all(d == k for n, d in G.degree) ``` And ```is_regular(G)``` is not much different once you get an arbitrary node: ```python n1=nx.utils.arbitrary_element(G)  if not G.is_directed():      d1 = len(G.adj[n1])    # could be  d1 = G.degree(n1)     return all(d1 == d for n, d in G.degree)  else:      d1=len(G.pred[n1])       in_regular = all(d1 == d for n, d in G.in_degree)       d1=len(G.succ[n1])       out_regular = all(d1 == d for n, d in G.out_degree)       return in_regular and out_regular  ```  That said, the way to handle the numpy requirement is to skip the tests when numpy isn't available. You use ```numpy = pytest.importorskip('numpy')``` or similar. Take a look at the tests for another module that imports numpy.  But I don't think you need numpy and it will be faster not to have to create the (potentially large) numpy array just to compute degree.
comment
For small graphs the overhead is negligible. But for 10K nodes it is significant and for 100K it probably won't fit in many computers. And we've got the in_degree and out_degree as just the length of a dict so no need to create and sum a matrix. :}  Some other comments/requests: - you define k-regular and regular in their functions quite well :) For the k-factor code, can you break down "spanning k-regular subgraph" with another sentence or two to make it easy to read. Leave the sentence as is... but add something to explain k-regular and spanning subgraph. - can you find a more descriptive name than gadget? Also the method ```resolve_node``` takes ```g``` as an argument but doesn't seem to have a node in sight. It makes it look like "g" is the node you are resolving. I think this is only used immediately after creating the gadget. So maybe it can just be called from __init__ with the graph passed into the gadget constructor. But I may be missing some reason to have it separated out to call later... - you don't need the ```g.add_node(outer)``` or ```inner``` just before you add an edge between them. The add_edge method adds the nodes too. (ok either way really not big deal) - For the exception for not perfect matching, can you make the message refer somehow to k-factor? As it is, someone calling it may be looking for a k-factor and not know what to do with a response that a perfect matching does not exist.   This is nice.. Thanks!
comment
I think the name "gadget" should not have been used in the paper -- but it was... :}  So, it's fine where it is.
comment
OK... last bits:    - add these functions to the docs by creating a section in ```doc/reference/algorithms/regular.rst``` Use ```reciprocity.rst``` or others as a model to work from.  To test locally you have to install sphinx and others in ```requirements/doc.txt```. But Travis-CI will check if it runs without checking locally. - Add something to ```networkx/algorithms/__init__.py``` to get the function to be included in the main namespace.  Ask if you have questions, or would prefer I do this stuff. I think we're close to merging.
comment
We've tried to keep the syntax creation/maintenance minimal by using Python native syntax (essentially list/dict comprehensions).  ```python age_lt_30 = [node for node, age in G.nodes(data="age") if age < 30]   degree_le_4 = [node for node, deg in G.degree() if deg<4]  claire_nodes = [node for node, name in G.nodes(data="name") is name == "Claire"] ```  If this doesn't fit what you've got in mind comment more or reopen... Thanks!
comment
G.nodes and G.edges are iterators over nodes and edges. Would that work with ```itertools.islice```?
comment
Yes, we would be interested in those algorithms.  Perhaps put them in ```algorithms/cluster.py```. Choose your own names for them but something simple like signed_clustering and signed_average_clustering.  The top of that module doesn't match our proclaimed style (described on the github networkx wiki page) so that could be updated as well. That code hasn't been touched in a while.  Thanks!
comment
Thanks very much @ericma! And happy to meet you @seunghwak  cuGraph looks promising. Hopefully we can make it easy for people to use both of these packages.
comment
Are you intending to add directed or undirected (you say undirected for both existing and your implementation)? Also, what algorithm are you using -- has it been published somewhere or are you creating a new algorithm?
comment
That sounds good.   It also sounds like it might make sense to have it included as a separate function. Have you thought about how to make the API?  Also, I would suppose this should go into the euler.py module. Is that what you expect? If you are planning to use existing bipartite minimum weight optimal matching algorithms then maybe it should go with that code instead... ?...
comment
Qs:  1. "w is the weight that I seek to minimize"...  What is w? a string? a function? 2. You should add a warning to the docstring that edge weights should not be numeric due to floating point errors. Rather they should be integer values. 3. I agree that the new function should go with the euler.py code.  Only API issue left is whether this should be a new function or make ```eulerize()``` use this algorithm.  I'm concerned that the current eulerize function does not optimize the choice of edge matching, but your new function would. Am I misunderstanding?  If that is the case, I'd prefer you create a new function... maybe ```eulerize_optimally```?  
comment
Yes, I think this could be useful.  Probably best to add a module in ```networkx/readwrite``` with tests in the ```tests``` subfolder of that.  Thanks!
comment
That seems like a good addition to the layout.py tools. My understanding is that you'd have to release the version in our package under the BSD license if its in NetworkX, but I'm not an expert. Thanks!
comment
If you make a pull request to a file that has the copyright used in much of networkx (which says BSD license) that is sufficient magic to release it under that license. Be sure to put yourself as an author in the way described in the networkx github wiki.  If you want to create a separate module with your own license attribute (would still need to be BSD license or compatible) you can adapt the module heading to be your copyright.  Thanks!
comment
The way I know for making a pull request is to fork networkx/networkx into your own account. Pull/push to that fork (or a branch within that repo) and then on the github page for that repo, a button allows you to create a pull request to the original repo.  There may be other fancier ways to do it involving the command line only, but I don't know them.
comment
Showing the visualizations doesn't help us figure out the degree... You need to work with the actual data, not a picture. It could be that more than one edge connects 2 nodes -- or that you are using a weighted edge.  Best case: find a small example that shows the same problem. For example, get the ego_graph of a node and then you can print out the entire graph with edge data and node data and see what the degree of each node is. 
comment
For a single node, it is probably better to set the node attributes directly. This function is typically used to set many/all the nodes.  You could, for example do:      G.nodes[c]['weight'] += 1  As far as the error message you are seeing, I suspect that the reference you are using is for a different version of NetworkX than the version you are using. Check the version using ```print(nx.__version__)```
comment
The function ```nx.single_source_shortest_path_length(G, source=0, cutoff=7)``` should provide the information you need. But it returns a dict keyed by node to distance from source. So you have to process it to get it into groups by distance.  Something like this should work:      from itertools import groupby     spl = nx.single_source_shortest_path_length(G, source=0, cutoff=7)     rings = [set(nodes) for dist, nodes in groupby(spl, lambda x: spl[x])] 
comment
Yes -- I see that our choice to make single_source_spl return a dict doesn't allow it to be an iterator. That was a hard choice to make -- long ago we changed many shortest_path functions to iterators.  Perhaps it is worth taking a look again.  But -- there is a private function which returns an iterator rather than the dict. It even yields the nodes in ring level order.  So something like this:      spl_iter = nx.algorithms.shortest_paths.unweighted._single_shortest_path_length     spli = spl_iter(G, {0}, cutoff=7)   # note: source should be a singleton in a set/dict/list     while dont_stop_me_now():   # some criteria         (node, level) = next(spli)         # do stuff to collect rings based on level...  can use that level never decreases 
comment
The pickled graph was created from an old version of NetworkX. Judging from the date on a version of a file named that which I found with a google search, that graph was created with NetworkX v1.11.  To convert the graph in that file to a NetworkX v2.x graph, you should get a version of Python that has NetworkX v1.11 installed and:  - read the pickle file - pickle the nodes, edges and graph attributes using something like:      pickle([sc.nodes(data=True), sc.edges(data=True), sc.graph])  # not tested  - start a new python session with a version of python that has NetworkX v2.x installed. - read in the pickled list of nodes/edges/attributes - create a graph from that info:      sc=nx.Graph()     sc.add_node_from(stuff[0])     sc.add_edges_from(stuff[1])     sc.graph.update(stuff[2])  - Then pickle sc to a file called ```Synthetic_Social_Network_v2.x.pkl``` or similar.
comment
Each node should appear in only one component and a single node cannot appear twice in the same component.  Almost certainly it looks like one node in one component twice and it is actually two different nodes. And/or it looks like the same node in more than one component and they are actually different nodes.  Basically, it is likely that reporting results using ```str(node)``` is not showing you enough information to distinguish node from each other.  Nodes in NetworkX are key objects in a dict and if they don't satisfy ```node1==node2``` they aren't the same even if ```str(node1)==str(node2)```
comment
The connected components are stored as sets of nodes, and sets cant have the same object more than once. I'm not sure what the problem is, but it is deeper than what is apparent.
comment
It sounds like it might be best to view your nodes as edges and edges as nodes.  In your graph, to get from edge to edge, you have to pass through a node which has a weight.  Turn that around:  Think of the edges as nodes.  Those nodes are connected to other nodes. Which other nodes?  The edges in your original picture that were on the other side of a node.  This construct is called the *line graph* of the original graph.      1-2-3-4 .  consider G as the 4-path with an additional node 5 coming off node 2.        \-5             Call the edges A, B, C, D where D is (2,5).  Then the nodes of your new graph LG (the *line graph* of G) are A, B, C, D. The edges in the line graph LG are created from 2 and 3 (the only nodes in G that connect edges). The node 3 in G corresponds to the edge (B,C) in LG.   The node 2 in G corresponds to three edges in LG: (A,B), (A,D) and (B,D).  It essentially becomes a triangle in the line graph because it is a node of degree 3.   The *line graph* is a graph theory concept that changes nodes to edges. There is a NetworkX operator [function to construct the line graph.](https://networkx.github.io/documentation/stable/reference/generated/networkx.generators.line.line_graph.html) . But it doesn't move the data to the new graph. There are too  many possibilities for how to do that, so you have to do that yourself.  Assuming you can get the node weights to become edge weights of the line graph, what is the next step? I guess you could use the shortest_path functions to find the shortest path between nodes... That would be the shortest path between what you now call edges.   I guess that's not quite what you want.  But maybe there is some way to make that work. 
comment
sounds great -- any takers?
comment
How hard would it be to  1) copy the graph 2) iterate over the edges and replace each custom object with its string representation 3) write the result as graphml That might be the most straightforward way. 
comment
I think that would be cool...   So, just to make sure I understand: Any edge/node attribute that is not one of the types we already handle, we replace the code that raises an exception with code that put the *repr* representation of the object into the graphml file. I suppose you can use the *str* value, but it seems to me the *repr* is really what you want.  The docs should clearly say that this is how objects are handled. Simple tests should be easy to construct as well.  Sounds good.   Thanks!!
comment
Please go ahead and write some code! :)  If you have questions, ask... and its fine to submit a PR that is incomplete for feedback.  It is easy to update and add commits to the PR.  There might even be a way to attach the PR to this issue, but the easy way is to make a PR that simply says "Fixes #3237" somewhere in the description.    Thanks!!
comment
Hmmm... what is the difference between a global property for the graph and a property for the graph. The graph is only one object, so it is hard to envision any sort of "default value". Should we simply store these values in the ```graph``` dict?  Also, not sure if you are talking about reading or writing these beasts... :)
comment
I'm no expert in lxml but does your suggested approach cause the file to be read many times? It looks like it only gets parsed once, so maybe this is a good way to proceed. It seems quite different from what I recall about this code, but I haven't stuck my head in there for a while. :}
comment
Yes...  None is explicitly disallowed as a node in NetworkX.  It will cause all kinds of strange and perhaps subtle troubles. ```A node can be any hashable object other than None``` 
comment
This strips ```"``` Do we also need to worry about ```'```?  @johnyf I agree -- It should be related to https://github.com/erocarrera/pydot/issues/66 or https://github.com/erocarrera/pydot/issues/72  over on pydot's repository. Let's get it fixed on pydot so that upstream (downstream?) code like ours doesn't have to worry about it. :)
comment
The code ```nx.Graph(G.edges)``` will not include isolated nodes in the layout. Perhaps better to use what is called a "Fresh Copy" in [the G.copy docs](https://networkx.github.io/documentation/latest/reference/classes/generated/networkx.Graph.copy.html).  Something like:      H=G.__class__()     H.add_nodes_from(G)     H.add_edges_from(G.edges)     pos = graphviz_layout(H, prog='dot') 
comment
Yes, we are interested in those features.  I think 4 (the laplacian spectral embedding) may already be included in spectral_layout(). Our plotting code is supposed to be independent of our layout code... so the example which uses spectral_layout does plot the result, but the layout algorithm doesn't require plotting...  So, I think the two embedding algorithms could be included in our layout algorithms. The first two seem to naturally fit with the code on stochastic block models, though  it may make more sense to create a new module for them.  Thanks!
comment
There is a thread about a similar issue for version 2.39 of Graphviz: [on pygraphviz-discuss](https://groups.google.com/forum/#!topic/pygraphviz-discuss/gSIyrJp6mVo)  What version of GraphViz is being installed?
comment
My installs are using graphviz 2.36. I'll try to get an environment set up to test this. I've got multiple versions of Graphviz, but can't easily tell which one pygraphviz is using...  Anybody know a quick way to have pygraphviz report which version of Graphviz it is using?
comment
Looking at #2852 again, I recall that the bug exists in Graphviz 2.40.1 but is fixed in the development version.  See https://github.com/pygraphviz/pygraphviz/issues/111  When Graphviz releases 2.42 we need to restrict to that or later. What can we do now?  restrict to 2.38?
comment
Changing the ```timeformat``` from ```dateTime``` to ```date``` allows it to run without error. Not sure if it does the correct thing, but no errors.
comment
That makes sense to me..  I guess we should accept either 'dateTime' or 'date'
comment
You don't say much about how you are converting formats. It sounds like the bug is in the format conversion code.  
comment
Do you know enough about the formats to tell me what the change should be or where the bug might be. For example, is NUL the right thing to put in the gexf file? How should \000 be handled?
comment
I agree that this is confusing. I'm not actually sure of the history of this "dynamic" implementation were we assume list attributes signify "dynamic" information.   Perhaps it would be better to require a graph attribute called "mode" be set to "dynamic" before the code parsing list attributes.  Short term fix: get a better error message when attributes that are lists don't fit the structure of dynamic data.  Long term fix: allow attributes with lists as values even for static graphs.
comment
Probably easiest to form the paper-author subgraph and then see if you need the matrix.      G=biggraph(...)     paper_nodes = {n for n in G if G.nodes[n]['type']=='paper'}     author_nodes = {n for n in G if G.nodes[n]['type']=='author'}     pa_graph = G.subgraph(paper_nodes | author_nodes)      # if its really big, maybe use the oneliner     pa_graph = G.subgraph((n for n in G if G.nodes[n]['type'] in ['author', 'paper']))  The subgraph is a view on the original graph, so it doesn't use up memory. If you plan to change it then create a copy:  ```pa_graph = pa_graph.copy()```
comment
The NX2.x data structure was an attempt to make this easier to achieve.  The first place to explore is the ```dict_factory``` functions. If you can make a custom backend that replaces, e.g. G._succ and G._pred you can make a new class to use that backend with code like:  ```python class MyGraph(Graph):     adjlist_outer_dict_factory = my_backend_adjlist_factory     adjlist_inner_dict_factory = my_backend_neighbors_dictlike_factory ```  In [the Graph docs](https://networkx.github.io/documentation/stable/reference/classes/graph.html) there is a section on "SubClasses" which might help explain more. Check out the ThinGraph example in that same section of the docs.  Comment more here or reopen, or open a new issue if you have other questions.
comment
With the old system, it wasn't clear whether the pandas dataframe should be an edgelist or an adjacency structure. So, you see from the list in ```__all__``` that there is now a ```from_pandas_edgelist``` and ```from_pandas_adjacency``` .  You should pick the one that corresponds to how you are storing your data in the dataframe. Pandas is very flexible, so it is possible you have it stored in yet another way. But we think those are the two common ways to store graph data. Hopefully this naming convention helps clarify -- the old name did not. :}
comment
The docs for ```graphviz_layout``` say that the root argument works for some layout algorithms. Do you happen to know which? Are you using an algorithm that takes advantage of the root argument?
comment
It looks like this is an issue that has to be tracked in the GRaphViz documentation (or else we would be spending time tracking and updating what they have available.  Alternate ways to achieve "this" depends on what "this" means. Different layouts have different ways of arranging the nodes, so it really depends what layout you are using.  
comment
People use both definitions. The measure different things. It depends what you need it for. There is a keyword option ```nx.average_clustering(G, count_zeros=False)``` to allow you to choose which you want. ```count_zeros=True``` is the default.
comment
Correct -- though it is not too hard to build things using ```cdict = nx.clustering(G)``` and ```deg = G.degree()``` to filter only the nodes with degree >=2 and average those.  
comment
The two pieces of code you show are effectively identical. There is no way that those two pieces of code create different output. They don't do what you say they should do either.... no list of isolates is ever created.
comment
I can't tell what your issue is. It looks like a coding problem on your end... but I can't tell.
comment
It makes sense to have a ```topological_overlap``` function that acts on NetworkX graphs. The conversion from pandas/numpy/etc can be done by existing functions. It should be written in a module named something like ```topological_overlaps.py``` and put in the algorithms directory.  It would be nice to have a reference rather than code to work from. This concept should have been defined mathematically somewhere. I'm not in favor of defining a concept using the code that computes it, and we should have a reference to cite if we're to include it anyway. So, I guess that is step 1).  Then we need someone to attempt to translate the C coe or the math definition to python. That is usually easier than you might think.
comment
NetworkX is all pure python.  We use other packages (like numpy, scipy, matplotlib, pygraphviz,  pydot, etc) that use C backend.    Your link has a clear definition in Eqn (2) that might be easy to compute in NetworkX. Something like:      # untested...     deg = dict(G.degree())     for i,u in enumerate(G):         for j,v in enumerate(G):             if u == v:                 tom[i, j] = 1             else:                 numer = len(set(G[u]) & set(G[v])) + (v in G[u])                 denom = min(deg[u], deg[v]) + (v not in G[u])                 tom[i. j] = numer / denom
comment
Please see [the documentation for read_gml](https://networkx.github.io/documentation/stable/reference/readwrite/generated/networkx.readwrite.gml.read_gml.html) . In particular, the note at the bottom about non-ascii character
comment
The GML standard is 7-bit ASCII text. If you want to push the boundaries to include other forms of text, you'll need to use a custom treatment for decoding your information? [I suspect "destringizer" is not even a word..]  It would be worth looking at the [literal_destringizer](https://networkx.github.io/documentation/stable/reference/readwrite/generated/networkx.readwrite.gml.literal_destringizer.html#networkx.readwrite.gml.literal_destringizer) option linked to in the [read_gml documentation page](https://networkx.github.io/documentation/stable/reference/readwrite/generated/networkx.readwrite.gml.read_gml.html) -- hopefully that is all you need. 
comment
We need more specifics to fix the problem. Can you create a short example that does the same thing?
comment
The ```subgraph``` method returns an **induced** subgraph.  That means it includes the listed nodes and any edges that connect between those nodes.  Edges from the listed nodes to other nodes are NOT part of the subgraph.  In your case, there are no edges between nodes 'a' and 'd' so no edges are in the subgraph.  You might consider ```pnet = nx.projected_graph(net, ['a','d'])```.  To get the common neighbor node 'D' that makes the projection have an edge, look at the ```multigraph``` optional keyword.     
comment
You should take a look at the [shortest_path](https://networkx.github.io/documentation/latest/reference/algorithms/shortest_paths.html) and [traversal](https://networkx.github.io/documentation/latest/reference/algorithms/traversal.html) functions.  I'm not sure what you mean by "all paths", but there is a function called [nx.all_simple_paths](https://networkx.github.io/documentation/latest/reference/algorithms/simple_paths.html) which might help. 
comment
There is currently no ```__hash__``` function in NetworkX. There is also no ```__eq___``` function. So according to the Python 3.7 docs:      User-defined classes have __eq__() and __hash__() methods by default; with them, all objects      compare unequal (except with themselves) and x.__hash__() returns an appropriate value such      that x == y implies both that x is y and hash(x) == hash(y).  So, Graphs are allowed to be contained in dicts and sets, but equality is determined by being the same object -- not by equality of objects. This is a default minimalist approach for non-builtin objects in Python and I don't see a good justification for changing that. Equality of graphs opens up the mutable can-of-worms and also the isomorphic can-of-worms. Explicitly turning off the default behavior to raise an error when hashing removes a potentially useful construct -- graphs can be nodes in another graph.  I think if someone wants to use ```hash/eq``` then they know what features of hash they want provided and they can likely subclass and implement their version of hash more easily than we can predict what they want.  BTW - NetworkX has a ```freeze``` function that makes the graph immutable.  
comment
Yes, it could be added as an example in the ```examples/subclass``` folder. That would be helpful for people trying this -- and also trying other "simple" subclasses.
comment
It looks like your Pajek file is not a valid Pajek file. But it is hard to tell from this information. Can you try making a very small graph in Pajek format and then opening it in an editor and copy/pasting it over here?
comment
For some reason the pajek file is not getting the list of nodes. After the line ```*Vertices 5```, there should be 5 lines describing the vertices. Instead the file goes straight to the ```*Edges```.  In your post on orange3 you say the pajek file was created using R and igraph. Perhaps they need more help in specifying the node labels?  Somehow you have 5 nodes but no node_labels in the "local variables" listed at the end of your error trace.
comment
There is no plan to use a Fibonacci heap in the networkx dijkstra routines. But that doesn't mean we can't -- only that we don't currently have such a plan. Do you have an easy Python implementation of Fibonacci heap in mind?
comment
This is certainly possible to draw if you can figure out the position of the nodes. NetworkX doesnt currently restrict our position layout functions to preserve multiplex features. If you want to implement it, or if you find an algorithm published somewhere, let us know.
comment
Can you please change the documentation instead of changing the default? The history of commits shows that #3981 changed the default to "full". But it neglected to change the documentation. Let's bring it up-to-date by  changing the documentation.    Thanks for bringing this issue to light!
comment
This feature is not included directly from NetworkX. Can you suggest an algorithm to implement?  If not, there may be a way to adjust the code from ```all_simple_paths``` or ```shortest_path```.
comment
Sure!  Thanks! -- do you have a good algorithm to try to implement for it? This is potentially memory intensive as well as potentially time intensive.
comment
If the recursive version is short and easy to read let's keep it in (not exposed in ```__all__```) and use it in the tests. I think it's worthwhile to have multiple versions for both testing and pedagogy.  Thanks!
comment
The "system defined attributes" actually isn't a well defined term here. What "system" are you referring to? It sounds like you are trying to draw perhaps using matplotlib, but perhaps using another drawing package. If it is matplotlib, then is the "system" you are talking about matplotlib?  And the attributes are defined according to what matplotlib makes available. They aren't really "system" attributes so much as matplotlib attributes.  To see the attributes commonly used check [the drawing documentation](https://networkx.github.io/documentation/stable/reference/drawing.html). For less common attributes you should look at the matplotlib documentation
comment
Would you be interested enough to submit some code in a pull request?
comment
The ```nx.contracted_nodes``` function does not provide this feature, but it would not be hard to implement:  (untested)  ```python # merge v into u with edge weights the average of previous edges to u and v. # for directed or multigraph this would be slightly more convoluted H = G.copy() for _, w, uw_weight in H.edges(v, data='weight'):     if u in G[w]:         H.add_edge(u, w, my_edge_func(uw_weight, G[w][u]['weight']))     else:         H.add_edge(u, w, uw_weight)  H.remove_node(v) ```
comment
The handling of self-loops is controlled in ```contracted_nodes``` by a keyword input parameter. So, whether to include that if statement depends on how you want to handle self-loops. And I guess your suggestions handles pre-existing self-loops while the parameter handles created self-loops. Add to those complications the tweaks for directed and multigraph and you can see why we haven't tried to include all the possible ways people would want to contract nodes. :}  If you can come up with a simple interface that allows the flexibility we would probably all like, then a PR is greatly appreciated. But it is not usually a quick fix in cases like this.  Maybe the thing to do is include examples of how to build your own... But if you think hard about it, it seems like its often possible to find a flexible intuitive interface. :}
comment
Please comment here (or reopen) if anyone has ideas.... Or just open a new PR/issue...  Thanks!
comment
What doesn't the current cycles routine support? I think it works fine with planar graphs with or without node positions. It just ignores that information. What is it that you want/need that the current code doesn't provide?
comment
I don't understand the definitions being used in the examples provided. You say we need to know the positions of the nodes to find the faces. So how would this functionality work then? Do you provide a position dict as input?  How is a face even defined if it requires positions?
comment
But a planar graph doesn't have coordinates associated with the nodes. The definition is only that there exists such an embedding into the plane. Indeed there are infinitely many such embeddings. All of them satisfy the v-e+f=2. But what does it mean to return the faces? Its seems like cycle_basis does a pretty good job.   Do you know of applications/extensions that would use the faces?  Would those be able to use the cycle_basis?
comment
Yes -- I see that you supply the graph AND an embedding.  But the embedding is not sorted as coordinates, but instead a dict of ordered neighbors for each node.  Yeah -- I think this is a reasonable approach. Unless you have another suggestion I would say to put it in a module networkx/algorithms/faces.py     You might want to "See Also" in the doc strings with some of the cycles.py functions and vice versa if that makes sense.
comment
I'm going to push this out to a later version until we can get a PR to look at.
comment
Can you check your example code? It doesn't actually use read_shp anywhere. Your stackexchange post suggests that you are using Python3.0. NetworkX requires 3.4 or later. Can you try with 3.6? It will be hard to recreate your error to investigate it without a simple shp file that causes that error. Our tests are working for python 3.4-3.6, but they are not very extensive. 
comment
Just to be clear, there is only one line that should be changed -- the rest are only adding comment symbols.  Is that right?      - inf = Ce.C.max()  # always has at least one inf     + inf = min(min(Ce.C.sum(axis = 0)), min(Ce.C.sum(axis = 1))) + 1  
comment
That is a very rough, but accurate description.  Asking for a merge is called creating a Pull Request. You should probably read the documentation section called [Developers Guide](https://networkx.github.io/documentation/latest/developer/index.html).  Thanks!
comment
Look at #2316 for the current status. It's been almost a year since the last activity so you should give it a try.
comment
This Issue discusses shortest paths for Multigraphs. The question is whether the shortest_path routines should return something like [(u, v, key, weight), ...] for each edge in the path.   My impression is that shortest_path for most multigraph problems would use the edge with minimum weight between two nodes. Can we write a wrapper for the output that moves along the found shortest path and adds keys and weights after-the-fact? For that matter, wouldn't it be possible to shrink the multigraph to a graph by only keeping minimal edges between each pair of connected nodes?  Maybe I don't understand the use-case for multigraphs and shortest path.
comment
It is not available currently and could be added as a module in the algorithms/traversal area. Thanks! 
comment
Go for it!
comment
It looks like you are using the "master" branch of your fork of NetworkX for both pull requests.  The best way to handle pull requests is to make a separate branch for each pull request. Then, even if it takes a few months to get the pull request merged, you can still work on other code independently of the first project.  Some webpages even say you should consider deleting the master branch on your fork of the main repository.  I don't go that far, but I must say, it would save a little bit of time I take to keep that branch current.  And it is true that whenever I want the true master, I use the networkx master branch -- not my fork's master branch.  So: starting from master, add a remote to the main networkx repository (maybe called ```upstream```.  Then update your master branch:  ``` git ch master; git pull upstream master``` Now you are ready to make a new branch:  ```git checkout -b iterative_ds``` Make changes...  commit...  push up to your fork:  ```git push -u origin iterative_ds``` Then on github, you can create a pull request from that branch.   OK... but what about now -- that the local repo is kind of mixed up. Create a new branch from your current state. Change back to the master branch. Use ```git log``` to find the sha of the most recent commit before  the work you did on iterative stuff.  Use ```git reset <sha>``` to reset the HEAD of the master branch to that point.  Then pull from upstream and master should be good to go.  Then I think you'll need to create a new branch and either pull or cherry-pick from the new branch you created with the iterative stuff in it. You can search google for most any git questions and get good stackoverflow answers. If worst comes to worst you can clone a new copy of your fork, copy the files over to a new branch in that copy of the repo and recommit.  Hope that isn't needed... :)
comment
@sdsingh we have a question about implementing IDS for NetworkX.  What should these functions return? IDS is similar to BFS so we could use it to return the things BFS returns, but then people could just use BFS. Can you give an example of where IDS is being used and what that example would want?   Thanks!
comment
I think we're not getting any feedback here, so based on reading online, it seems that IDS is most effective in providing early results which can then be used to pursue other approaches. So an iterator yielding results is needed.   I'm guessing that some people will want the edges and some will want predecessors (nodes along with which edges discovered them). Let's start with yielding edges like the ```dfs_edges``` does.  You'll need to keep track of the edges already yielded when you iteratively deepen the search so you don't yield them again.
comment
Thanks for the response!    So I think that means we don't want to store the edges we have already reported because that will also take O(b^d) memory. Right?  Or maybe I am confused. Don't we need O(b^d) memory just to hold the graph anyway?
comment
Yes -- I think that is the first best thing to implement.  Go for it. :)
comment
This would be very cool. I've read enough to know I'd like to see it in NetworkX. But I haven't tried implementing anything. Some of the proposed low-stretch tree algorithms would be needed as they build on those. 
comment
>As an example, the more advanced algorithm immediately returns the original graph if m <= 2028 / (0.38 * epsion^2) * n ln(n).   Hmmm...  a quick calculation says a 1% error needs 1billion nodes to even make reaching that cutoff possible. Maybe that's not such a good algorithm to implement.  Have you looked at the other links: Koutis (cited by @harrymvr above) or the Spielman code/papers (cited above by @chebee7i )
comment
Yes, The implementation the spanner construction would be of interest. Thanks very much for diving into this!!
comment
Sounds good -- looking forward to it. :)
comment
There are a number of low stretch spanning tree algorithms that would be cool. So I think we could have enough algorithms focusing on spanning trees that a directory in algorithms would be good.  Tree is a fine name-- but might pull up thoughts of tree algorithms that have little to do with spanning. If it becomes a problem we can split and move later.  I'm  +1 for a directory in algorithms holding spanning tree stuff. 
comment
Looks like it is waiting for code to be submitted as a Pull Request.
comment
@lonnen is correct. In order to consider edge attributes when matching edges, you must create the conditions for the match. That is done with the ```edge_match``` parameter of the ```optimize_edit_paths``` function. The ```edge_match``` parameter must be a function that inputs the edge data dict for the two edges, and return True when those should be considered to be matched. Importantly, the documentation says "If neither edge_match nor edge_subst_cost are specified then edge attributes are not considered."  So, if you want to match edges only when labels match, something like this ought to work: ```python edge_match = lambda dd1, dd2: dd1['label'] == dd2['label'] ```  Try it with a toy example first to make sure you understand what it does.
comment
I'm going to close this, but anyone with more comments can add them or reopen as needed.
comment
I think #3179  takes care of this?   Are you using version 2.4 or later?
comment
ford_fulkerson is a name used long ago in NetworkX (deprecated in v1.9). Try the edmonds_karp() function [described here](https://networkx.github.io/documentation/stable/reference/algorithms/flow.html)
comment
I think this is a good idea. Should I merge this one or wait for you to add more?  I have another question for you based on your experience with this. For long messages or highly indented raise commands, you often must use a line-break somehow with the raise.  I've been moving toward setting a ```msg``` string before the raise rather than using line breaks. Is there a preferred method?  ```python     raise NetworkXError(         "this is a long message ........................................................."         ) ``` vs ```python     msg = "this is a long message ........................................................."     raise NetworkXError(msg) ```  Thanks!    
comment
This looks very nice!  Thanks!  I have a question about the second test change inside the test_multigraph function. You add a test using 100 different random seeds. Does this test the seed argument or tests all possible different orderings of the labels/nodes? I'd prefer not to have many tests of a random process in our test suite...  Depending on the goal, could we instead either: go through all permutations of the labels (I think there are 24 of them as opposed to 100), or: check that seed is working using some other method?  Do you want to add part of your paragraph from #3330 describing the "best node first" strategy to the docstring?  Any other additions expected to this PR? It's very close to ready as far as I can see.
comment
Thanks @cool-RR !!
comment
See also #4000 which suggests avoiding swallowing exceptions throughout the code.
comment
In your call of ```nx.draw_networkx``` you have ```labels=list(...``` but the labels keyword argument should be a dictionary keyed by node to the label. Something like: ```labels=parts14``` though I may not be understanding what parts14 is.  ```labels=zip(G, parts14.values())``` is another possibility.
comment
That feature is not built-in. But the idea behind building it yourself is not too bad...  - Create each edge with a 'time' attribute:  ```G.add_edge(u, v, time=[a, b])``` - Store the current time e.g. as a graph attribute: ```G.graph['now'] = t``` - Build the time-checking into a ```subgraph_view``` which requires that the edge have a 'time' attribute that includes the current time.  (this is untested)  ```python def time_filter(G, node, nbr):     '''Return True if current time is between time endpoints for edge (node, nbr)'''     start, end = G[node][nbr]['time']     now = G.graph['now']     return start <= now <= end  Graph_at_now = nx.subgraph_view(G, filter_edge = time_filter) ```  The idea is that you can change the time of the graph and still use the same subgraph_view.  Changing the graph while also changing the time might lead to trouble. Depends what you are doing.
comment
What version of NetworkX gives this warning? It looks like #3764 fixes this error. It is in the latest (v2.5dev), but not the stable (v2.4) release. 
comment
Good -- v2.4 is the "stable" version.   v2.5 is in the works on github and will be released this Summer.  It already has the fix in it.  Thanks for the report!
comment
Thanks @ericmjl for taking a look at this.   The other things to look for are  - proper documentation (add the module to the rst file, use one line short summary at top of docstring followed by blank line, list parameters and return values) - tests that use random numbers should specify a seed and not repeatedly test many random inputs in these tests (though that code could remain as commented code). - The API should be simple and where relevant match current API. - The algorithm should not be already implemented somewhere else in NetworkX and if there is overlap, it should be determined whether to refactor common parts or leave them separate (and duplicate). Duplicate code is not always bad, but often it is better to avoid it.  Can't think of anything else to look for other than correct implementation at this time. Thanks!
comment
Yes!  Thanks @arunwise, and also Thanks @ericmjl for your work with this too. :} 
comment
Thanks for this!  I have some questions: - Why doesn't this work for multigraphs? It seems like it would. - This algorithm as originally written changes the graph G by adding a node attribute 'voterank' which looks like the voterank when the algorithm completes, but I'm pretty sure that isn't a useful value. The point is to select the nodes, not figure out the voterank of those nodes not selected after all selected nodes are selected. If I am correct that this isn't useful information, I would prefer that it not overwrite any potential node attribute already stored in G. Thus, could we change the algorithm to use an internal dict keyed by node to the voterank. That would probably faster as well.  - Could we remove the word "seed" in the docstring. And phrase the process as "Select `number_of_nodes` nodes in `G` using VoteRank"  using "select" instead of "compute". - Could you remove extraneous parens, e.g. ```if G.is_directed():```? - I think you don't need to set ```AvgDegree = 0``` before setting it, and you can remove ```float``` when getting the average (we only support Python3.x now).  I realize many of these relate to updating the original code rather than to your changes to the code, but while we're changing things, let make is as good as possible. Thanks!
comment
I think that approach to multigraphs is something the user could implement easily. I was thinking that you could process each edge as a vote. Then multiple edges give multiple votes for the same node.   Both the votes and decrease in voting power can work with a multiedge acting this way.  You are correct that the paper doesn't include either method though. I wonder if anyone has done that and studied its effectiveness.
comment
It looks like there is a rogue print statement in voterank. :} This looks great! Thanks for taking on the documentation and tests and expansion to multigraphs. I think this is a good setp forward to the voterank part of NetworkX. :}
comment
The doc_string says: ```A context manager for temporarily reversing a directed graph in place.```  You probably want a ReversedView, [most easily obtained](https://networkx.github.io/documentation/stable/reference/classes/generated/networkx.DiGraph.reverse.html) using ```G.reverse(copy=False)```.  Perhaps we should expand the documentation of the contextmanager ```reversed``` to provide other alternatives in a ```See Also``` section. 
comment
I'm circling back to this finally... sorry for delay...  If I understand correctly, the difficulty is that ```nx.ancestors``` reverses the graph in-place. And it shouldn't if we want to allow multiple threads to call ```nx.ancestors``` using the same graph.  Do I understand correctly?  If this is the issue, we should probably check whether this same defect arises anywhere else that uses ```nx.utils.reversed```. Also we should improve documentation about it too. Maybe (though maybe not) we should allow a way to designate that a function like ancestors reverses in-place or as a copy.
comment
This sounds great -- As you dive into creating the new data structure for Quick Cut, I would suggest that you try to build it out of native Python structures (like dict) wherever possible. They are quite fast. As for how to name them, why don't you (for now) rename the existing as ```original_ker...``` and use the new methods with the current name. That way we can test against the older method easily and users have both available if they need/want to compare.
comment
This looks very nice @boothby :)  Any comments @zcollins0 ?
comment
Yup -- it looks like the code actually does write "source" and "target" to the edge attributes and then overwrites them with the previously existing data.  Maybe it should raise an exception if the columns already exist. It's not good to overwrite data without telling someone.
comment
Yes -- that's the function... line 262 is where it gets overwritten, but the keys could be checked a few lines before...   I think the general ```NetworkXError``` makes sense here --  that flags it as an exception of the NX api and I don't think it  makes sense to create a special exception  just for this case.  Thanks!
comment
Fixed in #3935
comment
@fredrike can you weigh in on this?   It looks like any elected node should have its voterank set to ```[0,0]``` (see fig 1 in paper).  But the code doesn't do this.  What are we missing? or is it a bug?
comment
```__repr__``` is supposed to create a string that could be used as code to generate the object. That is very hard to do well for a graph object with arbitrary attributes. It's better to use the  ```nx.info(G)``` function is you are interested...
comment
We won't be implementing this.
comment
Thanks very much for this report!  Nicely put together... (and followed up when a fix was proposed) Thanks...
comment
This looks like you changes from using ```G[u]``` to ```G,predecessors(u)``` but that won't work for undirected graphs. Then you removed the tests of undirected graphs and changed them to directed graphs. So, while it is passing all the tests now, it still won't work for undirected graphs. Am I correct, or am I missing an if-clause somewhere that makes this only get used for directed graphs?  I'll get a chance to dig around in this sometime soon. But it would reassure me if you left the current tests of cycle_graphs and just added the new tests of DiGraphs.  It's Ok to have two assert statements in the same test. Just leave the existing one and add the new one.  Let me know if I'm missing something.
comment
Yes -- you are right. It should just be a walk (remove "closed") for both ```communicability()``` and ```communicability_exp()```.  That's been there for over 9 years. :}
comment
You can add the documentation. It is automatically created from the docstring. You just need to tell "sphinx" to auto-generate that file. Do this in ```doc/reference/algorithms/centrality.rst```. Use the other modules as examples.  The Python PEP8 Style suggestions are listed above and mostly deal with spacing issues. Check them locally using flake8 or pycodestyle.   Thanks very much for this!
comment
What is the trouble?  It looks fine to me.
comment
Thanks for this!  Can we use the [utility function currently](https://github.com/networkx/networkx/blob/master/networkx/algorithms/shortest_paths/weighted.py#L40) in ```algorithms/shortest_paths/weighted.py``` called ```_weight_function```?   I think that could remove a lot of the extra code that is deciding which way to compute weights. It might even be faster, though that is hard to predict in Python. The idea would be to call this centrally defined function on the input ```weight``` and then use the returned value as a function that computes the weight for each edge. The checking for multigraph is done once when ```weight``` is defined.  It's also possible it doesn't work for this case, but I think it does work.  I guess you'd have to import with a long name like:  ```python from networkx.algorithms.shortest_paths.weighted import _weight_function ```
comment
This looks good... Thanks!! While I've got your attention, I've been thinking that a better name of this function would be something like ```simple_paths_by_length```. Does this make sense to you?   I don't want to do it in this PR.  I'll talk to more people and do it as part of prep for v2.5. But do you think of the old name? of  the proposed name?
comment
Yes, I agree that backward compatibility is important.   Maybe when we go to v3.0... The problem with ```shortest_simple_paths``` is that it yields paths that aren't the shortest.
comment
Can you separate this branch from the branch that fixes the documentation?  Probably a ```git rebase -i HEAD~6``` and then remove the commits that fixed the documentation. That'll make this PR restricted to only code relevant to this issue.  Thanks!
comment
Excellent -- thanks for this! I especially like the tests...  Comments/suggestions: - The two tests can be nicely combined by using ```pytest.raises``` in a similar way to the ```test_exceptions``` method which appears about line 25 in the testing file. You can also follow that style to remove need for importing ```NetworkXError```.  - Creation of the test graph can be more compact using  ```python G = nx.path_graph(10) G.add_weighted_edges_from((u, v, u) for u, v in list(G.edges)) ``` - Can you add a test of the NetworkXError using the default source/target names? ```python nx.set_edge_attributes(G, 0, name="target") pytest.raises(nx.NetworkXError, nx.to_pandas_edgelist, G) ``` and similarly for ```name="source"``` .  - The error message (in the function itself -- not tests) is rather long.  And we're slowly switching to [formatted strings](https://docs.python.org/3/tutorial/inputoutput.html).  So, how about (?) ```nx.NetworkXError(f"Source name "{source}" is an edge attribute name")'```
comment
You are correct that NetworkX v2.4 supports Python 3.5. But with v2.5 we are moving to support Python 3.6-3.9. So anything that gets merged now will be Python 3.6+.  f-strings are ok for that. Thanks!
comment
Thanks!
comment
Thanks for this idea and implementation.   Can you motivate this change in a sentence or two? (Why should we do this instead of sticking to the style of the doc examples for GraphML?)  Also, will this impact reading the resulting file? How will it impact the attributes of the read Graph?   (minor PEP8 style change) can you remove the "is True" when checking ```self.self.named_key_id is True```?  Finally, add some tests to make sure this does what we want (and keeps doing it when other changes come)
comment
Thanks very much for the code and your responses!
comment
Thanks for this very simple example!!  Looks like a bug. 
comment
I believe Dijkstra is correct for non-negative weights. It should be Ok with zero weights or positive weights if I understand correctly.
comment
It seems to me that we have found two things here.  1) ```_dijkstra_multisource``` has a bug in its predecessor finding routine. I believe this goes beyond zero-weight edges. It we have ties for shortest path it may or may not report both predecessor for some nodes.  2) ```all_shortest_paths``` should not allow zero-weight edges. This is a concept that doesn't make sense. With a zero-weight edge, you will get an infinite number of shortest paths if any shortest path includes the zero weight edge. To make the answer determined, it is perhaps better to do a node-contraction for the two nodes on either side of every zero-weight edge.  I feel this is a strange enough case that we should handle it with documentation rather than e.g. checking for zero-weight edges.  Comments?  If I've got the above correct, then we should retain the fix for weighted.py in #3783 and change the documentation for all_shortest_paths to suggest that zero weight edges be handled using node contraction.
comment
Very nice!  I'm just catching up here.    I think we use the term ```simple path``` to be a path that does not repeat nodes. We have a function ```all_simple_paths```.  I guess that for shortest_paths you usually only get simple paths, so it really is a corner case that appears with either zero or negative weights.  I like your fix for ```_dijkstra_multisource```.  We should search for other routines that might use that feature.  I thought Bellman-Ford would catch infinite loops due to cycles because they could occur with negative weight around the cycle too. 
comment
The wikipedia entry on Bellman-Ford says that the algorithm can identify and report negative weight cycles. I think our implementation does this.   We should make our implementation also report zero weight cycles.  And we should construct ```all_shortest_paths``` to report all shortest simple paths with a comment to that effect near the top of the doc_string.
comment
I like what you've got in  #3783 factoring out the path-tracing from ```all_shortest_paths```. (sorry for long delay in review)  I think this is ready to merge.  I have one more only somewhat related question... Right now there is a function ```shortest_simple_paths``` in module ```algorithms.simple_paths.py``` that yields simple paths ordered by length. That is different from only returning the shortest simple paths (which is what ```all_shortest_paths``` does.    @jarrodmillman would it be reasonable to rename ```shortest_simple_paths``` to ```simple_paths_by_length```? That would require a deprecation period I suppose. But it might reduce confusion with these shortest_path algorithms.
comment
Let's push off renaming discussion to a different PR. This is fine as is. 
comment
This all looks good -- and I see you've got a fixme to look at goldberg_radzik too. Also good.  I have some minor comments/questions for you: - the internal function doesn't seem to need input G. Could we remove it? - it's name is also very close to ```all_shortest_paths```. What do you think of ```_build_paths_from_predecessors```? - we currently only call this function using a single source. Should we make the input a single source? (I mean, is there a case you anticipate that uses multiple sources?) - The name of the set of processed nodes is "hits". Is there an origin of this name? It's not a big deal, but would "seen" be better? we have a function "hits" in NX for link analysis (finding hubs and authorities based on links) and we use "seen" in other similar.  Very nice work!! 
comment
Indeed, the documentation does assume that the head/tail names follow the standard arrow analog with head being the destination.  Is source/destination more clear than head/tail? Maybe...  Probably best to include both as you have suggested. :} 
comment
It looks like you are probably asking it to remove the same node more than once. Am I missing something? I you remove a node and then try to remove it again, it will silently ignore the request to remove it a second time.  Also, please just copy and paste the text rather than sending a screenshot of the text. We can't interact with a screenshot.
comment
Try checking the nodes that are to be removed (often duplicates). And also try checking which nodes actually were removed.  ```python nn_1 = g.number_of_nodes()  setnn_1 = set(g)          # or set(g.nodes)  ... #then later nn_2 = g.number_of_nodes()  setnn_2 = set(g)          # or set(g.nodes)  if nn_1 - nn_2 != len(to_be_removed_list):     print("nodes actually removed:", setnn_1 - setnn_2) ```  It looks like ```np.random.choice``` is allowed to choose with replacement.
comment
Thank you!
comment
Yes -- indeed the function ```gaussian_random_partition_graph``` calls ```random_partition_graph``` with positional arguments in the wrong order. seed and directed are switched. Would be good to change that call to using keywords:  ```python gaussian_random_partition_graph(sizes, p_in, p_out, directed=directed, seed=seed) ```
comment
Thanks!
comment
See #3754 We have a number of places where we should be using a weight function (or string) and currently we're only using is as a string. We're definitely fixing this for next version.
comment
See also #3797 for a related feature request. I'm reopening this until fixed along with #3754 and #3797
comment
small tweak -- I think this came from bellman-ford, but it is not needed.  ```G_succ = G._succ if G.is_directed() else G._adj``` . Should be just ```G_succ = G._adj```  It's not needed because ```G._adj``` is always ```G._succ``` for directed graphs. It was designed that way so you can just use ```G._adj``` for successors whether G is directed or undirected.
comment
This looks good.  Anything else before its merged?
comment
It seems like a good idea to add these functions.   Thanks!  You can/should keep the ```_find_path_start``` function private, and in the tests when you need to call it use something like:      find_path_start = nx.algorithms.euler._find_path_start  The doc_strings need work -- Use one-line descriptions, followed by blank line and then paragraph defining the terms you use. Then define the parameters and return values, notes, "see also"s, references and examples.  Look at existing functions for how to make it look. It would be good to have a reference for most of these since they are definition heavy.  You might need to update the relevant *.rst files in /doc/reference to make the docs show the functions. 
comment
That depends on what you mean by "preserve the orientation". Is it the printing of the edge 2-tuple that you want?  And what do you want to keep about the undirected nature of the graph? Why wouldn't a DiGraph suffice? (It certainly prints the way you want...  but presumably you want the undirected nature of the graph for some other purpose?)
comment
Actually an undirected graph has to store both directions in an adjacency data structure anyway. So, the undirected Graph object just does all that bookkeepping for you -- but doesn't "save" any storing of both directions.   Glad to hear that you found that the edges are ordered if you specify which nodes to look at. I can verify that the code is set to report edges with the requested node being the first node in the edge 2-tuple.  The order that you supply the nodes to the ```t.edges``` determines which 2-tuples are reported.    I'm going to close this, but you can still post to it if needed and we can reopen.
comment
Your documentation is from networkx-1.9 (see the link url...)  The problem is that google's search algorithm still points to networkx 1.9 and 1.10 when you search for functions. We were able to get it to stop pointing to v2.x earlier than latest version. But 1.9 and 1.10 often still come up.
comment
Great!  Thanks!
comment
This looks good!  Thanks very much! I'm ready to merge unless something else is coming.   We can always do another PR for other things too.
comment
Rats -- I forgot to ask you to add the new function to the expanders section of ```doc/reference/generators.rst```.  Otherwise it won't show up in the documentation. Can you make a very short PR with that? Thanks for this!
comment
You must have an older version of networkx.
comment
Its not a networkx limitation -- its a Pajek standard. You can [relabel the nodes](https://networkx.github.io/documentation/stable/reference/relabel.html) to be integers and store the string as an attribute in the Pajek file.
comment
That's an excellent idea!  I think the cost of a try/catch over the casting should be small and the error message probably would help some people a lot.  :}
comment
This looks good -- Thanks!  Can you change the docs to "container of edges"?  Strictly speaking a list is an iterable, but not an iterator. That language is easy to mis-state, so I'd prefer "container".
comment
3 places in that file with that typo I believe.... Thanks!
comment
Do you get the same error using pip with a different package?  I think it is unlikely to be a networkx problem... 
comment
Hmmm.... Thanks for that! What's the best way to handle that -- do we have to get rid of the decorator that handles paths? Would it be easy for you to check for pydot or pygraphviz before calling networkx?  suggestions anyone?
comment
OK.. got it...    How about: ```python try:     import pygraphviz     networkx.drawing.nx_agraph.write_dot(self.graph, path) except ImportError:     try:         import pydot         networkx.drawing.nx_pydot.write_dot(self.graph, path)     except Import Error:             LOGGER.error('Could not build the graph in DOT format. Neither `pygraphviz` nor `pydot` modules are installed, and at least one of these is needed.') ``` 
comment
I believe it is due to the way Python stores entries in dicts. My understanding is that the order is not the same across machines even though it is (since Python3.6) the same from one run to another. I am actually not certain about these claims. I know for sure that prior to 3.6 the order of the keys in a dict changed based on many things. I know that it changed in Python 3.6 to be more consistent from run-to-run. But I am not sure whether dicts are guaranteed to use the same order from machine to machine now. I think they aren't.  
comment
Which version of Python?
comment
It looks like ```preflow_push()``` uses Python sets to store "active" and "inactive" nodes. And unlike dicts, sets are not deterministic even in Python 3.6+ (according to [this stackoverflow post](https://stackoverflow.com/questions/14956313/why-is-dictionary-ordering-non-deterministic))  You might be able to alleviate some of the trouble by setting the [PYTHONHASHSEED](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONHASHSEED) environment variable.
comment
Thanks!
comment
Can you provide a small example that shows the problem. It can just be two edges.
comment
It looks like you are using infinite values for the edge costs. Is that intentional? Might be better to pick a large finite value. Depends on what you are trying to do. Why not just remove that edge?
comment
The best way to store the labels of the nodes is as a node attribute.  Even if you have other node attributes, you should just give this attribute a different name and store it as another attribute. The more you discover about the nodes, the more attributes you should have.   You can, of course, store the node labels in a dict separate from the graph keyed by node to the label. There is not much difference in performance or memory between storing a property of a node as a node attribute and storing it separately from the graph data structure.
comment
Be careful not to assume that the nodes are ordered as you expect them to be. The nodes are ordered, by default, in the order of G.nodes which is probably not numerical order.  Try: ```python print(nx.to_numpy_matrix(G, nodelist=range(8))) ```
comment
This should be easier now that #2766 has been merged.
comment
I have changed the name of #2766 to better reflect its code -- which DOES implement a random_state approach following the advice from #1550 and #1764.  Sorry for the confusion.  I'm trying to ping this issue because we now have it "solved" for the layout functions, and we have a decorator to help implement it. We need to change the generators and look for other places that use random values. 
comment
It looks like pypy now works with numpy, so we won't be hurting pypy users if we use numpy.Random_State instead of random.Random.  Any other potential problems with switching from stdlib random to numpy's random? 
comment
I'm thinking that we should implement separate tools to handle ```random``` and ```numpy.random```.  Then individual code can easily choose which environment they want to work with. The other option is to choose one ecosystem for this package.   The layout code uses ```numpy.random``` -- and it also uses numpy matrices. The graph generator functions use ```random``` and that allows them to work in environments that don't work with numpy (though those are getting few and far between).   I'm thinking of using ```@np_random_state``` and ```create_np_random_state``` alongside ```random_state ``` and ```create_random_state```.   Thoughts?
comment
I think you summarized it pretty clearly -- in particular that the remaining question is whether to support two generator types or commit to a single generator type.  I don't think adapting the generators to look the same is a good approach. They have many different methods and it'd be a lot to maintain a unified API that works with either. The different methods make me realize that there are almost certainly advantages to each library. Some of our contributors will want to use ```random``` and others will want ```numpy.RandomState```.   This makes it tricky to see how to allow someone to use the same random number generator in all functions. Can you explain the need to use the same RNG for, say, generating the graph and initializing its layout?
comment
I'm digging into this again and it looks like the ```simpler_interface``` argument is essentially making two ```create_random_state``` functions. By that I mean that the methods available from ```np.mtrand._rand``` and ```random._inst``` differ. So if a contributor wants to use the ```random``` interface for their code they call with the ```simpler_interface``` set.  If they want the ```numpy``` interface they use the default.  It's generally not possible to allow the **user** to pick the RNG so we're back to using two RNGs.  Perhaps our codebase doesn't use more than ```choice```.  But if we use ```randint(a,b)``` anywhere, the arguments are different (numpy has top value exclusive while random is inclusive) then we'll have to either support users using two RNGs or force contributors to use ```numpy```.   I'm leaning toward your example's approach of allowing contributors to use wither of two RNGs (```random``` and ```numpy```) and users will need to supply state for both (if they use functions that use both).  I think I prefer to make two functions though rather than the ```simpler_interface``` argument to make it all more obviously explicit.  Thoughts?
comment
My reading of the example code is that a contributor calls ```create_random_state(rng, True)``` when they want the ```random``` package and ```create_random_state(rng)``` when they want ```numpy```.   The returned value is different and can't be used interchangeably. So they are effectively two separate functions.  Doesn't that mean the user will have to set the state for 2 RNGs?  Also, we won't need lots of ```getstate``` and ```setstate``` with the random package. That code can take a RNG as an argument just like the code that uses numpy. Create a new RNG with random.Random(). Use the global RNG with ```random._inst``` just like ```numpy.random.mtrand,_rand```.  It's all the same, but the API is different. so we can't quite use them interchangably.  
comment
> The returned value can be used interchangeably because they implement the same interface.  The RandomState returned does NOT have the same interface. That's the problem we are facing.   One example is that ```randint(1,3)``` for ```random._inst``` has 3 possible outcomes while for ```np.random.mtrand._rand``` it only has 2.  Maybe I need to look at the APIs more closely, but I believe that the ```numpy``` versions returned arrays of values while ```random``` do not. We are essentially creating a 3rd random number generator API (some sort of lowest common API between the two).  Am I missing something here?
comment
> That's fixed by using a facade like I did in SimpleRandomState.  But it's not fixed. The facade either has to implement the entire API or we end up with some 3rd API that is a least-common-api approach.  For this example facade, we lose the ability to use the ```size``` to return an array of values. So now -- to make our code work for either RNG, we must have ```size=None```.  Every contributor that uses numpy.RandomState will not be able to use the size argument.  I like the idea that contributors can use either random package's API without our having to maintain it. I like the idea that users can control the random_state. Merging those two goals seems to mean that a "seed" becomes two integers -- one for each package. 
comment
>It is fixed. You only ever call create_random_state with simpler_interface=True in those cases where >you rely only on the random interface. You don't need the size parameter in those cases.  Are you saying that any code that "relies only on the random interface" has to use the "simpler_interface"?  Those two interfaces are different.  You are saying that contributors can't use the ```random``` interface.  I'm confused. How is this helping? It seems like any contributor of code has to use the numpy interface which the simpler_interface then translates if needed. We might as well require all contributors to use the numpy interface.  I want contributors to be able to use whatever package they prefer...   >Contributors that rely on the full RandomState interface will not set simpler_interface to true, and >they can therefore use the full interface.  So you are saying that any user who wants to use a single RNG will need to use a numpy RandomState object.  I guess I was getting hung up on making it work either direction.  Let's clarify the goals.  [By the way -- thanks for sticking with me on this. I'm not agreeing yet but I feel that it is helping and we're making progress even if it does take a bit for me to catch on.]  Cases: 1) our code written to use RandomState, user wants to use RandomState. 2) our code written to use RandomState, user wants to use random. 3) our code written to use random, user wants to use RandomState. 4) our code written to use random, user wants to use random.  I think your proposal is to have cases 3) & 4) rewrite the code to use the simpler_interface and case 2) isn't supported.  Can we think about this instead:  1) and 4. Pass through without issue 2) isn't supported 3) create a facade to allow ```random``` syntax to be used with the RandomState RNG (inverse translator of the above example).  I'm hoping this: allows contributors to use what they want while allowing users to use RandomState for everything if they want to.  Users would not be able to use ```random``` for everything.
comment
>>I think your proposal is to have cases 3) & 4) rewrite the code to use the simpler_interface and case >>2) isn't supported. > >Yes, exactly. Note that case 2 and case 3 are currently unsupported.  Agreed!  :)   and to summarize:  Cases 2 and 3 are not currently supported. Both our proposals add case 3.  One difference is that under your proposal, 4 would require rewriting existing code that uses ```random```. Another difference is that under my proposal NX would have some functions that use the numpy ```RandomState interface``` and some that use the "quircky" ```random``` interface.   Implementation: - create tools (helper functions and decorators) to allow easy support for a single RNG. - update all functions that use a global RNG without allowing a "seed" parameter. - update all  functions that use "seed" parameter so that they allow an RNG. - (your proposal) update all functions that use the ```random``` interface to use the ```RandomState``` interface.  Note that we might want to do the last bullet even with my proposal.  Newly contributed  code could use the ```random API``` but existing code for v2.2 would all use ```RandomState API```.  I think the next step is to figure out how much work the last bullet would be -- and potential for inserting bugs.  This feels like we're getting somewhere. 
comment
How about this for doc_string description of argument:      random_state : random_state, integer or None (default)                                         Indicator of what random state to use for random number generation.                If a random number generator (either `random.Random` or                            `numpy.random.RandomState`) it can then be used again.                             If the package `numpy.random` or the Python package `random`,         use the default generator for that package.  If an integer, set the seed                           for a random number generator used solely for this function call.                 If None use the default RNG of the `numpy.random` package.   Also, what do you think about leaving the argument name as ```seed``` for functions to allow backward compatibility?
comment
I tested that (so long as you specify a value for the seed parameter other than ```np.random``` or ```random```) the default random number generators for both ```np.random``` and ```random``` are not affected by NetworkX.  The default behavior is to use the default generators which of course affects their state.  I tested this by setting the state of both default RNGs, calling every NetworkX function that uses a random number, and then making sure the default RNGs create the same next value as when I set the seed and immediately create a next value.  I hope this is a reasonable approach to test that. I didn't set up that kind of test in Travis-CI due to time constraints. 
comment
Yes -- it looks like we should swap lines 64 and 65 in ```connected.py``` Thanks!! 
comment
@boothby has a slightly better solution in #3859 where in addition to switching lines 64 and 65, we change ```_plain_bfs``` to return a set instead of yielding each node. Look good?
comment
Thanks!
comment
Can you make a 2 node, 1 edge version of the example and make sure it doesn't work for you? Something we can copy and paste and test. It works for me using:  ```python import networkx as nx import matplotlib as mpl mpl.rc('text', usetex=True)  G=nx.path_graph(2) pos=nx.spring_layout(G) nx.draw(G, pos) nx.draw_networkx_edge_labels(G, pos, edge_labels={(0,1):' $x^2$ mode'}) mpl.pyplot.show() ``` 
comment
Your node factory returns the same object (```all_edge_data```) for every single node. Perhaps you are basing that off of code for ThinGraph (where the goal is to save memory... so every edge does have the same attribute dictionary).  Why do you want to subgraph the nx.Graph class? It looks like you can just use the built-in attribute dict interface. Are you looking to provide default attribute values for all nodes and edges?  Is there something else you want the new class to accomplish?  ```python G=nx.Graph() G.add_nodes_from([1,2,3,4], NodeType=GPNNodeType.PLANT) G.nodes[1]['NodeType'] = GPNNodeType.CUSTOMER # sets all nodes to same value nx.set_node_attributes(G, GPNNodeType.JUNCTION, 'NodeType')   # set different values for one attribute on the specified nodes  nx.set_node_attributes(G, {2: GPNNodeType.CUSTOMER, 3: GPNNodeType.PLANT}, 'NodeType')   ```
comment
You will need both ```__hash__``` and ```__eq__``` to work correctly. Test it outside your graph.... ```python d={node:1, othernode:2} print(node in d) print(othernode in d) ``` 
comment
This looks ready to go. I changed the title to be more informative. I don't think this affects doc_strings or tests other than the one you changed. Thanks!
comment
The [documentation for katz_centrality](https://networkx.github.io/documentation/stable/reference/algorithms/centrality.html#eigenvector) says that there is an argument ```max_iter``` and another ```tol``` for the tolerance below which the iteration stops. You can increase ```max_iter```, increase ```tol```, or even switch to the ```katz_centrality_numpy``` method if that works better for you.
comment
Verified in Python 3.6 also.
comment
I didn't write this code and haven't used shp files much, but it looks like you need the nodes to be 2D position tuples. You can assign a label as a node attribute. But the node itself is treated as the position.      import networkx as nx     g = nx.Graph()     g.add_node((1.,1.))     g.add_node((2.,2.))     g.add_edge((2.,2.),(3.,2.))     g.add_edge((1.,1.),(3.,3.))     nx.write_shp(g,'test.shp')  
comment
The [docs for the write_shp function](https://networkx.github.io/documentation/stable/reference/readwrite/generated/networkx.readwrite.nx_shp.write_shp.html) state: ``` Nodes and edges are expected to have a Well Known Binary (Wkb) or Well Known Text (Wkt) key in order to generate geometries. Also acceptable are nodes with a numeric tuple key (x,y). ```  So I think there is fair warning(?) maybe?
comment
How big is your edge attribute data? Anything that adds memory to each edge is critical with 8.9m edges. During the read process, the function constructs a NetworkX MultiGraph to hold all the information. There is some additional storage to keep track of everything, and then near the end it does copy the MultiGraph to whatever type of graph object is desired. Have you tried it on a graph half as big to see how it scales?  The memory used is very dependent on edge attribute storage space.
comment
OK... I guess the ID is stored as an attribute. So that takes however many bytes the length of that string is. Probably not much. Each edge requires a edgedata dict so that is 240 bytes. The neighbor dicts are 240 bytes when created, but expand if there are more than enough neighbors to cause the dict to allocate more space. So, as a slight overestimate, let's suppose that each edge uses about 500 bytes. The final NetworkX graph will take about 500*10m ~= 5 GB of RAM. And we do copy the graph at the end (in a way that should allow immediate destruction and garbage collection...but it does require the memory to do the copy).  So, the 10-11 GB estimate seems reasonable for this size of graph. We've got [some hacks to keep the memory footprint smaller](https://networkx.github.io/documentation/stable/reference/classes/graph.html?highlight=thingraph), but they aren't built into the GraphML parser. 
comment
https://networkx.github.io/documentation/stable/tutorial.html#graph-attributes hope this helps.
comment
??? What is ```links```? Also, include your code inside   \`\`\`python  stuff \`\`\` to make it appear nicely 
comment
Good -- that makes it clear that links is an edgelist stored in a dataframe. Pandas data frames can also store adjacencies so... We split the function into two functions. ```nx.from_pandas_edgelist``` (which is what you want) and ```nx.from_pandas_adjacency```  https://networkx.github.io/documentation/stable/reference/convert.html#pandas
comment
Could you add a line or two to the docs saying that this method doesn't select uniformly randomly from all possible lobsters. And maybe some language that p1 and p2 are used repeatedly until failure to add an edge which stops that branch of the process. Then @silent567 should be happy with this too.
comment
I was thinking of just within the docstring for that function yes. Open to other ideas, but that seems the most simple. 
comment
Thanks! This looks good.    As for the ```is_lobster``` functions, they could be moved to a module in the algorithms section of the code. But I think that only makes sense if someone implements more algorithms for lobsters and caterpillars. But I must say I don't know much about the literature on this topic.   Let's commit it as is and then open a new PR if other algorithms are desirable. Thanks!!
comment
I believe it can produce such monster lobsters :) What makes you think that it can't? It creates random lobsters -- you can't request a specific lobster.
comment
So, you agree that it creates lobsters -- just not the lobsters you would like to see?  None of the references you point to specify anything specific about creating lobsters other than it is a tree where when you remove leaves twice you end up with a path.  It looks like you are proposing a new random model of lobster generation. Would you mind describing your model, where you got the algorithm from, why you think it is good, how it might be included in NetworkX, etc?
comment
The current implementation can/does implement multi-leaf lobsters. But it only generates at most one second level leaf for each first level leaf.  As is true for many of our random graph algorithms, we don't claim uniformly likely outcomes from the set. That is often too difficult, though it is the ideal we'd all appreciate. Perhaps we should be more clear in the documentation.  I would like to replace/improve the lobster generator if there is interest. And it would be nice to use a method established in the literature. As you say, influence implies responsibility. Since you are suggesting the change, would you be willing to search for established methods of creating random lobsters? Thanks either way -- it's good to have this on our radar.
comment
I think it could just be ```(n, nbr, G[n][nbr]) for nbr in seen``` I guess it could make the new graph take up more memory. But I think it is worth it. If someone cares a lot about that they  can strip the attributes before raising it to a power.
comment
Ahhh...  I see now... You aren't putting data on the edges that are added. Only on the dges that are copied from the original. I'm understanding better now. Sorry for not understanding better before. My suggestion for a shorter edge prescription doesn't work then... :{  Also, I now understand why you are thinking the memory won't matter much. I was worried about putting an edge attribute on all the new edges. That's not what you are saying...  But my better understanding also makes me less inclined to do the attribute copy by default. Why do you want those old edge attributes if 1) they are already on the original graph, and 2) the new power edges don't have them?  If you need them only for original edges, then look at the original graph. And if you want to expend them to all edges, its pretty easy (and problem specific) to add the edge attributes you do want. Something like:  ```python for u, v, dd in G.edges.data():     U.add_edge(u, v, dd)  % add data from original edges in G for u, v in U.edges:     if v not in G[u]:   % update any edges not in G         U[u][v][stuff] = function_to_compute_stuff ```  But I may be missing something again...
comment
The index in the matrix is not the index in the graph.  Your graph nodes are strings if I understand right.  Even if they were numbers, they don't get lined up with the corresponding row/column of the adjacency_matrix. To specify an order of the rows/columns, use the ```nodelist``` optional argument to ```nx.adj_matrix```.  
comment
Interesting idea. Can you say how this would help, and/or why this is a good thing?  The implementation seems to work fine. But of course, every layout has a scale. So this would probably need docs to explain what the scale would be for each layout function. ?? or maybe not and users try it to figure it out?
comment
In their paper, they use L_o to denote the size of the display area. That determines the scale of their layout. So, whether you choose to scale or not, you are still choosing a scale. Isn't it better to control the scale?  Your ```distance_G -distance_layout``` is a good example. The value changes a lot depending on the scale of the layout. It also depends a lot on the scale of distance_G. You better know what those scales are to interpret your results. Most likely you will want to set the scales for both ```distance_G``` and the layout to make them comparable. 
comment
Good -- then I think we understand each other.  I believe the paper has L related to L_0 in a way that depends on the number of edges so L can be much smaller than the size of the whole layout.   I'm guessing that the code knew it would rescale at the end (to make L_0 be 1... or maybe it is 2 because it can go 1 each direction from the center (diameter is 1)), so it may not explicitly set L_0, just using 1 without writing it. But it has been along time since I looked carefully at that code. You'll probably need to look at the paper to get the relationship between L_0 and L and work from that.   :)
comment
You should try a more recent version. This was fixed in #3307  try ```conda update networkx```
comment
Nice !!
comment
I cannot reproduce this result. The link to the edgelist file is no longer active. I have tried to recreate it with randomly generated edgelists. I have also tried looking through the logic of the algorithm to see what might be going wrong.   The first thing done in ```uf.to_sets()``` is essentially what is in this Issue's ```fix(uf)``` function, except it is done with all nodes instead of nodes 1 to 500.  That means there should be no difference between the code with ```fix(uf)``` inside the loop and outside the loop.   I suspect something else is/was going wrong to make it appear to fix the problem.   Can anyone reproduce any kind of error with this info?
comment
I have downloaded both versions of the solution and both the couresra unionfind.py and the networkx union_find.py give the same output.  I notice that in NetworkX v2.4 there is a fix of union_find.py for set extraction. #3224  @tommyjcarpenter can you verify that you were using NX v2.3 or before?
comment
OK... That would have been v2.3. I have now tested that it is broken on v2.3 and fixed on v2.4 and the github dev branch.  @tommyjcarpenter   Thanks for posting this so nicely and completely!!  It also helped give @boothby an incentive to look into unionfind and discover a bug in ```union```. The bug in union, if I understand correctly, leads to an inefficiency in the tree structure. I believe it still gave correct results, just didn't choose the best tree structure for future lookups.  If this is the case, then we can just replace the list comprehension with a set comprehension.
comment
Yes -- there are many modules that use numpy. Any that do are set up so that if numpy is not installed you can still import networkx. Take a look at a module that uses numpy. e.g. ```drawing/layout.py``` (and there may be better examples -- this is one I was looking at recently).  Mostly, you just import within the function that uses numpy instead of at the module level.  Testing is a little trickier because you don't want to run the tests that require numpy if numpy isn't available. But pytest actually makes it fairly straightforward.  Again, take a look at an existing test module, e.g. test_layout.py .  Ask if you have questions
comment
When I click on the codecov "Details" link and then on the "Diff" label, it shows which code has been run during a test and which hasn't. The error handling of no numpy obviously haven't been tested when the environment includes numpy, so that's not a worry.   The only suggested change is to test a case when cannibalism is False and there are not self-loops. The "Avoid copy otherwise" comment code doesn't get run during any tests. Minor for sure. But perhaps easy to fix??
comment
Looks like [Line #252](https://github.com/networkx/networkx/blob/4a6008938dfe4f19515a733dd8d3271d171ba0f9/networkx/algorithms/centrality/tests/test_trophic.py#L252) should be changed to use ```matrix_d instead of matrix_c```  Anything else planned for this PR? We can add other trophic functions later if they are planned. Thanks very much!!
comment
This looks good.  I only have one thing. For the docstrings, you include inline math using $stuff$. I think we need that to be :math:`stuff` for the [sphinx documentation](http://www.sphinx-doc.org/es/stable/ext/math.html) system.  Nevermind...  Dollar signs are good.
comment
Yes, we would be interested in this. Your placement in trophic.py looks good. You should use references in the docstrings -- maybe [1] and [2] in your post above.    Thanks very much!
comment
Thanks for the Issue...  and for the Edit--Proposed Fix.    I can verify that this is happening. Haven't been able to look at it hard yet.  Seems like a bug apparently not covered by any tests...
comment
As far as I can tell, this is failing when the edge_match function is checking that the edge data matches. But the code should be checking that both edges exists before it checks that the edge data matches. Your fix will work in this case, but only because this case has both in and out edges from node 2. It wouldn't be too difficult to find an example that doesn't work if we switch to core_2.  We should add another ```if``` statement to make sure ```core_1[neighbor]``` is in ```G2_adj[G2_node]```.  In fact we need to add that check wherever we use the edge_match function.
comment
That'd be great! You might start by adding your example as a test... make sure it fails and then put in the fix. :)
comment
Not sure I've thought through this all the way, but shouldn't we return False also when the edge doesn't exist in G2?  Maybe something like: (but please check I'm thinking about this right.)  ```python if G2_node not in G2_adj[G2_node] or \                         not edge_match(G1_adj[G1_node][G1_node],                                        G2_adj[G2_node][G2_node]):                     return False ```
comment
Thanks very much for this!
comment
```subgraph_view``` allows any filter function. ```G.subgraph``` is the common use case. The time spent creating the subgraph view is not usually important compared to the time using it. So we make the common use case fast in ```G.subgraph``` and the more flexible case is slower.
comment
This sounds like good proposal.  Thanks you!
comment
closed by #3806 
comment
I guess the logic would check if 'periodic' was True or False and if neither, then assume it is a list of length 'dim'.  It needs to be backwards compatible and not slow down the usual case too much (I don't think it will).
comment
Thanks for this!!  How about for doc_strings: ```     If `periodic` is True, both dimensions are periodic. If False, none are periodic.     If `periodic` is iterable, it should yield 2 bool values indicating whether the     1st and 2nd axes, respectively, are periodic. ```  And ```     If `periodic` is True, all dimensions are periodic. If False all      dimensions are not periodic. If `periodic` is iterable, it should     yield `dim` bool values each of which indicates whether the     corresponding axis is periodic. ```
comment
How about   ```python if np.iterable(node_size):  # many node sizes     source, target = edgelist[i][:2]     source_node_size = node_size[nodelist.index(source)]     target_node_size = node_size[nodelist.index(target)]     shrink_source = to_marker_edge(source_node_size, node_shape)     shrink_target = to_marker_edge(target_node_size, node_shape) else:     shrink_source = shrink_target = to_marker_edge(node_size, node_shape) ```   Fixes #3805 
comment
This looks good to me-- anything else before merging?
comment
This is a bug -- that function uses "weight" as if it is a string -- not a function. The documentation clearly says it can be either a string or a function. We need to update it to use either a string or a function. Not sure on the history here... Thanks! 
comment
At some point we decided to leave the threshold module out of the networkx namespace but leave it in the codebase. Looks like ```moral``` was left out by an oversight.  Any comments? 
comment
I think it just amounts to adding and removing an edge something like this:  ```python # setup  G=nx.path_graph(9, create_using=nx.DiGraph) G[1][2]['weight']=44 G[1][2]['id']=33 MG=nx.MultiDiGraph(G) MG.add_edge(1,2,'edgekey1',**{'weight':77})  # flip digraph edge u,v u,v = 1,2 G.add_edge(v, u, **G[u][v]) G.remove_edge(u, v)  # flip multigraph: edge (u, v, k) u,v,k = 1,2,'edgekey1'  G.add_edge(v, u, k, **G[u][v][k]) G.remove_edge(u, v, k) ```
comment
The data structure we use is adjacency dicts. So switching edges involves changing two entries. The data is not copied anywhere (thank you Python)... only pointers to it. It really should be quite quick.
comment
Thanks!
comment
I have verified this problem. It stems from using ```circular_layout``` as the initial positions for the nodes when ```dim >= 2```.  But "circular" layouts are two-D with zeros in all dimensions above 2.  Perhaps we can find a good initial position that works for ```dim>2```.
comment
It is true to say that the circular_layout is the initialization for the kamada_kawai_layout. But it isn't clear how to implement the third dimension for circular_layout.  I mean, it already does work for 3-D, it is still a circle... just in 3D.  Your idea of force-directed as init may work, but check that spring_layout does what you expect in 3D first... :)
comment
How would you do a spherical layout? A circular layout is fairly straightforward -- evenly spaced about the circle -- but what does a spherical layout look like with say 10 nodes?
comment
Yes, I agree -- Great Job @boothby !!
comment
This looks good.  Anything else before we merge?  The smoke tests could all use removing ```vpos = ``` for each test. That will improve the pycodestyle for the module too. If you don't get to it, I'll do that. Thanks!
comment
That's a bug in grid_graph.  The first line is ```dlabel = "%s" % dim``` and should be ```dlabel = "%s" % (dim,)```.  But I'm not sure why that line is even there... ```dlabel``` is never used.  It looks like that line is a mistake that appeared 2 years ago when taking out the G.name attribute in the grid_graph and moving it to lattice.py. We should simply remove that line.  As a workaround, you can use:  nx.grid_graph(((2,3),))
comment
This was changed from v1.x to v2.x with a ```node=nodes``` fix temporarily put in place. With the removal of support for Python 2.x, we have taken the opportunity to clean up some of the deprecated code, including this removal of the old ```G.node```.  Please update your code to ```G.nodes``` as that is the up-to-date attribute to use and has been since v2.0.  I'm sorry that you would prefer we have named this version v3.0 instead of v2.5. I hope this actually turns out to be a simple search and replace operation for you. Thanks for the idea of creating a property for ```node``` with associated warning. It would have been better to do that for v2.0-v2.5.  
comment
When I run that code, it deletes node 4 and replaces the edges with two edges (3, 3). What do you get? What do you expect? What version of NX are you using (it looks like you are using Python 2 based on print statements)?
comment
Those problems were fixed in #2586 and #2598 You should probably upgrade to a more recent version of NetworkX.
comment
Can you be more specific? The documentation seems pretty clear: https://networkx.github.io/documentation/stable/reference/algorithms/clique.html
comment
That check should be unnecessary because the for loop iterates over ```self._succ[n]```. If there is an entry ```self._succ[n][u]``` then there better be an entry ```self._pred[u][n]```.  Can you say more about why this error might be happening? Are you manipulting the data structure outside the usual API in any way? Can you come up with a small example that shows the error?
comment
The idea behind that choice of default is to allow code that handles both Graph and MultiGraph. That is, the return value of ```G.edges.data('weight')``` has the same structure whether G is a Graph or a MultiGraph. I think you are pointing out that ```G.edges.data()``` might more naturally include the keys for MultiGraphs. We decided that making people use ```G.edges.data(keys=True)``` would make the code more clear (at least avoid one type of  confusion). If you only use MultiGraphs it might seem unnecessary, but many users use Graph and MultiGraph. Also many developers write code for Graphs and then extend it for MultiGraph. This choice of default makes that easier too.
comment
Hmmm...   I tried using random graphs of similar size to yours and just timing the shortest_path functions gives astar about 5-10% more time. When I used a grid_2d_graph(1000,100) I got astar about 80% of the time of dijkstra. Neither is even close to the extreme difference you are getting. I just assigned edge weights using random.random(), so floats.  I'm using the dev version of NetworkX from github, but this code hasn't changed for a few releases. Are you using an older version of NetworkX? I'm using Python 3.7.  Anything else funky about your graph? lots of edge attributes or anything?  I don't see what could make such a big difference.
comment
OK... Sounds like nothing is different between our environments.  I suspect it is the structure of the network then. But I can't test that in any effective way. Do you get longer times for A* with random graphs too?
comment
It looks to me that something is different about your graph that makes dijkstra quite variable in time spent. Perhaps the computer is running into memory issues and having to swap to disk? It is hard for me to investigate because I don't get it being that slow or that variable in speed for the networkx I generate. 
comment
Thanks for this!  I think the problem occurs in bellman_ford when a target is used to build the path. The path ends with the input target object rather than the matching target object stored on the graph. Changing [one line](https://github.com/networkx/networkx/blob/b59935c8b114a3fbef5a5e6da3e3028d42cfc076/networkx/algorithms/shortest_paths/weighted.py#L1305) of the bellmen_ford function could fix things. ```python dsts = {n for n in G if n == target} if target is not None else pred ```  Another workaround is to call bellman_ford with the object you want as the target: ```python nx.shortest_path(G, Foo(0), {n for n in G if n == Foo(1)}.pop(), method="bellman-ford", weight=w)  ```  But this is probably better solved by "fixing" the equality checker of your objects than messing with the shortest_path routines. The bellman_ford code assumes that equality of the nodes implies that returning either object is equivalent. And we know that might not be true if you do some fancy custom object. But shouldn't that imply that you correct your definition of equality?  The correction I am suggesting is to make ```__eq__``` test everything that is "important" about the object to make sure they are the same before returning True.  You get to determine what is important. And if you have different meanings of that in different situations then you should probably rethink your object structure... (?) 
comment
NetworkX definitely does *not* only use the hash of a node.  It stores the node in a dict, so it uses a hash for sure. But dicts don't just use the hash. When there is a "collision" (two keys with the same hash) it checks if they are the same key using ```__eq__```.  That's why we say that nodes must be hashable. But we don't say they only need ```__hash__```. I believe they need ```__hash__``` and ```__eq__``` and nothing else, but I might be wrong about that. It's whatever Python dicts use.
comment
Yes, please add them to ```triads.py``` along with tests in the ```tests/test_triads.py``` module.   The PEP8 style guidelines can be tested locally using pycodestyle (pip install pycodestyle).   In addition the doc_strings for each function should start with a single line description with <80 chars (sphinx uses this for short descriptions for links to a function in the docs), then a blank line and then a paragraph description with more details.  Thanks!!
comment
This sounds good -- I think the first step might be a ```nx.triadic_closures``` function PR (that ignores triad type? -- not sure how the triad type affects things). :)
comment
Closed by way of #3742 
comment
Clicking on the codecov /patch details and then on the "Diff" tab, shows the code with each line colored by whether that line was run during a test. Most lines are green (tested), some are red (never ran during a test) and some are yellow (conditional statements that ran with one outcome but not all possible outcomes).   It looks like it wants you to add tests for bad inputs. For example, test what happens when the input graph is not a valid triad in ```triad_type()```.   You can use the ```pytest``` function ```raises()``` to test that a function call raises an exception.  I think you should also comment out the functions that aren't implemented yet (and remove them from ```__all__```). That will improve the code coverage, but more importantly won't lead to confusion for people using the latest version. You can uncomment them when you implement them.  I would suggest you fix any errors and get docs as you like, then we merge this PR before you start working on the other functions. Ask again if I didn't explain clearly.
comment
One other comment -- you have long strings of if statements but they don't cover all cases. I mean, they might cover all the possible cases, but the code doesn't use "else"... it finishes with ```elif```.  What should the code do if it falls through all the ```elif``` clauses? A brief look at the code without thinking hard makes it look like it could return ```None```. 
comment
Great!  I figured it was something like that. To make it clear to future coders looking at your stuff.  Can you remove the last if statement in each of these cases? Maybe leave a comment as to which case it is.  ```python for (e1, e2, e3) in permutations(G.edges(), 3):             if set(e1) == set(e2):                 if e3[0] in e1:                     return "111U"                 if e3[1] in e1:                     return "111D" ``` becomes ```python for (e1, e2, e3) in permutations(G.edges(), 3):             if set(e1) == set(e2):                 if e3[0] in e1:                     return "111U"                 # e3[1] in e1:                 return "111D" ``` 
comment
The code as written doesn't run. Can you change the imports at the top to something like:  ```python """Functions for analyzing triads of a graph."""  from itertools import combinations, permutations from collections import defaultdict from random import sample  import networkx as nx from networkx.utils import not_implemented_for  __all__ = ['triadic_census', 'is_triad', 'all_triplets', 'all_triads',            'triads_by_type', 'triad_type', 'random_triad'] ```  Then using ```pytest tests/test_triads.py``` shows a number of errors many due to checking ```is_triad``` on the input graph which doesn't have to be a triad!  Simple fixes I believe... :) Thanks!
comment
Thanks for this and Thanks @ericmjl for working with @bakerwho 
comment
This is a bug. The code checks whether there is one node in the shell but doesn't scale the radius by the number of nodes in the graph in that case (and it does scale the radius when the shell has more than one node).  Thanks for this!
comment
The old code treated shells with single nodes specially because it couldn't use ```rescale_layout``` with a single node (of course, then it didn't rescale at all -- which isn't right either). You are correct that we don't need to rescale using the generic rescale function in shall_layout because we can easily set the scale directly.  Your code can be improved by using ```enumerate()``` to avoid the fairly slow ```list.index``` method in two places.  If you want to make a pull request that would be great. Otherwise I will make the fix.
comment
Very good!  Thanks! (our travis-ci and appveyor use pytest to check doctests -- so all is good that way) :)
comment
Not quite sure I understand. Is it the node id or the edge id that is causing trouble. Your first post talks about edge ids, but the second about node ids.  It has been long enough since I looked at this code and we've got other very similar type of issues with other formats, so I may not have the right answer... Probably looking at the code and searching for node and edge id handling names would be understandable.  I believe the edge ids are stored on the graph as an attribute when reading a file. The name 'id' is not a special attribute name though so when writing the file it is not clear what attribute to use as the edge id. The currently written graphml file is valid graphml and it preserves all the information. A second read/write cycle would return the same output.  (I think this is correct...)  This kind of stuff is tricky to implement and difficult to maintain. Would love some code that handles it using readable code.
comment
Yes, it seems that graphml allows ```id``` attributes to be stored in multiple ways and we have chosen one of those ways when we write files. We read either way. So, in general reading and writing is not invariant, but once we read and write, further read/write cycles should leave the file invariant.
comment
Perhaps this should be in a new Issue (or maybe a new pull request?)...  It's not clear to mean what you (@kiryph ) are questioning/suggesting.  What should the edge ID be? Why isn't it enough to have the edge data as attributes? What is your expected behavior?  To answer your question bluntly: We can't treat edges the same as nodes because, well, they aren't the same. :)
comment
That looks like a good suggestion to me! Can you also add tests to make sure we check what happens with each case? This should be a good PR.
comment
Just a quick comment about why the codecov test is complaining. I haven't looked at the code yet, but am interesting to see this be merged.   The codecov complaint is that the new code (the patch) contains testing for only 90.62% of the new lines. Clicking on the details gives a report page from which you can select the "Diff" tab and it shows each line of the file colored "green" if that line has been tested, "red" if not tested, and "yellow" for statements (like if statements) that control the flow differently and only one of the two outcomes occurred during the tests.  In this case, the only lines untested are the last 3 lines of the file starting with ```if __name__ == "__main__"```. Those aren't needed since we are testing using the test files with pytest. So it should be enough to delete those lines.  
comment
This look good!  Thanks!  I found a couple of things to change:  - You should enter the new function into the docs by adding it to ```doc/reference/generators.rst``` (add the module and the function)  - Your module is named the same as your function causing one to hide the other. Perhaps make the module name ```sudoku_graphs.py```.  This also allows future functions involving other variants to be added to yours.  - The docstring for one of your tests extends to column 100 on a single line. ```Generate Sudoku graphs of various sizes and verify that they have the expected properties.``` I suggest rewording it to be shorter. Perhaps something like: ```Verify that various size Sudoku Graphs have expected properties.``` 
comment
Hmmm...  I think this got merged with some other branch in a way that github now shows the unrelated changes as part of this PR instead of part of the previous PRs. It's better not to merge a PR with a new version of master. If you need/want new functionality try some form of ```git rebase```.  I think it'll all be cleaned up.  I ran the following after cloning your repository ``` git ch master git remote add upstream https://github.com/networkx/networkx.git git pull upstream master git ch sudoku-generator-issue-3756 git rebase -i master ```  Then deleted all the commits listed there and replace with: ``` pick f8ad1fc Add Sudoku graph generator pick 8fd1f41 Remove doctest and create_using parameter pick a41e847 fix typos pick 7fc1162 rewrite sudoku tests as a class pick 47dd76b improve code style pick a52dd11 fix typo in docstring pick 7f78463 fix typo in docstring pick a031dea Add reference pick 530423c Parameterize tests for sudoku_graph pick ea0c538 remove __main__ pick 11e2eb8 Reverting change to sudoku_graph.py pick 08c58df remove __main__ from sudoku_graph.py pick 3057a5b Improve documentation of sudoku submodule ```  Checked it with ```git diff master``` All your changes should be there and the changes from others should not appear. Then pushed it to your repository and the PR should update just fine.  Can you just check the "changes" for this PR now and make sure I got everything?
comment
Yay!  And Thanks!!  @Radcliffe 
comment
Both pygraphviz and pydot are supported by networkx. You should be able to use either depending on what you have installed on your machine. 
comment
Fixes #3746  Thanks very much for this! :)
comment
You are turning the graph into a directed graph after reading it as an undirected graph. Converting an undirected graph to a directed graph is done by adding edges in both directions.  Instead use ```node_link_graph(js_graph, directed=True)```. That will load it as a directed graph. Also, in this way no post-processing is needed.
comment
This looks like a bug related to how the node attributes are formed. The GML code doesn't actually store lists. Writing a node property that happens to be a list  produces GML code with repeated lines for the same property. When reading in the file, if there is one value for a property, how can we tell whether it should be created as a list or simply take on the single value?      In [390]: import networkx as nx      ...: g = nx.Graph()      ...: g.add_node('n1', properties=['a','b'])      ...: nx.write_gml(g, '/tmp/test.gml')      ...: read_g = nx.read_gml('/tmp/test.gml')      ...: type(read_g.nodes['n1']['properties'])      ...:       ...:      Out[390]: list      In [391]: !cat /tmp/test.gml     graph [       node [         id 0         label "n1"           properties "a"           properties "b"       ]     ]      In [392]: import networkx as nx      ...: g = nx.Graph()      ...: g.add_node('n1', properties=['a'])      ...: nx.write_gml(g, '/tmp/test.gml')      ...: read_g = nx.read_gml('/tmp/test.gml')      ...: type(read_g.nodes['n1']['properties'])      ...:       ...:      Out[392]: str      In [393]: !cat /tmp/test.gml     graph [       node [         id 0         label "n1"           properties "a"       ]     ]
comment
Note that this description of "Lists" is more closely aligned in Python with a description of dicts. The key/value pairings indicate a dict. Also, the portion of the spec you reference does not include a spec for "Key". What are the allowable types for a Key?
comment
We could introduce a key that indicates the value is a list. This would be special, i.e. not part of the GML spec, but I think we are getting into areas beyond the spec:      graph [       node [         id 0         label "n1"         properties [ "list" [                         0 "a"                         1 "b"                         ]                     ]       ]     ]  The replacing "list" with "dict" could be read as a dict. We might want to make sure that the "list" has integer keys starting from 0. Maybe something like ```sorted(set(keys)) == list(range(len(keys)))```.  It feels like there may be a better way to do this though...
comment
Note that this solution does allow dashed lines with an arrowhead for the default arrowstyle, '-|>'  But it doesn't quite make the dashes well for some of the other arrowstyles (like 'simple' or 'fancy').  Still, I think it is definitely better than what we had and user's who override the default arrowstyle will understand what is happening.
comment
This seems like a good idea and easy to implement as you describe. Thanks!
comment
That's be great!
comment
Can you fix the link to the pdf also? Thanks for this! and Thanks @MridulS 
comment
You haven't given us the information that would be helpful yet (your map and your code don't help us see what the output is nor see what the actual connections in the graph are.  Importantly, we can't run your code and we have no way to tell how the map actually relates to the NetworkX graph you read in.) But perhaps you can look up the info you need yourself.  Since you have a list of the nodes with degree 2, check them.  See what ```G[node]``` is for those nodes.  ```python for node in two:     print(f"neighbors of node {node} are: ", G[node]) ```  It might be worthwhile to pick one such node and investigate that thoroughly -- both in your picture and in NetworkX. The picture clearly shows 3 edges, but NetworkX only records 2. Which one is missing? Why would there be a difference between the two?  Almost certainly it comes down to you making an assumption about what the network looks like that is not what the computer is recording. Maybe that node in the picture is really two nodes?   Good luck...
comment
Yikes -- you are correct. That code won't work as written.  Well, it works for multigraphs, but not for graphs. :{  If you can put together the pull request that would be great!  Thanks!
comment
Looks Great!  Thanks very much!
comment
I can verify that this is a bug in NetworkX. We should either be ruling out MultiGraphs or we need to adapt the shortest_path routines to work with them. We have it all working in the shortest_path routines, but due to performance issues (from a long time ago -- who knows if they still exist), the betweenness_centrality code uses it's own custom code for djikstra.  And that code doesn't work for MultiGraph or MultiDiGraph.   We should either  1) make the specialized code work for MultiGraphs,  2) remove the specialized code in favor of the more generic code, or  3) force the user to convert MultiGraphs to Graphs before running betweenness_centrality.  The potential downsides of 1) and 2) are performance issues. We should make sure this doesn't slow down the non-multi-case too much.  
comment
What would you prefer for this kind of output. It seems pretty clear that the call to GraphViz returned an error code. This tells you that the error code is 1. What else would you like to know? Keep in mind that we are an interface to pydot and thus GraphViz.  You will need to understand and get information from Graphviz to figure out what is going on. I don't think it makes sense for us to try to keep up with their error code system.
comment
Hmmm...  #3594 dealt with this issue. It has to do with Python3.8 versions of xml which changes the order of the attributes--- used to be alphabetical, now in order they are added to the dict if I understand correctly... Anyway, it is passing our tests with version 3.8 on Travis-CI. So there is probably a version issue going on.  Can you check that you have python3.8, networkx 2.4 and also report what version of xml you have? I'm not sure what else to check the version of...
comment
Thanks for doing that, but why is it passing our Travis-CI tests with version 3.8.  Something doesn't match up...
comment
@DrPiranoid can you take a look at this?
comment
I found k-truss in a book:  [Community Search over Big Graphs By Xin Huang, Laks V.S. Lakshmanan, Jianliang Xu](https://books.google.com/books?id=FdCnDwAAQBAJ&pg=PA24&lpg=PA24&dq=clique+truss&source=bl&ots=s9N2KBRqUw&sig=ACfU3U0qDkNmBE-_5sbrRRyo9i8kM3lCxw&hl=en&sa=X&ved=2ahUKEwizwsj3juPlAhXIMd8KHeYHDWEQ6AEwBHoECAgQAQ#v=onepage&q=clique%20truss&f=false) that also uses Cohen's definition.  So, I think we need to go with the established definition unless there is a reason stronger than preference.
comment
@ogreen Understood.  Could you add to the description in the doc_string some more detail. - make it clear whether the subgraph in an induced subgraph. - explain that some papers (can refer to the reference 1 paper) use a different definition that is a simple change of the original Cohen definition (refer to 2) and that the implemented definition is Cohen's. Thanks!
comment
Thanks!
comment
If a graph is not connected, the dict will not have as many keys as there are nodes in the graph. The dict only returns keys that can be reached by the source node.  The doc description is: ```Compute the shortest path lengths from source to all reachable nodes.```
comment
I'll have more time to look at this later, but just a first comment: In python, function calls are expensive relative to many other languages. So, ```G.predecessors(v)``` should be replaced almost everywhere performance matters by ```G.pred[v]```
comment
Hmmm... Looks like there is more work being done when, starting from vertex, we go forward two steps than if we go forward one and backward one. Is ```better_dag_pred``` adding the same edges many more times?  I can't repeat your results. The DAG I use gives about the same running time for all 3 routines. The NX  and ```better_dag_pred``` are about 10% faster than ```better_dag```.  How are you creating your graphs?
comment
Are we done with this Issue?  An associated PR #3445 has been merged. But I'm not sure if this is a sepaarte issue or not.
comment
I have investigated further. Using your ```random_dag``` graph creator I get the same differences in speed that you report. (The DAGs I was using were very simple dags, so probably not good examples.) The two methods you use are vastly different in times, just as you report.  I then counted how many edges each method added to the graphs. Notice that the final graphs created are the same, so any difference in number of edges added is just the algorithm doing redundant work.  With 80 nodes, p=0.7 I got ``` Generated DAG with 80 nodes and 2201 edges ahead 2 method: added 896 edges. pred_succ method: added 55731 edges. ``` With 800 nodes p=0.7 I got ``` Generated DAG with 800 nodes and 223673 edges ahead 2 method: added 95460 edges. pred_succ method: added 59292027 edges. ```  So, I am going to close this Issue. While I don't have a small example to show that one algorithm is slower than the other, this evidence shows that for the examples we are looking at it creates fewer redundant edges to add if you look forward 2 descendants than if you look forward 1 and backward 1. 
comment
Did you mean for this to be a directed graph?  Looking at the code, I think that is what is giving the troublesome result.  While the code does not explicitly restrict to undirected graphs, it looks like it doesn't handle directed graphs the way you are expecting.
comment
Why do you expect it to work for a directed graph?  What definition of coloring do you want to use? These algorithms don't work with directed graphs. You can of course, convert to undirected using ```G.to_undirected()``` and then it should work fine.
comment
If you are using coloring with a desire that connected nodes not have the same color, then you are using the definition for undirected coloring and it is appropriate to convert the DiGraph to a Graph. If copying the DiGraph is too slow, you can use ```DiGraph.to_undirected(as_view=True)``` which doesn't copy the graph, but results in a read-only view of the DiGraph using a Graph interface (either direction of an edge in the DiGraph implies an edge in the Graph). 
comment
This looks good. Thank You! Things still to do: - check PEP8 code style using pycodestyle (can install using pip or conda) or flake8.      pycodestyle lukes.py tests/test_lukes.py  - add the new module to the sphinx documentation via doc/reference/algorithms/community.rst  - format docstrings so the first line is <80 chars and the second line is blank. This may seem restrictive, but that first line is used wherever the function is linked to as a short description. It seems to enforce compact descriptions too. So, put all the restrictions involved with Lukes algorithm into the paragraph after the blank line. Make the first line short and informative. :) 
comment
Before refactoring the tests again -- I'll ask you to refactor them in a different way: using pytest instead of unittest.  If that's too much to ask, I will do it.  Mostly it involves replacing ```self.assertRaises``` with ```pytest.raises```, pulling the methods out of the class (unless you have some ```setup_method``` you'd like run before each function -- which you don't seem to need), naming the functions that have arguments a name that doesn't start with "test" and then replacing ```if __name__ == "__main__":``` code with test functions with names that *do* start with "test".  Let me know if you have questions or if you'd like me to take that on.
comment
This error is something unrelated to what you are putting in the PR. It showed up in the last day or two. I think it has to do with a change in how matplotlib behaves, but I haven't tracked it down yet.  Don't worry about the failure in the third Travis-CI environment.   I'm ready to merge this.  Anything else you know of before we do that?
comment
I agree that the TypeError down the road is better than a cumbersome check at the beginning. So I would say the "todo" comment can be removed. Thanks!
comment
Thanks for this!  and   Whoops on my part...  Changing G[n] to return ```G._adj[n]``` un-protects the data structure (of course). We'd be back to having trouble with people using G[u][v]={} and other variations.  So, I think we should keep it as G.adj and change the documentation to recommend the faster ways.  Sorry for my mistake...
comment
We can easily speed up G[u] to be as fast as it used to be.  (in G.__getitem__ use G._adj, not G.adj)  This Issue should lead to a PR which does that. That is a much better fix than changing the docs.   Thank you for bringing it up!    ```python     %timeit G[2]     %timeit G.neighbors(2)     %timeit G._adj[2]        # the fastest: direct access to the data      %timeit [n for n in G[2]]     %timeit [n for n in G.neighbors(2)]     %timeit [n for n in G._adj[2]] ```  With the resulting times      630 ns ± 6.83 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)     238 ns ± 3.86 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)     62.7 ns ± 0.626 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)      13.1 µs ± 104 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)     12.6 µs ± 123 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)     11.9 µs ± 65.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)     
comment
G.adj is read-only as is G.adj[u] and G.adj[u][v] but G.adj[u][v]['attr'] is a read-write dict. They all use G._adj to look up the info, but they protect against corrupting the data structure.
comment
Whoops...  We can't rewrite ```G.__getitem__``` to return ```G._adj[n]``` because while that is faster, it no longer protects the data structure.   We'll have to change the docs instead...  :{
comment
Thanks for this -- wow...   its weird...  Do I understand correctly that the edges and edge colors are drawn the same in both versions, but the colorbar is different. And the colorbar is the only thing that is different that you have noticed...???  Does the same thing have with a small graph or say 2-4 edges?
comment
Thanks @kaklise !!  Can you (@dbhart )verify that by adding these two lines before the ```plt.colorbar``` lines does the trick?      edge_collection.set_cmap(link_cmap)     edge_collection.set_clim(link_range[0], link_range[1])
comment
Excellent!   Thanks! Are you up for creating a PR or should I do that?  Not sure how to add a test of the colorbar being correct... :{
comment
Thanks for this!  It's about time we revisit this important algorithm.  Can you run your test codes against a version that changes nextlevel to be a set instead of a dict?  It only requires changing the two lines where nextlevel is assigned to. Lines 3 and 6 below. I obtained speed up by factor of 2 on 100 nodes and factor of 16 on 1000 nodes. The use of dicts here is a throw-back to before sets were a python data structure.  I just wonder what your speed testing code would show using sets vs queues.      seen = {}                  # level (number of hops) when seen in BFS     level = 0                  # the current level     nextlevel = set(firstlevel)     # set of nodes to check at next level     while nextlevel and cutoff >= level:         thislevel = nextlevel  # advance to next level         nextlevel = set([])         # and start a new set (fringe)         for v in thislevel:             if v not in seen:                 seen[v] = level  # set the level of vertex v                 nextlevel.update(adj[v])  # add neighbors of v                 yield (v, level)         level += 1     del seen 
comment
No cacheing with old sets -- but sets (and dicts) are wicked fast!! They are the heart of functions and have been optimized and then optimized.  The multilayer perceptron is a great special structure to test! The layers can make it tricky to be fast too.   One more change to set:  Can you run a version of set with a check for all nodes being seen after each layer?  Replace the while statement with:      n = len(G)     while nextlevel and cutoff >= level and len(seen) < n:  That implements your 2nd idea that we should quit when all the nodes are seen. So the resulting "set" code is doing your ideas: 1) replace the nextlevel data structure, and 2) stop after all nodes seen. But with ```sets``` and ```len(seen)```.  Hope to get one best version by combining good parts of the rest. :}  
comment
This looks good -- thanks for all the timing. Another potential difficulty with the PR is that deque seems to require that nodes be sortable. Is that true?  [ My Mistake! I misread your comment line about the queue being sorted by level and was thinking of a heap instead of a deque.]
comment
That sounds like a good plan.  Any of these options are better than the current code. :)
comment
The deprecation warnings have been in place for v2.0, v2.1, v2.2, v2.3. I'm sorry for any inconvenience. The functions you refer to are easy to update.      G.add_path(path) --> nx.add_path(G, path)  and similar for add_cycle, add_star, and the selfloop functions as well. The reasons to remove them involve shifting our tests from nose to pytest and yes, there probably is a way to get around issues there and keep the functions, but I'd rather spend time on getting new things in place than mucking with having tests handle deprecated functions that have been deprecated for 4 versions.  The comment about v3 was from so long ago that we have changed our expected release schedule since then. Python itself has talked about never increasing the first digit again.    I'm sorry about the confusion and the pain to have to rewrite your code. But I hope in the end this doesn't qualify as pulling the rug out from under you.  
comment
Also, can you provide the command you use to try to write the GraphML format file?
comment
I get different error messages depending on which version of Python I use, but they all amount to the same thing.  The most clear error message is from Python 3.7:      networkx.exception.NetworkXError: GraphML writer does not support <class 'rdflib.term.URIRef'> as data values.  So, the graphml writer doesn't know how to convert objects of this type into a string that can be stored in a graphml file. If you expect this to be read in by another program, you'll probably need to figure out what string representation of the URIRef that other program expects.  Workaround #1:  If you can convert the URIRef to a string when adding the attribute to the graph, then ```write_graphml``` should be able to handle writing the string. But you'd lose the URIRef functionality (at least directly from the edge label).  Workaround #2: Create a list(or dict) of URIRef labels and store the index of that label as the edge label. Then if you have both the graph and the URIRef list, you can access the URIRef functionality.  I'm going to mark this as closed because we've figured out why the error occurs, and its not something we expect from the current library spec.  But you can still continue to post comments to get help and to help others. Thanks!
comment
With those edges in claim_g I don't get any error using ```write_graphml``` 
comment
Glad you found a workaround. Perhaps writing the edgelist converts enough to strings that it can then be written to graphml. In any case, I'm glad it works!
comment
Seems reasonable to change the expected test result to ```cyclic_graph(3)``` instead of the complete graph. Both are right, but cyclic shows what is being tested.
comment
This looks good.  Can you check the PEP8 style guides using pycodestyle? It mostly shows spacing irregularities: <80 chars per line, spaces after commas and around operators.  Thanks for this!
comment
Yup -- ready to go...  Been a busy week.  Thanks for the gentle nudge. :) 
comment
Hi! Thanks for this and sorry for the delay. The PR seems to have differences related to the release 2.3 made about the same time as your PR.  Could you rebase to the current master or maybe just start fresh and ```git push --force``` onto this branch? You could also start a new PR if that is easier. I think you have added one function and a couple of tests, but it is hard to parse with all the other cruft in the github diff. 
comment
Yes!  Anywhere it checks ```self.version == '1.1'```, that should be ```self.VERSION```. I guess that means we need a test that slices work.  Thanks!
comment
The [docs for write_shp](https://networkx.github.io/documentation/stable/reference/readwrite/generated/networkx.readwrite.nx_shp.write_shp.html#networkx.readwrite.nx_shp.write_shp) state that nodes must be of the right type to make this work. My experience is that if you don't need the shapefile, you should not bother to change your nodes to make it work with the shapefile standard.  Instead store it another way.    For example you can pickle it using [write_gpickle](https://networkx.github.io/documentation/stable/reference/readwrite/gpickle.html) 
comment
The normalized rich_club coefficient involves scaling the unnormalized rich_club coefficient by the coefficient of a graph with the same degree distribution as your graph, but with some edges swapped to randomize it. This allows comparison of the normalized value to 1 so greater than 1 means the rich_club effect exists in your network.  In your example, the rich_club coefficient is zero for some value of k, so the scaling doesn't work. You could either turn off the scaling using ```normalized=False```, or change the number of edge swaps ```Q```.   Most likely you will just want to use the unnormalized option.  If anyone knows a correct value to set rich_club to when the normalization factor is zero, we could catch this error and replace with that value.  
comment
The graph structure only says which nodes connect to which others. The "crossing edges" depends on the position of the nodes, which is a property of the picture, not of the nodes themselves.  If you want to stick to evenly spaced circular layouts with complete graphs, there is likely a formula for crossings you could derive using ideas from geometry. But I don't think it will be part of a network package only because it depends on where you place the nodes.
comment
Edges crossing is closely related to G being a "planar graph" (can be drawn on paper without any edges crossing). So a planar graph is sort of the opposite of a graph with edges crossing. We do have some basic planar graph algorithms. It would definitely be harder to keep track of the crossings though.  Lots of topology to mess with our heads. :)  It would definitely be good to have a layout routine that places nodes to avoid crossing edges! Best regards
comment
Can you say a little more about why dominating_set is misleadingly named and why you think it is an approximation algorithm? It seems like the function that returns a dominating set is reasonably named dominating_set.  The module name dominating.py could be renamed dominating_sets.py (notice the plural so it isn't the same name as the function.  In general, I'm not in favor of renaming functions and modules in order to improve the organization of the package. We could easily spend all our time managing the names when we could be implementing algorithms. :}  If you find something confusing, go ahead and report what is confusing and back it up. If you find something organized differently than what you think it ought to be, hold on to that thought until you find something confusing.  
comment
I see. You were expecting ```minimal_dominating_set``` while this just returns ```dominating_set```.  I agree with you that approximate algorithms should be placed in the ```approximation``` subpackage. I'm going to claim that this function is not approximate because it doesn't claim to look for the smallest dominating set. It is an exact algorithm for finding a dominating set.   Thanks for clarifying. This is useful.
comment
We don't have any generators involving Opinion Leader networks. I've never heard of them. If you can supply more information go ahead and leave a comment. It might not be difficult to write a generator to do what you need.
comment
This looks to be more of an issue with OGR than with NetworkX.
comment
There are a number of tree algorithms placed in the files ```networkx/algorithms/tree``` and these algorithms could be placed there. It looks like you would need to implement a new data structure -- at least if you were going to match the linear time dependence stated in the paper.  There is an alternate data structure to represent threshold graphs in ```threshold.py```. I don't see any reason to exclude these.
comment
If you are computing a closer estimation, wouldn't you specify the X array? Then it is not zero. Also, if beta is set to zero then your first solution is the same as the existing result. The second solution will change the result from a Katz centrality to something new.
comment
Well -- we don't get to call it Katz centrality if it is something different whatever your personal preference is. What is Katz Centrality?
comment
Hmmmm.... That doesn't agree with the NetworkX docs, nor the Newman book cited in the doc_string. It looks like the Wikipedia version sets beta=0, though I'm not sure. In that reference, beta plays a role in reducing the problem (in eigenvector centrality) of nodes with zero in-degree contributing nothing to the centrality of the nodes they connect to. In that description, it is clear that beta is not just for convergence. It impacts the final value fairly strongly in some cases.  It would be great to connect the Wikipedia definition to Newman's definition and find out which (or both) are related to Katz's version. :}  Is it true that starting with 0 will lead to "beta" on the next iteration?  In any case, it is good that beta is available as an input so it can be changed. 
comment
If someone finds a good citation for good choices of initial values when computing Katz Centrality, we welcome that as an improvement. I'm going to close this issue, at least for now.
comment
Yes. NetworkX uses  ```sphinx.ext.napoleon``` instead of ```numpydoc```. I'm not sure if there is a way to use either, but it sounds like a maintenance nightmare.  I'm going to close this, but if you know an easy way to support either napoleon or numpydoc add another comment. 
comment
According to [the docs for projected_graph](https://networkx.github.io/documentation/latest/reference/algorithms/generated/networkx.algorithms.bipartite.projection.projected_graph.html#networkx.algorithms.bipartite.projection.projected_graph) you can use the argument ```multigraph=True``` to create a multigraph that has each edge key being the node of the neighbor that connected them.  So something like this could work (untested):      g_over_r_multi = nx.projected_graph(g, r_set, multigraph=True)     g_over_r = nx.DiGraph(g_over_r_multi.edges())      e_labels = {(u,v): list(g_over_r_multi.adj[u][v]) for u, v in g_over_r.edges()}      pos = nx.spring_layout(g_over_r)     nx.draw(g_over_r, pos=pos)     edge_info = nx.draw_networkx_edge_labels(g_over_r, pos=pos, edge_labels=e_labels)      
comment
This issue seems to have been solved.
comment
Try drawing a smaller graph. If that works, then the edges are probably being shown--you just can't see them. Also try zooming in on your image. Maybe the edges are there.
comment
For the edges to not show up on that picture, they would need to be covered up by nodes. Instead of zooming, for example you could compute how many isolated nodes are in the network. Maybe even just draw the non-isolated nodes.      G.remove_nodes_from(list(nx.isolates(G)))    # untested     nx.draw_networkx(G)
comment
I'm assuming that the image did actually show edges that were just not visible. I'll close this issue
comment
Controlling the drawn networks is done using the layout routines as described in [the drawing section](https://networkx.github.io/documentation/latest/reference/drawing.html). There are a couple of interfaces to graphviz available there: pygraphviz and pydot Whichever one you decide to install, hopefully the options available in graphviz_layout will suffice to do what you want.  :)
comment
Yes -- use something like ```pos = pygraphviz_layout(G)``` and then ```nx.draw_networkx(G, pos=pos)```
comment
The ```splines``` argument/attribute in GraphViz determines how the edges are routed as they travel from node to node. The edges do not affect the position of the nodes, and the position of the nodes is all we get back from ```pygraphviz_layout```. Matplotlib is controlling how the edges are routed. So, to get the GraphViz edges you would need to stick to drawing with Graphviz.  Luckily, pygraphviz does allow you to [draw with Graphviz](http://pygraphviz.github.io/documentation/pygraphviz-1.5/tutorial.html#layout-and-drawing). Something like this should work:      PG = G.to_agraph()     PG.draw('filename.png', prog='dot', args='-Gsplines=ortho') . # untested...
comment
I think we've solved this issue
comment
I believe the extra time comes from the ```test``` function imported using ```from networkx import *``` in a few of the test modules.  In a number of files, we use the idiom ```from networkx import *```.  One example is in ```generators/tests/test_classic.py```.  When I run ```pytest -v``` from the command line, it stops for a long time and reports that it is running ```networkx/testing/test```.  I think we need to remove ```from networkx import *``` from the test files.
comment
Perhaps it is not good to have a function called ```test``` in our main namespace... We didn't until recently. It allows people to run the tests using  ```nx.test()```.   Thoughts?
comment
Wow -- you guys are fantastic -- go out for an evening and you've got all the tests working! Way to go!  What's next? Do we need to look for better tests to take advantage of pytest features?  Can we figure out a way to avoid rerunning tests in a superclass for every subclass? Maybe that's just a matter of removing "test" from the name... but not sure.
comment
Great -- I will take a look.  I've been trying replacing ```__init__``` with ```setup_method``` and it seems to work, but I'm not sure on distinction between cls.G and self.G variables. Will read more.  Have a great couple of days!  Best wishes and Thanks For All the Fish! Dan
comment
Looks like ```pytest.raises``` can replace ```nose.assert_raises```  As for ```setup_method``` vs ```setup_class```, if I understand correctly, couldn't we just use setup_method in all cases and not have to figure out when the contents get changed by the test? I guess it costs slightly more time testing. But it would avoid difficult bugs in the tests, no? 
comment
The doc build seems to be failing because it is running nose on the newly pytest tests. Is a nose tester built into the doc build?
comment
I put an example treatment for doctests into convert_matrix.py.  See what you think.  Most of the warnings are about deprecated idioms in our code. Some include our own deprecated function from 2.1 which were "supposed" to be removed in v2.2. I squashed a number of these. I believe all of our own deprecated functions are now gone. There are still some warnings about deprecations from other packages that I haven't gotten to.  Hope you have power soon... :)
comment
Any thoughts on how to handle the numpy.matrix deprecations? I guess we could have lots of warnings and leave the code as is. But maybe its better to rewrite the code using arrays (still leaving the matrix creation functions available, but maybe deprecating some e.g. to_numpy_matrix)... 
comment
I'm offline till Saturday. Good stuff here.. :}
comment
We have had proposals before to allow the shortest path algorithms work with multigraphs. My impression is that it is better to use the graph algorithms and either morph the multigraph to a graph before calling the routine, OR use the "weight" functionality to create a function that looks at all edges between the two nodes and selects the shortest weight. The former requires altering the whole graph once before starting the search.  The latter requires recomputing which multiedge is best each time the two nodes are examined.  I'm not sure of the details of your methodology, but can you give a big picture description of why you need to do this and how your method might improve over these two alternatives?  Also, it would help if there is a reference to a published algorithm for this method of handling multigraphs. 
comment
I was envisioning one copy of the multigraph to a graph with computation of the "best" edge between each pair of connected nodes. That "best" edge weight could be computed and stored in different attributes -- one for each way you'd like to find the best from the multiedges.  So, you don't need to copy the graph many times. You just need to create an edge attribute for each method you'd like to compute the "best" multiedge.    The second method (using a weight function to scan through the multiedges and find the best one "on-the-fly") has redundant computations compared to changing to a Graph.  But it doesn't have redundant calculations compared to handling the multigraph. With a MultiGraph you must examine each multiedge between two nodes. With a weight function approach you can also examine each multiedge between the two nodes. Those methods are comparable in terms of computations. The redundancy is when comparing to converting to a Graph -- where finding the "best" edge is done once (for each way you want it computed) for each pair of nodes.  You identify the real problem directly in your final paragraph.  We are treating edges differently for MultiGraph and for Graph.  The distinction between 2-tuples and 3-tuples makes it hard to construct algorithms that work for either. There is a lot of checking in the code. This is a fairly deep concern with many implications for the code. We've thought about ways to handle edges more generally, but it almost always impacts performance for the simple Graph case.  I am fine with making a version of steiner that works with multigraphs so long as it doesn't impact the performance for highly used functions like Djikstra. I encourage you to find ways to avoid searching every multiedge in a djikstra manner and instead process each node-pair that has an edge. They amount to the same work -- but the latter allows you to use the djikstra built for the simple case.
comment
I'm going to close this PR based on a decision not to encourage using shortest path algorithms on multigraphs, but instead convert them to simple graphs with appropriate edge weights.
comment
In what sense is ```"{}".format(var)``` more proper than ```"" % var```? They are both valid Python structures and each has advantages. Can you cite a deprecation of %?  I don't think the term ```graph alias``` describes what you are showing. The output is not very helpful (partly because we haven't made printing graphs very helpful). Maybe it has the ```id``` of the object?  Anyway, you probably should not spend any more time on this without justifying that it is a useful enterprise. Perhaps you can spend the time implementing a graph algorithm instead?
comment
I've decided to leave the % string formatting operator because many times we only need a binary operation and it is perhaps more readable. Similarly I am going to leave G instead of graph. While I know it is bad coding practice to use short variable names, this is so embedded in our documentation and code and I haven't run into any downsides in terms of misunderstandings.  So I'm going to close this PR.
comment
Ahh. I understand better now. But the PR removes the example for ```enumerate_all_cliques``` as well. And that does increase exponentially in number of cliques as number of nodes grows. So, maybe only remove it from the two ```find_cliques(_recursive)``` functions.  Is there a good example of a type of graph that increases the number of maximal cliques exponentially as the size of the graph grows?
comment
Why?
comment
The milestones list October 15 for version 2.4. I think we are on schedule for that.  It's possible it would be later though... 
comment
Nice -- thanks...     How portable is this? For example, you use ```brew``` to install ```gdal```?  Will that work for everyone?   Does it need to happen when the ```source activate``` is active or can it be done any time?  Can you specify what is generic and what is special for each OS?  Finally, I think the ```Guide``` link is an [internal link and can be marked up](http://docutils.sourceforge.net/docs/user/rst/quickref.html#internal-hyperlink-targets) as such.
comment
@jarrodmillman  Are we ready to merge this Developer Setup guide? Any remaining tasks?
comment
@jarrodmillman could you see if this PR is ready to merge?  It says conflicts exist, but I can't find any... 
comment
Hard to see what's going on without seeing it... :) You've got 4 URLs that aren't hyperlinks and at least the first ones don't work. You haven't shown us any code that you tried. How can we tell what's going on?  The code adds an edge for any line that 1) has two or more numbers separated by spaces, and 2) doesn't have a comment character (# by default) in front of those two numbers. What you describe should work -- and your loading it as an array and feeding it to add_edges_from() makes me think it is working... but that something else is wrong. Your two methods should return the same graph.
comment
I need to see your code for an example file that shows the problem.  When I run it on your "file" or four numbers it works fine. Same for the file with 4 uris. But I can't see what your code is. Maybe you are doing something with read_edgelist() that it isn't expecting.  Does it work for you with just the 4 numbers?
comment
Aha -- you've got ```#``` symbols in your text file which is the default character for the comments argument. ```nx.read_edgelist``` is trying to handle each hashtag as a comment symbol.  Use:      nx.read_edgelist(filename, comments='@')   or any other character that doesn't show up (or more than one character is one is too restrictive).
comment
I don't think your definition of clustering_coefficient aligns with ours.  You are counting edges in the induced subgraph of the inclusive neighborhood. Our definition counts triangles incident on the node of interest. This can be thought of as counting edges in your induced subgraph that don't connect to the node of interest. In other words, counting edges in the induced subgraph WITHOUT including the node of interest.  Your definition inflates the clustering coefficient for high degree nodes because it counts all the edges from the node to its neighbors. 
comment
Reading that pdf of the textbook (pages 29 and 30 are the only places where clustering coefficient is discussed), it is clear that they are discussing the results of the Watts and Strogatz work on small world graphs and clustering. But the definition does not align with that work. I do not think we should replicate this definition -- which appears to be a mistake.
comment
Looks like if you want both, you'd use ```single_source_dijkstra```. Maybe that should be in the docs for dijkstra_path.
comment
Yes -- you are right... I understand the issue.  It'd be nice to read 'INF', 'NaN' and '-INF' as floats (and maybe 'inf', etc too)...
comment
Thanks very much for this report!   It looks like the lines added to the "reader" at line 800 of the file should not be there. The ```decode_attr_elements``` handles them so we don't have to.  Also, the lines in the writer should check the type to ensure it is float before converting.  And we'll add your example as a test.  Thanks again! 
comment
That doesn't sound good.... Do you have a short piece of code that shows  the problem? Thanks!
comment
Good -- thanks for that code. It is clear that near line 325 the code presumes that the dataframe has a column that is not indicating source or target. And if that column doesn't exist it will cause the ```zip()``` function later on to terminate before any edges are added.  This is a bug and should be easy to fix.   Let's just raise an exception when no ```edge_attr``` columns exist?  That might be cleanest...
comment
Should this be renamed as ```is_container_of_ints```?  That's a real question... What should it be called if it accepts more than lists?  Also, the function is used in only a couple of places in the code. In at least one, a return value of False triggers an attempt to convert from numpy.int64 to int. With this change perhaps that code can be simplified. 
comment
Closed via #3617  Thanks for your help!
comment
See #3631 which I think handles these cases. I don't think the refactoring of the "if" is needed. Thanks for the help.
comment
Hmm.. It looks like the old name is used in the code -- not just the documentation.  And I don't think the new name is in the code. Why do you think it is a Typo?
comment
Got it!...  Sorry -- I missed the part of the code where they switch from strings to function names. Thought it was simply a transfer, but it's a lookup where it does change the name.  Thanks!!
comment
I agree with you that there is a bug in how the code rescales the shell layouts. If there is a single node in the shell, it moves that node to the middle of the diagram.  I would like to fix the behavior by changing the ```shell_layout``` function rather than touching ```rescale_layout``` though. There are a lot of functions that use rescale_layout and they might not work as expected if we don't shift one node layouts to the origin. Instead I am hoping that a similar ```if``` statement in the ```shell_layout``` could determine whether rescale_layout gets used at all. If there is one node, then set that coordinate appropriately and otherwise call the rescale function.  Do you see any downside to this approach?
comment
I'm suggesting that we *do* scale when there is only one node in a shell. Just not using the ```rescale_layout``` code. Of course, the outer shells have a different radius than inner shells, so we have to scale each shell.  It should be something like:      if len(node) > 1:         pos = rescale_layout(pos, scale=scale * radius / len(nlist)) + center      else:         pos = [(scale * radius + center[0], center[1])]    # or something similar 
comment
A PR would be really helpful.  Thank you for being flexible.
comment
Yes -- ```scale=scale``` is better for line 805.
comment
You are correct that the new node should not get self-loop edges and by adding an edge to ```u``` before duplicating the neighbor edges, we make it possible to create a self-loop edge ```v-v```.  This doesn't align with the doc_string order either.  That should be fixed... Probably good to check the article to check whether the code aligns with the paper, or the doc_string aligns with the paper (are self-loops created for the new node or not).  I pretty sure the order of selecting ```u``` and then adding ```v``` doesn't impact the results.  But we might as well make it agree with the doc_string description.
comment
Yes -- I agree with you that you can't choose u to be v.  But the code doesn't allow that because it doesn't choose from all nodes, only from ```range(0,newN)``` which excludes the new node.
comment
Ahh -- thank you for persisting -- now I understand where my confusion is coming...  The code actually doesn't use the ```random.randint``` function.  It uses ```seed.randint``` where ```seed``` is created by the decorator ```@py_random_state```. The function ```numpy.Random.randint``` does not include the upper endpoint, while the ```random.randint``` does. I got those two confused...  So... better to switch the order to what the docs say, and also to make that code clearer... Otherwise we could (as the current code allows) choose u to be ```new node``` which is not good.  Thanks again... 
comment
I believe it hasn't been implemented because of ease rather than design.  It would be great to have that addition to NetworkX.  Thanks!
comment
Fixed in #3183 
comment
Thanks for this!!  It's very helpful to use copy/paste of text rather than screen shots in your postings because we can't copy/paste the code from your screen shots.  What happens with the try-except code?  
comment
If it detects the AttributeError and it is inside a try/except clause it will catch the error (unless you spell the except clause incorrectly...:) So, there is something happening that you don't think is happening.   In this case, the iterator isn't actually traversed until after you leave the try/except clause. So the AttributeError occurs after you are into the parse_edgelist function.  You can make the tuple-comprehension into a list-comprehension by using square brackets -- though for large files that might use a lot of memory.   So... let's try another approach:  Use an "if" clause inside the tuple-comprehension. Does this even work?      lines = (line if isinstance(line, str) else line.decode(encoding) for line in path) 
comment
Great!  Now for the tests -- :) The tests are in tests/test_edgelist.py Can you create one that tests reading in a file using mode 'r' instead of 'rb'? The graph can be simple. The key is to make sure the decode doesn't raise an exception.
comment
This is the approach I was intending. Thanks for all of this!
comment
Excellent!  Thanks so Much!!
comment
I think this dependency should be removed.  We only use it to raise an importerror if the version of pydot is too old. We already have a version requirement on pydot in requirements/extras.txt We shouldn't create a dependency to check another dependency
comment
This seems to be fixed already. 
comment
Thanks --  Would it be possible to add a simple test that makes sure we don't mess this up later? Even just computing one very simple case with resolution=2 and checking values would be helpful.
comment
This addresses one major part of #3153. It implements the generalized case for the ```modularity`` function.   Does it make sense to generalize other functions in this module?  What about in the ```modularity_max.py``` module?  If you'd prefer those changes to be in another PR thats fine.
comment
Let's work on getting #3260 merged
comment
I would claim it is a definitional difference rather than an implementation error. The question is whether self-loops should be added to the closure.  The documentation link you refer to is the link to eppstein's webpage, right?  Looking at the Wikipedia page and it's reference to Nuutila (1995): http://www.cs.hut.fi/~enu/thesis.pdf I tend to agree that these loops should be included.  Thanks!
comment
We should make this clear at the very least -- and probably change the current default behavior. But as the link you reference above suggests, some people do not expect self-loops in the closure. Just because the mathematical meaning of a word like closure seems clear doesn't mean the word will be clear. Besides, your definition of "reachable points" may be different from others.    It seems to me there are two types of self-loops that arise and we need to be clear about which is treated which way.  Is a node reachable from itself in zero steps? If so, we need self-loops on every node. I believe this is called "reflexive transitive closure".  If we enforce that paths must have positive length, then we only get self-loops when there exists a cycle that includes that node.  I gather that you are saying that we should include self-loops due to cycles in the graph, but exclude self-loops if no such cycle exists.  If this is what you are suggesting, then I agree.
comment
I agree..  An option ```reflexive=True``` could include all selfloops, ```reflexive=False``` could be the default with self-loops only for nodes on cycles.  And we could add an option: ```reflexive=None``` which does not include any self-loops -- if that doesn't make the code to convoluted.  In any case, it means we'll have to separate this from using ```dfs_preorder_nodes``` because we need to see when cycles are formed.
comment
Thanks!
comment
I would much prefer to change the documentation to match the code than to change the code to match the docs.  This is because it reduces backward compatibility problems, thus perhaps saving many people time when they upgrade to the next version. What do you think?
comment
Thanks for that summary. Let's see if I understand correctly:  Current unexpected behavior is:  - With no fixed nodes:     - anti-gravity is not scaled nor impacted by changing A     - threshold cutoffs are hard coded and thus not impacted by changing A - With any number of fixed nodes:     - scaling is disabled so scale of results differ greatly from no fixed node case.     - the unexpected behaviors for no fixed nodes still apply to this case as well.  In addition it seems to me that the docs aren't clear about much of this. It is not clear that scaling edge weights (A) will affect spring force but not anti-gravity. And it is not clear that the hard coded thresholds even exist nor what their impact is.   The reason we disable rescaling when fixed nodes are present is because rescaling would affect the positions of the fixed nodes.   Questions:  - Is there a better way to handle scaling with fixed nodes? - Is there a better way to handle the trade-off between spring force, anti-gravity and thresholds?  If we were designing the API from scratch what would be best? Would we add keywords for anti-gravity and thresholds? Would we allow turning on scaling with fixed nodes? This is not the first time we've adjusted the scaling part of the API so I want to try to get it right.    BTW, I like the center_spring solution to the unconnected graphs issue. Your Petri Dish example is quite compelling and could be included as a drawing example in our gallery. :)  
comment
We are definitely computing position scale, not plot scale.  We are in danger of mis-implementing the algorithm. This is ```fruchterman_reingold``` after all. So, I think we should NOT choose A -> 1/A^3. Also, the length should not be proportional to entries in A. With fruchterman_reingold the A values give spring constants not distances.  Questions we need to address:  - should we scale the positions if there are any fixed nodes (which would change the position of those fixed nodes)?  I think the answer is no. If people want to fix a node at a position then we won't scale. - Do we need other keyword parameters to allow adjustment of scales? (now we have k, A and scale. These should provide enough flexibility to determine the spring scale, anti-gravity scale and threshold cutoff scale...  maybe?) 
comment
Maybe you should write an article about the Ortiz Layout Algorithm. Then we would definitely include these ideas. :)
comment
It is not an error to report only edges that reach new nodes. Just a different definition than you want to use. My understanding is that this is at least a common definition of a BFS ordering of edges.  Maybe we just need to update the docstring so it says more clearly what it does... :}
comment
I think this is already taken care of in ```edge_bfs```.  We need to make the docstrings of both these functions much more clear and they should point to each other and explain the difference. 
comment
It seems that @nandahkrishna has correctly answered your comments 1), 2) and 3).  Whether you want to believe those answers is another matter.  Closeness is defined as inward distance.  It is in fact the reciprocal of the average (though certainly that could be described in other ways). The definition of "component" takes care of your concerns about ```n```.  The documentation describes accurately what the code does. We didn't just make up the algorithm. Follow the citations in the documentation to see more information about definitions, etc.  Finally, it would be helpful if you approach the issue without just stating that everything is wrong. Investigate further to see what we did and why we might have done that. Perhaps your notion of closeness is not the same as ours.   
comment
Those are very helpful links and also example code @luanamarinho  :)
comment
I've looked at all three links provided which discuss closeness centrality. This list contains the article by Freeman that we reference in the docs. I am not physically close to my copy of Wasserman and Faust to check that reference, but if I recall correctly, that reference is about how to handle graphs that are not connected; still it is possible they discuss directed graphs.  [Freeman](https://www.sciencedirect.com/science/article/pii/0378873378900217) doesn't consider directed graphs.  [Gephi](https://github.com/gephi/gephi/wiki/Closeness-Centrality) refers to Wikipedia for the definition of closeness centrality and that page clearly defines it as inward distances. I tried to find where in the source code they compute closeness and was not successful.  [neo4j](https://neo4j.com/blog/graph-algorithms-neo4j-closeness-centrality/) also does not discuss directed graphs, though the language clearly uses the words "distance from a node to all other nodes".   The [Wikipedia definition of closeness centrality](https://en.wikipedia.org/wiki/Closeness_centrality) clearly uses inward closeness as the definition, referring to 2 papers from 1950 and 1966. I have not tracked down those papers.  I have not been able to find a commonly used definition of closeness centrality that uses outward distance from a node. I have found a Wikipedia article that uses inward distance to a node as well as a neoj4 blog that talks about outward distance from a node.    @luanamarinho, are you able to find more definitive references for which way this is best defined? Otherwise I'm inclined to continue to use the Wikipedia definition that Gephi cites. It would be best to lean on refereed published works, but even code is helpful here... :}  
comment
Let's add more to the doc_string to warn people that closeness centrality uses inward closeness.
comment
I believe the docstrings for betweenness_centrality_subset mentions this. They state that the normalization may appear to be strange, but is designed to make the results of ```nx.betweenness_centrality_subset(G, G.nodes, G.nodes)``` agree with ```nx.betweenness_centrality(G)```.  When all nodes are in both ```sources``` and ```targets``` then we traverse paths in both directions. So normalization should account for this.  What do you think?
comment
The strange value that @joelmiller saw and discussed in the StackOverflow post appears half as big as expected. This is due to the factor 0.5 in ```_rescale```.  The expectation seems that the unnormalized value comes from computing the fraction of all paths from source to target that go through each node.  (There is no problem with the "normalized" version of the code as far as I can see.)  The total number of paths between source and target is counted differently for directed and undirected graphs. Directed paths are easy to count. Undirected paths are tricky: should a path from "u" to "v" count as 1 undirected path or as 2 directed paths?   For ```betweenness_centrality``` we report the number of undirected paths when G is undirected.   Now, when we switch to the ```subset``` case, if the source and target subsets are the same, then we again presumably want to count undirected paths. But if the source and target subsets differ -- for example, if sources is {0} and targets is {1}, then we are only counting the paths in one direction. They are undirected paths but we are counting them in a directed way. To count them as undirected paths, each should count as half a path.  I believe that is the reason for the factor 0.5 in ```_rescale```. If you want to know the number of paths, you need to count each directed path you find as half of an undirected path. Otherwise when you add up the counts over all the subsets you will have a higher count than the number of paths in the graph. 
comment
I'd like to make this a documentation fix.  1) add a long description similar to what I wrote above about undirected paths, counting, etc and the "reason" networkx counts half-paths.  2) add more warnings about normalization in the docstrings. Especially in the ```_subset``` docs we should add an entire paragraph to say that the default for normalization is different for ```_subset``` than for the non-subset function.
comment
I believe the docs say that the input to ```modularity``` must be a list. Is there any benefit to allowing the input to be an iterator? I would prefer to require a list as input over allowing an iterator  that we immediately turn into a list. 
comment
Right -- I can definitely see that something in the documentation could emphasize this more than just the type of the input argument. Is there a simple example you know of that we could put in the doc_string that would highlight this?  For example, calling ```modularity``` with the input being the output of a function that returns a generator -- so you have to enclose it in a list...
comment
Thanks -- I'll try to incorporate it or something similar into docs...
comment
You are correct that the documentation is wrong. The actual formula used in the code is:  (trace(M)-sum(M^2))/(1-sum(M^2))  This documentation error occurs in ```attribute_ac()``` as well as ```attribute_assortativity_coefficient()``` 
comment
Well... unfortunately we called our module ```tests``` instead of something more private like ```tester``` or ```_tests```.  We should probably fix this naming issue while ding #3102 .  Thanks for pointing this out.  To directly answer your question -- this import is relevant to be able to run the tests on the package. Something like it appears in each of the packages that you probably use. But they use a different naming scheme.  For example numpy creates the test function using the module ```_pytesttester```.  If you don't plan to use the ```test``` function you can safely remove that import statement. We will work to fix the naming issue of a module named ```tests.py``` in a coming release -- though probably not v2.2 as that is close to completion.
comment
Perhaps even better, moving ```tests/test.py``` to ```_nosetester.py``` and changing the import of ```run as test``` in ```__init__.py``` to use ```_nosetester``` instead of ```tests.test``` should do the trick.  That separates the testing code from the code that runs the tests too. And the name is private, slightly more descriptive and similar to what numpy uses. 
comment
I am going to classify this as a request for documentation.  @aparamon can you close it when you feel it has been satisfied?  Thanks.
comment
The docs look wonderful to me!
comment
See #3400
comment
I'd love to see this turned into a PR so we could include the improvements. The time is right for getting it into v2.2.  Otherwise we'll bump it to v2.3.
comment
The original posted error is casued by turning an edge into a set. The comment line there says this is to speed up the "in" in case of large objects. But edges have at most 2 nodes and most importantly a self-loop edge becomes a single element set when you turn it into a set. That's why the ```X.add_edges_from``` creates an error.   The solution is to leave the edge as a tuple.  The second issue discussed here is more related to drawing self-loops in networkx.  This is subtle and tricky and in some sense beyond the current state of the drawing package. nxviz may have solved drawing self-loops. I'm not sure.  I'm going to close this issue.
comment
Thank you!
comment
Great!  Thanks for this! The new change Python3.8 to a syntax error when using "is" with strings and small integers is an effective way to make us avoid errors we might fall into.  The phrase ```Exception as e``` is one way to avoid a bare ```except:```, but we don't usually use the ```as e``` part (which allows later access to the content of the exception.) So, changing ```except:``` to ```except Exception:``` is the correct replacement in most places.  BUT: toward the end of the files, a bunch of ```import``` statements are enclosed in try/except clauses. Instead of Exception, can you make those ```except (ImportError, IndexError):``` (I think that is the right syntax).  Then you can remove the comments that indicate those are the exceptions being caught.  Finally, in line 1203 where you wrap the output of an example, I am surprised nosetests lets us do that! I thought that linebreaks messed up the comparison of actual output and expected output. But the tests are clearly passing.  Nice -- I learned something today! :)
comment
That's very nice!  That should allow us to make more of our doc_strings comply with PEP8/pycodestyle. I think we don't have many of those cases left. But I now know how to handle them.  Your changes look good.  And this last change is a petty, minor spacing preference. But, what do you think about inside tests/test_pydot.py on line 82, changing:             e2 = sorted([                 (e.get_source(), e.get_destination())                 for e in Pin.get_edge_list()])  to:             e2 = sorted([(e.get_source(), e.get_destination())                         for e in Pin.get_edge_list()])  with the "for" lining up under the character AFTER the bracket opening the list comprehension. What is most readable?
comment
Right now the ```average_shortest_path_length``` function allows methods dijkstra, bellman-ford or unweighted. It would make sense to add the option of using Floyd-Warshall.   The default method for most shortest path computations is dijkstra.  It should stay that way -- that is what "most people" will want. If you have a dense network then you should change from the default.  Thanks!
comment
Yup -- two PRs are appropriate for these two items.  I think the definition of average shortest path length should depend on a literature search if possible. The application to directed graphs ought to do what is standard in the literature -- or provide both possibilities if both are popular in the literature.  (I'm actually not sure why an average of 0.5 for the graph a->b is bad.... isn't that what you would want?)
comment
Ahh..  you are pointing out that the other path (from b to a) has infinite shortest path length so the average should be (1+\infty)/2 or something like that... that's why it shouldn't be allowed.  Am I understanding?  The wikipedia entry on average shortest path has a directed graph example and they explicitly say to assume d(u,v)=0 if there is no path from u to v.  I have also found papers, e.g. [Stat. Mech of Complex Networks](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.74.47), that compute and compare the shortest path length for directed networks that clearly are not strongly connected -- like the WWW with hyperlinks being directed edges. 
comment
In the case of strongly connected graphs, the current code gives the same result. So restricting it to only work for that type of directed graph means we are removing something without adding anything except possibly protecting a user from misunderstanding what is happening. I would prefer to do this with the docs rather than with code.  The same could be said for disconnected undirected graphs. But there we have an obvious alternative -- take average of each component.   What do we suggest to users who want the average for a weakly connected graph?
comment
I am thinking that we already have an alternative for undirected graphs.  The code gives an error and the docs show you what to do with an explicit example. (Compute the average on each component separately.)   That doesn't work so nicely for directed graphs. Many people will want to recreate what has already been done in previous papers (using the denominator n(n-1)). If we make it raise an error for a weakly (and not strongly) connected network, then what do we put in the docs to suggest for those people to recreate this measure that is apparently popular?  Maybe 2 examples with something like this:      # commonly used definition     path_length = nx.shortest_path_length(G)     s = sum(length for u, pl_dict in path_length for length in pl_dict.values())     print("Average path length: ", s / (n * (n - 1)))  or for a more "correct" denominator use:      # perhaps more sensical definition (dividing by number of actual paths)     path_length = nx.shortest_path_length(G)     (s,num_paths) = (0,0)     for u, pl_dict in path_length:         num_paths += len(pl_dict)         s += sum(pl_dict.values())     print("Average path length: ", s / num_paths)  I'd like to avoid adding more optional arguments if possible.
comment
The [technical report with specs for GML](http://www.fim.uni-passau.de/fileadmin/files/lehrstuhl/brandenburg/projekte/gml/gml-technical-report.pdf) linked to in the docs states that values need to be int, float, string or lists. And ```n42``` is not a string like ```"n42"``` would be.  It also looks like each node/edge should have a ```label``` attribute and the file karate.gml does not.  Perhaps our reader is being too demanding to 1) require quotes around a string and 2) require every node and edge to have a ```label```. But it is following the spec.
comment
Sounds good!   Thanks!  We want a flexible reader, but it should write the official spec. :)
comment
I think the issue is that the reader gets to the token n42 (without quotes) in the gml file and creates this error:      >NetworkXError: expected an int, float, string or '[', found 'n42' at (4, 5)  It expects n42 to be surrounded by quotes so it can be identified as a string. That would follow the official spec.  We want it to handle n42 by checking if it is an int, float, string or '[' and if not, to treat it like a string even though it doesn't have quotes around it.  I'm not sure that creating a default label name would help... I am sure that we don't want to make the resulting node anything other than 'n42'.  That is, we just want to treat the unquoted text as a string and make that the node in the output NetworkX graph.
comment
We usually (for each format) try to write the graph using the standard for that format. These standards are publicly available and I think linked to from the docs. We sometimes (often?) deviate from the standards upon reading a formatted file to allow NetworkX to read files that are written by commonly used software that does not follow the standard.  @rhodrim .  Can you explain what is precluded in NetworkX -- preferably, find the code that precludes it? It seems to me you are confusing a node object with a node label. NetworkX allows any hashable object to be a node. You can also store node attributes (such as a label). The node is not the attribute and vice versa. The relabel_nodes_... function changes the node objects in the graph -- it does not change node attributes.  Perhaps by "labels" you mean the text written on each node when drawing using matplotlib -- see networkx.draw_networkx_labels()  Perhaps you mean the "label" attribute presumably used to create GML files -- see networkx.set_node_attributes() Be sure to get the right version of the docs for you networkx version.
comment
The original posting still applies. If you do what it says, the same error arises.  To close this I guess we need to allow reading these non-standard files.
comment
Yes!  It'd be great to fix this issue. Take a shot at it.  What is on the Wikipedia page doesn't seem to impact this Issue. It is certainly true that we can't read in the GML file available on Mark Newman's webpage. That file may be invalid GML, but we could probably read it anyway if we create our reader to be sufficiently flexible.  We should not change our writer (as far as I understand) because we want it to create valid GML.
comment
This looks good. I have one more request:  Can you check the style guidelines using pycodestyle? (which checks the PEP8 style guidelines) I'm guessing some of the tests will just have to have lines longer than 80 chars, so leave those.  You can either fix the style complaints for parts of the file you didn't touch or just focus on the parts you did touch and leave the rest. We don't want to do a wholesale style guide change, but our goal is to fix each module we touch.    Thanks for this!! 
comment
Both of those... And just for those two files. :)
comment
If you look at the "diffs" tab on the codecov/patch report, the left side colors code lines green if they are tested and red if they aren't. It looks like we don't test attribute handling where the attribute value is a list.  When you added handling of the special floats, that added many lines to the list-handling code that aren't being tested.  That's why the %age is so low for the codecov diff.  Let's add one more attribute to the test_specials routine:      list_value = [(1, 2, 3), (2, 1, 2)]     G.add_node(3, key=list_value)  and a line at the end to test it was processed:      assert_equals(H.nodes[3]['networkx_key'], list_value)  This adds an attribute to node 3 with two features: - the name of the attribute is "key" which gets special treatment currently untested. - the value of the attribute is a list which brings a bunch of code into the covered category.
comment
OK... Thanks for that! It looks like there is a LOT of code in this module that is not tested. I'm just going to merge.  We can open another issue/PR for improving the tests in test_gexf.py  
comment
Yes, that would be very cool! I believe gml.py similarly needs tests to increase the coverage.
comment
This looks good to me.
comment
You should try to separate the pull requests you are working on into different git branches. (You used your master branch for both Pull Requests, so this one has the previous PR commits in this PR.)  Using a separate branch for each set of changes helps you keep the PRs from depending on one another. It also allows you to pull changes from the main repos  master branch into the PR branch when you need to without affecting other PRs.  Can you include the example given in #3573 as a test for whether this PR addresses all concerns? 
comment
Don't worry too much about the v3.8 issue yet. But I see you have been trying to solve it. So... is there an obvious feature of v3.8 that is causing the trouble?
comment
Thanks for pointing out the connection to @jarrodmillman and PR #3594 with the GEXF v3.8 stuff.  Jarrod, Are we OK to merge this in spite of the Travis errors, or do we need more work on the tests to make sure this passes before merging? 
comment
I don't think your correct result is a correct breadth first search -- at least the way we are defining it. In particular, you have a cycle in your list of edges. The edge filter2 -> extractor_dest ends at a node already discovered from node filter1, so it does not belong to a breadth first search and should not appear in the list of successors.
comment
I know your graph doesn't have cycles. I mean that your proposed "correct" result has cycles. In other words. The thing you think is correct is not correct. (If I am correct... :)
comment
You probably don't want "all" paths....(they could retrace their steps in a cycle forever). But maybe ```nx.all_simple_paths()``` would work. [More options here.](https://networkx.github.io/documentation/latest/reference/algorithms/shortest_paths.html)
comment
See #3600 
comment
@ericmjl   This might be of interest to nxviz also.
comment
Thanks very much!  This looks great!
comment
Hi! Thanks for contributing this functionality!  It's going to take me a little bit to work through the code, but I have some comments based on my first read-through.  - the function name and the module name are the same which will probably cause trouble down the line. Can you come up with a module name or function name that are different? For example, is this function part of a general group of functions so that in the future someone could put another function into this module? e.g. random_as_graphs.py  - The docstring for each function should start with a single line description (<80 chars) followed by a blank line and the a paragraph explaining for fully what is going on. Sphinx uses the first line in the docs whenever it links to this function.   - Use pycodestyle to check the PEP8 style guide for Python style. You can install pycodestyle with pip and it runs on the command line. We're not draconian about style, but if there isn't a reason to break the style we might as well stick to it. Usually things it complains about are 80 char lines, spacing or short variable names. 
comment
This looks good!  Thanks!!  I have three items -- the second is probably just pickiness, so you don't have to do it. 1) can you shorten the first line of the docstring for the ```random_internet_as_graph``` function so it is one line? Maybe abbrev IASM and then describe what that acronym is after a blank line. 2) can you move the ```__init__``` method of your class to the top of the methods? (optional)  3) names...   Does it make sense to use ```random_internet_AS_graph``` instead of lowercase ```random_internet_as_graph``` which reads like the word "as" instead of the letters A, S...  Use whichever you think works best for users... Names are arbitrary, but surprisingly important...
comment
Excellent!  Thanks very much for this PR.
comment
This works with ```with open('network.edgelist', 'rb') as fh:``` I think you need 'rb' to make it work.  Is there a way to easily handle either 'r' or 'rb' in python3?
comment
That seems good.  Let's use:      try:         lines = (line.decode(encoding) for line in path)     except AttributeError:         lines = (line for line in path)
comment
My understanding (and timing from long long ago) is that try-except is faster than if-else when it doesn't throw an exception and slower when it does. Also, I don't think this is a matter of checking the Python version. Which processing should be done depends on the input.  Yes! Please create a PR for this. :) 
comment
I don't see any reason not to make the lists into sets. Thanks!  I suspect there is also a speed-up to splitting the large loop into one for MultiGraphs and one for Graphs, rather than a single loop with lots of checking whether G is a multigraph. But I haven't run any tests specifically for this. 
comment
Closed by #3512
comment
It looks like this branch has the Floyd-Warshall optimization changes in it too. Can you create a branch starting from the current networkx master branch and just include your last commit? Thanks!
comment
Looks like there is a bug in minimum_spanning_tree that is showing up when the first node in a graph is connected to the rest by an edge with edge-weight NaN.
comment
Thanks for that!  I found the error... nodes should discard(v) instead of remove(v) and then we need to check if v is in visited or not in nodes. This is only an issue if there is a node with a single edge with weight NaN. 
comment
Idle for too long...
comment
Fixed in #3576 
comment
The order of edges is not preserved [in networkx.](https://networkx.github.io/documentation/stable/reference/introduction.html#networkx-basics) This is similar (and directly related to) dictionary ordering. The order is likely to be consistent from run-to-run especially once Python 3.6 changed dict ordering. But it can change from implementation to implementation and it certainly does not depend on sorting of nodes.
comment
It is related to the order that the dict's store neighbors. So the answer is "no"...  An undirected graph will report the edges in an arbitrary order, no matter how they are added.   What are you trying to do? Most likely there is a way to work around this difficulty. Perhaps       edges = [set(e) for e in G.edges]
comment
Check out #1090 and #1092 for previous attempts at Louvain. Please help us out with this if you can.  
comment
For the PRs referred to above, a larger weight is not more distant. Larger weight means closer -- see the reference (Blondel) paper.  I don't think you need to convert your weights.  But if you ever do, new_weight = 1/old_weight is a standard way to convert.    I'm going to close this issue in favor of #1090 and #1092
comment
Fixed in #2821 
comment
Take a look at [the documentation ](https://networkx.github.io/documentation/stable/reference/introduction.html) especially the introduction and the multigraph/multidigraph references.   For a MultiDiGraph, the edge data includes the key and an attribute dict. This is related to the fact that specifying one (1,2) doesn't tell you for which edge between 1 and 2 you want edge data.  If you know the edge keys you can use ```G.get_edge_data(1, 2, 0)``` and specify the key. Also, rather than get_edge_data you can use G[1][2] and/or G[1][2][0] if you prefer.
comment
To be considered for adoption, you need to describe what the PR is about -- provide justification that this enhances rather than de-hances (my made up word) -- etc.  Also, your variable names are fairly cryptic abbreviations. Can you ease readability by choosing more meaningful names? (that won't affect whether this gets accepted or not -- just a suggestion really)
comment
Great! Thanks for this update...  Github is good for keeping track of who has said what and submitted which changes.  One step is to add comments to the bottom of the pull request even if you already have comments you could edit. That way github sends an email to people "following" the pull request.  Editing a comment doesn't trigger any notifications of the changes.  Looks like you have figured out how to nicely display code in the comments! Thanks for that performance data.  Since we're going on about names and its good practice for github anyway, could you change ```Du```, ```Pu```, etc to ```dist_u``` and ```pred_u```.  Also, in your test, you name one variable ```path``` when it is predecessors. Please change to ```pred```.  Please add back the comments about defaultdict magick near lines 108 and 115. If they are incorrect please let me know. But it is better to have them than not IMO.  If I understand your test and change in procedure, you are removing ```u``` as a key of the ```pred``` dict if it has no predecessors. The function is supposed to return all-pairs results, so I think it needs to have all nodes as keys - even if their result is empty. So I think I'd like you to remove that change (and adapt the corresponding test). I may not be understanding completely though, so push back if you have other reasons for this change.  Thanks!  
comment
Very helpful comments...  Thank you!  I believe the thinking was that converting the outer dict was good for removing the defaultdict nature, and doesn't affect performance. Is it also possible that the resulting dict-of-defaultdict would be useful? (not sure). I haven't tested it, but perhaps converting to dict-of-dict takes too much time.  I did not realize that the two returned dicts differ in how they handle nodes without predecessors. I can argue that pred should not have nodes if there are no preds, and that dist should have all nodes. But I don't know whether that is true in practice.   I would prefer not to add lots of post-processing. And I'd prefer to leave dist with all-pairs information. Maybe the best solution to make them consistent is to add empty entries to pred for each node. But again, I'm not sure that consistency between t he two dicts is necessary. Maybe they should be different. Can you think of a rationale for making them the same or for making them different? The rationale should be based on how they would be used by the user.
comment
Maybe you should try leaving ```pred_w``` and ```pred_u``` out. They only save time when the ```if``` is true, so it might not save much time compared to the time needed to remove empty dicts later. Check the speed difference, but it is an idea for how to handle the empty dicts. 
comment
Can you remove the .kate-swp file from the PR? Also, please put back the comment lines about defaultdicts at about line 108 of dense.py Thanks
comment
Looks like the same change in ```newman_watts_strogatz_graph``` should also be made.
comment
Looks good!  Thanks!
comment
I guess that k>n would usually be a mistake that should raise an exception so the user can correct the mistake. If we make the code return a complete graph for this case, it could hide such a mistake. You can remove it for your case by handling the exception and assigning a complete graph. I know that is more work for the user, but perhaps better than hiding a mistake for someone else. 
comment
Apparently warnings are often turned off in the default environment. I don't know how true that is and have not checked the details, but we don't use warnings very often because they don't seem effective in the sense you are speaking of. (They are more effective for warning developers.)   I don't know why the case k == n raises an exception -- perhaps because it is not really a Watts-Strogatz process in that case?
comment
But when ```k==n``` it isn't a random graph... so it isn't a Watts Strogatz graph. Can you use?      nx.watts_strogatz_graph(n, k) if k < n else nx.complete_graph(n)
comment
Sounds good -- let's check for ```k==n``` explicitly in the code and return ```complete_graph``` in that case to avoid swapping edges.
comment
Your screen shot of the code makes it impossible to copy and paste the code to test it. Perhaps you can just copy and paste the code into the Issue rather than make a screen shot image.  Almost certainly the order of the nodes after relabeling is:  ```list(G) == [1, 2, 3, 4, 6, 5]``` which would make the rows and columns of the adjacency matrix show up as your output. If you want to make the adjacency order the rows and columns in a specific way, use the ```nx.adjacency_matrix``` optional argument ```nodelist=[1,2,3,4,5,6]```.
comment
I don't see any methods in the distance_measures package that uses the @not_implemented_for decorator.  Can you give a short example code that fails?
comment
I think you must be using a version of networkx that doesn't have the ```resistance_distance``` function. It was included in v2.4rc1 in April 2019. That is available from github.  v2.4 is planned for release in October 2019.  If you already installed your github version of networkx locally, it is quite likely that your Python interpreter isn't finding that version when you import. You can try: ```print(nx.__version__)``` to make sure you have v2.4rc1
comment
Can you add some tests to make sure this feature doesn't get lost in the future? They can be quite small examples...
comment
Thanks!
comment
Fixed in #2943
comment
Looks like you need to add ```spiral_layout``` to the ```__all__``` variable at the top so the ```__init__``` file can do its import magic and get the function imported to the bigger networkx package.
comment
fixed in #3307 
comment
Add in #2943
comment
Implemented in  #2943
comment
Implemented in #2830
comment
Node attributes are stored in an attribute dict (e.g. ```G.nodes```) that is intended to be keyed by the attribute name to an arbitrary python object as the value. So something like this should work:      G.nodes[node]['json_info'] = json_dict_like_object     # or... if the node has not been added     G.add_node(node, json_info=json_dict)
comment
Interesting.... what do you mean by the top level. Can you give an example?
comment
I still don't get it... do you want to add another layer to the current dict-of-dict structure that we use for node attributes? So to access attributes use: ```G.nodes[0]['payload']['hi']``` That is completely available to do right now using code similar to what you just wrote.
comment
OK...  I think I finally understand. :) We have a mechanism for replacing the dict-like structure used for most of the Graph class data storage. In the beginning of the code for the graph class a number of dict-like factory variables are assigned values. One of them is called ```node_attr_dict_factory```. That is the one you would use to replace the node attribute dict with a node-attribute json_dict.  To make a new class with the change use something like (untested):      lazyjson_factory = lambda : lazyjson({})     class MyGraph(nx.Graph):         node_attr_dict_factory = lazyjson  The ```lazyjson_factory``` function should take no arguments and return a dict-like structure. I used lambda here, but you could probably just use ```lazyjson_factory = lazyjson``` if that doesn't require an input.  If you prefer not to create a new class, but instead change one graph at a time use code like this:      G = nx.Graph()     G.node_attr_dict_factory = lazyjson  Hopefully we have included enough functionality that this works smoothly for you.
comment
Yes -- #3556 and #3458 address this issue. Thanks!
comment
Thanks!
comment
Thanks very much for this!  Would this be a good time to look for other places this should appear? Or should be do that in a separate PR?
comment
Could you put it in other places as part of this PR by commit'ing to your doc/update branch? Then it will be conveniently stored and updated here and we can do all of the additions as a single change.  Thanks! 
comment
Those look good!  :)
comment
I'm hoping #3458 is taking care of all these iterable warnings. Are you seeing the warning with NetworkX 2.3 or with the github repository (dev version)? Thanks!
comment
Yes -- good.  Then #3458 should take care of the problem. Thanks!
comment
Thanks for this! Can you refactor your code to add the functions to the existing ```joint_degree_seq.py``` file, and to put the tests in the ```tests/test_joint_degree_seq.py``` file?  I haven't looked deeply at the code yet. In general, I like to make a new function as you have done, but is the code sufficiently similar to the undirected version to make them a single function with keyword argument ```directed```? (probably not). 
comment
That sounds reasonable and the refactoring looks good too. Just a couple more things to work through.  - For Sphinx to get the formatting of the docs right, we need to start with a single-line <80 char description of the function. Follow that with a blank line and then a longer paragraph (or two) that describe in more detail. Sometimes it is a pain to find a short description of the function, but it usually works out better anyway, so sphinx enforces that format.  The short description gets listed along with the function name anywhere Sphinx creates a link for the function.  - PEP8 is a python code style set of guidelines. You can check a module or directory of modules using ```pycodestyle joint_degree_seq.py``` and you can install pycodestyle using ```pip install pycodestyle```.  The usual trouble-spots are 80 character line limits and spacing between operators. Check the test file as well as the module itself.  - We need a pointer in the function in the docs reference. So, find the ```doc/reference/generators.rst``` file and in it the section for joint_degree_sequences and add the two new function names.  Let me know if you have questions.
comment
Thanks! You can make Sphinx work locally, but it involves more work than a simple pip install. I usually don't examine the docs locally unless it is a doc-related PR. The first Travis-CI test runs sphinx, so you can check for errors/warnings there. The resulting docs don't actually get deployed unless the branch is the master branch. So we do a little check of that before releases. Not ideal, but it's what we've got. :}  
comment
This looks like a bug. Thanks for the report!  Line 392 of ```nx_pylab.py``` should be changed from      if not nodelist or len(nodelist) == 0:      # to something similar to this (untested)     if len(nodelist) == 0 or not nodelist:   Your workaround works in the meantime.
comment
Now that I look at the code a little closer, there is a cleaner fix: Can you leave off the ```or not nodelist```, at this point in the code the variable nodelist should be a list and the only way it can pass the ```not nodelist``` test is if the length is zero.  So let's just check for zero.  Also, ```return None``` on the very next line should just be ```return``` because that defaults to returning None.  Thanks!  (you can commit the new changes locally to the same branch and then push to your ```origin``` and it should update this PR automagically.)
comment
Thanks!
comment
Hmmm...   With that code removed, ```python2 setup.py install``` installs networkx just fine on Python2.7 when it shouldn't. I'm not sure what you mean when you say that ```python_requires``` should take care of it.  Perhaps you should create a PR that works so we can see what you mean.
comment
I think running ```python2 setup.py install``` does not involve pip. (I have pip version 18.1 and setuptools version 39.2)
comment
Be careful...  It's not a sure thing that the nodes will be in the same order when you relabel as integers. So, you created a mapping from node integers to positions, but it may not be correct. To do this more carefully, try something like:      G = nx.convert_node_labels_to_integers(G, first_label=0, label_attribute='pos')     pos = nx.get_node_attributes(G, 'pos')     node_df = pd.DataFrame(pos.items(), columns=['node_id', 'geometry'])     
comment
You've got way more code in there than I can parse easily.  Can you compute the length of the path using the weights you specified? Can you compare that length to another path you think is shorter?
comment
Can you compute the length of the path using the weights you specified? Can you compare that length to another path you think is shorter?  The number of edges in the path should not matter so long as you have successfully chosen an attribute for the edge weights.
comment
It's hard for me to tell exactly, but I think you are computing the distance of these paths using distances as calculated from LineString?  Can you compute the length as NetworkX sees it.  That is, see if NetworkX agrees that the length of the route you specify is shorter. Most likely, the values you think are being used are not the ones actually being used. We need to check both ways of storing the network to make sure they agree.       # find length of path "pathnodes"     distance = 0     for n,nbr in nx.utils.pairwise(pathnodes):         distance += G[n][nbr]['distance']   Actually -- I just realized that you don't define 'distance' as an attribute on the NetworkX graph. Am I correct?  If so, when you use it for "shortest_path" it will not find the attribute and substitute the value 1 for each edge.  Essentially that means you aren't doing a weighted path.   Can you check that you actually define the length of each edge in the attribute that you use for the shortest_path calculation?   
comment
Hmmmmm....  interesting. It sounds like "distance" may be a different value than LineString's "length".  I'm glad to hear that NetworkX is finding the shortest route based on how it understands distance. But it means you will have to do some digging to figure out which measure of path length is better for your application.
comment
You can get/set update all graph attributes easily using the dictionary ```G.graph```      G.graph['rankdir']='BT'  But a look at the to_pydot code makes it clear that for pydot we use ```G.graph['graph']``` as a dictionary of default values for graph attributes. So I think you need to use      G.graph['graph'] = dict(rankdir="BT")  Notice that a few lines down in th code from your link it sets default values for Nodes and Edges using ```G.graph['node']``` and ```G.graph['edge']```
comment
Looks like a good idea. I think the < operator won't work if ```value is None```. Maybe we need a test to check that this works and a fix if it doesn't.
comment
Ahh...  good... thank you!!
comment
I think I understand your suggestion for improvement. I don't understand your code though. ```cont = cont or False``` is the same as ```cont = cont```. Similarly ```cont = cont or True```  is the same as ```cont = True```.  Can you check your logic?  
comment
Good --  could you also add your example as a test (that simply tests that it converges). The tests are in ```tests/test_label_propagation.py``` Thanks!
comment
Yes, that is what I am intending.   I think testing a loop that is supposed to stop is OK even if previous versions of the code didn't stop. If the new test gets applied to the old code the test will run forever and Travis-CI will time out. I don't see any other way to test that an infinite loop occurred when it shouldn't have. Thanks!  
comment
I'm optimistically putting this under release 2.3. Please don't be disappointed if it ends up being in 2.4. But I would like it to be merged and its just a matter of time and how fast the release happens etc.  Thanks!
comment
doctest errors:   Looks like you'll need to add ```import networkx as nx``` perhaps after ```import itertools```. That allows the doctest to pick up ```nx```. There maybe a fancier way, but that works.   Then you need to add ```nx.``` before ```isomorphism``` so it can find that name, and ```ISMAGS``` before ```_get_permutations_by_length``` so it can find that name.  The formatting of output is a little more tricky. You are supposed to have the test output formatted exactly as the output of the doctest.  You've got your results nicely with line-breaks, but the doctest output is all on one line.  And, if you put it all on one line in your expected answer, then you aren't following PEP8 guidelines for readability.  Also, the order of the list and the order of the elements in the dict that is inside the list may differ from run to run and so are hard to pass the doctest system.  One option is to have the test create a variable to hold the answer -- then check that the answer is what its supposed to be without outputting the answer.  If order is giving trouble, replace ```list``` with ```set```.  For example:      >>> largest_subgraph = set(ismags2.largest_common_subgraph())     >>> answer = {{1: 0, 0: 1, 2: 2}, {1: 0, 0: 1, 3: 2}, {2: 0, 0: 1, 1: 2},     ... {2: 0, 0: 1, 3: 2},{3: 0, 0: 1, 1: 2}, {3: 0, 0: 1, 2: 2}}     >>> largest_subgraph == answer     True  Your mileage may vary. :}
comment
Yes, I'm interested in this addition to the library! I'm familiar with the problem of finding general symmetries of a graph -- that can be prohibitively long. If I understand correctly, your PR would use the symmetries to examine subgraphs, reporting one from each symmetry equivalence class.    Thanks very much!
comment
It would be great to see an early stage PR. Tests are restricted to unit test style tests using ```nose```. That also tests any examples included in the docstrings. Each directory has a ```tests/``` directory with test modules. The tests are not exhaustive but rather make sure that basic functionality works and that we don't break something inadvertently. The tests are run after each commit to any PR so the idea is to make short routines that test one idea.  It looks like Hypothesis is much more involved than what we use -- but I didn't look at it closely.
comment
Not exactly sure -- but it looks like your edge weights are strings. They need to be numbers to be able to add them together to get lengths. Start by just reading in your dot file and looking at the resulting edge attributes. Something like ```T.edges(data=True)```.  If the weights are strings then convert them to numbers before calling ```longest_path```
comment
Hi -- A couple of comments, mostly about documentation, but the main question is where this stands relative to the existing algorithm.  Is this algorithm guaranteed to give the same results as closeness_centrality? Is it guaranteed to be faster than closeness_centrality? Are there cases when you would want to use one as opposed to the other?  Can you also briefly discuss whether this should be a new module or be included in closeness.py?  docs:  - The docstring for your new function should start with a single line description. Then there should be a blank line and then a paragraph or two describing what the function does in more detail. The single line is used by Sphinx when creating links to the function in the docs. You should also stick to <80 chars in a line so try to make the description short.  Then you can explain more later.  - you can add much of your first sentence to the start of the second paragraph to make it become the new "first" paragraph that describes what is going on.  - can you add some description of what incremental refers to and when you want/need to use it?  - The top of the module should follow the format described in the github networkx wiki... let me know if you have questions.  - I'm not totally convinced this should be a separate module. It might make sense to add it to closeness.py.  Either way we need to add something to /doc/reference/algorithms/centrality.rst.  Either a new line naming the function, or a new section describing the module.  
comment
After thinking about it more, and based on the idea that the output is the same as for closeness_centrality, could you put it in the module ```closeness.py```? And the tests in the corresponding test module. You will need to add a line to the centrality.rst doc file with the new function's name. I think this will be more clear and better for the package. Thanks!
comment
Yes, I think it should be a separate function so each is clear. We should add each function to the other's "See Also" section in the docstring.
comment
These changes look good. Anything else to do for this PR?
comment
It looks like the documentation system had an error processing the lines that include      Let G' = (V, E ∪ uv). T  I believe the trouble is the unicode symbol for "union" which probably needs to be replaced by a LaTeX command for union (maybe \cup) and the expression should be put into math mode with something like   :math:`G' = (V, E \cup uv)` the docstring for closeness_centrality uses math mode but in my quick look at that I don't see any "inline" math; only full paragraphs of math equations. The inline syntax is :math:`<stuff>`
comment
Now it looks like the minus sign is a unicode character too. (see bottom of error log for first config in travis-ci.) 
comment
OK... I'm ready to merge this PR.  Anything else before I do that?
comment
Thanks!   I see what you mean about ```generate_unique_node```.  Maybe there is a better naming convention... Like finding the first 2 integers that aren't already nodes? Can you explain the deletion of tests/test_line.py? 
comment
I definitely see the problem. If your suggestion about  0 → (0, 0), (0, 1) works let's use it. If it's a pain to implement or there are other issues, then we can keep the uuid method...
comment
I can verify that edge attribute types with the same name but different type cause ```write_graphml``` to create two differently named edge attributes -- one to hold edge attributes of each type. Furthermore, the second attribute does not get an entry in the graphml file describing it in a <key> element. That may be why ```read_graphml``` is giving the ```no key d10``` error.  Anyone have suggestions on how to make the writer add the edge attribute to the list of <key> elements? The edge attribute has the same name, but different type.
comment
@iamjli you correctly mention that networkx should handle the missing values (or any other case where the same attribute name includes values of different type).  Do you have any suggestions for what the "right" way to handle missing values is?
comment
Should we make ```write_graphml``` raise an error if the type of an edge attribute is not consistent? That would force the user to go back and change the attributes. Kind of a pain, but no surprises that way.  Is that the "right" way to handle missing values?
comment
Questions about ```descendants_at_distance```:  - It looks like you don't need a deque. I mean that the order of of the nodes is not important so appendleft could just be append with a list or even add with a set.  Is there any other reason to use deque that I am missing? If not, then we should use list or set. - As noted in and near your TODO, the could be changes to use ```G[vertex]``` instead of ```G.successors(vertex)``` and it would work for directed or undirected graphs. Then it should be put into breadth_first_search.py since it is quite general and perhaps helpful to other routines.  ```transitive_closure``` does feel like a misfit in dag.py.  Can you recommend another place for it?  In ```transitive_closure_dag``` you use a list topo_order so you can index it. But it is perhaps better to use ```reversed(topo_order)``` instead of the indexing. That allows you to not require a list, only an iterable.   The first line of the doc_string should be split from the rest of that first paragraph.  Sphinx uses the first line, followed by a blank line, as a short summary when it creates links to the function in the reference.  So:  one-line, blank line, paragraphs separated by blank lines.  This looks quite good. Thank you.
comment
Hmmm.. not sure the reversed vs indexing suggestion helped then... and I learned something. :)  Let's leave transitive closure where it is for now. If we get anything else related to it I'd want to refactor into a separate module.   Can you do one more check using pycodestyle to check for PEP8 compliance. Is there anything else for this PR?
comment
The problem is your line:      l = [set(x) for x in G.edges()]   This is no longer a list of edges. If an edge is a self-loop it looks like (35, 35) but after set(x) it looks like {35} and in the next line you form a tuple with it so it look like (35,) The G.edges() is going to iterate to 2-tuples so you don't have to worry about long tuples "x". Form the edges something like this (untested):      node_at_pos = {v: k for k,v in enumerate(G.nodes())}     edges = [(node_at_pos[u], node_at_pos[v]) for u,v in G.edges()]       # or use ( ) instead of [ ] for speed     # edges = ((node_at_pos[u], node_at_pos[v]) for u,v in G.edges())       ...     X.add_edges_from(edges)
comment
If the test has random numbers generated it may fail on some architectures and not others just because of the random numbers chosen. To recreate the problem perhaps run it locally with a seed set for the random number generator. Find a seed that creates the error and stick with that seed for the duration of your debugging.    tricky....
comment
Good work!  Thanks!  One question about your last comment... how can multiple optimal solutions have different lengths? Wouldn't the shortest one be optimal?
comment
Both of those paths have the same length. So they are both optimal. In what sense do they have different lengths?
comment
OK... sounds good.  In cases like that we should allow either path as a correct solution.  Right?
comment
Could you change the test to check that the answer is one of the two optimal paths? The paths are short enough that you could define each and then check that the answer is one of the two.  Something like:      path1 = [ ... ]     path2 = [ ... ]     assert_in(nx.astar_path(G, source, target), (path1, path2))  Then commit and push to the same branch and it will update the PR directly. Thanks for asking... sorry for not being more clear.
comment
This PR looks like it has both moral_graph code and chordal_graph code.  The moral_graph code was already merged (in a different file) in another PR. Can you remove that code from this PR?   Also, can you check pep8 (mostly line length) for this PR and fix the indentation in the doc_string for the alpa parameter.     Thanks!
comment
Thanks for this! Which test showed you that the current function has a bug? I think the tests for line.py are in tests/test_line.py Can you move your new tests in there? 
comment
No new pull request needed. If you commit to your local and then push to your github copy of networkx it will update the PR on the networkx Github automatically.  Thanks!
comment
Thanks for the gentle bump. I was hoping to get time to check through that code and understand where/why the bug arose in the first place. But I don't think that is going to happen. So...  Can you change the line that you already changed from ```v not in G_partition.neighbors(u)``` to ```v not in G_partition[u]```?  Lookups on a dict are much better than lookups on an iterator that is created from the dict anyway... :} .  That's one reason I wanted to check through that code. It seems non-optimal and this might have other bugs...  Thanks very much! 
comment
Perhaps the multiple component version of inverse_line_graph should be included as a Notes section in the docstring telling people (in very general but clear terms) how to do it themselves.  I'm not sure how it would be done exactly... call inverse_line_graph on each component separately?
comment
Ack!  I hope I didn't mess this up completely. Did you have more to change/address with this commit? (maybe the K_1/K_2 case?) 
comment
Great!   Thanks!
comment
Thanks for this! It looks pretty good and I don't even see any PEP8 (pycodestyle) things to change though you should check if you haven't.  I think the Notes section should not start with a blank line.   The biggest addition I suggest is to include some of the text from your PR description in the docstring -- perhaps under the Notes section, but maybe in the description just before the parameters section. In particular I find the description of "full" vs "perfect" and the citation to be really helpful.  I'm not sure what the best way to handle Scipy 1.1 vs 1.4 is. @jarrodmillman do you have any thoughts?
comment
In an ideal world we would have a scipy and base python implementation of each algorithm, but it is certainly fine to just implement a scipy version. The vast majority of our users have scipy available. Those that don't would not be able to use the function. We could leave it at that, or implement a python version. But since you suggest the Scipy version would be very straightforward I think it is reasonable to start with that and add a pure python version if you think it would be helpful.
comment
Yes, I think this would be useful. You should add the function, some simple unit-tests and/or smoke tests and a link in the doc/reference files. You might want to look at another module that uses scipy to see how we arrange the testing so the tests get skipped if scipy is not present. Thanks!
comment
```networkx/algorithms/bipartitie/spectral.py``` has the code that handles imports well for testing. 
comment
Here's a similar very simple example that shows the same bug:      Gedges = [(1,0,11), (1,2,1), (2,0,6)]     G = nx.DiGraph()     G.add_weighted_edges(Gedges)     def heur(u,v):         distance = {0:0, 1:36, 2:16}         return distance[u]     nx.astart_path(G,1,0,heur)    # returns  [1, 0]  instead of shorter [1,2,0]  Definitely a bug here. I think when the code finds the target it never checks whether the heuristic estimate of distance is correct... It just stops.  Why don't any tests find this bug? Am I missing something?  We need a PR with a test and some fixed code.
comment
pydot is an optional dependency; like numpy, you can run networkx without it so long as you don't use any functions that require it. If you try to run it an exception is thrown telling you that you need to install pydot.  Perhaps [this page](https://github.com/networkx/networkx/tree/master/requirements) can help.
comment
It is fairly easy to implement a linear algorithm and end up with a quadratic algorithm if you aren't careful with data structures. Let's fix this so it is linear (or n*log(n) if that is a preferred algorithm)
comment
Let's go with the simpler change PR which is #3535  Thanks very much everyone!
comment
Can you rebase to remove the Floyd-wasserman optimization stuff? that's a different PR.
comment
When you pull from master it gets new commits added to the upstream master. But it doesn't get rid of any commits you have made locally. That means if your local branch has commits from previous pull requests they will still be there.  To start a new branch for a PR, use:      git checkout master   # switches you to the local master branch      git pull upstream master   # update to latest upstream commits     git checkout -b new-pull-request   # create (and switch to) a new branch based on local master  
comment
The commands are something like what follows. If you do this on a local clone of your github repository and it gets messed up you can just delete it, re-clone and change to the right branch and try again. It will only affect your github repository if you push the changes. If you've got other stuff un-pushed in your local repository you can make a new one cloned from github and start "fresh".  Navigate to your ```prufer-with-linear-complexity``` branch. I'm assuming you have a branch named ```master``` with the networkx commits pulled from upstream.      git rebase -i HEAD~10     # start an interactive(-i) rebase from 10 commits before current commit.  I put the number 10 in this example, because I counted 10 commits on the pull request page. The idea is to start the rebase from when this branch deviated from the master branch. There may be other (better) ways of doing this that I don't know about, such as replacing "HEAD~10" with "master". But I'm not a git expert. This is just the way I know how to do it.  The result is an editor with a page like a commit message only at the top it lists all 10 commits between the start of the rebase and the current commit. Instructions appear below that. Basically you want to "drop" the commits that you don't want in the pull request; and you want to "pick" the commits you want to keep.  You've got a couple of commits which merge master into this branch. I'm not sure how to handle those, but if they were easy merges to make, it's probably best to drop them too. So long as they don't make changes to the code you change in your latest commit the pull request will be OK (I think)  After you save quit from that editor, git will attempt to complete the changes you indicated and then if it detects a confusing action, stops at that point and has you correct and then ```git rebase --continue``` (or something similar to that -- it tells you the command you need to use to continue). If it doesn't find any conflicts it opens an editor asking you to create/edit a commit message and then commit the changes. That completes the rebase and hopefully the difference between your branch and the master is only the commits you "pick"ed.
comment
See #3535
comment
You are telling us what you think you are doing and the resulting missing edges should not happen in that case.  So your theory is just fine. It's the implementation that is causing you trouble (probably).  Can you show us what you are actually doing instead of what you think you are doing? Don't include the whole edge list. Maybe a short version of the code with only 3 edges in the edge list, but all other case the same. 
comment
Hi...  The current implementation doesn't involve edges weights at all. Only node attributes. Does the edge weighted version you show come from a reference somewhere?  Finally, can you say a few words about your workaround? It looks like you set up a new graph to mimic the original edge-weighted graph using an unweighted graph. But what's going on there?
comment
It looks like your timing doesn't include translating the NetworkX graph to a numpy matrix.  If so, that's perhaps not the right comparison. I guess if you start with the data in a numpy matrix, it is useful, but then I would expect it to be faster.  Currently the function (and all others in that module) ignore edge weights. I am fine with you adding the functionality of computing assortativity by interpreting the edge weights as numbers of parallel edges. You will need to document it carefully so people understand that is what is being assumed. And you'll need to make the interface simple; with defaults that align with what currently happens.  Before you start that though, can you make the following timing comparison. -  start with a graph G with weighted edges. time 2 things:      1) nx.numeric_assortativity_coefficient     2) converting the graph to a numpy matrix, then computing the coefficient. Make sure they give the same result too.  Thanks!
comment
Thanks for this -- it looks like computing with numpy is quite efficient.  I'm still having trouble with how node assortativity is defined when accounting for edge weights. The Newman article doesn't actually discuss edge weights except to say that one can account for them. Other searching shows that some people have used edge weights to replace degree with "degree strength" -- meaning the sum of the edge weights replaces the number of edges when computing degree.  I think this is what you are proposing.  Can you find a reference where they use/define this type of assortativity?  I'm also wondering if this is something universal to the assortativity collection of routines we offer. Are other users probably going to want edge weighted versions of the many assortativity routines we offer? Perhaps we should be looking at changing this on a deeper level? 
comment
It is accessible from ```nx.LFR_benchmark_graph``` The code is in networkx/generators/community.py Do you have the right version of NetworkX installed?
comment
I looked at the history of this function ( #3411 ) and it used to be in  networkx/algorithms/community/community_generators.py Try importing it as: import networkx.community.LFR_benchmark_graph or perhaps from networkx.community import LFR_benchmark_graph  As of #3411 (this past Spring) it is housed in  networkx/generators/community.py and it is accessible as nx.LFR_benchmark_graph 
comment
It looks like you have two connected components. [{1, 2, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24},  {3, 4, 5, 8, 11, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37}]
comment
A quick look at the error log suggests that ```s``` has length zero, probs because G[u] and G[v] have length zero. Do you have isolated nodes? maybe they have no neighbors? I haven't looked at the code to see what happens when there are isolated nodes. I've only looked at your error message.
comment
Thanks @Pascal-Ortiz for the minimal example. That helps a lot with debugging. And this does look like a "Bug". As @Dragotic points out it should be able to handle this case. That said, the fact that this is not yet a stable release doesn't have much to do with whether there are bugs.  Bugs exist in all software. The correct approach is not to trust any software. Test it. We do as best as we can to eliminate bugs, but they show up anyway.  It seems like this one should be fairly straightforward to track down and fix. We need a PR for it. Thanks for reporting the bug! We might be able to find a work-around before a fix too... Any help is appreciated. 
comment
Excellent!  Thanks for finding this and that is a good fix. :)
comment
Looks like you built this pull request on top of another pull request. You should create a new branch for each pull request. In this case it doesn't matter much because most of those commits are already in the networkx repository. So, don't worry about it this time. In the future see some of the comments at https://github.com/networkx/networkx/pull/3523#issuecomment-514621366  This looks good.  Anything else before committing? 
comment
The hard part here is not providing the code for shortest-paths on a DAG but how to include it in NetworkX in a way that can be found easily and doesn't confuse people. For the DAG code see [this comment](https://github.com/networkx/networkx/pull/1118#issuecomment-41171119) on #1118. Maybe the algorithm you found is even better, I didn't look.   So, does it go in dag.py? How will users find it? Will it check for DAG as part of the function? Will it allow the user to supply the topological sort on nodes? There are many docstrings to change to advertise it including functions in weighted.py and unweighted.py. 
comment
Putting it in weighted.py sounds good to me.  Unless there is a quick way to check for a dag, I think we should not have ```shortest_path``` check for a dag before deciding which routine to call. But maybe we can put something in the doc_string that tells users that for DAGs we have a special routine available.   Sounds good! 
comment
You'll need to change the .rst files in the doc/reference/algorithms part of the repository to reflect the new function so sphinx will know to find the function and include its docstring.  Yes, I was referring to the ```shortest_path``` function in ```generic.py``` but there may be other places where it is appropriate (maybe the module docstring for all shortest paths). It might be good to add it somewhere in the docs for DAGs that this function exists in the shortest path section.
comment
Thank You!!
comment
Thanks for this! I've got a number of comments below.  Module/file header: take a look at the networkx github wiki for our standard header/copyright/author recognition. Not all our files have it but we are working toward that.  Check the PEP8 code style using pycodestyle (used to be called pep8). Most changes will be spaces after commas, length of lines, indenting and stuff like that.  In your import of numpy you call your function hits_numpy. :)  You should probably also rearrange that 2 line exception to make the first line define the error string and the second raise the exception:      msg = "non_randomness requires NumPy: http://scipy.org/"     raise ImportError(msg)  Code ```if not k:``` should be ```if k is not None:``` Call of ```label_propagation_communities``` can be shortened to ```nx.community.label_propagation_communities```  Define the value for Eqn 4.5 on its own line. Then put that into the return statement. Can you please come up with two names for these two measures of non_randomness? Hopefully the paper named them, but even if it didn't, try to find names that will convey the spirit of the measure and/or how it is computed.  :) 
comment
Thanks -- looks good.  Can you add the NetworkX people to the copyright?  Can you check Pycodestyle again (spaces after commas and between operators)
comment
Don't lines 78 and 81 in non_randomness also get flagged by pycodestyle?  They have lots of operations without spaces around them.
comment
Thanks!
comment
Thank for this cograph stuff! :) . This looks quite good.  Can you make the module header match the description in the NX Github Wiki... The copyright and author recognition and stuff like that. Make sure the docsrtrings have a single line to start the docstring. Then a blank line and then a paragraph or two describing in more detail what is being done. Your ```full_join``` is great; the ```cograph``` needs work. Also "starts of from" in cograph docs should be "starts off from".  You should replace ```len(list(R.nodes()))``` with ```len(R)``` (and similar with G instead of R).  Shouldn't you test more than the number of nodes? e.g. maybe the case n=2 with two possible answers? I haven't thought about this much.  You'll need to add to the docs to make this show up in the reference.  Look at /doc/reference/ and adjust or add the appropriate ```.rst``` files for these new functions.  Finally, do the usual pycodestyle (PEP8) check to style issues.
comment
I think this is ready...  Anything else from your perspective?
comment
You will need to supply more info for us to help you. Maybe a short example piece of code that shows the trouble.  Most likely you will find the trouble as you are preparing the example to show us what is wrong :}
comment
You never added any nodes. G is created as a graph with no nodes or edges. When you use ```nx.set_node_attributes``` it sets the node attributes for all the nodes in G, but there aren't any yet.   Try creating the graph with something like:      n = node_feature_df.shape[0]     G = nx.empty_graph(n)
comment
Take a look at the ```add_edges_from``` and ```add_weighted_edges_from``` methods at https://networkx.github.io/documentation/stable/reference/classes/graph.html 
comment
```G.edges``` should accept the keyword argument 'data'.  I believe your ```G``` is not a NetworkX graph object.  Try ```G.edges(data=True)``` from the command line. If you get the same answer then G is not a NetworkX Graph Object.
comment
I'm closing this in favor of the documentation changes described in #3841 It can be reopened if needed.
comment
You haven't given enough info to say whether it is a bug or a feature... :}  Is it possible that you have negative lengths?  Djikstra doesn't give shortest paths in that case.
comment
Thanks for this example. I can verify that it gives a strange result with 2 paths reported for ```all_shortest_paths```: the one from ```shortest_path``` and another which is apparently shorter than the one returned by ```shortest_path```.    Just to make sure it isn't a problem with double precision round-off when comparing nodes, I converted the graph to integer nodes and I find the same issue.  Also, the first 141 nodes of the second path are the same as the first path. The last 120 or so nodes differ...  Any ideas how to track this bug down (I'm assuming it is a bug until shown otherwise)?
comment
Hmmm... more mystery.  When I calculate the length of the paths given using the 'length' edge attribute I get the lengths of the two paths as the same. (And I do believe that Djikstra is implemented OK... we have used it for many many years without issues -- OK... so I would LIKE Djikstra to be bug-free so am projecting that desire... :)  What is LineString(path).length computing?  Could it be something other than the length computed by networkx?  Some code for computing path length from path:      def get_length(graph, path, weight='length'):         dist=0         for u,v in nx.utils.pairwise(path):             dist += graph[u][v][weight]         return dist 
comment
Thank you for this.   - Can you put the docs from the "Notes" section as a paragraph after the doc_string heading. That is, start with the current one-line description, put a blank line, then put a paragraph description of what the definition is. Furthermore, can you be more specific about what "linking" 2 trees means (how are they connected/linked?)  - I think the code can be simplified -- since you know the sizes and all nodes are integers.  Something like the following should be faster:      def binomial_tree(n):         G = nx.empty_graph(1)         N = 1         for i in range(n):             edges = [(u + N, v + N)  for (u, v) in G.edges]             G.add_edges_from(edges)             G.add_edge(0,N)             N *= 2         return G 
comment
You'll also need to add a line to the /doc/reference/generators.rst file to get the documentation to include this function.  Thanks!
comment
I'm not too worried about providing seemless conversion from any graph markup language to any other. If one accepts ```node_default``` as a valid key and the other doesn't it's fine to raise an exception and let the user figure out how they want to change names.   Perhaps I am missing something here... so feel free to reopen this issue and explain a little more about what is special about node_default and edge_default and how they might be handled in a way that could make this work.
comment
I don't understand this question.  I will close it but you can reopen it if you have more to add.
comment
Should anything be done about this? I will close it (still searchable). Reopen it if there is something we should do.
comment
Hmmm... Not sure I understand. The order of a dict should have no impact.  Can you give code for a small example?
comment
Hi!  Thanks for this -- it looks promising. :)  Here are some comments:  - you can put your name in CONTRIBUTORS.rst in the base directory if you want (don't have to) - When using ```not_implemented_for``` the docstring says:      Multiple types are joined logically with "and".     For "or" use multiple @not_implemented_for() lines.  So, you have restricted MultiDiGraph, but not DiGraph or MultiGraph. I think you'll need two lines instead.  - I usually test locally using ```nosetests --with-doctest *optional_filename*```, but that only tests one environment, so I rely on the CI testing to check the other environments. Realistically I usually only test the files where I've changed stuff, so I guess I rely on CI to test everything even for my environment. - The ```from nose.tools import *``` at the top of your test file pulls in lots of tools like ```assert_true``` and ```assert_false``` which you should use instead of e.g. ```assert_equal( --- , False)```.  There is even an ```assert_raises``` which you can read more about and/or look for examples.  Some people would say you should replace the ```*``` with each specific tool you use, but we're not too picky about that. - In your tests, you use 3 lines for each test and the lines are spread out because you have many cases. If many more tests get added it may be hard to track. You can reduce the number of lines for each graph to check and complete testing of each graph before moving to the next. For example:      is_at_free = nx.asteroidal.is_at_free     assert_false(is_at_free(nx.cycle_graph(6))     assert_true(is_at_free(nx.path_graph(6))     ...  - You should add a ```asteroidal.rst``` file so this function shows up in the docs. Look at /doc/reference/algorithms for examples. If you have questions, ask -- it can be tricky at times. - In the docstring, you state that the function checks if there are AT, but the name is ```is_at_free```. The phrase "check if there are AT" kind of implies True means there are AT. Perhaps reword that first line? - In the docstring Parameters section, after ```certificate : boolean``` you should add: ```(default: False)``` to show it is optional. - The docstring indicates the return value is boolean when sometimes it isn't. Perhaps you need to document the certificate case more carefully.   - line 82: you use an entire if structure to return True or False when you can just ```return first_cond and second_cond and third_cond```.  Some would say you can in-line the conditions so when the first is False python doesn't have to compute the other two. That could make the line very long, so other approaches include nested ```if```s, or strategic line breaks and spacing to retain readability. Probably can just be left as is too. - line 150: I think you have an extra layer of subgraph not needed. You only need the nodes from the components, so you don't have to create a subgraph to isolate those nodes. The loop can start immediately after G_reduced is created in line 149:      for cc in nx.connected_components(G_reduced):             label += 1             for u in cc:                 row_dict[u] = label  Note that the inner loop can just iterate over ```cc``` instead of ```cc.nodes```.
comment
Hi ... Sorry for the delay in my response.   I've been thinking about the ```certificate``` keyword and the 2-tuple return values for ```is_at_free```.  I think it would be better (and match our API elsewhere) if you create 2 functions:      is_at_free(G)  # return True if there are no AT in G     find_asteroidal_triple(G)  # return a 3-tuple of nodes, or None if G is AT-free  Basically, ```find_asteroidal_triple()``` would be the function you have now (with simplified return code).  And ```is_at_free()``` can just call find_asteroidal_triple and then check if the output is None. What do you think?  Finally, you define a function ```_is_asteroidal_triple``` which is a long, but single logical expression. That function only gets used once. It's probably easier to read and faster (python function calls are somewhat expensive) to inline the function (put the 3-line logical statement in the main code and remove the function).  Have you tried this on larger graphs of say 100 or 1000 nodes? Is performance going to be an issue? 
comment
This looks pretty good.  Thanks very much for your work!  - Do you want to expose other functions besides ```is_at_free```? Perhaps ```find_asteroidal_triple```? - The if statement in ```is_at_free``` is really testing whether the result is ```None```, so it should use ```is None```. Actually that whole function is probably better written as a one-liner:      return find_asteroidal_triple(G) is None  This is close to being merged.
comment
Looks good.  But you need to add ```find_asteroidal_triple``` to the .rst doc file. :)
comment
I find it hard to know what some of the suggestions might become. But my first impression is that the first bullet is quite in line with what NetworkX provides. So that fits well. The second sounds like it would make a good "example" (in the examples folder) but developing a complete utility program would probably be better supported as a separate package. The third also seems large enough and self-contained enough to be a separate package, but I don't really know what your aiming for. The tsfresh is clearly a package for time series analysis. If you are envisioning a package that could capture lots of computed quantities for networks, then NetworkX tries to do that. :) If you envision a utility that would choose which computations to make and collect the data for many graphs, that feels like a front-end for networkx that might be better as a separate package -- though obviously there would be exchange of ideas and code between the two.
comment
Your description makes sense -- though I haven't looked at any code yet. I suspect the solution is something like using triples where the middle element is a unique number for each node.  ```nodeid = {n: i for i, n in enumerate(G)}``` sets up the values and then the tuple is ```(priority, nodeid[node], node)```  
comment
Thanks!
comment
Thanks for this!  A number of lines in the doc_strings need to be indented. In particular, after each parameter and return type, and after the references.  Take a look at the other doc_strings for existing functions.  ```set(G)``` is perhaps better than ```set(G.nodes())```. should be faster and perhaps easier to read. ```set(V - S)``` is not needed since V and S are already sets:  ```V - S``` is better.   Do these work if S contains something that is not a node in G? Do they raise a reasonable exception?  
comment
The test failure was due to a test (in an unrelated part of the code) which uses random numbers not setting a seed for te random number generator. We have a couple of those -- I am trying to find them and fix them.  So Thank You for finding one and sending the screen shot.  Almost always, restarting the CI test (an option when looking at the CI-travis report or CI-appveyor list of test results for each environment) without any change to the code will generate new random numbers and the trouble will not appear. You effectively made that happen by creating a change to the code which initiated a re-run of all the tests.   I will open an issue to fix ```random_degree_sequence_graph``` so it sets a seed on the RNG.
comment
Now its going to seem like I'm back tracking on what I said earlier. But I'm actually not. :}  I am hesitant about so much CPU power checking the input for ```NodeNotFound```. I think that without those explicit checks, each function already raises an appropriate exception. Now that you have the tests in place for NodeNotFound, can you try removing the code that checks for nodes in S that aren't in G? That would retain the speed while still making sure we have good exceptions.  It is possible that one or more exceptions is NetworkXError instead of NodeNotFound. If so, change the test to check for NetworkXError. Otherwise you could try to change the exception in the other parts of the code. But I think that should be done in another PR. That will be a rather big change I suspect.  If you want me to do these last picky things, just let me know.  Thanks, 
comment
Sounds good. Leave the code in ```group_betweenness_centrality``` and take it out in the others. And do it in this PR. No advantage to waiting.
comment
Can you revert the exception changes. Just let it be a NetworkXError (and check for that in the tests). I've got it on my list to go through the code and make the exception system consistent. I don't want to add any code now that adds to what has to happen then -- especially if it means adding code to check the inputs.  No one will be confused by NetworkXError with a meaningful message.
comment
See comment: https://github.com/networkx/networkx/pull/3484#issuecomment-504570647 about this same change being needed elsewhere. I am going to merge #3484  but we still need to check the other functions in that module (and fix them in another PR).
comment
@jarrodmillman is this a simple "yes"? Any other implications?
comment
A quick test for timing the ```_weight_function``` fix used for dijkstra suggests a more than 10% slowdown for graphs if we accommodate multigraphs. I suggest we make ```betweenness_centrality``` ```not_implemented_for``` multigraphs (as this PR does). So I'm going to merge this.
comment
This looks like a good addition. Placement in the package is good. I only have a couple of comments/requests:  - remove the unused import of ```bisect``` - add a test using a MultiGraph to check those 2 lines of code - add test of square matrix and right size matrix for _laplacian_submatrix (can use ```assert_raises``` or ```@raises``` decorator) - check PEP8 style using the pycodestyle package or similar style checker. I see / operators without spaces and some lines are longer than 79 chars. There might be other things.  Thanks!
comment
This looks good. I don't see anything else to change.  Thanks very much for this addition!
comment
I'm going to wait a couple days to help me make sure it doesn't interfere with another PR coming in. (k-truss). I think they will happily live next to each other but github may complain until one of them gets  rebased.
comment
Thanks very much for this. We also need an addition to the docs so t his will show up in the reference docs. Go to /doc/reference/algorithms/core.rst and add your function to the list of functions there.  There may be some changes to improve performance, but this code looks pretty good. I suspect if we improve in, say, speed, that we will lose in memory footprint or something. One thing to consider is going through the edges one node at a time so you don't have to lookup the neighbors as often. Probably not a big change in speed, but hard to tell without looking.       for u in H:         nbrs_u = set(H[u])         for v in nbrs_u:             if len(nbrs_u & H[v]) < k:                 to_drop.append((u, v))  Also, with this "once node at a time" approach, I think you end up removing each edge twice -- once in each direction. To avoid this you can store the seen nodes in a set and don't consider them later in the other direction.      seen = set([])   # and then inside the loop, change the if to:     if v not in seen and len(nbrs_u & H[v]) < k:     # finally add a line in the loop on u     seen.add(u)  Another idea -- just before the function definition use the decorators  @not_implemented_for('directed') @not_implemented_for('multigraph')  See core_number() for an example. I know many other functions in that module don't use this feature, but that's because they are old and this is a new function, so we might as well use the tools we've got.  Ask if you have questions.  
comment
You are correct that H.edges only outputs (u, v) and not (v, u) for the Graph class. The "same edge twice" problem arises if you switch to "one node at a time" processing of the edges.  I was wrong to say your current code does that.  I'll change the comment.
comment
I'd like to go ahead and merge this. We can update to tweak performance if needed later in another PR.  Are you OK with that? 
comment
Looks like #3476 fixes this issue - once it is merged
comment
Excellent! -- Thanks for tracking this down and putting it into a PR. I believe this fixes #3475
comment
Fixes #3466   Thanks!!
comment
The error for the random test is because someone (probably me) merged the test without making it set a seed for the RNG. So occasionally it doesn't pass the test.  It should be investigated further -- but not as part of your PR.   The codecov test complaint that is showing up now is because many of the lines of code which ```return False``` aren't being tested. Can you add tests that make sure each way to make it fail is tested?    Thanks for this!
comment
Non-trivial tests are probably beyond the scope for this sort of github interface anyway. I'm not looking for anything beyond very simple tests that just test one thing at a time (unit-tests).  For example, if you look at the Codecov report "diff" for this PR: https://codecov.io/gh/networkx/networkx/pull/3435/diff It shows each line as green if the line was executed as part of the tests. Red means it wasn't executed and Yellow means it is an "if" statement (or other branch) where only one of the two possibilities was encountered in the tests.  The first line that is red is line 371 and that simply means you don't have any tests which cause this criteria for ```subgraph_is_monomorphic``` to return False. The fix is to add a test using this function where the subgraph is not monomorphic. Scanning through the report it looks like lots of criteria that make the routine return False are not tested.  Coming up with tests should amount to finding simple cases where they aren't monomorphic. (that's not always easy-- but it shouldn't be too tricky for most of those cases.
comment
Thanks very much!  Is there anything else for the code itself?  I know the docs are not very complete in this module -- they don't follow the usual style of other NetworkX functions. And I don't think we should try to fix that in this PR. But I hate to add code without some basic additions to the docs.  Can you add a paragraph in the module's head doc_string about monomorphisms, what they are and when they would be of interest? Also, for each function with a new argument ```mono```, could you add a line in the docstring stating what it means and how to use it.  Finally, can you check pep8 compliance using ```pycodestyle``` (installable via pip). Thanks for all this!
comment
If someone requests a monomorphism_iter, that changes ```self.test```. Can they later request an isomorphism_iter?
comment
Ahh... I see now -- I was only looking at the diff... not the whole file.   Good...  But now I have another question:  Might someone want to do a graph-graph monomorphism? Or is that just a graph-subgraph monomorphism and we don't have to worry about that showing up some day. (I just want to make sure "mono" is sufficient rather than using "mono_subgraph" or something.)
comment
OK.. I think this is ready to merge. Anything else to add before I do that?
comment
Yes, this looks like a good addition.  Would this be worth making into a decorator like ```not_implemented_for```? That may be more trouble than its worth. I'm fine with whichever way it's implemented.  Is the name ```order``` descriptive enough that it won't be confused with something else? It seems like ```topolocial_sort_order=None``` is too long. ```top_sort``` seems short enough to be convenient, but still descriptive enough that it won't be confused with another concept. I'm OK with whatever you choose to make the keyword. I'm just asking for you to think about it.   
comment
This looks ready to merge. Anything else to add here?
comment
Thanks!
comment
OK... I think we are very close. A few small things to clean up.  - In doc_string definition, should ```C / len(nbrs_u) * len(nbrs_v)``` have parens around the multiplication? (both are in denominator right?) - At start of example line 1115, you use "from zero to *n* where..." It should be "from 0 to *n-1* where" - can you split the example on line 1124 to bring it into the 80 char line limit for PEP8.  Maybe like:  ```         >>> lol = [[sim[u][v] for v in sorted(sim[u])] for u in sorted(sim)]         >>> sim_array = array(lol) ``` lol stands for list-of-list, but any short name could work.  Thanks! 
comment
Thanks!
comment
Thanks for This!!  Please help me work through this as it has been a long time since I looked at betweenness and I didn't write the code in the first place. :) As far as I can tell, you have replaced division by len(P[w]) with multiplication by sigma[v]/sigma[w]. Your description suggests this handles multiple shortest paths (through the same predecessor?) in a way the current code does not. That all seems reasonable and the example you show also makes sense -- though I don't understand why we scale undirected graphs by 0.5 for this subset measure; but that is independent of this PR. :}  Could you comment on why it is correct to remove ```s``` from ```target_set```? Your update of the test function doc_strings is great.   Could you make sure the changes still adhere to pep8 standards (probably have to adjust line length in some cases)?   Thanks again!
comment
Thank You!
comment
There isn't anything in the spring_layout algorithm that would ensure two nodes are separated from the others. However, the positions from spring_layout are scaled to be at most length 1 from the center in the widest dimension that results from the spring algorithm. That is the maximum distance is found from the center considering all dimensions, and all distances are scaled to make that 1.0.  So, I'm not surprised that the farthest node from the origin has one coordinate being 1.0. I am a little surprised that two nodes seem to be pulled farther than expected from their groups. If spring_layout is doing that I suspect a bug. But more likely it is some artifact of the data that makes two nodes separate from the others.   Can you ensure that these two nodes are not different from the others in their group?  Can you find a simple case that shows the same behavior to explore more? When I run spring_layout on a path of 4 nodes it gives: In [22]: G=nx.path_graph(4) In [23]: nx.spring_layout(G) Out[23]:  {0: array([ 0.43834003, -1.        ]),  1: array([ 0.1555344, -0.3549743]),  2: array([-0.15576722,  0.35528535]),  3: array([-0.43810721,  0.99968895])}   These values are scaled so max coordinate absolute value is 1 (center is origin by default). But no point is pulled toward (0,0) which it seems to be in your example.
comment
What code are you running? (I can't debug it unless I can reproduce it.) . Something like this?  ``` G = nx.Graph() G.add_weighted_edges([0, 1, 0.2), (0, 2, 0.1), (0, 3, 0.1), (1, 2, 0.1), (1, 3, 0.1), (2, 3, 0.1)]) pos = nx.spring_layout(G) print(pos) ```  What output do you get?
comment
Your code didn't work for me (syntax errors) but if I understand right, this is the same process.  Can you tell me what is strange about the position output?  ``` G=nx.Graph() G.add_weighted_edges_from([(0, 1, 5.0), (0, 2, 10.0), (0, 3, 10.0), (1, 2, 10.0), (1, 3, 10.0), (2, 3, 10.0)]) print(nx.spring_layout(G)) {0: array([1.        , 0.15926037]),  1: array([-1.        , -0.15926037]),  2: array([ 0.11901171, -0.7472776 ]),  3: array([-0.11901171,  0.7472776 ])} ```
comment
It shouldn't matter for spring_layout so long as it is python3.5+ and networkx 2.2+ Can you show your output (along with the code that created it.
comment
Sorry for the delay on this...
comment
nx.draw_spring does not put labels on nodes by default (neither does nx.draw). Use ```nx.draw_spring(G, with_labels=True)```
comment
Hi -- thanks for thinking about this. My understanding is that union-find with {by-rank or by-weight} and {path compression, path_splitting, or path_halving} all have the same level of complexity. So, if what you and Wikipedia say is true about a tight bound, then it seems likely the bound is tight for all these algorithms.  I'm not able to spend the time to dive into this too deeply. My inclination is to stick with what we've got unless there is an advantage to -by-rank.  Can you explain or point me to something that does explain which is better?  My understanding is that they are the same.
comment
Looks good!   Just a few comments for now...  - we don't need __future__ anymore: we have dropped support for python 2, the future is NOW! - check your code style using pep8 or pycodestyle (which is the new name for pep8) - Replace the ```_``` variable name with something meaningful. e.g. ```for path in paths:``` The time to use ```_``` is when you won't use that variable again  e.g.:      for u,_ in G.edges:         print(u) # code that doesn't use the second node of an edge  
comment
OK... we're close.   - Did you check the pep8 style? (```pip install pycodestyle``` then either ```pep8 group.py tests/test_group.py``` or ```pycodestyle group.py tests/test_group.py```) - Could you change some names in the tests...   First, could you rename the file ```test_group.py```?   (easier to find if you are working on the group.py file.)  Second, can you make the test names more informative.  For example, the first could be:  ```def test_group_betweenness_single_node(self):```   In each case, you can change the number to a short phrase. That helps when Travis/Appveyor report an error and helps with readability generally.
comment
Thanks very much for this!  Also -- now you know the process... you can add more functions with a new PR. :)
comment
One image of a formula without definitions or context is not enough information to say if this is useful. Do you have a reference/article that describes this centrality measure?
comment
Yes -- these links would be good to include in the docs for the function(s) too.  It might be best to create a new module in networkx/algorithms/centrality/ named something like group_betweenness.py  If you have questions about how to organize the code or how to use github, ask.  It's often helpful to create a "close to finished" PR (rather than wait for it to be polished) so we can look at the code and ask questions -- and you can ask questions.  Adding new commits and pushing to your github automatically updates the PR in networkx after the PR is created.
comment
group.py sounds like a good module name.  Thanks!
comment
Hmm.. sorry, I'm not sure what the impact of this change is. (It's been a while since I looked at this code.)  What is the current behavior that this changes? What is the new behavior?  It looks like this change makes it so people can't specify a single number and have it apply to the whole collection. 
comment
@haiyangdeperci do the changes that @joseppinilla made for alpha values satisfy your needs?
comment
The code coverage check simply means that your new code has 86.2% of it's lines covered by tests. That is supposed to be an incentive for you to write tests that execute the lines of code not executed by the other tests. These are usually special cases, raising exceptions, branches of if statements that rarely happen, etc.  Think about if there are lines of code you should be testing that you aren't. If there are, include tests. If  not, don't worry about it. we can/do merge PRs that don't pass this fairly arbitrary coverage level.  Thanks for your contributions!
comment
Yeah -- I have seen that happen before with other PRs and couldn't figure out what codecov was doing to flag those two files. Didn't have time to track it down yet...
comment
You'll need to add/update some rst files to get this new function into the documentation.   Also, why do you use ```eigvalsh()```?  Is there a better function -- perhaps one that can avoid the call to ```.to_dense()```
comment
One more change:  Can you shift the heading ("authors", imports, comments, etc) to match the [suggested ordering on the wiki page](https://github.com/networkx/networkx/wiki#user-content-top-of-the-module).  Essentially, match the ordering/style in [richclub.py](https://github.com/networkx/networkx/blob/master/networkx/algorithms/richclub.py)  Note that one aspect of that is the copyright will still have the same names (and you can add yours to that list) but the list of authors can/should be you since you wrote this code.  A "Tidy up linalg rst files" PR would be welcome. :)
comment
Thank you for making the docs consistent -- and also fixing the documented default value for edge color.  It is good to keep the options up-to-date with what matplotlib has to offer.  Given the ```scatter``` warning on 1d-arrays of length 3 and 4, I think we should stick with requiring ```len``` to be 1 (if not string_like).  Your example code from #3394 would then print a warning. But making ```node_color``` a 2D-array with 3 or 4 elements would work without warnings:      nx.draw_networkx(G, node_color=(0.0,0.0,1.0,1.0))  should give a warning unless there are 4 nodes in G.      nx.draw_networkx(G, node_color=[(0.0,0.0,1.0,1.0)])  should be allowed and use the same blue color for all nodes.  What do you think? 
comment
Hmmm.. I think we should make it work as flexibly as possible.      nx.draw(D, edge_color=[(0.0,0.0,1.0,1.0)])     nx.draw(D, edge_color=(0.0,0.0,1.0,1.0))     # one color except if 4 edges in G. Then 4 nums into cmap     nx.draw(D, edge_color=[(0.0,0.0,1.0)])     nx.draw(D, edge_color=(0.0,0.0,1.0))       # one color except if 3 edges in G.     nx.draw(D, edge_color='green')     nx.draw(D, edge_color='g')     nx.draw(D, edge_color=['green'])     nx.draw(D, edge_color=['g'])  should all work for node_color and for edge_color.  I think we can do this.
comment
I think that will make node_color and edge_color follow different rules for color assignment -- in an undocumented way (without referring to matplotlib docs). I do like the idea of using the defaults that matplotlib has chosen. It'd be nice if node_color and edge_color encoded colors the same way, but that isn't a design goal per se.  The value we should add to matplotlib here is checking/implementing any Graph Theory constraints before calling matplotlib.  Are there any in this case? Previous developers (often using older versions of matplotlib) have made it possible to assign colors as a single value or as an array of values. Matplotlib does this now. Maybe there isn't any reason to add to the possibilities provided by matplotlib. (well -- except if we want node_color to be similar to edge_color in the way it handles colors?)
comment
This looks good. Can you add test(s) that runs the commands you put here as examples? That will help make sure we don't regress when making other changes later. Thanks for this.
comment
We should include some tests for undirected graphs even if ```edge_color``` is passed directly to ```matplotlib``` just so we know when they have changed their API.  Your added test function goes way beyond just testing color (Thanks!) so maybe just add another function that does something like:      for G in (self.G, self.G.to_directed()):         # smoke test of color inputs          nx.draw(G, edge_color=[(0.0,0.0,1.0,1.0)])         nx.draw(G, edge_color=(0.0,0.0,1.0,1.0))         nx.draw(G, edge_color=[(0.0,0.0,1.0)])         nx.draw(G, edge_color=(0.0,0.0,1.0))         nx.draw(G, edge_color='green')         nx.draw(G, edge_color='g')         nx.draw(G, edge_color=['green'])         nx.draw(G, edge_color=['g'])         nx.draw(G, edge_color=['g','k','r']) # matplotlib can handle these         nx.draw(G, edge_color=['g','k','b','r'])    
comment
Changing the default alpha from 1.0 to None makes sense to me if it causes matplotlib to assign the value 1.0 to alpha.  Those changes look good. Do we need tests for alpha now? Did your commit finish the alpha handling in a way that does what #3370 wants? I couldn't tell from your comment whether this is done or still needing to fix something. Thanks!
comment
It looks like you've got an off-by-factor-of-two error in the tests (or in the code).
comment
Looks like a unittest issue with different versions of unittest.  Might be better to switch to the nose paradigm which you asked for help with. I'll take a look to see how easy that would be. (probably easy).
comment
I have some questions about barycenters generally that relate to this code:  I can't find this approach to barycenter online. Is there another term that is used for this calculation that has a larger online presence? Is there a literature that uses it? Is it covered by more than West's book?  Why is the barycenter here the subgraph induced by the nodes instead of just the nodes? For example, the center of a graph is the set of nodes with some property. Would it make sense to have this function return the set of nodes.   The user would certainly be able to convert to a subgraph. (As is, the user can easily get the nodes from the subgraph too, but at the cost of creating the subgraph they might not need.)  Thanks -- and sorry for the delay in getting to this.
comment
If ```barycenter``` can use a dict of shortest path lengths effectively (which it looks like it could) then it'd be cool to unify the API and reduce time for those who already computed that dict.  This looks good. I've shifted it to milestone v2.3 and will work to get it merged soon. Thanks!
comment
It is failing because of different printing styles in numpy across versions. We've had that trouble before. Probably Travis changed the version of numpy.  Anyway, as you say -- it is not part of this PR.
comment
This looks good -- but now we will have to change the doc/rst files. Changes needed in doc/reference/generators.rst and doc/reference/algorithms/community.rst 
comment
Can you elaborate? Why do you expect it there? What rule are your expectations based on? How does this choice break that rule.  It's probably also good to describe more carefully whether it is only one function that should be moved or a module, or what...
comment
Thanks -- this helps. I'll take a look at the history and try to figure out why it might historically be like this. Looks like to move it we would move community_generators and test_generators to the generators directory.
comment
Yes -- there is nothing in the history that indicates this should be where it is. It makes more sense to pull the code and put it in generators.community.py If you could put together such a PR, that would be great. Thanks!
comment
The bipartite generators are currently named in such a way that we can't easily combine them. Basically, they use many of the names we already use for non-bipartite generators. But in that subpackage the functions refer to bipartite generators.  Let's leave them there. I think the idea for putting LFR benchmark in community was a similar philosophy that code could be compartmentalized somewhat using imports and namespaces. But it hasn't been implemented widely in the NetworkX package. I think you found the places I know of for generators.  There are other places where we have isolated the namespace somewhat like the isomorphism checkers.    Let's move the community_generators and keep the bipartite generators where they are.
comment
The ```keys``` argument to edges only works for MultiGraphs. And it already is a valid argument for edges/in_edges/out_edges with MultiDiGraphs.
comment
It looks like your troubles are not related to previous issues. When you subclass an object you need to think about the arguments for instantiating the new class. If you want it to handle inputs in a similar way to the super-class, you need to build those into the arguments for your sub-class.  As written, you require an argument ```foo```, and do not allow any other arguments. So, any code that treats your new class as a NetworkX class (either with or without arguments) will not work.  Think about what you want the value of ```foo``` to be when your new object is instantiated using syntax for NetworkX graphs. Make that the default value for ```foo```.  That will get rid of the TypeError complaining about required positional arguments.  Next, you need to build into your sub-class the ability to handle input meant for the sub-class. Take a look at ```nx.Graph.__init__()``` and make the subclass ```__init__()``` mimic that calling syntax (while adding ```foo``` of course).  Once ```MyGraph(edges)``` and ```MyGraph()``` work you should be ready to go. 
comment
That looks better -- but it specifies an ordering of arguments that makes ```foo``` first and then the super-class arguments. If something calls ```MyGraph(edges)``` then edges will be the value of ```foo``` instead of ```data```.  It would probably be safer to actually specify what input arguments you want in which order, rather than using ```kwargs```. And put ```foo``` last on the list so calls that treat MyGraph like the super-class will work properly. 
comment
This looks pretty good -- I have only two suggestions:  - can you make sure it passes a pep8 style checker (pycodestyle which used to be pep8; or flake8 or maybe others).  I think linelength may be the only change -- but worth looking.  - Can you change the description from 2 functions that do the same thing to 2 functions which generate Harary graphs, but one maximizes connectivity holding number of nodes and edge fixed while the other minimizes edges holding connectivity and number of nodes fixed.  This might lead to other doc changes, but I'm not sure.  Thanks!
comment
OK... now some more (hopefully this will be it):    - remove the .vscode line from .gitignore     - Add blank lines after each parameter entry  So that:      n: integer        The number of nodes the generated graph is to contain     m: integer        The number of edges the generated graph is to contain     Becomes:      n: integer        The number of nodes the generated graph is to contain         m: integer        The number of edges the generated graph is to contain         - Make sure each function's docstring starts with a single line (<80 chars)   and then a blank line.  You may have to move the detailed description of the    function to the third line -- and replace it with a first line that is more compact.    For example:      Returns the Harary graph $H_{n,m}$ that maximizes the node     connectivity with $n$ nodes and $m$ edges.     This maximum node connectivity is known to be $[2m/n]$. [1]_  Becomes:      Returns the Harary graph with fixed number of nodes and edges.      The Harary graph $H_{n,m}$ is the graph that maximizes node connectivity     while keeping fixed the numbers of $n$ nodes and $m$ edges.      This maximum node connectivity is known to be $[2m/n]$. [1]_
comment
I know of two latex codes for fractions like that and then the way you have it, which is actually pretty good since it doesn't make the line any taller:      ${2m \over n}$ and $\frac{2m}{n}$ and $2m/n$
comment
I did misunderstand... sorry about that. In LaTeX there are packages that provide special symbols, e.g.  $\left\lceil 2m/n \right\rceil$ and $\left\lfloor 2m/n \right\rfloor$ But that makes the docstring hard to read in text/code form.   I would suggest using the more 'code-like' form: ceil($2m/n$) and floor($2m/n).  That is what we have used elsewhere e.g. ```edge_augmentation```.
comment
Thanks very much for this!
comment
Yes.... I apologize for not checking this. Each module gets automatically included in the docs by Sphinx  if you create a corresponding file/entry in the /doc/reference/ directory.  In this case you should add to /doc/reference/generators.rst  The next question is which section of generators.rst to put the new functions. You'll need to add a new section that will cover your new module. Follow the syntax for other modules in that file. You can test locally by building the docs locally.    Thanks for noticing and asking... :)
comment
Why do you feel it is impossible? What's an upper bound on the number of maximal cliques?[ Some discussions (and results)](https://mathoverflow.net/questions/154709/upper-bound-for-maximal-cliques-on-perfect-graphs) show it is much bigger than numb nodes or numb edges. 
comment
This seems to be a matplotlib feature/bug. Updating the tick_params doesn't seem to do anything to the axes. This is true whether you use ```ax.tick_params(...)``` or ```plt.tick_params(...)```   @elplatt any suggestions?
comment
This code for ```is_semieularian``` and ```eularian_path``` doesn't handle (raises an error) the cases:      - undirected with all nodes having an even degree     - directed with all nodes having ```in_degree == out_degree```     - directed with exactly one node where ```|in_degree - out_degree| == 1```  Looking at the existing code ```is_eularian``` and ```eularian_circuit```,  it doesn't handle isolated nodes correctly. The functions report that no eularian_circuit exists when eularian circuits shouldn't care about isolated nodes -- they deal with edges. A nice way to handle that is to start by removing isolated nodes from G (make a subgraph of G with only nodes with positive degree).  These concerns are more than will easily allow inclusion for v2.1.  I'm bumping this to v2.2 milestone. 
comment
I'm afraid this code (and the original code from #1488) isn't correct.  By that I mean that it doesn't find all eulerian paths nor identify them correctly as semieulerian.  Try ```G = nx.DiGraph([(0,1), (1,2), (2,0), (4,0)])```  which has path ```[4,0,1,2,0]``` but the code says there is no eulerian path.  Similarly  ```G = nx.DiGraph([(0,1), (1,2), (2,0), (0,4)])``` has path ```[0,1,2,0,4]``` but is reported as no path.  I tested this with the original code from #1488 and with the new code. The problem seems to be in the heart of the algorithm, -- where the starting node is chosen. It doesn't handle all the cases for number of nodes with off-by-one in_degree and out_degree.  I'm not sure where to go from here -- this has been around for a looong time. Perhaps someone can complete Edwardo RIvera's code or take the code from @humberto-ortiz and make it work for these cases.  In the meantime I propose to reject these Pull Requests and apologize for all the work requested when we didn't check the fundamental algorithm...
comment
Thanks!
comment
Hmmm..   I suspect this is happening with many other packages. For example, pandas and numpy both planned to stop supporting python2. Neither plans to change the major release number -- numpy-1.17 will no longer support python2 and Pandas will have 0.24 be the last to support python2.  Maybe it will work for them because of people using ~=0.24.2 which is equivalent to >=0.24.2, == 0.24.* Maybe we can do the same with people expected to use ~=2.2.0.  But we haven't been using 3 numbers for our releases.  Thoughts?
comment
I think the main problem specified in this Issue is that using pip with ```~=2.2``` won't work for networkx. Users should stick to other version specifiers like ```==``` and ```>=```.  Is that an accurate description of the problem?  If so, we could add to the paragraph something like:      The pip shortcut version specifier syntax ```~=``` won't always work with our numbering system.     It is better to use syntax such as ```==``` or ```>=```.  I agree that we shouldn't create yet another place to update with version numbers for each release! :}
comment
Sounds good to me.  Thanks!
comment
This is starting to look specific enough (with 3 Issues lined up as well) that I'm going give it a Milestone 2.3  designation.  We should get this done.
comment
Thank you for reporting this!  It is a bug. The offending line should allow edge 3-tuples from multigraphs by the code:      src_node, dst_node = edgelist[i][:2]  We should check that section of code for other similar bugs and add a test that reveals this issue.
comment
This looks pretty good!  Thanks!   Can you get it to PEP8 standards? (you can install a pep8 style checker using ```pip install pep8```.  probably it only will need you to follow the 80 character limit on each line.  Can you say something about the need for the locally written and maintained queue utility? Could it be replaced by Python's ```heapq```?   If so, that could remove maintenance issues for us later.  
comment
Thanks for that description.  I would prefer not to add a dependency.  How often do you have to remove a non-root element?  I'll look at the code some more...
comment
OK..  That makes sense to me.  Could you add to the doc file ```doc/reference/algorithms/community.rst``` to make these functions appear in the documentation reference?  I think it is then ready to go.  If that's too much, let me know and I'll do it.
comment
Thanks for this. 
comment
Hmmm..    Perhaps the best way to figure this out is to take an extreme case and see what ```p``` is. For example, if ```p=1``` do you get the ratio or 1 minus the ratio?  The function requests ```p```, so you can choose to set it however you like. But I think you are seeking info about what impact that choice has on the structure.
comment
Maybe both references would be helpful.
comment
I think we should go with Aric's suggested approach above:  > Another approach is to loop through all of the edges before writing to discover the existing id  >  attributes. Then for edges without ids generate new ones with some algorithm  > that don't conflict with existing ids.  But I'm still going to bump this to v2.3 so we can get v2.2 out soon.
comment
Does this allow two directed edges in opposite directions between a pair of nodes to be seen as two edges? What other use cases do you see for this? Do the styles have to be the same for all edges, or is there a way to make them different even though the arrows are in the same patch? (I guess you can always draw some edges with one style and then draw others with a different style using a second function call...) 
comment
Does the picture you show use a single call to ```draw_networkx_edges```? Or is it many calls?  That is, could ```connectionstyle``` (which should be ```connection_style```) be a list of strings?
comment
This actually looks like a bug to me -- at least on first look. The dijkstra method works for an undirected graph.  Does it even work for a directed graph? Say: a single edge (0, 1).   Also, there is no function ```single_target_dijkstra_path_length```, so the quick fix is different. Now that we have an efficient way to reverse graphs (nx 2.x), it might be best to simply check the reverse flag at the beginning and just use ```G=G.reverse()``` or similar to handle that case.   But first thing is to check that a directed graph is treated correctly for weighted edges.  Thanks!
comment
Your edited comment is key:  The djikstra shortest path computes outward closeness, not inward as we advertise.  I think that means that currently if our graph has all edge weights 1, we get reversed results for using weighted as compared to unweighted methods.   Hmmm...   Fixing this could provide a backward compatible headache. We should document it with strong warnings that for NX2.3+ using directed weighted edges gives correct (inward) values whereas for previous versions the directed weighted edges give correct (outward) values and ```reverse``` had no affect.  Assuming ```reverse=False```, in order to compute the inward closeness, we will need to reverse the edges in the weighted case. For the unweighted case, we essentially do that inside ```single_target_shortest_path_length```.  Maybe it would be cleaner to reverse the edges in both cases and use ```single_source_shortest_path_length``` for the unweighted case.    When ```reverse=True```, then we would NOT reverse the edges...   A little turned around perhaps, but it stems from closeness centrality being defined as looking at inward connections.    Thoughts?
comment
I'm leaning toward getting rid of the ```reverse``` argument. We can reverse graphs easily now so users can simply call it on ```G.reverse()``` instead of ```G```. Fewer arguments also makes the interface that much cleaner. In the code we can reverse any directed graph input in order to provide the inward closeness as advertised. 
comment
Unfortunately, there are two versions of ```graphviz_layout``` in the networkx package. Which are you referring to?  It might be better to call them ```pydot_layout``` and/or ```pygraphviz_layout```.  Looking at the code, it seems that ```pydot_layout``` ignores any ```args``` while ```pygraphviz_layout``` at least forwards the ```args``` keywords on to the pygraphviz package.  It also seems to me that the doc_strings in ```pydot_layout``` and its version of the wrapper ```graphviz_layout``` should be updated to at least list the possible arguments correctly.
comment
Looks good!  :) . Thanks --  I haven't worked through all the code, but I notice that you've got tests and integrated the autodoc rst files.  You still need doc_strings as you know.  I would also encourage you to pay special attention to the naming of functions. ```procedure_P``` is not very descriptive to a generic user. But if it is a standard name used in many (whatever many means) sources, then that's OK. If it is only referred to that way in the one paper, then there is perhaps a better name that describes what it is doing. Similar for variable names -- one letter names are often discouraged as being non-descriptive, but we use them for example for graphs in lots of our code because the literature does that.   Your placement of the code and general style looks good -- again, I haven't gone through it in detail.
comment
Yes, I think that would be a good place to include your equitable-coloring functionality.  Thanks very much!
comment
That just means that the tests only covered 88% of the code (well...  88% of the code was actually run while doing the tests).  The drawing routines often have lower coverage than the rest of the codebase. I guess you could add tests for "corner cases" to make sure more code is reached during the tests. But I wouldn't worry about it too much.  We can review and merge even with the coverage goals not passing.
comment
The drawloop PR now has the Betti number change in it.
comment
This needs tests and thoughts of how universal the choice of angles on the loop work. 
comment
Does #3307 make this obsolete/unnecessary?  If not, open it up again.
comment
Fixed in #3278  Sorry about this. It'll be fixed in version 2.3 (coming pretty soon).
comment
We should mention #1054 in this PR's conversation. This PR attempts to implement at least part of the wishlist in #1054.    I have not looked at the code in depth yet, but I would request that you recast it as a module ```moral.py``` in the ```algorithms``` directory with the tests in ```algorithms/tests/test_moral```. You should also make it part of the main namespace and add/change ```*.rst``` files in the ```doc``` directory so that it shows up in the docs too.  That is, remove the __init__ file, move the moral.py and test file and add/change the rst files in /doc so it can find the new functions.  Thanks for this!  Anyone have comments on this algorithm and implementation?  
comment
The core of the function can be replaced by many fewer statements that might be faster:      HH=G.to_undirected()     for preds in G.pred.values():         HH.add_edges_from(itertools.combinations(preds, r=2))  Note that this handles the case of len(preds)<=1.  It also avoids creating a list of preds for each node. Coding style varies quite a bit by coder, so see if you like that suggestion (and maybe time it).  Also -- can you change the function name from ```get_moral_graph``` to ```moral_graph```? Again, if you think ```get_``` is better, let's hear your reasons. But our names typically don't have ```get_``` when we are computing/creating that object.  You can consider putting your name in ```/CONTRIBUTORS.rst``` . You don't have to.
comment
The error is not related to your code.... its a version difference between testing environments that has cropped up recently.  This looks good to go.
comment
Thank you -- see #3339 
comment
Thanks for this!  Can you provide a brief description of what VoteRank is? Perhaps in the doc_string for the function, and maybe in the PR as well. 
comment
@fredrike Thanks for the nudge.  Are we waiting for feedback from @pbrodka? I am having a crazy month, but will try to get to it soon. 
comment
Lack of time and focus I believe is the only real hold up.  I ask for you to check the spelling of ```influetial``` and ```turnself``` Could you also replace ```list()``` with ```[]``` in the code? Everything else is ready to merge as far as I'm concerned.  Thanks --
comment
Thanks for this!
comment
We don't require nodes to be sortable. So if a graph didn't have nodes that could be sorted, you'd need a way to circumvent that when using your strategy. One idea would be to create a 2-way mapping from the nodes to an ordered set (say-- the integers 0 to n-1 where n is number of nodes). Then sort based on that ordered set. node_list = list(G.nodes)   # node number to node => node_list[number] node_map = {n: i for i, n in enumerate(G)}   # node to node number  =>  node_map[node]  You'd spend more time looking up the node values in the mapping, but less time counting triangles. I don't know whether it would be faster in the end or not.
comment
The docs for these function describe default values that affect the path calculation. The most obvious one is which edge attribute to use as a weight.  The default for ```shortest_path_length``` is all edges have weight 1. You need to specify more info if the default values aren't giving you what you want. 
comment
@winni2k can you comment on this?
comment
Did you all figure out whether ```all_simple_paths``` should stop when it reaches the first target or continue through targets to other targets to get all paths?  I think that has to be made clearer in the docstring:   What exactly are we returning?!?
comment
Hmmmm... This seems strange. I don't think there is any recursion in the pagerank function. Could the recursion be coming from some other code? Am I missing something that recurses?
comment
Definitely strange. I don't see how this could arise -- and I cant reproduce it. If your DiGraph has only 32 nodes can you describe it for us?  Any node attributes? edge attributes? simple code to construct?
comment
I don't get any error message. What version of networkx?    ``` In[23]:    G.nodes Out[23]: NodeView((729168, 1231426, 335987, 253879, 1235290, 1235053, 551054))  In [24]: G.edges(data=True) Out[24]: EdgeDataView([(729168, 1231426, {'weight': -1.076}), (729168, 335987, {'weight': -0.192}),  (729168, 253879, {'weight': 0.28}), (729168, 1235290, {'weight': -0.209}), (729168, 1235053,  {'weight': -0.133}), (729168, 551054, {'weight': 0.14}), (1231426, 335987, {'weight': -0.017}), (1231426,  253879, {'weight': 0.273}), (1231426, 1235290, {'weight': -0.004}), (1231426, 1235053, {'weight':  -0.027}), (1231426, 551054, {'weight': -0.974}), (335987, 253879, {'weight': 0.138}), (335987,  1235290, {'weight': 0.851}), (335987, 1235053, {'weight': 0.521}), (335987, 551054, {'weight': 0.738}),  (253879, 1235290, {'weight': -0.096}), (253879, 1235053, {'weight': 0.235}), (253879, 551054,  {'weight': -0.047}), (1235290, 1235053, {'weight': 0.382}), (1235290, 551054, {'weight': 1.059}),  (1235053, 551054, {'weight': 0.084})])  In [25]: nx.pagerank(G, alpha=0.85) Out[25]:  {729168: -0.13400158128832415,  1231426: -0.25457331794498744,  335987: 0.4078567642526132,  253879: 0.116677068471361,  1235290: 0.4142263868147926,  1235053: 0.20887382796901605,  551054: 0.24094085172552873} ```
comment
This looks good to me. Are you done with this PR -- ready to merge? Anyone else want some time to look at it?
comment
I'm not sure if this should be called a bug or just an undeveloped / disallowed feature of write_graphml . By the way, I do get this error with an undirected graph. And I think you want your example to write the graph ```g``` and not ```H```.  The code ```g=nx.contracted_node(G,'a','b')``` contracts 'a' and 'b' correctly. But it also adds a node attribute to node 'a' so that ```g.nodes['a']['contraction']``` is ```{b: {}}```.  In other words the contraction attribute holds the node and node data for the node that no longer exists in the graph.  This causes a problem when using ```write_graphml``` because graphml does not support dicts as attributes.  To fix it, remove that attribute -- or replace it so it doesn't use a dict as attribute value. For example, add a line:      del g.nodes[dup[0]]['contraction']  Or to replace {b:{}} with a tuple of contracted nodes:      attrs = g.nodes[dup[0]]['contraction']     g.nodes[dup[0]]['contraction'] = tuple(attrs)  
comment
Thank you!
comment
Thanks!!
comment
Will this make it hard to coverage to tolerance for some cases (?small graphs?) 
comment
I think this case can be easily covered by reducing ```tol``` when you have a problem (say due to a large graph).
comment
I'd rather not get into rearranging code for succinctness. Readability is a better measure of code-quality (and sometimes agrees with succinctness). In this case, I'd rather not make the change -- if only to avoid a precedent.  Thanks for the suggestion! 
comment
I don't like the idea of hard coding these options into our code. The user can do this (along with many other matplotlib options) after the plot is created. If we do it in our code, then we have more to maintain and we haven't added functionality per se.  We could provide parameters for all the different ticks options, but that sounds like it could lead to requests for options about many plot features.  My leaning is toward a piece-meal approach where each function does a minimal thing and the user puts the functions together to create what they want.  Does this make sense or is there more to the story?
comment
Right...  That makes sense to me... I'll take a closer look and probably merge soon...
comment
The [docs for minimum_cycle_basis](https://networkx.github.io/documentation/latest/reference/algorithms/generated/networkx.algorithms.cycles.minimum_cycle_basis.html) warn that the cycle list does not order the nodes in the order they appear in the cycle.
comment
Perhaps we should put a redirect to https://networkx.github.io/documentation/latest/auto_examples/index.html
comment
It seems to me that it is a feature that this raises an exception if the nodes are not present in G. Otherwise it might hide a bug in someone's code.  Can you use something like this instead?      source in G and target in G and has_path(G, source, target)
comment
The ```attr_dict``` argument to ```G.add_edges_from``` is not supported in v2.2. It has been replaced with      G.add_edges_from([(0,1), (1,3)], custom_weight=10)     G.add_edges_from([(0,2), (2,3)], custom_weight=1)  You can see this because in your example, the edge attribute dict has one key: ```attr_dict``` instead of the key ```custom_weight``` as desired. When ```shortest_path``` looks up the edge, it doesn't find an attribute called ```custom_weight``` so the weight defaults to 1.
comment
I'm fine for adding this graph to that module. Looks good.  Add a note that you're ready to merge when that's the case.  Thanks!
comment
This error comes from the convert_* part of the code which is in the main ```networkx``` directory. It is in the test for converting from a pandas dataframe where a NetworkXError is supposed to be raised if the input column name: ```edge_attr``` is not a valid column name of the pandas data frame.  Travis reports that the error of NOT creating a NetworkXError only occurs in one configuration of the test platform. That makes it suspicious -- it could be a feature of the test environment and have nothing to do with the code you are writing.  I believe that is the case here -- the code you are adding has nothing to do with whether ```{}``` is a valid hashable name of a column for pandas data frames. You just happened to get lucky to be running the test just after the test environment changed in some way to create this bug.  We should open a separate Issue to address getting the test environment right.   As far as this PR is concerned, when you are ready for the merge, look at travis again and make sure this is the only failed test. Then merge even though the Travis CI test failed.  You can merge anyway.  Thanks!
comment
Do you know a way to see what this will look like on github before merging it?
comment
Thanks very much @ericmjl !!   :)
comment
No need to "squash" the commits.  github does that by default for each PR when we merge it. Thanks!
comment
@Moradnejad see [this stackoverflow question](https://stackoverflow.com/questions/2081640/what-exactly-do-u-and-r-string-flags-do-and-what-are-raw-string-literals) about ```r" "```  @BoboTiG   Thanks for this!  The shortening of output means the user can't see what the full output should be. I'm probably OK with that, but wonder what options there might be for a workaround that does show the full output.
comment
The ```frozen``` changes are new today -- so you'll need to rebase to the latest version of master: I think it should just be  1) update a master branch;  2) change to your PR branch;  3) ```git rebase master``` .   # but things may come up here... I usually use ```git rebase -i master```  Also, it looks like your changes for ```add_star``` aren't passing tests because it is adding a path instead of a star.
comment
See #3285
comment
Don't worry about the test failures on travis. They are not related to your pull request. It looks like the version of numpy has changed on one of the builds in travis and so matrices are printing differently. That's the first failure. The second is a re-appearance of problems with testing kcomponents where valid random graphs never get generated. I restarted it, hoping it will work this time, but I'll look into that further in another PR.  Thanks for this!  looking forward to your tests.
comment
Fixes #3277
comment
Looks good!  Can you also add a test for an empty list? That will cover the try/except clause.
comment
Excellent!  Thanks very much!
comment
Thanks!
comment
Thank you --  Python 2 is no longer supported. So we just need to adjust the imports to use the Python 3 API.  
comment
Fixed in #3278 and #3171
comment
The test failure is not due to this change. I think the numpy version on Travis changed so it prints differently. We'll fix that in another PR.  Could you also make sure all the imports of ```collections``` are correctly changed?  (You probably did already, but I just wanted to be sure...  #3171 did a bunch of that already.)
comment
I will open a new issue.  This issue is pretty old and closed long ago. It certainly should be fixed.
comment
Thanks!
comment
Thanks!
comment
Check [the centrality page](https://networkx.github.io/documentation/latest/reference/algorithms/centrality.html#subgraph)...   It might be called ```subgraph_centrality``` or ```communicability_betweenness_centrality```.
comment
I think you are looking at old documentation.  Your original link is to v1.9 docs which don't describe what is available for the recent v2.2 release or the latest v2.3dev on github.
comment
I think [```grid_2d_graph``` ](https://networkx.github.io/documentation/stable/reference/generators.html) should do what you want.   Use the ```periodic=true``` optional argument to make it a torus.  ```grid_graph``` also has the ```periodic``` optional argument. They may be equivalent for what you want... but ```grid_graph``` can do an n-dim lattice.
comment
It creates a "periodic domain"... that's the name used, for example, when solving differential equations when the independent variable cycles back on itself (going around a circle or using an angular variable).  Also, we couldn't come up with a better name for it. :)
comment
We are not supporting Python2.7 anymore so it is fine to switch to the ```collections.abc``` namespace. We should not use the ```six``` package.
comment
With the release of NetworkX 2.2, we no longer plan to support Python2.7. That doesn't necessarily mean we start removing support for 2.7, but it could. We should figure out what we're doing with 2.7...  
comment
There us nothing (in the latest version) on line 611 that would give this kind of error.  Check your version using ```print(nx.__version__)``` or similar... and let us know.
comment
Thank you -- that helped me see what I was missing...  You added this to an existing Issue about deprecation of ```is_hold``` and ```hold```.  So I missed that your concern is deprecation of ```is_numlike```.  It's probably better to open a new Issue, but this will do fine.  Luckily, changes to address this deprecation are in #3174 and fixed in #3179  This was after v2.2 and will be in v2.3.
comment
closed by #3256
comment
That seems good to me.  Here's a plug for a project related to this stuff that needs some energy: -- there is a package called GraVE which integrates ideas and code from Matplotlib and NetworkX to ease and enable better drawing.   https://github.com/networkx/grave 
comment
That's almost certainly a name collision between the approximation namespace and the connectivity namespace. Don't have time to look now. But adding the approx namespace to the whole broke a lot of links... so I suspect a namespace collision.
comment
I think protecting people against using hashable-dicts as attribute dicts is better done by documenting that they "don't do that!"....  :)  We're adding a fair amount of argument checking of node types for everyone using ```add_nodes_from``` in order to protect the few people who 1) want to use hashable attribute dicts, 2) want to use ```add_nodes_from```, and 3) want to specify the hashable attribute dicts as part of the ```add_nodes_from``` input.     I haven't checked the time difference for normal users, but I prefer that in the description for ```node_attr_dict_factory``` we warn people that if they make their dicts hashable, they shouldn't use ```add_nodes_from``` with those dicts.  Indeed, maybe it would be better to tell them that if they make their dicts hashable, they should subclass to redefine ```add_nodes_from```.  I'd like to hear other's opinions on the ```node_attr_dict_factory``` addition and whether we should protect users from themselves. Perhaps I'm being too rigid... 
comment
find_cycle uses a depth first search for the cycle. It does not guarantee that it will find the cycle you want. It takes one fork of the branch in a deterministic, yet arbitrary way.  I'm not sure what you are looking for long term, but you could either try  1) to construct the graph so that it chooses the correct path (it generally follows the fork with the earliest added edge -- you can tell the order by viewing gcopy[path[1]] to get a list of the successors).     2) you could use ```nx.find_cycle_basis``` and examine which is the shortest and contains the starting node.  There may be [a different cycle oriented function](https://networkx.github.io/documentation/latest/reference/algorithms/cycles.html) that could give what you want.
comment
Thanks!   It looks like it has been updated on more recent releases. https://networkx.github.io/documentation/latest/reference/readwrite/graphml.html  
comment
Be careful... The definition of subgraph is an "induced subgraph". That means ALL edges between the nodes selected are included and must appear in the isomorphic image graph.    So, the graph made up of a ring 1-2-3-4 with additional edge 2-4 does NOT have the ring as a subgraph.  The edges of subg are isomorphic to a subset of the edges of G, but subg is not an "induced subgraph" of G.  I believe your example should be returning "False". No fix needed.
comment
If you know E' and V' then you can make a new graph:  G=nx.Graph() G.add_nodes_from(V') G.add_edges_from(E')  But that is not the same as looking for a dual-subset (V', E') that is isomorphic to some other graph. I haven't seen any algorithms that do that. But they may exist.   On Sun, Nov 4, 2018 at 9:26 PM, hhzhang123 <notifications@github.com> wrote:  > Be careful... The definition of subgraph is an "induced subgraph". That > means ALL edges between the nodes selected are included and must appear in > the isomorphic image graph. > > So, the graph made up of a ring 1-2-3-4 with additional edge 2-4 does NOT > have the ring as a subgraph. The edges of subg are isomorphic to a subset > of the edges of G, but subg is not an "induced subgraph" of G. > > I believe your example should be returning "False". No fix needed. > > Thank you very much! If the definition of subgraph is an "induced > subgraph", it is fine. > But i really need the subgraph： > G=(V, E)， G'=(V', E')，V' ∈V，E'∈E，then G' is the subgraph of G. > > Doew networkx has any api to do that? > > — > You are receiving this because you commented. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/3218#issuecomment-435736614>, > or mute the thread > <https://github.com/notifications/unsubscribe-auth/AA32XQcvpG4VJdLUrWoQYy3mSR-DWatmks5ur6HjgaJpZM4YLQ7Y> > . > 
comment
Thanks!    - you reference the BA paper. Does that have the dual in it? Any reference for the dual? - could you add some tests in e.g. ```tests/test_random_graphs.py```?
comment
:) I don't think you need to put a manuscript out there in order to cite it here.  But when you do publish it, it'd be great to come back here and add it to the documentation. :)
comment
It looks like a second function ```multi_...``` slipped into the last commit.  It needs docs and tests. Or you can pull it out and add it in a later PR.  You can revert a commit and push to your branch with the --force option. You can also make a new commit that erases parts you don't want. 
comment
Thanks very much!!
comment
Oops.. Sorry about that -- I didn't check the docs.  Open a new PR I guess.  The way to add this function to the docs is to edit the file:  doc/reference/generators.rst Find the section for the module where your function is and add it to the list of functions.  The Sphinx doc making program automatically generates the rst files from the docstrings -- but you have to list which functions to include.  To run this on your local machine you'll need to ```pip install sphinx``` or conda or whatever package system you use. Then in the doc/ directory use ```make html```. The result is put in doc/build/html  If that is any trouble at all let me know and I'll make the change.
comment
This looks good.  Can you put in a test or two to make sure we keep this (now correct) behavior? I haven't looked at the tests carefully in a long time, so if its not easy just say so.
comment
Excellent -- Thanks very much!
comment
fixed in #3185 3185
comment
We even have a node factory variable set up to do this: ```node_dict_factory```. We just don't use it. And I'm pretty sure we did used to use it.  It is certainly intended to be used.  I consider this a bug -- certainly doesn't agree with the docs that describe it.  We haven't got a factory for ```self.graph``` and maybe we should. It's only ever created once (there is only one such object) and it can be overwritten easily as ```G.graph=mydict()```. But for consistency I guess we can make a factory for it part of the class.
comment
Ahh... got it...    so something named like:  ```node_attr_dict_factory``` would love to simplify some of that code in ```add_node```, ```add_edge```, etc too. :)
comment
I'll try to start a more complete comment here in the main conversation. There are many things I like about the proposed changes. But it changes  much more than the PR was intended to do. The main focus was to allow  a custom node attribute dict factory similar to that for ```edge_attr_dict_factory```. Also a graph attr dict factory seems reasonable.  Some of the "extra" changes break/change functionality, some are simply style changes without obvious benefit. I will focus mainly on the things I'd like to see remain the same rather than on the large number of good changes. That is unfortunate in terms of the tone here, but I think it most efficient. Don't take it the wrong way...  The doc changes look good.  Leave the assignments (that seem to do nothing) which move the class variables that hold factories to instance variables (L293 in digraph and similar in graph).  Leave the node checking with ```self._succ``` and ```self._adj``` rather than shift to ```self._node```  Leave the code that adds the node in ```add_nodes_from``` rather than pushing it to ```add_node``` If ```add_nodes_from``` becomes a loop with calls to ```add_node```, we might as well remove it. Users can do that.  Leave the code in ```add_edge``` that adds a new edge_attr_dict... that was optimized -- not sure it still is of course, but those changes require more testing and should be in a different PR.  Leave the code to add edges in ```add_edges_from``` rather than off-loading to add_edge.  Leave the code in ```copy``` unchanged.  Thanks very much for this!
comment
Thanks... This makes it easier for me... I restarted the hung test.
comment
NetworkX does not make use of the subgraph/cluster capabilities in dot files.  
comment
I'm confused.   1) Your example has no self-loops. And I don't see how handling them at the start saves time. 2) Your idea to switch to ```sccG``` instead of ```subG``` is only used for looking up neighbors as in ```sscG[startnode]```.  But looking up neighbors is the same speed for sccG as it is for subG. The speedup occurs when we compute the strongly connected components, but that already uses ```H = subG.subgraph(scc)``` so we've saved that time already.  Are you seeing a speedup? Where is it coming from?
comment
Ahh...  got it.  By throwing out length 1 scc you reduce the number of sccs a huge amount when there are directed paths.   But that made some self-loops not appear in the tests, so you added code to yield them.  So it's the handling of singleton components that speeds up the code.  And I am mistaken about the ```sccG[nextnode]``` as well. I was getting confused between connected_component (which is for Graph) and strongly_connected_component (which is for DiGraph).   Thanks for helping me straight it out.  In my poking around, I notice that one of the tests:  ```test_simple_cycles``` on line 62 checks for all entries in cc to be in ca.  But it doesn't test that all entries in ca are in cc.  Could you add a line after the loop with something like: ```assert_equal(len(cc), len(ca))```.  Similarly for line 93 and maybe other places.  Thanks for the speed-ups!!
comment
I think I meant L83... and you found that one.  Sorry about that....  This looks ready to merge.  Anything else?  
comment
Yes -- this is the right place -- you can ask for someone to review the PR in the upper right. But @ericmjl has already done that -- probably in response to the Issue you created.   My first impressions about this change are that it introduces an awkward API for a fairly special case. But I'm open to discussion about that. The API has two ways to specify the target. Maybe we should split this into two functions the way the shortest_path functions got split out: the existing API for single targets and a new function for multiple targets case. Before doing that we should make sure that this is not just me not seeing the API correctly.  Can you describe the API in a simple way? How would you tell someone how to use this function?
comment
I'd like to handle the singleton vs container input question the way we handle it in other parts of the code (like ```nbunch_iter```). We check if the argument is in the graph. If it is in the graph we assume it is a singleton and put it in a list. After that "check" the code can be written for containers.      if target in G:         target = [target]  Then I'd like to keep the keyword name as "target" but make it clear in the documentation that it can be a container of nodes. The idea there is that "a target" does not have to be a single node. Just as an archery target includes many concentric rings -- "I hit the target" means one of the rings was hit.  It also makes backward compatibility more straightforward. :)  Does this make any sense?
comment
Great!   Thanks --    You could use ```node``` as the name...   or ```t``` ?? But using ```targets``` internally seems to read OK too.
comment
This code looks good. And you've got the docstrings updated which should be all we need for documentation.  Can you add some basic tests to the file tests/test_simple_paths.py? Simple short tests of key functionality is what we're aiming for.   I think that is all that's left before we merge.  Thanks!
comment
We usually do put all the tests related to a single construct (ok... and with the same setup required) into a class named TestAllSimplePaths.  But obviously it is not always consistent across the package. So, if you want to refactor the tests that's fine, but you can leave as is too. 
comment
Thanks!
comment
It looks like their suggested fix is to use ```isinstance(alpha, numbers.Number)```. We don't need ```cb.``` in front of ```isinstance```. We do need the ```numbers``` package.  Thanks for this...  I'll put the label for "Needs PR"
comment
hee hee -- looks like we were both working on this at the same time.  (see #2984)  What do you think?  My fix specifies the order of the columns in the doctest. Your change uses an OrderedDict so that the default column order is the same as expected. I believe the DataFrame can also be created with a list instead of a dict to constrain the order.
comment
I think the new error indicates that OrderedDict will be harder to maintain long term. And I also appreciate your sense of aesthetics about having source and target printed first in the examples.  I'll put the order as you created it (source, target, cost, weight), but not add dependence on OrderedDict in this part of the code.  I suspect that in N years (as N -> infinity) we won't have OrderedDict anymore.
comment
see #2984 
comment
Does ```seen``` need to be a list in this function? It might speed it up quite a bit if a set is used. I guess that would require node_cut to be a frozen_set. And maybe it is irrelevant -- I haven't done any profiling.  If I understand correctly, the change that fixes the algorithm is to move out of the loop over antichains the code that puts x and v in the auxillary graphs. I don't know the algorithm well, but:  once we yield a node_cut do we need to consider any other antichains for those values of x and v? That is, is it possible to have another antichain produce a node_cut with that same value of x, v, H and R?  Thanks for this -- it helps a lot.  
comment
Yes -- the time seems to be mostly in those two set comprehensions and the graph lookups.  All the suggestions here cut the time t less than half.  Not a huge savings, but given the now longer time maybe worthwhile...  I think the first set comprehension is taking time because ```in antichain``` is slow when ```antichain``` is large.  We can invert the ```cmap``` to make that faster. The second set comprehension is also slow -- partly because of using the ```predecessor``` function but partly because set.update is faster than a nested for loop.  Because this is such an inner loop, I suggest we replace ```predecessor(n)``` with ```._pred[n]```.  I know we could use ```.pred[n]``` (which is a read-only version of _pred) but it might be worth using ```_pred``` here.  A couple of other things get about 5-10% improvement:  ```H_Reversed``` is only used once and then only to do a lookup:  ```H_reversed[u]```.  We should just use ```H.pred[u]``` and not create ```H_Reversed```.  Also, both H.pred and H.nodes can be sped up by pre-loading them before the loop over ```X```:  ```Hnodes = H.nodes``` and ```Hpred = H._pred```      # just after cmap created     inv_cmap = defaultdict(list) .   # at top have to:  from collections import defaultdict     for n, scc in cmap.items():         inv_cmap[scc].append(n)  ============      # replace the first set comp     # S = {n for n, scc in cmap.items() if scc in antichain}     S = set([])     for scc in antichain:         S.update(inv_cmap[scc])          # replace second set comp     #S |= {x for n in S for x in R_closure.predecessors(n)}     moreS = set([])     for n in S:         moreS.update(R_closure._pred[n])     S.update(moreS)  ===========      # here's the line with H_Reversed replaced     cutset.update((u, w) for w in Hpred[u] if w not in S)  ### Full patch  Everything included the patch becomes:   (is there a better way to show a patch here?)  ``` @@ -2,6 +2,7 @@  """  Kanevsky all minimum node k cutsets algorithm.  """ +from collections import defaultdict  from operator import itemgetter  from itertools import combinations   @@ -101,8 +102,6 @@      # for node connectivity.      H = build_auxiliary_node_connectivity(G)      mapping = H.graph['mapping'] -    # Reversed H for helping finding cut. -    H_reversed = H.reverse()      R = build_residual_network(H, 'capacity')      kwargs = dict(capacity='capacity', residual=R)      # Define default flow function @@ -122,6 +121,8 @@          seen.append(X)          yield X   +    Hnodes = H.nodes  # for speed +    Hpred = H._pred  # for speed      for x in X:          # step 3: Compute local connectivity flow of x with all other          # non adjacent nodes in G @@ -151,6 +152,9 @@                  # residual flow network R and call it L.                  L = nx.condensation(R)                  cmap = L.graph['mapping'] +                inv_cmap = defaultdict(list) +                for n, scc in cmap.items(): +                    inv_cmap[scc].append(n)                  # Find the incident nodes in the condensed graph.                  VE1 = set([cmap[n] for n in VE1])                  # step 7: Compute all antichains of L; @@ -166,21 +170,27 @@                      # define a node partition of the auxiliary digraph H                      # through taking all of antichain's predecessors in the                      # transitive closure. -                    S = {n for n, scc in cmap.items() if scc in antichain} -                    S |= {x for n in S for x in R_closure.predecessors(n)} +#                    S = {n for n, scc in cmap.items() if scc in antichain} +                    S = set([]) +                    for scc in antichain: +                        S.update(inv_cmap[scc]) +                    moreS = set([]) +                    for n in S: +                        moreS.update(R_closure._pred[n]) +                    S.update(moreS) +#                    S |= {x for n in S for x in R_closure.predecessors(n)}                      if '%sB' % mapping[x] not in S or '%sA' % mapping[v] in S:                          continue                      # Find the cutset that links the node partition (S,~S) in H                      cutset = set()                      for u in S: -                        cutset.update((u, w) -                                      for w in H_reversed[u] if w not in S) +                        cutset.update((u, w) for w in Hpred[u] if w not in S)                      # The edges in H that form the cutset are internal edges                      # (ie edges that represent a node of the original graph G) -                    if any([H.nodes[u]['id'] != H.nodes[w]['id'] +                    if any([Hnodes[u]['id'] != Hnodes[w]['id']                              for u, w in cutset]):                          continue -                    node_cut = {H.nodes[u]['id'] for u, _ in cutset} +                    node_cut = {Hnodes[u]['id'] for u, _ in cutset}                        if len(node_cut) == k:                          # The cut is invalid if it includes internal edges of ```
comment
It is much better to have the code that doesn't skip those antichains. I think this is just a hard problem. The patch I put together is not likely to make this useable for graphs it takes too long for -- just speed up the ones we can already do.  There is more work to do here in terms of looking for algorithmic shortcuts and so forth. But I think this fulfills the goal of correcting the missing cuts, even though the time is longer.
comment
Nice layout code for the comment!  I didn't know you could hide details. :)  Yes, I agree that the next place to look for speedup is the antichains. Also, none of this is helping with the complexity -- that will take more thought!  I think this is ready to merge.  Anything else?
comment
Yes -- you are correct.  Thank you!!
comment
Looks good so far.  Can you add a test to make sure we know if we break using names later?
comment
Thanks!  This is very helpful...
comment
In fact, NetworkX stores node, edge and graph attributes in an associated dictionary. I'm not sure why you think they are stored as a list. But they aren't.      G.graph['graph_attr']     G.nodes[n]['node_attr']     G.adj[n][nbr]['edge_attr']     # adjacency structure gives access to edge attribute dicts     G.edges[n,nbr]['edge_attr'] 
comment
The decorator is ```open_file``` (it appears in the line before the function signature ```def ...``` with an ```@``` in front.  The decorator is defined in ```networkx/utils/decorators.py``` and opens the file if needed, or passes the file object through otherwise.  You have clearly found a work-around for the error. But it'd be nice to fix this...  In looking at both this code and the code for pygraphviz, it looks like we should have the decorator specify mode ```'w+b'```.  But I'm not close to machines that can easily test whether this works for the different cases and versions of OS/python/etc.  Can you easily verify that changing the 'w' to 'w+b' in the ```@nx.utils.open_file``` decorator fixes this bug?
comment
Thanks -- but it looks like more is needed. The docstring says that a DiGraph is returned. But clearly two objects are returned... a DiGraph and a root object.
comment
Thanks very much!
comment
Thanks for this! I'm worried that ```next(iter(``` won't return the same ordering on the nodes. I know we talked about whether using ```min``` was a way to arbitrarily select a node -- and decided it was. But doesn't that arbitrary choice have to be the same if we run into the same set later?  Maybe it doesn't, but I can't tell from a quick look at the code.  Another way to ensure a consistent ordering of nodes is to create an ordering dict using something like: ```node_order = {n: i for i, n in enumerate(G)}``` Then ```set_min``` becomes      # perhaps this is short enough to put the code inline?     def set_min(node_order, nodes):         return min(nodes, key=node_order.get)  What do you think?
comment
I'll dig into the code more this weekend.  Question: if we don't have to keep the order the same, then we shouldn't be using any ```min``` function at all.  Just ```nx.arbitrary_element``` or ```next(iter(```.  Would this work?
comment
Thanks @chebee7i  !!  The doctring is pretty clear about needing nodes with a total order.  It even talks about relabeling to obtain ordered nodes.  Sorry about not reading that! :{  In addition to the quote shown above from [1], the article [2] also clearly requires a total ordering of the nodes.   So... should we rely on the docstring to tell people how to relabel? Or should we build in an ordering? We could create one using the order of ```G.nodes``` and adapt ```min(someset)``` to use that ordering.  The cost in time would be 1) creating the ordering dict, and 2) requiring ```min``` to use a comparison function rather than finding min of the nodes themselves. The benefit would be easy handling of nodes that aren't ordered.  Am I missing any costs or benefits here?  
comment
I think we can hide the total orderable requirement from the user without changing the mapping/nodes, etc. We change the ```min``` function so it can find a min from a total ordering.      ```min(T2_inout)``` becomes ```min(T2_inout, key=total_order.__getitem__)```  The nodes don't change. So there is no issue with making the internal mapping available. That maps nodes, and doesn't care about ordering. The only place where ordering seems to matter is when generating candidate pairs. As you mention above, it is important to avoid considering equivalent pairs multiple times. The total order does that.  Performance: I tried to check performance. Performance in Python often seems to be counter-intuitive. Specifying the ```key``` argument in ```min``` did indeed seem to slow it down about 1% for graphs with 100 to 1000 nodes if the nodes were added in order. BUT if I shuffled the node numbers so they weren't ordered integers, the ```key``` argument method was about 1% faster. Presumably finding the min is slightly faster if the container is already ordered low to high. The vf2 code iterates over nodes from the graph and my ```total_ordering``` dict was based on the iteration order of the graph. So I conjecture that it is slightly faster to do ```min``` with a ```key``` that matches the iteration order of the nodes in the container, then to do ```min``` without a ```key``` on a container that doesn't have its iteration matching the desired order.  None of the changes I tested made a difference in speed of more than 2%. And no version was consistently ahead of the others.  Even stranger, I went ahead and implemented a version that didn't use ```min``` to sort the nodes. It just pulls the first node out of the container based on iteration order. I hoped that would show an example where the order mattered. It hasn't... and I have run it many times on many different size graphs.  Perhaps iteration order is sufficiently reliable for selecting the same element that it doesn't break the algorithm. (I was checking that the list of isomorphisms is equal using vf2 with ```min``` and using vf2 with ```next(iter```. The list of isomorphisms always was equal.)    Back to timing -- I did not expect that the arbitrary element method was not faster(!) than computing the ```min```.  Starts making me wonder what takes time in Python.  Perhaps calling ```next``` and ```iter``` requires lookup time that cancels iterating over 400 nodes?    I also was surprised to find that changing       T1_inout = [node for node in G1_nodes if node in self.inout_1 if node not in self.core_1]  to       T1_inout = [node for node in self.inout_1 if node not in self.core_1]  Actually slowed down the code about 2%.  It seems to me that the second should be faster.  Optimizing Python code is hard to do...  Bottom line: We should probably use the iteration order of ```G.nodes``` as the total ordering for nodes in the graph. It doesn't require orderable node objects. It doesn't consistently take more time -- seems to depend on whether the graph iteration ordering matches the node object ordering.  Time difference is about 1% either way.  It does cost memory with one dict keyed by node to integer.
comment
@wbernoudy does this variation of the changes do what you need it to for your use-case?  If so, maybe we can merge it before the upcoming release.
comment
Your ```largest_hub``` object is a list of nodes. The ```ego_graph``` function [expects a single node](https://networkx.github.io/documentation/latest/reference/generated/networkx.generators.ego.ego_graph.html#networkx.generators.ego.ego_graph) so the square brackets around your long node name mean it is actually looking for the LIST as a node... note the string you think it is looking for.
comment
It is not a single node if it is a list. A list cannot be a node because it is not hashable. Try ```hub_ego=nx.ego_graph(g_problock,largest_hub[0])```
comment
Please make comments like this in the PR itself, not in another new issue.
comment
Thanks!
comment
Your pointer to docs is for Version 1.10. The stable release is v2.1 and we're close to releasing v2.2.  The [documentation aligns with the actual behavior](https://networkx.github.io/documentation/stable/reference/algorithms/traversal.html) where ```dfs_*``` routines return dicts and ```bfs_*``` routines iterate over ```(node, result)``` pairs. 
comment
It is a little strange that the two sets of functions return different types of objects. Seems like in a better world they would both return the same thing -- perhaps the generator version because it is easy to build the dict from the generator. There may be an efficiency reason they are different, but I don't recall.
comment
This looks great -- thank you very much! Can you make sure the new source satisfies PEP8 standards? That most often means checking that the linewidth is 79 chars or less. But there are other potential issues. You can use ```pip install pep8``` or ```flake8``` or maybe others.
comment
No reason I know of that we don't check flake8 in CI except we haven't included that.
comment
See #2861
comment
Should add tests for calling it with an empty graph and calling it with a disconnected graph. (this will make the coverage of the tests pass the codecov standards).  Also we need to add it to the ```__init__.py``` and ```/doc/reference``` pages.  I will add those tests and others soon if you don't get to it, so no worries...   Thanks!
comment
Whoops... there are other issues here we need to address.    The license is not the same as the BSD license we give the rest of the code.  Can anyone say anything useful about the compatibility of these two? Do we need to switch the license or can the two live side-by-side?  There aren't any tests yet.
comment
Yeah -- that'll teach me not to look before questioning....  its the same as license we use.  Sorry for the confusion...  Thanks very much in advance for tests... :)
comment
It looks like you are trying to use ```unittest``` for the tests... You should remove that in favor of using ```nose``` (though ```nose``` is not being activately maintained so we will likely have to move to ```pytest```).  But for this PR use ```nose```.  The tests should be put in a separate file ```tests/test_second_order.py``` or similar. Each folder of code has a ```tests``` subfolder.  Look at the other test modules in ```tests``` for examples. You need to test the special cases as you are doing, but it would also be good to test the main algorithm with some known cases. The docstring does one such example -- perhaps there are others?  If it's too much trouble to get that all working, I can add the tests to make codecov happy. 
comment
That looks better -- you are passing the tests when numpy is installed.  We need to skip these tests when numpy isn't present. Take a look at other modules (e.g. ```current_flow_betweenness.py```) to see how that is done.  You've got the SkipTest code in the test class but not in the module itself. I think it is an easy fix.  I haven't looked at the actual tests yet
comment
Don't worry about the 3% on the coverage. It is low because of the checks for when numpy is not there. Those checks have to be there but coverage complains. We can override and merge even with that failure.   Why did the Florentine test fail? Is there an example where the second order centrality is known? 
comment
Looks good -- anything else to change before I merge? I'll look through one more time before I do.
comment
Sorry not to have raised this earlier: Could you change the name of the new module so that it is not the same as the function? -- avoiding potential namespace collisions.    Given the other names in the centrality folder, maybe change the module to ```second_order.py``` and similarly to tests file. The ```__init__.py``` entry will also need to be changed, but the documentation file (```centrality.rst```) should be fine since it is pulling from the centrality module and not the second_order file.  Thanks!
comment
One more thing -- is ```deepcopy``` really needed in ```_Qj```? Unless it is a matrix of containers, (which I think doesn't make sense here) could you use ```P.copy()```?
comment
Thanks for this!  Just a couple comments -- mostly about getting the function integrated with the whole networkx API.  - You should include the new function in the ```__all__``` variable defined at the top (to have it imported to the networkx namespace. - You should add "See Also" sections to both rescale function's doc_string to refer to each other - You should update the documentation page ```doc/reference/drawing.rst``` by adding the name of the new function to the list of layout functions - You should add a few simple tests to ```networkx/drawing/tests/test_layout.py``` to make sure this function runs and gives reasonable answers. - The first line defining ```pos_v``` unpacks the position tuple only to repack it as a tuple again. Could do:  ```pos_v = np.array(pos.values())``` or just remove the line and in the next line replace ```pos_v``` with ```np.array(pos.values())```. 
comment
I believe the problem is that ```key``` is not an attribute of the edge. Unfortunately, we use ```key``` as the variable name for the third argument of the method ```add_edge```. So, your code is not adding an attribute to the edge. Use ```G98.edges(data=True)``` to see the problem.  See also #1583 for more discussion of this "feature".  I suggest not using the "key=" syntax. Instead use a 3-tuple-like argument for multiedges:      G98 = nx.MultiGraph()     G98.add_edge(1, 2, 1)     G98.add_edge(1, 3, 5)  Or, if you really want an attribute to hold that value, name it something other than ```key```.
comment
Are you able to make the characters show using ```plt.text```? NetworkX is using matplotlib's text feature to draw the characters. I think the next step in debugging is to figure out whether the problem lies in the config for NetworkX or in Matplotlib or in something deeper.      plt.text(0,0,'雨')     plt.text(1,0,'風')     plt.text(2,0,'⿱')     plt.show()
comment
I dont know of anything we can do with NetworkX to fix this.
comment
This look good!   I made some comments connected to individual lines of code.  You should also bring the code style up to the PEP8 standards. Its easy as ```pip install pep8``` and then ```pep8 euler.py```.  Most of the trouble is typically spacing issues and 79 char limit on length of line.  You can also read the [PEP8 style guidelines in their official form](https://www.python.org/dev/peps/pep-0008/?).   Also, the single-use function ```_simple_eulerize_simplegraph``` can be eliminated for simplicity by removing lines 230-233  The Codecov check (click on "Diff" heading of the details to see how Codecov thinks your tests are covering the code) is basically asking you to test the two NetworkXError cases.  It also asks you to test a case where ```n==m``` (which I think can't happen since you used ```combinations``` to create the data.  You should probably add simple tests that input a MultiGraph with multiedges to check that it is handled the way you want.  Just a simple one like a triangle with an extra edge would do it I think -- but maybe there's more there to test.
comment
Yes, lets make it an exception like the one you suggest.  Probably doesn't matter much, but an exception can be caught and is more likely to notify someone of a bug in their code than silently returning an empty graph.    :)
comment
What description of the Bellman-Ford algorithm is this based on when it was rewritten recently? We need to make sure it is really bellman-ford and not a close substitute.
comment
Does the cutoff argument make sense for Bellman-Ford since we allow negative weights? Should that be removed too?
comment
I think we should remove it completely. It will fail noisily so no surprises, and checking at the end doesn't save any time or resources, so is a pointless concept.  I'm also worried that this isn't the Bellman-Ford algorithm.  It seems to have been changed in a major way from the initial code a number of years ago and I want to check if there is a variant of the algorithm that this is following. If not, we should go back to a method with a provably correct solution in finite time -- or a negative cycle.
comment
Yes -- Please keep it with comments describing the variant of Bellman-Ford.  And your other suggestions for handling single node and multiple nodes in all these algorithms make sense too.  Good work!!  :)
comment
This looks good.  As far as I can tell you've covered all the suggestions made above. Are you ready to merge this?  And Thanks again.
comment
Yes - I agree that we should remove the tests for import numpy/scipy. We can just import and any error message will be pretty clear. Let's do that.
comment
In the past, people have asked that the layout functions all supply arguments for the center of the layout and the scale of the layout. This could be helpful, for example, when wanting a placement of multiple networks in the same figure.   This API asks for a width and height. Is there a way to make that work with a center and scale?  Maybe we would need: center, scale and aspect-ratio. There might be a better way...  Ideas?
comment
I get them both returning the value as the 3d entry in the edge tuple. 
comment
When I use that code, I get different output. Can you try      import networkx as nx     print(nx.__version__)     myg = nx.DiGraph()     myg.add_edge(0, 1, type=10)     print(myg.in_edges(data='type'))     print(myg.out_edges(data='type'))     print(myg.in_edges([1], data='type'))     print(myg.out_edges([0], data='type'))
comment
That's what I wondered.  You probably have NetworkX 2.1 install on one installation of python and NetworkX 1.11 installed on another installation of python. Make sure you are using the installation of Python that you think you are. Each one has its own set of installed packages.
comment
Your subgraphs are SubGraph Views.  These are similar to the ```dict``` views: ```keys/values/items```.  They cannot be pickled.   But the subgraphs themselves are graphs. So if you change from a View to a Graph structure it should work. The easiest way to do that is to copy the view: ```SG = G.subgraph(nodes).copy()``` In your case:      subgraph = graph.subgraph(["test1", "test2"]).copy()  A similar issue is true for the ReverseView data structure. You can't pickle the view, but you can pickle a copy of the view.
comment
1)  If your code uses a random number generator then the function should accept an argument ```seed``` which should be processed using the decorator ```@py_random_state(<index>)``` or ```@np_random_state(<index>)``` where ```<index>``` is the index of the ```seed``` argument. Use ```py_random_state``` if your code uses Python's ```random``` module.  Use ```np_random_state``` if your code uses Numpy's random module.  Then in your code when you want a random number, use ```seed``` as the random number generator (RNG).  For example:   ```seed.random()``` or ```seed.gauss()```.  The decorator is imported using  ```from networkx.tools import py_random_state``` or similar.  Note: if your code doesn't create random numbers but calls a function that creates random numbers, then you must still include an input argument for the seed processed the same way as above and then passed into the function that creates the random numbers. This is the only way to make sure that random numbers don't affect the global RNG if the user passes in a ```seed``` parameter.  2) When testing, it is probably a good idea to set the seed value. Then it is consistent from one test to the next. Ideally, you could test it with multiple different seed values that as a whole cover all the code. But sometimes that is very difficult to arrange. If its troublesome then don't worry too much about coverage.  3) I think it would be better to shrink the size of the tests to speed them up unless there is a specific trait of the large graphs that we want to test.
comment
Also, since this is a new module we need to update the ```/doc/reference``` files so they find and include the function in the docs.  I will try to make this and the above updates if you dont get to it soon. So no worries.    Thanks again for this!
comment
Awesome.  I also noticed that your module is named the same as your function.  Can you change the name of the module to something different (and slightly more general in case other functions get added)?  This name-change leads to small changes in the rst and __init__ files which are kind of a pain, but we don't clobber the name spanner that way.  Perhaps ```spanning.py```, but maybe there is a better name?
comment
I like ```sparsifiers.py``` as a general term with plural to hopefully indicate long term multiple approaches.  Thanks!
comment
Is this ready to merge then?
comment
That's a blunder...  probably happened when we alphabetized the list a little while ago.
comment
This looks good!  Thanks.  But ```math.isclose``` doesnt work for python 2.7 so can we make it      if abs(alpha + beta + gamma - 1.0) < 1e-9:
comment
Great -- It looks like the tests are NOT testing this well.  I'll try to add to that today before merging.  (unless you want to try that) Thanks!
comment
Thanks!
comment
Thanks for pointing this out! I agree that we need a fix here -- either raise an exception when order doesn't match or reorder the columns to match the rows.  I would lean toward your **Approach 1**. Do you see advantages of the second approach?  I guess my question is also getting at: are there backward compatibility issues we need to worry about here?  Thanks!
comment
This looks good -- Put together a pull request for Approach 1. Thanks very much!
comment
The number of nodes after ```compose_all``` should not be more than the sum of the number of nodes of the graphs you are composing.   ```compose_all``` should return the union of the node sets and the union of the edge sets.  Your code here is missing a parentheses but maybe your real code isn't  But induced subgraphs are not as you describe. Induced subgraphs keep the specified nodes and keep **all** edges between those nodes. If you want a subgraph that is just the path itself, use ```nx.path_graph(path)```
comment
This "feature" is documented in both networkx and pandas. It involves ```DataFrame.values``` using a flexible type for numeric values.   From [the docstring for](https://networkx.github.io/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html) ```nx.from_pandas_edgelist```:      Note: This function iterates over DataFrame.values, which is not     guaranteed to retain the data type across columns in the row. This is only     a problem if your row is entirely numeric and a mix of ints and floats. In     that case, all values will be returned as floats. See the     DataFrame.iterrows documentation for an example.
comment
That's the way to iterate over the matrix of values in the dataframe. Other suggestions? 
comment
You shifted from testing that it worked to testing its speed.  But assuming all the above works, could you put together a PR with the improved version?  Thanks! 
comment
Unfortunately, ```type(self)()``` does not work for graph views (subgraphs, etc). So this fix will break many parts of NetworkX.  The better approach is to redefine ```fresh_copy``` in the new subclass:      class X(nx.DiGraph):         node_dict_factory = collections.OrderedDict         def fresh_copy(self):             return X()  If there is a better class inheritance structure that would get around the need for ```fresh_copy``` I would like to hear about it. Currently, the Views are subclasses of the base Graph classes. 
comment
Thank you for this direct question on this issue -- I believe many subclasses will only need to overwrite ```fresh_copy``` to return an empty version of itself. The Views will often work. But it depends what you change when building your subclass.  Presumably you aren't changing how edges or nodes are reported, but perhaps you are. In this case, all bets are off and you may have to rewrite the whole class -- or much of it (see e.g. MultiGraph for an example).  Most of the time you are adding features beyond what is offered for Graph and leaving the graph theory aspects of Graph the same. Then overwrite ```fresh_copy``` should be enough.  If the parts of GraphView that overwrite Graph methods are going to be changed, then you would have to subclass the GraphView and SubGraph class as you describe.  Unfortunately I don't think there is a single answer to your question. It depends on what features of the Graph class you are changing. Still, I think most cases that leave the graph theory parts of Graph the same will only need to replace ```fresh_copy```.
comment
There are some reasonable ideas above -- but tricky to implement. I'll take a look at it in more detail. If you have code/ideas/tests that would be helpful.
comment
Thanks for this.  The new GraphViews should allow subclasses more easily now.  Try it out!
comment
Yes, that would be helpful!  [Edited:  sorry my previous comments were about a different issue. I got confused]  Please add an argument named ```depth_limit``` as the last argument to the function. The function signature is then: ```bfs_tree(G, source, reverse=False, depth_limit=None)```  Look around at other code for example, the github networkx wiki for odds and ends and ask questions here.  Thanks very much!
comment
Sorry... I got confused in my previous answer about depth_limit for ```bfs_*``` functions. Please see my edits to that message.  Thanks
comment
Take a look at the issues with the tag: "Good first issue", or "Needs PR". Some are quite involved -- others more straightforward.  Some very interesting ones that will get you reading the literature as well as coding are #1609 and #1457 
comment
@gmyr would you like to take a look at this one (depth limit on breadth-first-search) #2352 we've got a PR but it has been almost 3 weeks since any activity and I think the code would be quick to get right.  Let me know -- if you don't, I will get to it :) 
comment
I can verify that with networkx dev and python 3.6 the same trouble occurs (although it isn't always the same node).  Also, it doesn't happen with the isomorphic ```path_graph(5)``` where the nodes are integers. So the bug has something to do with nodes being strings.
comment
Yes -- and that docstring is really VERY minimal...  It doesn't even say what the input or output arguments are. There may be others in that category like ```G.successors``` but I haven't checked. Pull requests would be helpful...
comment
Hi!  Thanks for this....  I have been too focused on the upcoming release to look at this much yet. I'll try to answer your questions and then dig into it more soon.  Recursion: many of our algorithms start as a recursive method -- but recursion uses a lot of RAM (hence the recursion depth limit). So its better for many reasons to implement a non-recursive method. If you like I always suggest leaving the recursive code there (with recursive in the function name) as it can be used for testing/comparing and for pedagogy.  You can certainly construct your own depth-first-search.  It's not a long piece of code. And there are many possible variants depending on when you want something returned. So we actually have a number of places where variants of dfs show up in the code.  Many many years ago Aric made a version that was very flexible and could allow the user to construct whichever dfs variant they wanted. But it was removed (I think because Aric decided it was harder for the user and for us to maintain than taking the dfs code and changing it to do what you want -- perhaps other reasons that I don't recall too).  Neighbor sorted_list with constant time insertion requires a different data structure. That's fine if there's an easy way to include it and maintain it. There are a few places in the code where we use dict-of-set for the neighbors -- separate from a Graph object. What exactly do you need? Would heapq or deque (from collections) be sufficient? There was a recent post #3057 about using a Fibonacci heap. Perhaps they have some ideas.  If you decide not to create such a data structure then its still worth having this code in networkx.  File structure:  Start by including all planarity functions in ```algorithms/planarity.py```.  We can refactor if needed.  I guess my main question for you would be:  What are the big questions in planarity?  This seems like a good function to have.  Are there basic algorithms used a lot for planarity that we are missing? Low hanging fruit ideas are appreciated and we can mark them as good for people looking for something to contribute (like "Good First Issue").  I don't have much experience with them so appreciate any hints.  Thanks again!
comment
Nice -- I'll put the label "Good First PR" on this issue to see if anyone decides to take on Dual graph.  There is a StackOverflow answer [about linked lists in python](https://stackoverflow.com/questions/2154946/python-linked-list-o1-insert-remove) that suggests if you usually add in the same place, or change that place occasionally, you can get efficient O(1) insertion by using two lists. One "before" the pointer of where to add and one "after". Then to move the pointer, you remove from one list and add to the other. So long as it uses python its acceptable here.  Thanks and looking forward to more. :)
comment
I've only got questions (not suggestions) at this point. Still getting up to speed on the data structure and the algorithm.  It took me a while to figure out that cw and ccw referred to clockwise and counterclockwise respectively. That concept is a little confusing to me. How can you tell which direction you are going if you don't have an embedding to work from?  Is the direction arbitrary for the first edge and once chose determined after that?  That would align with planar graphs' symmetry of flipping your view to "from below" so cw becomes ccw etc.  Maybe in long run a few sentences to discuss cw and ccw could be helpful.  I'm not sure what the difference between half edges and directed edges is. Perhaps more importantly, I don't understand half edges yet.  Are you constructing half edges as you go around a face and then the other half edge comes from the other face and it must have opposite polarity (cw or ccw) to be valid.?.?  I hope that is close to correct...otherwise ignore the rest of this paragraph. :)    Could you use a directed graph in place of this structure -- where directed edges represent half-edges. You cannot have directed edges created where they already exist. And at some point (going around a different face) the other direction of the same edge will show up. So to check for a planar embedding you go around the faces and then check that each edge has the opposite oriented edge.  Is this correct?  Also, why do you need to know the "next node" of the neighbor node in the dict-of-dict structure? You say  > They are dicts that map a node v to a dict that maps each neighbor of v to the next neighbor of v in clockwise direction (or counter clockwise direction).  Couldn't you find the "next neighbor" by looking up the neighbor in the outer dict? Is a face determined by two edges? (three nodes?) I think there is something basic I'm missing here.  Finally, you often mention "constant time"...  I always like to tweak the system by asking "constant as what variable changes?"  Thanks :)
comment
Maybe you can tell me what you mean by a "half-edge".  That might help much of the rest. I'll get there...   :)  As for which data structure to use: I think the second gist you list is probably a better data-structure for this problem as it doesn't carry any of the "baggage" of subclassing graphs. But I leave that decision up to you since you know the algorithm and code the best.  Thanks!
comment
That's great -- it *is* what I was thinking in terms of directed edges.  I like the way it can make finding the faces easy too.  I think the NetworkX DiGraph might be a reasonable structure to store this "Dcel". Indeed, the code below creates such a structure (without the height/lowpt/etc filled in).      # set up graph     G = nx.path_graph(9)     nx.add_path([4,7,9])          DG = nx.DiGraph(G)       # creates two directed (half) edges in place of each undirected edge     nx.set_edge_attributes(DG, None, 'cw')  # set up edge attr to point to cw/ccw reference     nx.set_edge_attributes(DG, None, 'ccw')  # initialize reference to None     (4, DG[4][5]['cw'])    # shows the next half-edge clockwise of (4, 5)     (4, DG[4][5]['ccw'])    # similar for next counter-clockwise of (4, 5)     # make half-edge ((4, 7) just clockwise of (4, 5)     if DG[4][7]['cw'] is None:         DG[4][7]['cw'] = DG[4][5]['cw']         DG[4][5]['cw'] = 7     else:         print("cannot add half-edge. It is already present.")          # You could store the node attributes on the DiGraph too.     DG.node[0]['height'] = 0   I'm not saying it will be better to store all that on a DiGraph -- might not even be possible given all the manipulation you need to do while maintaining the order of the neighbors. But it is possible that it could work.
comment
NetworkX allows any hashable node other than ```None```. There are a number of places in the code where we use ```None``` as an indicator of no input so we had to disallow ``None``` as a node.  :) 
comment
This looks very good... Thanks!  The only thing I notice to check is: what happens if someone gives bad input? In particular, can G be a directed graph or a multigraph? I can imagine raising an exception in this case, but perhaps also simply converting to a Graph is enough to check planarity.  Maybe it would be best to raise an exception and in the docs note that to use it with a DiGraph or MultiGraph, convert to Graph first.  If you want to raise an exception, NX has [a decorator ```@NotImplementedFor('directed')```](https://networkx.github.io/documentation/latest/reference/generated/networkx.utils.decorators.not_implemented_for.html)      @NotImplementedFor('directed')     @NotImplementedFor('multigraph')     def check_planarity(...
comment
This is a good way to handle it.  I'm not sure how I missed those tests.  Looking online instead of in my editor I suspect. Anyway. Is it ready to merge?  I think it looks good to go.
comment
```katz_similarity``` also gives an equation for the vector (actually an equation giving one element of the vector in terms of all the others). But some improvement to the docs seems simple and easy to do. Would you like to take a shot at making the equation for eigenvector_centrality easier to read?
comment
Sounds great!
comment
The edge order is determined by how the ```dict``` stores the adjacency information. For Python v3.5 and earlier: ```dict``` does not guarantee an order, the order of the edges can possibly change from one session to another even though you read from the same file.  For Python v3.6 and later the ```dict``` preserves order. So if you switch to python 3.6 you should see the edges in the same order each session.  The function ```eulerian_circuit``` uses a deterministic method. It will return the same circuit if you call the function the same way.  I'm not sure there is a good algorithm to find all eulerian circuits. Certainly NetworkX does not have that feature. If you find an algorithm and implement it, consider contributing it to NX.  Note that you CAN change the ```source``` parameter of ```eulerian_circuit```. This will change the node at which the circuit starts. You may still end up with the same circuit just shifted.  But for a complicated example with many correct answers, it will likely be different.
comment
Yes -- this is a known issues:  see #2131  The GML spec doesn't allow underscores in attribute names. We allow reading .gml files that don't correspond to the official GML spec. But we write only items that follow the spec.  You should convert your attribute names to not include the underscore.      for n in G:         G.node[n]['userid'] = G.node[n]['user_id']         del G[n]['user_id']  We should also add to the documentation a note about this. 
comment
Oops... That's a mistake in the code I suggested.  Sorry.  The syntax ```f[n]['user_id']``` looks for an **edge** from ```n``` to ```'user_id'```.  You need to delete the node information:      del f.node[n]['user_id']  # similar to line above it in the example.
comment
If you take out the check for ```source in G``` then I think an input ```v``` that ```is not in G``` gives a ```KeyError``` instead of ```NodeNotFound```.  Is that what we want? I guess we should be testing all these errors...:}
comment
Yes - I think it would be helpful to have all those functions give the same type of error when ```source``` is not in the graph.  So I guess that means we should add it to dijkstra and bellman_ford. Can the check be put in the innermost function or do we want it in each outward facing function?
comment
This looks ready to merge.  Anything else planned for this PR?
comment
I added a short description in the release notes for the change in API and I (hopefully) improved the coverage delta target by changing ```elif``` to ```else``` in some places.   So, you can remove the absent-source guards...  When you talk about a potential refactor of the exceptions. What do you have in mind? The #1705 isn't very specific in what to do -- other than remove NetworkX from the exception names. And we shouldn't do that until v3.0 due to potential backward compatibility issues. 
comment
Yup --- That is a bug.  That should be ```adjlist_outer_dict_factory```.  Thank you!
comment
Any resolution here?  Should we change the docs somewhere to suggest trying the optimize version of the function? Should we just close this? What do you suggest?
comment
That's a great summary. Maybe I'll try to put a paragraph "docstring" for the module (at the top) that simply says what you jut said. That way people know a little more which functions to try. I'll take a stab at the difference between optimize_edit_paths and optimal_edit_paths too.  
comment
Try getting matplotlib to work with a simple non-networkx picture. It's most likely a config error for matplotlib.   It looks like your config is trying to use the 'ps' interface (create a 'ps' file) by default rather than drawing to an interactive GUI.
comment
I've put together a pull request to address this API inconsistency. I think it should make it easier to use dijkstra or bellman_ford interchangeably. Any thoughts before I merge that pul request? Thanks!
comment
Well -- I'm sure the behavior was intended for each individual algorithm, but I doubt anyone specifically looked at the fact that it was different for the two algorithms. Thanks for pointing it out! Hope this helps with #2346 
comment
Thanks for this!  The code and the idea to include it looks good.  My quick read suggests this is similar to betweenness centrality only you don't consider all paths, only paths starting from percolated nodes.  Is that correct?  Could you add a paragraph describing the relation between betweenness and percolation centrality?  Along those lines, but from a code perspective: would it make sense to include this in the betweenness module?  You mention "Time" in the docstring.  Can you describe what you mean by that? Is it part of the definition, or part of a setting in which this measure is often used?
comment
This makes sense...  I think we should leave this in a separate module from betweenness. But please add a paragraph (or more) to the docstring about the connection betweenness and percolation centrality. Also, make the time aspect explicit:  this is a measure of a shapshot in time of a typically evolving process as percolation occurs.  This is your chance to get other people to understand and use this measure. So make the description accessible and complete.  I'll go through the code in more detail at some point soon. If you haven't already done so, can you check that the code matches Python's PEP8 code style which you can read about from [the official PEP8 specification](https://www.python.org/dev/peps/pep-0008/?), or use a convenient tool to check it for you ```pip install pep8``` and then ```pep8 filename```  Thanks!
comment
Thanks!
comment
It looks like the adjacency dict-structure uses ```int``` for the nodes and ```int64``` for the neighbors.  Should be fixed...
comment
This looks like it is putting the entire graph into the ```graph``` attribute.   Your initial graph shouldn't use the special attribute ```graph```. Am I right? Does that create a subgraph within the bigger graph? I;m not sure why you have it there.  I certainly messes up reading and writing as you demonstrate.  What if it isn't there? Does the same thing happen one iteration later?  Do you have suggestions for how to handle this?
comment
I added a test to verify that this fix works.  We still add the graph/node/edge attributes to G.graph each time we use ```from_agraph```, but now ```to_agraph``` doesn't add them to the dot file repeatedly.  Thanks! 
comment
Can't you just pip those individual packages? NetworkX will work with those packages if they are installed.   (Also, I restarted the aborted python 3.5 test)
comment
Right -- I understand the trouble with requiring pydot 1.2.3 when pydot3 and pydotplus are your standards (that should be changed however, as the developers of those two packages are now working to make pydot the single package for python3 use of pydot).  We went through similar issues -- first using pydot, then pydotplus and now back to pydot.  How does this change in sphinx help you?  You still can't install with pydotplus, right?
comment
OK... thanks for helping me learn a little more about all of this.
comment
Did you want a 'nose' version in there too?
comment
Thanks!
comment
Thanks for this --  Some comments:  - remove all your imports (```from breadth_first_search``` ) and just use ```nx.bfs_edges``` or ```nx.bfs*```   You can remove the new file from tests that way too.  Go through the nx interface instead of directly    to the files. - leave blank line after your name in contribute.rst file. Also remove the ```:orphan:``` line at beginning. - your line ```step = list(neighbors(source))``` uses the neighbors of ```source```.  That doesn't work for depths greater than 2 -- or am i missing something. see [this stackoverflow question](https://stackoverflow.com/questions/10258305/how-to-implement-a-breadth-first-search-to-a-certain-depth/16923440) - The examples/tests in the docstring are fairly exhaustive. Might be better to include one example and put the rest of the more test-like examples into the test files. 
comment
See #3077
comment
I believe your question is not well-formed.  If you have node positions (an embedding), NetworkX can draw the graph with the nodes in those positions.  So I assume you don't have node positions. If you don't have node positions, but you want each distance to equal the length attribute of each edge, then for almost all cases, the graph can't be embedded in two dimensions.  You need many dimensions.  When networkx draws the map of the US, it starts with the positions of NY and DC and computes the lengths.  It would be a hard problem to find the positions of the cities if all you are given is the distances.  So, yes, NetworkX does not provide this functionality.  But I think you are probably looking for the wrong solution to your problem. :)
comment
Sorry for the spurious comments on code you had corrected before I commented on it. :}  Two bigger picture coments:  Install nose (```pip install nose```) and run the tests locally: ```nosetests --with-doctest``` runs the tests recursively down the directory tree starting from the current directory.  Then you will know what Travis will produce without waiting for Travis.  Instead of removing a test that has order issues, you can sort the relevant lists:      result = {n: sorted(s) for n, s in nx.bfs_successors(self.D, source=7, depth_limit=2)}     assert_equal(result, {8: [9], 2: [3], 7: [2, 8]})  Also, you should make sure you are following the PEP8 Python code style. You can use the style checker ```pip install pep8``` and then ```pep8 breadth_first_search.py```. And you can read [the official style guide](https://www.python.org/dev/peps/pep-0008/?) too.   Most of the suggestions will be spacing issues and length of lines less than 80 chars.
comment
Instead of making the test case so simple, try not using the ```dict``` function.  Something like:      >>> print(sorted(nx.bfs_predecessors(M, source=1, depth_limit=3)))     [(0, 1), (2, 1), (3, 2), (4, 3), (7, 2), (8, 7)]  Eventually, all dicts will be ordered (python 3.6+), but the older versions have arbitrary ordering so for consistency in printing we often need to sort the results.
comment
Okay!  good news -- it passes all the tests. :)  I only see one more issue to overcome. The documentation is having trouble finding the reference for ```depth-limited searching```.  Looking at the [sphinx documentation for citations](http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html#citations) it looks like the space is not allowed in the name:  ```case-insensitive single words consisting of alphanumerics plus internal hyphens, underscores, and periods; no whitespace```.  Can you rename it with ```-``` or ```_``` replacing all white space?  [Edit: you also need a blank line before the ```.. _Depth-limited-searching``` or sphinx doesn't see it as a reference]   The place to check for this is the first version in our Travis-CI tests. That one is run with Python 2.7 and has the special attribute that it compiles the documentation. If the active branch is networkx's master branch it also updates the latest documentation pages. But that doesn't happen until the PR is merged with master.  So for now we can't see the documentation, but we can see the error log as it is compiled.  In any case, can you change the citation name to remove white space? Also, I didn't see an error for this, but most examples of sphinx I have seen use a space after the double-periods.  It looks like you got rid of this space both for the new citation and the existing one. Could you either put those spaces back or explain why they need to be changed? If they need to be changed then we have a lot of places where it should be changed.  You can check the documentation from your own machine using installed python 2.7 and sphinx. After installing packages listed in ```requirements/docs.txt``` go to the doc directory and use ```make html``` and to check the examples, use ```make doctest```.   :)
comment
Maybe this would be best to implement as its own function rather than an extension of the existing euler functionality. This might be a reasonable project for someone looking to contribute.
comment
I think you should put the code in ```networkx/algorithms/euler.py``` with the tests in ```networkx/algorithms/tests/test_euler.py```.  Take a look at what is in there to see how it is set up. Also read the NetworkX github Wiki to general style ideas.   Read the links from the original poster.  Also search for "eulerization of a graph" or similar and read what those pages say about how to do this.  It looks like you will need to create a MultiGraph to be able to handle duplicate edges. I think you should ignore edge weights altogether unless you can find a reference that uses them. The tests should be unittests -- that is, tests of specific features and/or aspects of the code. The idea is that they should flag us if we change something that breaks your code. -- one way to do this is to check that the answers are right in some important specific cases. :)  vague enough?  Put your name with the authors at the top and if you want in the ```CONTRIBUTORS.rst``` file. If you have questions about overall structure ask here -- or submit your unfinished PR and ask your questions in the PR.  In this case, people often put ```[WIP]``` at the beginning of the PR title.  Thanks!
comment
Looks like Codecov wants a test of the multigraph cases. (which wasn't there before but now that we touched that code, it is flagging it.  I'll add that soon... 
comment
Thanks for doing this!!
comment
I'm confused...  Is this a problem when you ```deepcopy``` or when you ```pickle```?  The ```OutEdgeView``` object is likely to be the ```G.edges``` object. And it makes sense that trying to create that object before the graph exists will cause trouble. But I still don't understand enough about what the problem is or maybe what you are trying to do. Are you pickling as part of a deepcopy? 
comment
I haven't had time to look at it thoroughly.  But it seems like we need to make sure G.edges and G.nodes are not part of any pickle process. They get recreated when needed.  Might be simplest just to have them created every time we use them. But I think a ```setstate``` on TrackGraph could do the trick.  Maybe there is a way to put it on Graph so that it works for the other flavors of graph: DiGraph/MultiGraph/MultiDiGraph/OrderedGraph, etc.  Is this similar to your ideas for a solution?
comment
I'm not sure I understand the unpickling problem you are trying to solve. Can you simply add a ```setstate``` method for TrackGraph that avoids pickling the EdgeViews?  As far as I understand, pickling and unpickling works fine for the NetworkX classes. It just isn't working for your TrackGraph. Is that correct?
comment
See #2915 which might help fix this problem.
comment
@stablum The PR #3011 removed circular references between the base classes and graphviews. So pickle and deepcopy should be easier to subclass.  Would it be easy for you to try your TrackGraph with the latest version of NetworkX and see if the issue is fixed?  Thanks...
comment
@stablum I have simplified the process for subclassing Graph/DiGraph etc.  The GraphViews were adding too much overhead to avoid cyclic references and you weren't the only one having issues with subclassing. I'm hopeful that the new system will be easier.    For starters, get rid of ```fresh_copy``` and any getstate stuff. It you have a directed and undirected TrackGraph and you want ```to_directed``` to work, you should overwrite two methods to tell what class to use when converting directed-ness.      def to_directed_class(self):         return TrackDiGraph     def to_undirected_class(self):         return TrackGraph  Please let me know how this conversion goes. I think it makes it much better for you. This is going into NetworkX 2.0 which should be released soon.
comment
No one that I know of is working on exactly this.  But you should definitely look at #1621 which takes a bidirectional search perspective.  The length of time on line 270 is because ```visited``` is a list instead of a set. As you point out, that is an obvious place to look for speedup. It might be worth keeping the list as ```path_so_far``` or just ```path``` and add a second set structure called ```visited```.   [Note: for python3.6+ all dicts keep the keys ordered, so making ```visited``` a dict would serve both goals. I guess an OrderedDict would work for all versions. Someone could look into this as a quick fix -- just change line 259 to create a ```collections.OrderedDict```, then adapt the ```pop``` and ```append``` and ```yield list(visited)``` to make it all work.]  
comment
Thanks!
comment
This seems like a good way to handle an empty incoming list.  Thanks!
comment
Each of these functions uses the ```@open_file``` decorator to determine if the argument is a string-like name of the file or a file object. Your suggestion would lead to a change in that decorator so that if it is a pathlib object, a reasonable file object is created.  Would you be able to take a look at open_file and see if there is a good way to identify the argument as a pathlib object? Then pathlib would work for all networkx functions. 
comment
Actually, I think it might be better/faster/cheaper just to have the user put ```str(p)``` when they are calling the function, instead of adding a bunch of code to NetworkX which would eventually need to be changed to keep up with pathlib. The pathlib library helps you manipulate the path, but when you want to use it you turn it into a string by using ```str```.    So I think I will close this issue in favor of leaving specific code out of networkx and instead requiring the user to turn the argument into a string before calling these functions.
comment
I agree with keeping up with all base packages.  But isn't the proper use of pathlib to turn it into a string when you want to use it?      f = open(str(p))  or in a more sophisticated way:      with p.open('wb') as f:         nx.write_pickle(G, path=f)  How are you envisioning people use the path objects?
comment
OK... I put together an unsatisfying workable version.  Unsatisfying because pathlib objects should really be sent to ```open``` directly. But that doesn't work for python 3.4 or 3.5.  So casting to ```str``` is what I'm doing. Also, checking for patlib by looking for attr ```resolve``` because ```__fspath__``` is not present on py34 or py35. 
comment
I like the second version better -- easier to read and hopefully understand. I haven't checked details to make sure there aren't any strange differences.  How hard would it be to allow ```int``` column specifiers?  ----------------- The testing errors are due to Python 2 choking on:      for s, t, *attrs in zip(df[source], df[target], *[df[col] for col in cols]):  which could be replaced (if I understand correctly) with       for edgeinfo in zip(df[source], df[target], *[df[col] for col in cols]):         s, t = edgeinfo[:2]         attr = edgeinfo[2:]
comment
Another option?:           for s, t, attrs in zip(df[source], df[target], zip([df[col] for col in cols])):
comment
Thanks for this!    Could you add tests for these cases in the tests/test_degree_centrality.py file?
comment
Thanks for this. Seems like a good suggestion for fix: circular_layout should raise an exception for ```dim``` values it can't handle.
comment
Thanks for this!  Thoughts:  - we should add a quick test for ```dim==2``` in ```shell_layout```. And we should add a sentence in the circular layout description of ```dim``` that ```dim<2``` causes an error. - trying to infer the rules for a dim=1 circular layout may cause surprises. Let's just raise an exception. - uniformly spaced dim=1 case is easy enough -- let's avoid making a new function for it. - add a test for the dim=1 case for ```kamada_kawai_layout```.  There are "smoke tests" for kamada_kawai_layout in the test methods that check all functions. The added test can similarly be a "smoke test". Just run it with dim=1 and don't check the output. 
comment
OK... this looks good to me. I'll look into the codecov stuff -- but can't see why those lines aren't being hit by the tests.  Anything else? Take the WIP off the title when you are ready for me to merge.
comment
Can you explain more? Why should they have equal runtimes? do you have any suggestions for how to speed it up?
comment
I believe this depends on what you mean by "the same topology".  For example, it is pretty easy to create many different sized complete graphs, but you need to be more specific than many different sized trees. I believe this is all possible for you using NetworkX.
comment
You should check the [matplotlib documentation about colors](https://matplotlib.org/api/colors_api.html).  In particular, if you specify which color using a finer scale of colors (like RGB values) then lots of things are possible:      colors = [(0.1, 0.2, 0.5), (0.1, 0.5, 0.2), (0.5, 0.1, 0.2)]
comment
Take a look at [the docs for reading/writing](https://networkx.github.io/documentation/stable/reference/convert.html) The syntax you are using is for an older version of networkx that doesn't specify whether the dataframe holds adjacency info or edgelist info.
comment
Yes, I suspect either networkjx or graphviz could do this. You'll need to create the nodes and edges to be drawn and then draw them.
comment
It seems like this has a simple solution:   ```labels={k, v for k, v in labels if v in pos}``` If there is a simple solution in networkx code that works as well, I'd be fine to look at it, but it needs to work both ways -- that is, labels and pos are both dicts. We then need to decide which takes precedence. Perhaps better to make the user specify exactly what they want so we avoid surprises.
comment
The ```pos``` dicts can be manipulated to do this:  find the energy minimizing positions in the other dimensions, and then add an additional coordinate to each node based on the given property.  If you get an example working for this feature I'd help you include it in our examples.  Reopen this issue if you want to pursue this approach.
comment
closed in #2926
comment
shortest_path ignores node weights. It looks at edge weights if requested to do so.   So I guess the answer to your question is that zero-weight nodes are treated as regular nodes.
comment
The pagerank is originally for directed graphs. The [Wikipedia PageRank page](https://en.wikipedia.org/wiki/PageRank) describes extensions as do many other sources. It would be difficult to pin down precisely which source was used to create this code. Most likely it was more than one source.
comment
If you are not using at least NetworkX v2.1 you should upgrade  (See #2902)  There was a bug in ```from_pandas_adjacency``` before NetworkX2.1 Was the trouble you are seeing with ```to_pandas_adjacency``` also fixed?
comment
Take a look at ```multi_source_dijkstra```.   Maybe there would be a way to get a similar framework for the unweighted shortest_path functions too.
comment
Reopen if this is still of interest...
comment
The [documentation for networkx.katz_centrality](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.katz_centrality.html?highlight=katz_centrality#networkx.algorithms.centrality.katz_centrality) says that this error is raised when you have reached the maximum number of iterations. That is presumably occurring because your network is difficult to find the katz_centrality for -- for example, it has eigenvalues that have high multiplicity.  You can try increasing the ```max_iter``` parameter or try a different "basic" example.
comment
Setting the canvas size for use with matplotlib is a matplotlib feature. You can use their functionality -- or if you prefer, you can scale the values in ```pos``` in your code. (Scaling the values of ```pos``` will likely show the same picture unless you turn off the automatic choice of canvas size done by matplotlib.)
comment
```binary``` is a module that imports networkx as nx so it can access functions from networkx. Thus the name ```nx``` appears in the namespace of the module ```nx.binary```
comment
It sounds like you are confusing the graph G with the positions of the nodes. By default, graphs don't store positions of the nodes -- though you can add them if you like.  The various layout functions create position dictionaries keyed by node to position vectors (usually 2-d). If you create a rotation matrix/array using numpy you can multiply that matrix by the position of each node to get the new positions.  No need to change the index of the node -- just operate on the position vectors.
comment
```pop``` is typically a mutating operation for lists, dicts etc. Graph mutations should be done through the graph object itself rather than through the reporting methods ```nodes``` and ```edges```.  If you want to mimic removing a node from the graph and returning it you could use:      node = nx.utils.arbitrary_element(G)     G.remove_node(node)  or equivalently      for node in G:         break     G.remove_node(node)
comment
Is it possible that this comment is at the place were we "ignore it when we visit the node a second time"?  Certainly the comment should either be changed or removed. It may be easier to follow the code than the comment. 
comment
Thank you!
comment
That size network should not be a problem... but it depends on what your nodes are and what attributes you store.  Since you are using ThinGraph, you aren't storing edge attributes. So the first question I'll ask is:  What objects are your nodes?  They should be integers to reduce memory usage.  Other objects close in since to integers will work too. Strings and more complex objects can end up using a lot of memory, though I don't completely understand why.   You can try to build the graph with integer nodes (while storing a list of nodes for each integer) or if . you've already got the graph (and the memory), use [nx.convert_node_labels_to_integers](https://networkx.github.io/documentation/latest/reference/generated/networkx.relabel.convert_node_labels_to_integers.html)
comment
Ahh -- sorry, I read your original as 6 million instead of 60 million.  yes, I suspect you could be running into trouble even with the ThinGraph if you have 16MB.  Think of it this way: if you have to store two numbers per edge. Then you need 120M numbers. which gets you to 1GB just to point to the numbers (8 byte pointers on 64-bit machines). So after including the adjacency dict structures themselves (240 bytes per dict overhead) and the 28 bytes per number for storing the numbers, I'm not surprised you are getting up near 10GB.
comment
Yes -- we have spent many hours over the years looking at this.  :rant:  Python can be slow for some tasks, but it has some surprisingly fast data structures. You need to choose of course, but the tradeoff between people-time and computer time is quite real. When the questions being asked are changing faster than the development time for the C programs, it makes good sense to value people-time first.  :end-of-rant:  We haven't found a useful way to make a C/C++ interface for NetworkX work in general.  Once you make it general enough, it is better to keep it in python.  My suggestion for your development pipe-line is to write it in python. Do some simple cases. Do some harder cases. Once you see what you want and how much time it will take, try to speed it up. Do the following steps and stop when your problem can be done in the time you really need it by. Don't get sucked into making something work optimally if you don't need to. Profile your code to find the bottlenecks.    - Use Cython first with no changes to the python code. - Find the slow parts and use Cython to speed up those bottlenecks that take time. Inner loops are quite effective. - rewrite the inner loops/bottlenecks in C/C++.  Actually this can almost always be done at the previous bullet since Cython allows C code to mix with the python.  
comment
Your algorithm looks simple and indeed is described in [wikipedia's article on transitive reduction](https://en.wikipedia.org/wiki/Transitive_reduction) and boolean matrix multiplication.  However, simple to describe does not mean efficient.    Can you explain what you mean by efficient in this case? Also what is omega (and n) in O(n^omega)?  You have to create a dense matrix which uses lots of RAM and then do matrix multiplication, which might theoretically be order O(n^2.37...) but I doubt that numpy gets to that level -- probably similar to O(n^3).  Most networks of practical interest as networks are sparse. The wikipedia section on complexity in the sparse case describes the algorithm we intend to implement in NetworkX. It is O(n*m) where n in number of nodes and m is number of edges. Even for dense networks (m=n^2) so this is O(n^3) too. For sparse networks, it is much closer to O(n^2). Looking at the code, it is not clear to me that we have achieved this complexity, but I didn't look at it very long. Do you have reason to believe our implementation is not optimal, or does not follow the algorithm for sparse networks described in the link?  I wouldn't worry about discarding information -- the user still has the original graph. And multiedges are a special case. The question is whether our algorithm is O(n*m). And whether your boolean matrix multiplication is efficient.
comment
I thought the implementation described in wikipedia is O(n*m), so in this case O(|V| (|V|+|E|)) and Not O(|V|^2 (|V|+|E|)) as you describe.  Then the whole process for dense networks is O(n^3) because m=n^2. This should be the same complexity as the matrix implementation.  But, I'm concerned that our implementation is not actually doing what it should -- I mean that it seems to be doing more work than needed.  The code says:          for u in G:             u_edges = set(G[u])             for v in G[u]:                 u_edges -= {y for x, y in nx.dfs_edges(G, v)}              TR.add_edges_from((u, v) for v in u_edges)  Forget for a moment that ```u_edges``` should be named ```u_nbrs```. This seems to compute the depth-first-search for each node v many times. That is ```G.in_degree(v)``` times. It should be much faster to compute ```{y for x, y in nx.dfs_edges(G, v)}``` once for each ```v``` and store it. I believe this could be done using a more efficient ```dfs_*``` function too -- because we don't need the edges -- only the nodes -- but I haven't checked that out thoroughly.  For a dense network I think this means we have implemented O(n^4) instead of O(n^3) as intended.  I would prefer to get this algorithm correct before implementing a matrix based version that would be the same complexity for dense networks as this intends to be and worse for sparse networks.  If it is true that a dense version of this function would be better then we should include that as an alternative function.  And certainly we should include your 4 line implementation in the doc_string if it doesn't get implemented. It's so simple and intuitive!  Do you have code to time the various versions? Can we easily insert a scaling of the network -- say doubling the number of nodes while keeping density approximately the same. Then we could get the complexity of the actual function rather than the algorithm.   :)  
comment
If you do try to time these codes, could you try this version of transitive reduction:      TR = nx.DiGraph()     TR.add_nodes_from(G.nodes())     descendants = {}     for u in G:         u_nbrs = set(G[u])         for v in G[u]:             if v in u_nbrs:                 if v not in descendants:                     descendants[v] = {y for x, y in nx.dfs_edges(G, v)}                 u_nbrs -= descendants[v]         TR.add_edges_from((u, v) for v in u_nbrs)     return TR  Thanks very much!
comment
You should just change them back and make another commit with those changes.  It is true that this makes more total commits -- and there are ways to "squash commits" together if you want to search for that -- but it undoes what you already did as if nothing happened. And when github merges it squashes all your commits by default anyway -- so the long term history is just one commit for the entire PR.  Other items of note: - Check the PEP8 style of your changes (perhaps using the convenient program pep8 available as    ```pip install pep8```)  - Line 342: ```q = current_sort[-1]``` and the next line . ---  Isn't this just ```q=current_sort.pop()```? - Line 380:   isn't that just an ```append```? - Add space after the first line of the docstring. The first line gets used as a short description and the rest should appear as a paragraph, but separated from the first line with a blank line - Probably good to add an example to the doc_string.  :)
comment
You could ```yield list(current_sort)``` to make/yield a copy of the list.  That clarifies the separation of the list in the algorithm from the lists that have been released. tuple would work too of course.  Does this algorithm work for MultiGraph? We should either have a not_implemented_for, or include tests to ensure it does what we expect. 
comment
This looks good to me.  Anything else before I merge?
comment
Thanks for submitting!!! :)
comment
Thanks for checking into python 3.7 and reporting the problems --  And thanks for finding solutions.  This should be doable. :)
comment
I can't seem to build a good testbed for python 3.7. And I don't have travis doing it correctly either.   Can you check if this PR makes NX work on Arch Linux? Suggestions for getting a working testbed would be helpful too. :)
comment
Great!  Thanks very much for this! 
comment
The documentation you link to is for version 1.10 of NetworkX.  That link is clearly being used for other purposes now.  The [link to testing documentation](https://networkx.github.io/documentation/stable/install.html#testing) shows the correct link for nose software:  https://nose.readthedocs.org/
comment
This might be improved with #2730 solved. But Im going to close this issue. We won't be updating links on old documentation pages.  Thanks for letting us know...
comment
I agree -- the tests seem to be due to a change in pandas rather than anything from this PR.  Thanks for filling in this corner case!! :)
comment
I'm not sure if this is your problem, but as written ```list(g.nodes())[0]``` should result in a list and it results in a singleton. Instead use: ```list(g.nodes())[:1]```  With that change, the code works on my setup (python 3.6.5, matplotlib 2.2.2, networkx dev)
comment
More information and fewer exclamation points would be helpful. :) write_pajek does indeed recognize edge weights.
comment
You've got the order of your arguments switched.       nx.set_node_attributes(G, state_ini, 'state')
comment
I don't have any major problems with this PR. March and April are crazy busy -- will try to get to it. I don't think we should refactor other methods to use this one -- if only because of readability of e.g. erdos-renyi. But it's good to have the general algorithm available for more advanced use.  Looks like the conflicts to be resolved are easy to fix for this PR.
comment
This PR adds Stochastic Block Model generator.   It needs to treat ```seed``` as a rng-or-int to match the new way to handle seeds in other generators. It needs tests (start with all the exceptions that should be raised -- then some sort of smoke test to make sure it actually creates something reasonable -- and maybe a couple with seed set so it is deterministic and we check that it creates the right thing.) You should rebase to avoid the conflicts which came because another method was added in this file.
comment
I'm going to leave the seed argument as it is until we figure out choice of random_state in #1764. Other functions in this module will have to be changed in the same way as this new function. I think this is ready for merging.  Comments?
comment
Yes -- can you take a shot at that?  Thanks-- Also, if you can put parentheses on the print statements in the doc_string to make it pass the tests that would be great.
comment
You can handle floats in a doctest (code in the docstring) using ```round(expression, 5)``` where the number of digits can be changed from 5 to whatever you want. There is one example in networkx at networkx/algorithms/bipartite/cluster.py  bascially ```print(round(v[1]['density'], 5))```
comment
I like that interface change. :) Might break backward compatibility, but in a fairly obvious way. We will need to flag it in the release notes.
comment
```find_cycle``` has the same ```orientation``` interface, so this should probably be done there too.
comment
Does ```edge_dfs``` do what you want?  (i.e. is our naming scheme in need of revamping?)      >>> print(list(nx.edge_dfs(g)))     [(1, 2, 0), (1, 2, 1), (1, 3, 0)] 
comment
@chebee7i do you have some ideas for how to approach this?  Otherwise I'm going to bump it to networkx-2.1
comment
Yes -- the github issue shows that the milestone was modifed from networkx-2.1 to networkx-2.2.  Would you be willing to try to get that working?  We need a pull request.
comment
Is there a way to ```set_printoptions``` when we run the nosetests doctests? But still have it not set for general use...   I guess for now we can just load legacy mode always, but it seems better to use the new functionality when we can.
comment
Fixed by #2810 but popped up again in the sphinx doctest code a week later.
comment
Let's postpone create_using changes.  I have some ideas on better generators, better create_using and a more universal packaging of graph (a GraphBox?). For example, perhaps a GraphBox is just a tuple ```(nodes, edges)``` or for more universal treatment a tuple ```(graphattr, nodes, edges, succ, pred)``` with None in any of those position.   I know Aric has thought about improvements to the graph structure/conversion system along these lines and mentions some of that above.  But I don't think we are ready to put even the ```create_using``` ideas into practice for v2.0.  I'm going to move this issue to v2.1 and open a v-future issue for a GraphBox transportation system.  If people think we should do something for v2.0 on this please post something here. 
comment
To move this forward toward implementation, I summarize the discussion on graph generators as:  1. change create_using to accept a class constructor (or maybe either an instance or constructor to ease backward compatibility) 2. add G.update(H) and G.update(nodes, edges) methods to the base classes. 3. split the generator functions, e.g. into functions like ```generate_path(n)``` and ```path_graph(n)```. The generators return ```(nodes, edges)``` tuples ready to be input to the ```G.update``` method.  The graph constructors pick a specific graph type so we eliminate create_using -- perhaps making 1) moot.  I think we can do 1) and 2) for v2.2. There are relatively minor backward compatibility issues and the design seems clear from the discussion above.    I would like some more ideas for how to do 3) before implementing it.   I'm hesitant to replace the ```create_using``` keyword argument by doubling the number of generator functions -- one generates the nodes/edges and the other puts it in a graph? Do we switch all the generators to e.g. ```nx.generate_path(n)``` and NOT provide the ```nx.path_graph(n)```? That leads to ```G = nx.Graph().update(nx.generate_path(n))``` which seems ugly compared to ```G = nx.path_graph(n)```.  Suggestions?  
comment
This discussion should be continued in #3036.  With #3004 and #3028 we have the building blocks for many of the ideas here.
comment
Thanks for this bug report!  I can verify that an infinite loop occurs with this example.  In the method that looks for cycles when 1 edge is found, the edge is duplicated (line #388) when it shouldn't be.  I'll put a PR in shortly. 
comment
fixed in #3031 
comment
Yup -- that's  typo.  Thanks! I've created a pull request for it.  #3031
comment
Fixed in #2951
comment
Thanks for submitting this.  It's not close to my area so please excuse and explain my mistakes in interpretation.  My comments are mostly on coding and API.  I'm not sure the iterators should be built as classes. Typically iterators yield results based on an unchanging data structure. In this case, the data structure is changing throughout the iteration process.   So I suggest an API where a function is given to the algorithm with calling signature ```heuristic(graph)``` where graph is a dict-of-sets adjacency. The function should return a node or ```None``` to signify that the algorithm stop.  I'm not close enough to this code to know if my suggestion makes sense in this case. Can you make a judgement for whether this makes sense?  If data should be stored from one call to the next (like for MinDegreeHeuristic), you can create a class to store that data and have the function be ```lambda graph: MinDegreeHeuristic.best_node(graph)```.  But for other heuristics that don't store data from one call to another a function will do.  What do you think?
comment
Final to-do-list:  - Check line-lengths (<80 chars) and blank lines containing white-space and other PEP8 stuff. Check PEP8 style (perhaps using the pep8 utility easily installed with ```pip install pep8```).  - Doc_string for two public facing functions.     Make a one-line description, then a blank line, then the rest of the text describing the function.  For example with ```treewidth_min_degree``` End the lin after the first sentence and put a blank line before the next sentence which will appear in the paragraph following the one-line description.  - Doc_string for ```min_fill_in_heuristic``` (not as important because not public-facing, but ...)     Start with a synopsis sentence similar to the MinDegreeHeuristic class.     These are all minor things. I think it is ready to merge otherwise.
comment
travis has been giving errors for the past couple of days while building the documentation. I'm going to merge even with that error.  Thanks very much for this!
comment
This works fine for me (python 3.6, nx from git) I suspect it is a problem with drawing the unicode characters using the ```text``` feature in matplotlib. In particular, the unicode does not get lost anywhere.
comment
Did #2955 fix this problem?  If not, reopen please.
comment
If this is requesting that we compute the min and max degree as each node is added or removed then I'll just say that we aren't going to do that. Reopen if the request differs from that.
comment
```twopi``` is a GraphVIz program similar to ```neato``` and ```dot```.  So you could run twopi on the command line in a similar way that you run dot or neato.  BUT, it looks like you are using old code -- it's got the pydotplus package, for example. I suggest you update to the latest version of NetworkX and try again. The pydot world is changing as they shift maintainers. That might help solve your problem.
comment
Fixed in #3024
comment
Thanks for this!   It's always good to have a tutorial that is actually correct. :)  The various versions of ```to_*directed``` should probably always generate views (views are easy for the user to copy if needed. But the methods have defaulted to copies since before views (or even generator expressions) existed. We have left them that way for backward compatibility and perhaps somewhat due to lack of attention. Your question  helps with that. 
comment
Looks like some other typos in there too -- like "muts" instead of "must"
comment
Thanks for this!   The documentation is wrong.  deepcopy is not the default (you can get that by using deepcopy). The first paragraph of the same docstring you quote contradicts the quoted claim.      The copy method by default returns a shallow copy of the graph and attributes.   The first paragraph is correct, while the 3rd paragraph's claim about "default" is incorrect.
comment
How many cycles are you expecting?  The 1000 nodes and 1700 edges is not very big for NetworkX graph storage.  But if you are using 70GB of RAM then you must be trying to store all the simple cycles at the same time (perhaps as a list).  THAT is going to take a very large amount of RAM.  Once it fills up your RAM the code takes forever to run because it has to swap between disk and RAM.  Most likely you will need to find a way to process the cycles as they are generated rather than trying to store them all.
comment
The trouble here is representing the nodes with objects that have the same ```str``` representation. Perhaps the easy fix is to convert nodes to integers, get the layout from that and then convert those positions to the old node names:      H = nx.convert_node_labels_to_integers(G, label_attribute='node_label')     H_layout = nx.nx_pydot.pydot_layout(G, prog='dot')     G_layout = {H.nodes[n]['node_label']: p for n, p in H_layout.items()}  I'll put this into the doc_string as an example usage.
comment
Thanks for tracking this down to this level!!  Just to help me out, can you tell me what size the node attributes and/or edge attributes are? If you don't store any node or edge attributes, that indicates we've got a bad design for subgraph views. If the attribute dicts are big, what kind of size?  Thanks
comment
I have tried a case that takes 5 minutes in nx 2.0 in python 2.7 and 50 seconds in nx 2.2 in python2.7.  Same case takes 35 seconds for nx 2.2 and nx 1.11 in python3.6.   I'll try to get a better test setup soon.      import networkx as nx     n=1000     G=nx.grid_2d_graph(n,n)  # million nodes     ns=list(G)     sg=G.subgraph(ns[10:-10])     %%time     S=sg.copy()   
comment
If you replace the two lines in the gist code that 1) create the subgraph and 2) copy the subgraph, with the following, the speedup is tremendous:      G2=G.fresh_copy()     G2.add_nodes_from((n, G.nodes[n]) for n in largest_wcc)     G2.add_edges_from((n, nbr, d)             for n, nbrs in G.adj.items() if n in largest_wcc             for nbr, d in nbrs.items() if nbr in largest_wcc)     G2.graph.update(G.graph)  There were changes from 2.0 to 2.1 in the subgraph view's treatment of node lookup.  The intent was to speed up the common case of a small number of nodes in the induced subgraph.  This is the opposite case where a small number of nodes are NOT in the induced subgraph. So, perhaps I need to tweak the special handling again by looking for the smaller of "nodes in" vs "nodes out". 
comment
Ahh..   The code I put doesn't account for multiedges.      H2 = H.fresh_copy()     H2.add_nodes_from((n, H.nodes[n]) for n in largest_wcc)     if H2.is_multigraph:         H2.add_edges_from((n, nbr, key, d)             for n, nbrs in H.adj.items() if n in largest_wcc             for nbr, keydict in nbrs.items() if nbr in largest_wcc             for key, d in keydict.items())     else:         H2.add_edges_from((n, nbr, d)             for n, nbrs in H.adj.items() if n in largest_wcc             for nbr, d in nbrs.items() if nbr in largest_wcc)     H2.graph.update(H.graph)
comment
This looks quite reasonable. Thank you.  You proposed fix uses the syntax ```G = state['_graph']()``` which will not work because graphs are not callable. Should you call weakref on ```state['_graph']``` directly? Or am I missing something.        def __init__(self, G):     -     self._graph = G = state[_'graph']     +     self._graph = G = weakref(state['_graph'])
comment
The ```state['_graph']``` syntax is used for pickle and deepcopy where the ```state``` dict holds attributes needed to unpickle the object. I am not very familiar with weakrefs and in particular with pickling weakrefs, but my understanding is that they only impact garbage collection and that special linkage should work fine across pickling.  So, YES it should be fine to replace ```state['_graph']``` with a weak reference to it everywhere. Certainly for self._graph and most likely for ```state['_graph']```
comment
As far as I know we don't have any tests that specifically look at garbage collection.  We do have unit tests for pickling and un-pickling. So that side of it should be tested by travis-CI automatically.
comment
See #2915
comment
Fixed in #3011  If you could try it and see if the fix worked I'd appreciate it. Thanks
comment
Thanks very much for the test code -- Using ```gc.get_objects()``` I was able to track the garbage collection.  I also recommend ```objgraph``` which I didn't know about before to track the back-references to objects.   I was able to find the second circular reference =>  G.root_graph I think that should be fixable -- will try it in a bit.   Hopefully there won't be any others.
comment
I'm aiming for v2.2 release in June or July... Thanks for your help in tracking this down.
comment
It is not clear whether a subgraph should preserve the order of the nodes of the original graph or the order of nodes provided to the command.  Perhaps it is best to simplify the API by using more basic commands to build what you want. e.g.      SG=nx.OrderedGraph()     SG.add_nodes_from(ordered_nodes)     SG.add_edges_from((u, v) for (u, v) in G.edges() if u in SG if v in SG)  Is the better API a long list of methods each creating their specified subgraph orderings? Or is it better to ask users to build their own?   Perhaps we should remove the subgraph method entirely...
comment
Let's fix the documentation.  See #2985
comment
This commit https://github.com/networkx/networkx/commit/b7af06af9ecd885c3b311520cc01ded673987083 added exactly that capability to NetworkX.  Though it didn't make it into the v2.0 release. It is in v2.1 and also in the latest code.
comment
Also -- be careful with the docs -- you are reading an old version from jfinkels fork of the readthedocs webpage.  You should get the docs from networkx.github.io
comment
The underlying trouble is hopefully fixed by #3011 I'm going to close this but it could be reopened for more discussion if needed.
comment
There isn't a method defined to get all edges between two specified nodes. You can build your own using adjacency lookup:      [(u, v, k, dd) for k, dd in G[u][v].items()]
comment
This feels like a good (hack) fix for this case, but using ```id``` to identify nodes will potentially create problems from one run to the next if you do anything else with the dot file. It also doesn't pass tests without a way to read in that structure (ad I'm not sure we want to always read in that structure).   So, Hopefully people will find this PR by searching the repo if they need it. But I'm not going to merge it.  Thanks for documenting your ideas and submitting the PR!  :)  Also see #1568 for a solution using ```nx.convert_node_labels_to_integers```
comment
The ```attr_dict``` ketword argument to ```G.add_node``` is no longer supported in NetworkX v2.0+ (It now creates a node attribute called ```attr_dict```).  For recent NX versions you should use:      G.add_node(0, x=0.0, y=0.0)     #  or      G.add_node(0, **{x=0.0, y=0.0})  Of course, it'd be nice if the code didn't give a segfault!  On my system MacOS/anaconda, it gives the same KeyError without a segfault.
comment
Yes -- #2730 is an issue to try to improve the search engine results, but hasn't been implemented. I've notices that many packages have this problem of older versions coming up higher on the search list. :}
comment
Your new example also uses attribute types that aren't supported by ```write_graphml```. It supports ```float``` but not ```numpy.float64```.  I'm not sure why it is segfaulting.
comment
Could you try a different installation of python to see if that fixes the segfaults? I'm not familiar with Arch Linux, but it seems that this is a python issue rather than simply NetworkX.
comment
OK.. I'll close this then -- people can still find it by searching.
comment
This isn't written as a pull request -- rather it is a script that implements an algorithm. Someone could take this idea and create a pull request that integrates this algorithm into NetworkX
comment
Yes -- That should be "edge attributes" instead of node attributes.
comment
Thanks!!
comment
Don't worry about the failure on Travis-ci.  It you look at the log of the error it is not related to your code. It looks like one of the tests that uses random graph generation is no longer working sometimes.  The GML standard http://www.fim.uni-passau.de/index.php?id=17297&L=1 explicitly specifies the keys to allow  '[A-Za-z][A-Za-z0-9]*$' Are you advocating a change to the standard?
comment
Is there a reference for the definition of metric_closure or Steiner tree on a disconnected graph? My naive view is that Steiner Tree's definition works applied to disconnected graphs. Most definitions of the metric closure I have found add the extra edges with infinite (or very large) distances to make it a complete graph.  But couldn't we also just exclude all edges between components? It would work just as well for Steiner Tree. Are there other uses for metric closure?
comment
I agree...   As far as my fairly cursory search of the literature has found, people don't use these terms for a disconnected graph. It may be possible to create a useful definition in such cases, but let's just refactor so that the current error message becomes a more useful one. I think the cost of running nx.is_connected(G) is pretty steep for this feature. Most calls will use connected graphs. I suggest a check of the first dijkstra result to test connectivity. I'll put a PR together soon if this sounds good.
comment
Hmm...   Maybe I am misunderstanding, but isn't this:      p=nx.shortest_path(G, source, target, 'preservances')
comment
Ahh... now I understand better.  I think you will need to build your own version of Dijkstra for this. It would be cool to submit it as a Pull Release for inclusion in NetworkX.  I haven't checked this definitively, but my basic idea is to invert(i.e. reciprocate) the edge weights (preservances) and then find the minimal maximum. To find the "minimal maximum" use a version of djikstra which replaces addition with maximum. The code in networkx/algorithms/shortest_paths/weighted.py:729-819 should do it with the only change on line 798.  You could also create a number class where addition is overwritten as maximum and store all preservances as that class, but that seems like overkill.
comment
Good points -- though I don't think it is all that critical whether the end result is a list of edges or a list of nodes -- so long as it is consistent within the package and easy for people to write short code to convert if they want.   As far as API for maxl min or minl max, I would guess its best to write one of them and have the user convert their edge attributes (using presumably easy functions to write that do that -- like get/set_edge_attributes?). But that's my bias -- there may be a cool organization for functions that do either. 
comment
This was closed by #2252 
comment
The Travis and Appveyor tests have a number of configurations that they test. Some have numpy installed and some don't (look for the option: -optional_deps=1).  Your code is producing two types of errors. When numpy is not installed, it complains about no numpy. When numpy is installed it complains that spectral_forge_graph is not part of networkx. Each have fairly simple fixes.  To skip the tests when numpy is not installed use code like the code at the end of ```convert_matrix.py```.  That code skips is numpy, scipy or pandas are not installed, so you just need the part of numpy. Typically we put it at the end of the module.  To enter ```spectral_forge_graph``` into the networkx namespace, add it to the ```__init__.py``` file in the generators directory. Use a similar format to the lines already there.  Hope this helps and thanks for the contribution!
comment
Finally getting to this -- Thanks!!
comment
Arrows were included in #2760  
comment
I think those good results may be special for the specific graphs being tested. Can you describe why this would move the algorithm from quadratic time to linear time?  When I try the new version for random graphs, I get quadratic dependence.  I think the quadratic nature comes from rechecking all the ```w``` in ```v_nbrs``` for being in ```preorder``` each time we come back to ```v``` being on the top of the queue. So, for each neighbor we have to pass through all the previously explored neighbors.   I tried to create a version that stores the ```v_nbrs``` on the queue with ```v``` but wasn't able to get it working in the time I had. ```v_nbrs``` is used in more than one place and sometimes needs to use ```G[v]```  Thanks everybody for looking at this!
comment
@ArseniyAntonov   Maybe the best performance would include a combination of your approach with removing nodes in addition to storing ```v_nbrs``` as a set and popping nodes off it as preorder is checked. G'Luck -- :)
comment
Thanks for the gentle nudge! :) I looked more at this last week and almost got to a resolution.  I am leaning toward removing the part that copies the graph and removes the scc nodes. The storing of ```nbrs``` seems to be the change that removes quadratic dependence.   @ArseniyAntonov What do you think about that approach? 
comment
See #2994 
comment
See #1878 
comment
I believe the API changed from v1 to v2 of NetworkX. With a newer version you should use ```nx.from_pandas_edgelist``` instead of ```nx.from_pandas_dataframe``` but at least it ought to work. :)  Are you using a version of NetworkX from v1.11 or earlier?  ```print(networkx.__version__)```
comment
See #2981 and #2975  In summary, use the cutoff argument to all_simple_paths with more info in those answers.
comment
You should use the ```self_loops=False``` argument to the ```contracted_nodes``` function.  Otherwise the edge from 'a' to 'b' contract to the selfloop.
comment
Thanks for this -- it looks fine for inclusion, but the .pyc files will mess up the repository. Can you try reverting and recommitting (--force will be needed) to your ```issue-2917-label-keyerror``` branch? Or start the PR over again? Thanks!
comment
I'm finally looking at this again. Sorry for the delay.  Eliminating the assignment of ```(x,y)``` is insufficient to avoid the error: x and y are needed later.  Instead, we also need to avoid creating any label for nodes not in pos because we don't have a position for the label.  This means we silently leave out labels that are not in ```pos```. I think that could lead to surprises where labels that should be shown are not -- without any error message. The current behavior avoids surprises because the KeyError clearly flags the node as not in the dict ```pos```.  I prefer to avoid surprises by raising KeyError on potential mismatches of dicts.  If you want to use an uber-dict of labels with more keys than are in the Graph, I suggest the following and will put this in the docstring if it is deemed OK for these functions (edge_labels has the same problem).      nx.draw_networkx_labels(G, pos, {k:v for k,v in labels.items() if k in pos})
comment
This looks good -- ignore the error on Travis.  That is not related to this code.  Anything else before I merge?
comment
I believe the ```cutoff``` argument to ```all_simple_paths``` is what you are looking for.
comment
Got it -- I understand your question now...    We don't have a weighted version of ```all_simple_paths```.  The weighted methods are often based on Dijkstra, perhaps you can combine code from ```_dijkstra_multisource``` (in ```shortest_paths/weighted.py```) and ```all_simple_paths```.
comment
Doesn't making ```show_nodes``` a tuple increase search time for ```n in show_nodes``` dramatically? Most of the SubGraphViews performance is exactly these type of lookups.  In there a way to store the nodes as a set for lookup, and yet as an ordered list for reporting?  I know some recent Python versions have offered that kind of set/dict structure. I wonder if someone has backported it so that it is available for older versions of Python... 
comment
See #2985
comment
This PR has many many changes that are not at all related to what you propose in the description. I think your other recent PR has many of the same changes as well.  This makes it very hard to review the code. If you want to make spelling corrections, put those in their own PR and describe what it is. Then a PR like this one can involve few changes in a few files and will be much more likely to get reviewed.   Also, Thanks for the contributions!
comment
I'm going to close this pull request because: - once someone knows the Betti numbers for graph theory, the computation is easily constructed. - For someone who doesn't know the Betti numbers for graph theory, there isn't enough other connected functionality in networkx for this to fit inside a context which leads to the Betti Numbers.  Please comment if these ideas are not correct.  And Thanks for the Pull Request!
comment
Where was this previously reported. Is this fixed by #2936 ?
comment
I'm +1 for the fix with only writing attributes that are strings -- option 1)  I'm not sure what the comment about "coordinates" meant -- does it make one option better than another?  Anybody else have input on which option is preferred?  A simple example of what this is fixing is in #2945 
comment
Thanks!
comment
```G.neighbors(4)```     but if you want to avoid spelling differences across the globe try   ```G[4]```
comment
Thanks!
comment
This looks good -- Thanks!
comment
The wikipedia link does not imply you can convert to an undirected graph to compute the clustering coefficient. When converting to undirected, there is no way to tell whether the resulting undirected edge came from a single directed edge or two directed edges. A single directed edge leads to a different coefficient (based on the defn in wikipedia) than two directed edges in opposite directions. The convert-to-undirected method would provide the same answer as a directed graph where all edges were in both directions. I think the potential difference is a factor of 2. 
comment
Thanks very much!!
comment
Isn't it better to pre-process your MultiGraph so that the shortest edge between two nodes is the edge in the processed Graph?
comment
For the ```all_shortest_paths``` case I agree with you -- would you like to take a stab at it? 
comment
I think it would be better to provide functions that convert lists of nodes to lists of edges. Sorry I haven't had time to look at this more. 
comment
I thought we worked out that it is not impossible to convert lists of nodes to list of edges. You just need to find the shortest edge between each pair of nodes in the list. If there is more than one shortest edge then you have two shortest paths.  I think I am missing something...
comment
I think you are optimistic and perhaps not realistic about how "seamless" or "efficient" this method would be. I think both methods could be consistent -- but it would take an incredible amount of work to make your method consistent across all the functions.  The maintenance effort required would rise too -- and that is a real concern for this project.   Here's a compromise: you could rewrite the functions you intend to use in this way. Try to make the API for them easily extendable across all the shortest_path functions.  But mostly work to get it working for what you need.  You'll get working code and if it looks easy to use and not a maintenance drain then we can work to expand to the other functions.  I think that's what I mean by "take a stab at it".  Do enough to see how it will turn out (and solve your troubles even if it turns out to be too much for the package). 
comment
I think the shortest_path function is in ```networkx.algorithms.shortest_paths.generic```. The functions in unweighted.py do not have the weight argument.
comment
Do you suggest that we make this change and change it back after the sphinx fix is in? Would it be better to leave it as is since it seems to be an upstream problem?
comment
You need to turn your ```MultiGraph``` into a ```Graph``` before calling ```shortest_simple_paths```. If you just want ```shortest_paths``` you can use that instead. If you really want ```shortest_simple_paths``` then figure out how to handle any multiedges that might occur in your data.  One very simple way to handle them is to combine them into a single edge.      G=nx.Graph(g)  
comment
The generator is the fast way to access the paths.      for path in nx.shortest_simple_paths(G,'20','14',2):         # do something with the path  That way you don't have to hold all the paths in memory at once.  If you create a list of all the paths, it will require a lot of memory/time.
comment
Thank you
comment
Thanks!
comment
Yes -- we should put a note/warning in the docstring for ```set_node_attributes``` that the order was changed between 1.x and 2.x
comment
This looks good.  But could you take a look at #1764 and #1550 where it is argued that we shouldn't be doing this? What is the right approach to handling random number generators and numpy.random in particular? 
comment
Very nice!    This could be helpful elsewhere I'm sure... in addition to #1764 but certainly there.  It'd be nice, (for backward compatibility even for private functions) to leave the existing arguments in the same order and add the random state argument to the end. But if there are other reasons to add it earlier I'm open to that.  Also it should probably have a default -- perhaps of None -- even in private functions.  Does it make sense to pull together some helper code into the nx.utils to help with random states? I suppose it could even be a decorator for functions similar to the one we use to open files. But perhaps I am getting too ambitious and we should merge this first and see if we like it.  I'm fine with either approach.
comment
Don't worry about the travis test failure in mac osx. They changed their build environment.  Minor things:    - don't wrap the imports in try/except. (too much overhead and they don't cause errors if not in the module namespace. - For sphinx, the docstring should start with a single line description (that means <80 chars too). Then a blank line and then (if reasonable) a paragraph defining what the terms mean in the single line description.   Blank lines separate sections. There are competing styles for docstrings, but take a look at [this one](http://www.sphinx-doc.org/en/stable/ext/example_numpy.html) from numpy and sphinx.  This is great stuff!  Thanks for chasing the bugs too!
comment
OK... Is this all that is planned for this PR?   If so, I'll take one more check through it and then merge. :)  I added some doc tweaks and changed the name of ```check_random_state``` to ```create_random_state```.  Are you OK with that @harryscholes ?
comment
Take a look at #1764 for more on this issue
comment
See #1764 which will fix this using ideas fro #2766
comment
That ```iff``` stands for "if and only if", which is an abbreviation common in math theorems and proofs. Could you replace the ```iff``` with ```if and only if```?  Thanks!
comment
Excellent -- Thanks!
comment
The API for [the set_node_attributes function](https://networkx.github.io/documentation/stable/reference/generated/networkx.classes.function.set_node_attributes.html) changed to allow more flexibility. The order of values and name changed. It is probably best to be explicit about the order by using keywords:      nx.set_node_attributes(G, values=values, name=name)
comment
You need to create the create_using object. Put () after nx.DiGraph and it will work. As written it tries to use the class itself and that doesn't work so reverts to Graph.
comment
I don't have that happening. In fact, I don't see how that could happen with the code... So: what version of python and what does nx.__version__ show? Can you show us what code you are using to create df, etc?
comment
See #1883 and #1562.      You might try:  ```nx.add_edges_from(itertools.combinations(students,2))``` Or if you know it will be disjoint nodes:  ```nx.union(g, nx.complete_graph(stduents))``` or ```nx.disjoint_union```.  
comment
If we're going to remove blockmodel long term anyway, let's change the place in #1845 that uses it to use `quotient_graph`.   Putting in the alias for backward compatibility is fine too, but our codebase should use what we expect to exist long term. 
comment
We could also create a wrapper function `blockmodel` using old syntax which then calls quotient_graph with the correct arguments. 
comment
A search for "Networkx blockmodel" returns an example from the gallery and the code points out that the relevant function is now called ```quotient_graph```.  The deprecation notice was put in [PR #1886](https://github.com/networkx/networkx/pull/1886).   A search for blockmodel on the networkx documentation page returns 4 items one of which is ```quotient_graph```, another is the example found above.    In the ```quotient_graph``` docstring  it explicitly shows how to implement the blockmodel idea using quotient_graph.  It looks like it is not in the release notes. But otherwise fairly well covered.  Want to put it in a release doc? 
comment
The release doc for 2.0 would be the place: doc/release/release_2.0.rst  There is a section on Deprecations that lists function to be deprecated later. Start a subsection listing function removed for v2.0.   blockmodel is one of them.    Thanks!
comment
Thanks!
comment
#1662  should be good to go as far as iterable in_degree methods due to commit bfd855f  
comment
You should try the [layout functions](https://networkx.github.io/documentation/stable/reference/drawing.html#module-networkx.drawing.layout) in the drawing section.  Or you can google: networkx drawing layout
comment
Try the [email discussion group](https://groups.google.com/forum/#!forum/networkx-discuss) for a wider audience. Send email to: networkx-discuss@googlegroups.com
comment
This looks good.  Any other work to be done here?  Also, are there other places in the code that could use this (or a similar) utility?
comment
Thanks!
comment
That example does not work with NetworkX.  Perhaps it should, but at the moment it doesn't.  The first sentence you quote is true:   ```The neighbor information is also provided by subscripting the graph.```  But the example should read:   ```for nbr, datadict in G[node].items():```  Thank for reporting this.  Would it be helpful to include functionality to iterate over nbrs and a single attribute? It can be done now using two lines:      for nbr, dd in G[node].items():         foovalue = dd.get('foo', default=1)
comment
The syntax ```G.nodes[1]``` shows the NODE attributes for node 1, while ```G[1].items()``` shows the neighbors and EDGE attributes connecting to those neighbors.    Said another way, ```G[node]``` gives the adjacency information, not the node information.  Edge information:      for nbr,dd in G[node].items():         print(nbr, dd.get('color','red)  Node information:      print(G.nodes(data='color'))  Node information about neighbor nodes:      nodecolor=G.nodes(data='color')     [(nbr,nodecolor[nbr]) for nbr in G[node]]  OR      [(nbr, G.nodes[node]['color']) for nbr in G[node]]
comment
For large Graphs it is expensive to build the entire dictionary. The iterator allows you to process all the distances without creating a large data structure in memory to hold all that data.  One of the big changes in NetworkX 2.0 is moving to a more iterator/view approach to graph reporting for precisely this reason.
comment
Thank you!!
comment
line 290 has `def test_write_with_node_attributes` so this is definitely the right file to put tests in.  To test in a closed-form way, look at some of the existing tests in that file. They basically check that the text of the output is what we want it to be. We don't test whether that actually works with Gephi and if Gephi changes what works, our unit tests don't catch that.  That's OK.   For these kinds of unit tests we just want to test that we're doing what we think we're doing. 
comment
Fixed in #2875
comment
Let's try to focus on network algorithms rather than code style. It's fine to fix style if you are working on that code anyway, but we don't have the reviewer time to check these kinds of fixes and the potential problems they might create.  Thanks for your efforts.
comment
The ```reverse``` keyword is not valid in v2. With the new View classes, you should try ```nx.topological_sort(G.reverse())```
comment
```topological_sort_recursive``` was removed in PR #1756
comment
Ack -- Thanks for finding this.  What a mess...  It looks like the first removal ```test_random_graph``` should not be a removal, but a change of name for the second version of that function to ```test_random_digraph```  Thanks!
comment
Take a look at the documentation for Graphviz. The program ```dot``` that is used in your example code is described there. 
comment
Use integers as nodes. 50,000 nodes is not too big depending on the number of edges. 
comment
There is nothing to guarantee that the relabeling of nodes to integers will give the same integers for two (even identical) graphs. You can use some of the optional ```ordering``` arguments to [relabel_node_labels_to_integers](https://networkx.github.io/documentation/latest/reference/generated/networkx.relabel.convert_node_labels_to_integers.html) to try to get the same ordering.   But I wonder: do you need to relabel the nodes in the parsing code?
comment
If you've got sortable nodes (which you do) use the ```ordering``` keyword as mentioned above.
comment
This adds a dependence on ```six``` and we'd like to keep dependencies to a minimum. Also, its not clear that math.gcd is better than fractions.gcd.  I'd rather not add code that depends on python version.
comment
It looks like the test_agraph failing could be related to using GraphViz 2.40 because it only occurs for that one build. I will say: don't worry about that bug. It is almost certainly the testing setup rather than your code since you didn't touch that part of the code.  I have only looked at this quickly, and it looks good. I will ask you to try to follow the pep8 style guidelines (easy to install the pep8 program using pip).  Also, do you prefer to let us copyright the code? If you want to put a copyright on it that is fine, otherwise its OK to put the standard copyright notice in many other modules. Either way it should go in a comment at the top of the code just before the author info.  Thanks very much for the contribution!
comment
Thanks very much!!  This looks great. It would be good to add a quick test that writing and reading the node data works. It could be the example you give in this PR, but in the ```tests/test_shp.py``` file.
comment
The [documentation for all_neighbors](https://networkx.github.io/documentation/stable/reference/generated/networkx.classes.function.all_neighbors.html) states that the return value is an iterator over the neighbor nodes. That is what you see when you print it: a ```dictionary-keyiterator```.  To get a list (if that's what you want) use ```print node, list(neigh)```.  But you probably should work through the tutorial (available from the sidebar of the documentation). There are other ways to get the neighbors of a node that are better in many cases.  ```G[node]``` returns a dict keyed by neighbors with values being the edge attribute dicts. So, ```list(G[node])``` is a list of neighbors.
comment
Returning edge data for simple paths should be implemented by the user as post-processing of the simple path. We want our functions to be simple and efficient and the upside of avoiding looking up the edge data from the graph doesn't make up for the downside of building and returning data lists even when data is not wanted.  Try (untested):      [[(u, v, G[u][v]['data_attr']) for u, v in nx.utils.pairwise(path)] for path in nx.all_simple_paths(G)]
comment
I interpret the paper as you do. It also matches how Wikipedia defines local efficiency. Would you like to create a pull request with this change and a test that it is correct? Or would you prefer that I do that.
comment
Take a look at #2233 where this (mis)behavior was implemented apparently to avoid an error raised when the center node is not present. It also looks like we should forego the ```ego_graph``` altogether in favor of ```G.subgraph(G[v])``` which will not include the center node ```v```.
comment
It will almost certainly be easier for me to create a pull request due to familiarity with github and the codebase/tests etc. But I didn't want to do that if you were already working on it.  ```G[v]``` is an easy way to get all neighbors of v. It is actually a dict keyed by neighbors of v with values being the edge attribute dicts, but iterating over ```G[v]``` provides the neighbors of v in the graph. In the end we need ```G.subgraph(neighbors_of_v)``` and ```G[v]``` is an easy way to get the neighbors.  I'll put together a pull request soon.
comment
This Issue requires 1) figuring out if we are correct that the local_efficiency should not include the node in question in its neighborhood.  The comments here suggest it should not. 2) If it should not include the node in question then we need to look at #2233 to figure out why that error was occurring and provide a fix (if it hasn't been fixed already). That error involves a report of ```PathNotFound``` when the graph is not connected. I believe the fix for that spurious error is that the global efficiency computation should contribute zero for that pair of nodes (infinite distance provides zero efficiency). But that needs to be determined. It is also possible that a fix has already been put in place for the case of infinite distance between nodes. I vaguely recall that being discussed, but it may have been for a different routine. 3) We should provide some good citations to the info that provides the answers to these questions. 4) Finally, we get down to changing the code -- which I believe should be simple to change.  So, while the code changes here will be fairly simple, the present hurdle is figuring out what actually needs to be done.  If this is what you are looking for then take it on!  Other tips are to read the Wiki page for style features and to look at other modules for examples of how to put together a new module.   :)
comment
Fixed by #2843
comment
Yes, close the issue...  it will still show up if someone searches for it. And we won't be fixing the old version.
comment
What happens with      print(nx.__version__)  
comment
Try updating to the v2.1rc1 prelease candidate (or current master on github). Or you can wait until next week when v2.1 will come out with a fix. I get your output with v2.0 and I get this with v2.1:      [('A', 'C', {'weight': 0.5}), ('A', 'B', {'weight': 0.5}), ('D', 'C', {'weight': 1.0}), ('C', 'A', {'weight': 0.8}), ('C', 'D', {'weight': 0.2}), ('B', 'A', {'weight': 1.0})]  You can get the prelease using:   ```pip install --pre --upgrade networkx```  Note that for your original post (with ```MultiGraph```) because it is not directed, you will only get one direction of each edge (presumably the last direction added).  I'm not sure why you don't get both directions when you use a MultiGraph...
comment
Can you show us the Graph g and the code used to produce the graphml file?      print(g.nodes)     print(g.edges(data=True))     # whatever write_graphml code you used
comment
Thanks for that. It looks to me like you will need to include data attributes for the edges (and the nodes). The edge keys used by default in NetworkX are integers starting at 0 for each edge pair.  Those edge keys are what gets used in the "id" field in write_graphml. I'm not sure what the node name field is for yEd but if it is "name", then you will need to create a node data attribute called "name" for each node.      # not tested     G=nx.MultiDiGraph()     for n in range(4):         nodename = "{}node}.format(n + 1)         G.add_node(nodename, name=nodename)     dir_edges = reduce(lambda x, y: x + y, map(list, self._walked_path.values())) # list of tuples     G.add_edges_from((x, y, id) for id,(x,y) in enumerate(dir_edges))     nx.write_graphml(G, path='home/usr/graph.graphml') 
comment
merged in #2821
comment
It looks like this is pervasive:  `add_node, add_nodes_from, add_edge, add_edges_from, __init__, add_weighted_edges_from, add_star, add_path, add_cycle` And these are just from the graph.py file.  In most cases, it means we can't have attributes named: `self, nodes, n, u, v, attr_dict, ebunch` and I agree that n,u,v are poor choices to rule out.  But I think worse is for `__init__` we have ruled out `data` as a graph attribute name.  We could get around all of this by using `(*args, **kwargs)` and implementing our own argument parsing. For add_node it is not much slower (~15%), but it's ugly and cumbersome:   ``` def add_node(*args, **attr):     nargs = len(args)     try:         self = args[0]         n = args[1]     except:         e="add_node() takes at least 2 arguments ({} given)"%nargs         raise TypeError(e)     if nargs>3:         e="add_node() takes at most 3 arguments ({} given)"%nargs         raise TypeError(e)     # set up attribute dict     if nargs==2:         attr_dict = attr     else: # nargs==3         attr_dict = args[2]         try:             attr_dict.update(attr)         except AttributeError:             raise NetworkXError(\                 "The attr_dict argument must be a dictionary.")     #Now we're ready to add the node ```  Maybe the best thing is to change these names to more obscure spellings so the difficulty won't arise often.  Perhaps   ``` def add_node(self, node_to_be_added, attr_dict=None, **attr) def add_nodes_from(self, nodes_to_be_added, **attr) def __init__(self, init_data=None, **attr) ``` 
comment
Another approach to help with this issue is to use underscores for argument names.  `def add_node(self, _n, **attr)` 
comment
How is ```ekey``` for the keyword name of the edge key variable in all graph methods?   @chebee7i any thoughts?  I'm working on changing keyword arguments so they don't get clobbered. The one I'm having the most trouble with is ```key``` for ```G.add_edge()```. I should rename it because it can get clobbered by ```**kwargs``` and it is a potentially very common name for edge attributes.  Bu then G.remove_edge and others should change so they don't use ```key``` while ```add_edge``` uses something else. 
comment
In #2824 I'm leaving key as an argument for the edge key.  I think this could potentially be trouble, but changing the name introduces a lot of subtle bugs for anyone who currently uses ```key=``` (which a lot of our tests do). We haven't had requests for avoiding ```key``` yet. So I'm just going to leave it and fix the others.
comment
merged in #2823 
comment
It looks like the timing for all four ```connected_component_subgraphs``` functions is within a few percent of the corresponding ```connected_components``` functions. For worst case trials (10000 isolated edges) subgraphs can get up to 20% slower. For gnp_random_graphs they are within a few percent.  These tests were performed with the option ```copy=False```.  With the ```copy=True``` keyword the times for biconnected and strongly_connected took 7 times longer, while weakly_connected was 45 times longer and connected was 80 times longer. So, ```copy=False``` should be the default.  The most recent comment I can find on this is by @jtorrents back in May 2015 in ticket #1501 expressing a change in heart about making subgraph creation automatic.  > Yielding sets of nodes forces user code to be more explicit about subgraph copy handling, I think that this pattern: >  >     for component in nx.connected_components(G): >         Gc = G.subgraph(component) # or G.subgraph(component).copy()  >         # Do stuff with Gc >  > Is better than: >  >     for Gc in nx.connected_components(G): # Assume that we yield subgraphs with default to no copy  >         # Do stuff with Gc >  > Because the former forces the user to think, do I need to modify Gc? do I want these modifications to propagate to G? If we yield subgraphs with no copy as default and the user changes Gc it can lead to bugs/surprises.  I believe this is an argument for simply removing the ```connected_component_subgraphs``` method and NOT changing the current ```connected_components``` function.   Ideas?
comment
Instead of a ```directed``` argument, should this function have a ```create_using``` argument so people can make ```MultiGraph``` and ```MultiDiGraph``` too?
comment
```jit_data``` is writing the data while ```jit_graph``` is reading it.  So I would be fine with having only ```jit_graph``` support ```create_using```. That API also makes it easier for people to use their home-grown graph classes with this function...  Probably a small market, but I think its the better API. Thoughts?      def jit_graph(data, create_using=None):         ...         if create_using is None:              G = nx.Graph() 
comment
This looks like a bug.  Thanks for the report! I believe the cause is selfloops.  It looks like ```len(G.edges)``` does not count selfloops.  Short term fix is to use ```G.size()``` or ```G.number_of_edges()``` but tweak the following in more complex situations:      sum(len(nbrs)+(n in nbrs) for n, nbrs in G.adj.items()) // 2  or      sum(1 for e in G.edges) 
comment
This PR allowed all tests to pass for many days. But now it has changed again so that the error shows when the documentation build process tests the doctests.  In Travis it is the first config that produces the error in e.g. #2816 . Not sure why the legacy print options would not work in this environment. 
comment
The SubGraph view objects can provide a way to specify which types of nodes/edges to include in a subgraph. So, with a full set of info stored on a ```MultiDiGraph``` with node attributes for mode and edge attributes for crossnet ('dis-dis', 'dis-g', 'g-drug') you could access the network of 'dis-dis' edges using:      G = MultiDiGraph()     ... add nodes and edges     def disdis_edges(u, v, k):         return G[u][v][k]['crossnet'] == 'dis-dis'     disdis = SubMultiDiGraph(G, show_edges=disdis_edges)     print(disdis.edges)  This is a relatively new feature and isn't particularly well documented, but I think it is closely related to what you are looking for.
comment
The degree of a node is not necessarily the number of neighbors. If you have self-loops for example, the results you show would occur. 
comment
I've been wondering about what should be returned with ```_edges```...  We could return an edge_collection when arrows are not requested and an ```(edge_collection, arrow_collection)``` tuple if arrows are requested.  Another approach would be to make two functions -- one for edges and one for arrows. Each returns the collection it creates.  Each gets its own ```**kwds```.  Is there much overlap between keywords for line and fancyarrow? 
comment
I think for now let's try to get the features in with one function and a few arrow-specific arguments. I guess I didn't understand that for the arrows case we don't have a line collection at all...  Guess I need to look at the code more. :}  It seems like ```FancyArrowPatch``` has some features that help relative to ```FancyArrow```, so I'll leave the choice of which to use up to you. And if the bug with ```patchcollections``` is likely to be fixed then let's not make the choice depend on that.```
comment
I'm fine with returning None for the time being. Any container of the patches would be fine with me also. What do you think?
comment
Yes -- that is a good idea. If the example is to change the alpha values of the arrows individually, then I think that would take care of the last unaddressed target for this implementation.  Are there any others?
comment
I think the examples folder would be for something bigger while the doc_string works better for a small example. If you can create a 2-3 edge network with different alphas for each arrow, I'm guessing that would be short enough to put in the doc_string.  I'd prefer it there. Then as a separate PR we could create an example for the examples folder that highlights the arrows features more generally.  Does this make sense? Is there a better way?
comment
I think we need a little more info about TrackGraph. Does the same thing happen with DiGraph? Ideally, you could construct a short example that recreates the problem. Based on what you report above my first thought is that maybe TrackGraph doesn't call ```DiGraph.__init__``` in its ```__init__``` method.
comment
Reopen this if you can provide more info.
comment
It needs the decorator package. That is our only "hard" requirement. (meaning NumPy and SciPy are nice but not necessary. ```decorator``` is necessary).   Most installing systems (pip, conda, etc) will install it when you install NetworkX, but if you install from github or from source, you will need to install the decorator package as well.  Look for version 4.1 or later.
comment
Fixed in #2788
comment
I think the ```**kwds``` argument trouble only occurs when you have one NetworkX function calling multiple functions with the same kwds dict.  For example, that means ```nx.draw_networkx``` should not use ```**kwds``` because those arguments could be meant for nodes, or edges, or labels.  But ```nx.draw_networkx_nodes``` can use kwds because it only calls ```ax.scatter``` and we can document that any kwds not specifically provided will be passed to matplotlib's ```scatter```.  ```nx.draw_networkx_edges``` calls ```LineCollection``` twice -- once for edges and once for arrows. Perhaps arrow drawing should be separated from edge drawing. Or we can provide the same ```kwds``` to both?  ```nx.draw_networkx_labels``` and ```nx.draw_edge_labels``` simple pass kwds to ```ax.text```.  I think using ```**kwds``` whenever it is clear what function they get delivered to will be easier in the long run than trying to keep up with the matplotlib argument changes. If we don't think matplotlib will change their arguments anytime soon, we can simply provide all their arguments explicitly.  But even then it seems cleaner to use kwds and document that we are passing those to a specific matplotlib tool.  
comment
The reason this doesn't pass tests is that ```**kwds``` cannot be removed even though it LOOKS like it is not used. In fact it is used in ```draw_networkx_nodes``` to collect all the keywords for edges and labels that aren't intended for nodes. The function ```draw_networkx``` uses the same dict for attributes for all three types of drawing.    Our use of ```**kwds``` here is not documented and is confusing.   Best solution may be to pass keywords on to mpl through the function  ```_nodes```, ```_edges```, ```_labels``` but to pre-process them in ```draw_networkx``` -- allowing only select attributes to be passed along. First step is to see how many collisions between scatter/linecollection/text/arrowstuff there are.
comment
I'm closing this in light of #2788 adding edgecolors for node borders and the  ongoing work in #2757 I envision a reconsideration of ```**kwds``` for these routines that somehow protects/separates the keywords meant for each part of the drawing. 
comment
See #2748  Maybe this is the right short term fix for that issue as well...
comment
I guess the best guide we have to new contributors is on the wiki pages. I hope that anyone is able to make changes to them, though I haven't tested. If you are able to add things or improve what's there please do so. It probably needs attention. Thanks. 
comment
Merged in #2785 
comment
There is an issue #1583  related to reducing this kind of problem in our codebase.  But a short term fix is:      G.add_node(node_id)     G.nodes[node_id].update(data)
comment
I see you have added new changes fairly recently.  What is the status of this effort? and Thanks!
comment
Would you be willing to add a short paragraph to each docstring -- after the one-line description and before the parameters section -- defining what the graph-edit-distance is and what that function computes?  I think it is ready to merge otherwise... please continue to add things after the merge. It seems there is a fair amount of interest in this feature.
comment
Merged via #2782
comment
I'm going to close this. The reverse operation is a pretty light-weight view and shouldn't be avoided. I'd like to keep the reverse keyword out of the shortest_path routines too. So I'm closing this PR.  If there are other parts that I missed open it again.
comment
Yes - Appveyor changed something last week.  I'll open an issue for that.  Thanks for this! 
comment
If the edge already exists, then use ```G.edges[u,v].update(attr)``` If not and you need to avoid attribute names ```self```, ```u``` and ```v```, then      G.add_edge(u,v)     G.edges[u, v].update(attr)  see also #2562
comment
Ahh...   Yes, that should at least be reworded, if not removed.  That refers to the fact that ```G.edges[u,v]={'foo':'bar'}``` is not allowed.  You can't assign to ```G.edges[1, 2]```.  But you can change and update the dict that it represents.
comment
This all looks intriguing and useful.   Two things I have thought of to look into:  Have you done any speed tests for the case when restrictions are not needed?  (Some other algorithms call shortest_path repeatedly and so want it to not be slow.)  Can this be done for ALL the algorithms of network using a subclass of Graph/DiGraph rather than a change to each algorithm?  (The subclass could hold the list of nodes and/or edges that get restricted before returning edges/neighbors/etc.)  Nice work! Dan  On Aug 31, 2012, at 9:59 AM, aparamon wrote:  > 2012/8/31 Jordi Torrents notifications@github.com  >  > > Andrey,  > >  > > This implementation looks good to me, and the tests that you wrote are nice. The only thing to do for the unweighted case is to update the docstrings of shortest_path and friends. This will be very helpful for implementing White and Newman's approximation for node independent paths.  > >  > > Regarding the weighted case, do you think that it is worth to also support restrictions? We could use the same approach, but I'm not sure that it is as useful as in the unweighted case because you always can manipulate the weights to impose restrictions.  >  > Thanks.  >  > I think support for the weighted case should be also implemented, and  > I think it's not very hard to do. It would be very useful because  > currently you cannot get "there is no pass" exception via playing with  > weights. And you might not want to play with weights because it's side  > effect (frozen graphs etc).  >  > Also I think that all _shortest_path_ procedures should support  > restrictions. However, the amount of interface changes looks  > frightening to do at once. But I don't really see any practical  > alternative to passing the options. Copying graph and modifying it  > according to restrictions before passing to shortest_path appears to  > be just too damn slow :-(  >  > When everyone agrees on the interface I will also push my  > implementation of yen_simple_paths. I only need to come up with the  > better name probably :-)  >  > Best wishes,  > Andrey Paramonov > — > Reply to this email directly or view it on GitHub. 
comment
> > Have you done any speed tests for the case when restrictions are not > > needed? (Some other algorithms call shortest_path repeatedly and so want it > > to not be slow.) >  > My change was designed to not slow down the common case of no > restrictions. It only 2 checks per call are added which is absolutely > negligible. >  > > Can this be done for ALL the algorithms of network using a subclass of > > Graph/DiGraph rather than a change to each algorithm? (The subclass could > > hold the list of nodes and/or edges that get restricted before returning > > edges/neighbors/etc.) >  > I think it's not better compared to just modifying graph in-place > before calling shortes_path. I could design robust "with" context > class for this purpose. However this implies that I should rely on the > side effects, which I don't like (frozen graphs etc). But if anyone > agrees that teaching every _shortest_path_ procedure to use > restrictions is just mad, I think I could provide alternative > implementation (context class). >  > Best wishes, > Andrey Paramonov  I agree.  These arguments make sense. I'm +1 for this approach.  How many of the other shortest_path routines would be straightforward to update? Certainly, the ones that use _bidirectional_pred_succ.  Might be others too... Maybe more important is "how many wouldn't be straightforward to add this feature to?" :) Dan 
comment
I'm not sure if this is relevant for you, but you can view a road atlas with each street segment being a node.  Then intersections act as edges.  If there is no left turn than an edge doesn't connect those two nodes. This might relieve the issue of restricted sub-paths.  But there may be other issues with this approach.  I haven't used it myself. Dan  On Oct 23, 2012, at 12:04 PM, timstirling wrote:  > Just wanted to add that I have also found need for shortest path algorithms with path restrictions. Basically I have a road atlas with nodes ad road intersections and streets as the edges. One-way streets are handles using a Digraph. However there are other driver restrictions, e.g. sometimes you cannot make a left turn. >  > Given 3 nodes [1,2,3], it is feasible to traverse from node 1 to 2 and also form 2-3, but it is not possible to traverse the sequence 1-2-3. But it is feasible to travel the sequence 3-2-1. IT should also be possible to travel 1-2-x-3 for any other node x. >  > I modified the a\* algorithm to test if the current head of the explored path is in a set of restricted sub-paths. A single parameter gets passed to a*, the function that will return True or False if the current path is valid or not. The complexity lies in this function.Most of my restricted sequences are only 3 nodes, so a quick check to see if the latest 3 nodes in order exist in a set of restricted sub-paths (tuples). Some of my restricted paths are longer so there is some iteration that is not implemented very fast but these restrictions are rare. >  > I don't know how far the above discussion and implementation has gone towards providing such functionality but I wanted to let the developers know that there is demand for having shortest-path algorithms with restricted sub-paths. >  > — > Reply to this email directly or view it on GitHub. 
comment
I also thought there was more about views than I can find.  But I'll try to put down what I can recall of the discussion.  From a speed perspective, the idea is that views could be faster than constructing the entire restricted graph.  But the view has to construct/return a dict for each call to G[n] and it makes sense to save that dict in case it has to return it again. If you access G[n] for all n then you've accessed the entire restricted graph (and might as well have built the graph instead of using a view). So for speed, the advantage of a view is that you can slowly create the restricted graph instead of creating it all on the first call. But if you are planning to traverse the whole restricted graph you might as well create the restricted graph in the first place.  So I can see using views in this way if you don't expect to need to traverse the entire restricted graph. It's kind of like a lazy subgraph that only creates the subgraph as it is needed.  From a low memory perspective, the idea is that views might be slimmer than creating the restricted graph. So, we in this paradigm we should not save the neighbor dicts and instead construct them each time a call like G[n] is used. The slowdown is significant, but I don't have numbers for it. Because the memory usage of creating the restricted graph is less than the original graph itself, creating the restricted graph is at worst doubling your memory needs. So my feeling is that it isn't worth using views to save memory. A factor of less than two in memory costs is not enough to justify the much longer access times. Probably better to find a way to split the graph up somehow--maybe replacing the graph with the restricted graph and then reconstructing the original graph when done with the restricted one. Dan  On Fri, Apr 11, 2014 at 1:05 PM, chebee7i notifications@github.com wrote:  > @dschult https://github.com/dschult might have more to say on views. I > can't remember where that discussion took place. I found #335https://github.com/networkx/networkx/issues/335but I swear there was more discussion than that. >  > ##  >  > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/762#issuecomment-40226342 > . 
comment
On Thu, Apr 17, 2014 at 1:43 AM, ysitu notifications@github.com wrote:  > A graph view does not necessarily have to create a dict for each call to > G[n]. It can/should instead return an O(1)-space lightweight filtering > proxy. This in particular makes sense for algorithms like Dijkstra's, which > examines each edge only once. The same argument goes for materializing a > graph view into a graph. Filling up countless dicts of dicts is just not in > any way time- and memory-friendly compared enumerating items in existing > dicts. >  > Agreed...   And we talked long ago of making the NetworkX interface more > iterator based in how it returns almost all properties.  At that time > version 2.0 was far in the future and we said that shift could come at 2.0. >   Well--- now 2.0 is not far away. :)   So maybe we need to rethink the > basic interfaces like G[n] to make them iterators.  Aric has talked about > moving that direction for a long time.  Creating a view that has Gview[n] return an iterator while the graph itself returns a dict is certainly possible too. 
comment
This moved from #1067 to #1538 to #2069 which is scheduled for NetworkX 2.1 So I'm changing this to NetworkX 2.1 as well. (Also, there are some other cycle based issues on the 2.0 list.)
comment
See #2069 
comment
This looks like it doesn't conflict with #1851 but they both change the same file (weighted.py). I'm going to merge that request before these conflict.  You may have to do a rebase. Sorry about that.  
comment
This is not still needed. Our turning the shortest_path functions into iterators needs more thought.  We should not create a dict holding the path only to iterate through the entries (and then often create another dict from the iterates). To make these routines iterators that report as they compute, we would need to either split out the length/pred/path algorithms (as they once were).  Each would then yield the appropriate output. Or we could keep the algorithm in one helper function (as now) but yield the appropriate results for possible all three of length/pred/path upon discovery of each node.  If `paths is None` then we would not yield paths; if `pred is None` we would not yield preds.  Or we can leave it as is: shortest_path_length routines iterate, shortest_path routines return dicts. 
comment
This only returns the collections from ```draw_networkx``` and not from ```draw```.  So there is no net affect from the change to the ```draw``` function. Do you mean to only change how ```draw_networkx``` works?   The docs should be changed to reflect this change.  It might be nice to know how this interacts with matplotlib animations. But that could be a separate issue -- perhaps an example in the /examples directory.
comment
Looks like the underlying probability distribution functions impose the strict inequalities, so this may be more than a simple fix.  Should be bump it to v2.1?
comment
Thanks!!
comment
It looks like all those vertices are already added to ```ignored_nodes``` in previous iterations (except the last). Perhaps a solution like adding ```root[-1]``` at the end of the loop would work the same way without having to add the entire path each iteration.   Also, can you include a test that shows the error and then ensures that it works with the fix?
comment
This looks good to me.  Anything else you are planning for this PR?
comment
For the part that fixes #2451 it looks from the OP that the code raises an exception when g is NoneType because it can't find ```g.getGeometryType```. Does this fix avoid that exception?  It looks like this fix correctly handles the case when ```g.getGeometryType``` returns an invalid type, but would still fail when g is NoneType.  That said, I don't fully understand the OP example -- and you say this implements the fix suggested in #2451 so can you help me understand what is going on with that Issues?  Thanks!
comment
Yes -- Thank you!  I was getting mixed around with the strict argument's intent.  I understand much better now. This was helpful.  Also, thank you for the PR! :)
comment
That is certainly a bug -- Thanks for the report!  I think it was introduced two years ago in an attempt to make it more simple... but a typo snuck into the mapping on line 429:  ```mapping = {c: v for c, (u, v) in enumerate(order)}```   should be ```mapping = {c: u for c, (u, v) in enumerate(order)}```   Looks like we will need more tests (I haven't looked at them yet).
comment
Fixed in #2743 and #2744  You can get it from github, or wait for v2.1 which should be coming fairly soon.
comment
Thanks!
comment
See #2768   (and #1640)
comment
Moving this to milestone networkx-future because the PR for this ( #782 )  has that milestone.
comment
It looks like your np.full() command loses the dtype option. I think it just needs a dtype=dtype. minor: spaces after commas in the function calls is PEP8 standard.  Can you add some simple tests to show the problem--and the fix? 
comment
It sounds like all the edges are single (though weighted) edges.  The ability of MultiGraphs to hold different weights for different edges is lost so it might be better to call it WeightedGraph.  The distinction between integer weights and floating point might be important though. Certainly an integer matrix can hold binary adjacency info, but the class could also allow non-binary integer valued edge weights without much trouble.  The adjacency is still tested by whether the weight is non-zero.  Allowing integer weights other than one could use the same interface as the current classes. Floating point weights are nice--but in some applications cause trouble because of round-off.  Providing both yields four graph classes based on directed/undirected and integer/floating point.  It might be easier to allow any valid dtype and then just provide 2 classes: directed/undirected.  Then the user can choose the dtype and you don't have to worry about it. The algorithms involving edge weights should work for any numerical dtype.  Too much fun... :) 
comment
Can't a scipy matrix use any dtype?  If so, then maybe its best to implement the straight forward case using 0 as non-edge. Then if people want zero-like values that aren't zero they can use a dtype with that behavior.  (The idea being that we don't have to solve all possibilities now.) 
comment
Regarding different classes for different algorithms:  If the graph class uses non-zero (or ...) for adjacency then you don't need a separate class to implement the different algorithms. The unweighted version ignores the value (except to check if zero) while the weighted version uses the value.  There may be other cases where it's important to use only 0/1 values instead of 0/non-zero, but they are not the common cases. Maybe it is best to start with 2 classes directed/undirected and then see if the unweighted case is helpful.  I think you need two classes (directed/undirected) because add_edge will affect two entries in one case and only one entry for the other.  Also the definition of degree differs and edges() keeps track of duplicate reporting differently.  It is tempting (to me) to try to make one DiGraph class and then write the Graph class to use the DiGraph class, but each time I've played with that I end up deciding its better to just have two classes. :) 
comment
1. Maybe the initial attempt should just be the frozen graph.  Users can build the graph as a standard NetworkX graph and then transform that to the matrix form.  Then you can focus on the matrix part of the implementation and worry about the add_node, etc parts later. 2. As you mentioned earlier it is nice that e.g. Pandas allows hashable or index access. If the regular hashable lookup is not found then an index lookup is tried. But that's probably a feature for later too rather than a first attempt. 3. In my trials I decided it was best to keep a dict for looking up the index and a list for finding the hashable for an index.  If ordereddict works better for that it seems quite reasonable. 
comment
The discussion from #1878 that should be here is:  @jfinkels wrote  Don't add lines to this attribute. Your copyright ownership is noted in the file header.  @dschult wrote  I'm not clear on this spec. I understand that we're going to be slowly moving through the code shifting **author** lines to comments. But I never saw a discussion of how that should appear.  Are we going to list authors in a comment line, or list them as part of a copyright statement? If we list them as part of a copyright, how do we handle the year, etc... @hagberg I think you have talked to lawyer types about this sort of thing. How should we annotate authors in the source?  @jfinkels wrote  This is why I suggested #1675 (which I consider not really a "Documentation" issue, but a "Defect", for the problems you propose here). This is how, for example, Python Software Foundation does it.  The way it roughly seems to have been done until now is this: a contributor distributes the code that he or she authors (and on which he or she has a copyright) to NetworkX under an appropriate license (i.e. the Modified BSD License), and the copyright for that bit of contributed code (i.e. the author and year) is listed.  @hagberg wrote:  This is off topic from this PR but... I'd like to keep the barrier as low as possible for contributing.  There is no networkx organization at this point. If we wanted to consider a copyright license agreement we'd have to create something like that first.  The current situation is as @jfinkels describes. We accept code contributions from authors who must have the authority and ability to distribute the code under the modified BSD license. We don't require a copyright statement if the author does not provide one and are willing to add the standard NetworkX copyright if requested. I'd prefer not to include any code without an explicit copyright statement.  We have no clear license on the documentation other than BSD (which isn't quite appropriate for documentation).  @dschult wrote   So, if we have a copyright line in the header for the file, do we put their name in the comment listing authors? Or is the authors comment line optional if/when we have copyright lines?  @hagberg wrote  I don't see a problem with having both a copyright and an author. But that is optional as far as I am concerned. 
comment
I added a section to the "Developer Notes" wiki page on how to format the module header lines. It would be great to make that text match what we want to head for.  I think it is close already.  I invite people to please adjust/change that whole page as you think fit.... 
comment
The original post here points to CLAHUB as a service that collects and checks if contributors agree to the CLA as part of the pull request process. It looks cool--but (see [clahub issue #111](https://github.com/clahub/clahub/issues/111)) the maintainer is looking for a permanent host for the service. In that issue are pointers to other similar services.  Would someone like to find an appropriate service for us?  
comment
This looks good.  I think the branches should mimic the ones for the networkx repository. That is, when a release is made we should make a release of the notebooks too. The master branch in each should be the development branch.  Otherwise I'm going to have trouble keeping track of which release matches with which examples release.  :)
comment
If I understand correctly mybinder.org is a website where users can run a "live" version of each notebook through their browser.  It's too bad that the site isn't quite production ready yet. Perhaps we can wait on browser based functionality and concentrate on setting up automated testing where each branch of notebooks tests with a corresponding version of NetworkX.  I think that is what is needed to make the notebooks separate repository viable.    It looks like sphinx-gallery can put many of the examples on ReadTheDocs with a link to download the notebook in either ```.py``` or ```.ipynb``` form. That interface allows automated testing through Travis with a sync'd version of networkx (because they are in the same repository).  Can we do something like that with a separate notebooks repository?
comment
Ok maybe i am getting the gist of this more...   This system would allow us to automatically test the notebooks by putting a Travis.yml file in the repository. Should the branch of NetworkX be controlled in requirements.txt? Or maybe instead using pip to github directly in the yml file?   The nbval testing system for notebooks seems pretty good. Each cell is a test against the expected output.  Also, while the mybinder live-notebook-in-a-browser is still in beta, it is pretty good already. I was able to do what I wanted with the UI.  The issue with installing pygraphviz is probably that it needs GraphViz. Any I don't think GraphViz can be installed by pip (though maybe it can -- I'm no expert on pip).  The front page mentions that you can use either a requirements file or a DockerFile to set up the install. Somehow we need GraphViz installed before the pip command tries to install pygraphviz.
comment
This sounds like what it should do. What do you think it should do?
comment
You would specify the optional dependency by importing the package, but making sure everything loads OK if that import fails. Numpy and Scipy are optional dependencies in exactly this manner.  You could take a look at the ```networkx/linalg``` folder for example code.   If I understand what you mean by spectral clustering it seems like the ```linalg``` folder would be a good place to add a module for spectral clustering. If you think that another place makes more sense, propose it. :)  Thanks!
comment
Line 270 in ```_find_fiedler_function``` needs to add postprocessing of the regular expression:      if method is None:         method = 'pcg'  When method is None (the default), it causes ```q``` to be treated as if chol/lu methods were being used instead of pcg. But in the tracemin function 'pcg' is used. So the method is a cross between the two. The major impact I see is that 'pcg' is always set to find the "largest" two eigenvectors -- fiedler vector being the second one and the first being a constant vector. The bug makes 'pcg' look for more than two vectors.  The 'chol' and 'lu' methods are constructed to find more than two eigenvectors.   The result for the failing example is that when the default method takes more than one iteration (enters the deflate loop) the second column of ```W``` becomes all zeros and the matrix is thus singular. This creates "NaN" which never allow the finishing condition to be satisfied.  This bug is very old (~3 years).  It is possible that the default method has been giving incorrect results.  All tests I have done show that the default was either correct, or the process hung. This would align with the idea that the problem only arose if the program got into the "deflate" loop of of the tracemin algorithm, and that at that point it always hung.  So I am hopeful that it hasn't been returning incorrect results.
comment
Still getting rare (but not rare enough) hangs due to random initialization of ```X```.   Fix for now is to use a different method (```tracemin_lu``` or ```tracemin_chol``` or ```lanczos``` or ```lobpcg```) I'll try to track down more.
comment
Thanks for this feedback. Your careful description and analysis is very helpful.  The self-loop methods also were moved from the graph class to functions in the main namespace. I'm guessing they have smaller impact, but potentially similar arguments. Still, they are reporting methods, while ```add_path```, etc are mutating methods/functions.  It's hard to know where the line should be drawn. Simple interfaces are nice: as simple as possible but not simpler.  :)
comment
This makes sense to me. It looks like the input-check for isolated nodes was written without directed edges being considered. Would you like to submit your fix as a pull request?  Thanks!
comment
Any progress on a test case here?
comment
Yes -- that is often done.  Thanks
comment
It is true that pickle doesn't store the class definitions, only the data itself. So you won't be able to read a pickled v1 Graph using v2.  You can still use pickle to do that transfer of the node and edge information though.      # in v1.x     pickle.dump([G.nodes(data=True), G.edges(data=True)], file)     # then in v2.x     nodes, edges = pickle.load(file)     G=nx.Graph()     G.add_nodes_from(nodes)     G.add_edges_from(edges)  If you don't have node attributes and you don't have isolated nodes, you can just pickle the edges. Then creating ```G``` is simply ```G = nx.Graph(pickle.load(file))```  Your comment about ```numpy.float64``` indicates that you have numpy arrays in your node/edge attributes. Make sure you have a compatible version of numpy when you read the pickle file to the version of numpy when you wrote the pickle file. 
comment
Please try adding one method to your ```MyGraph``` class which will help it make copies of the right object.  The method is ```G.fresh_copy()```. It should return an empty graph object of the type you want to use for copies.      def fresh_copy(self):         return MyGraph()  Note that any subgraph views of MyGraph will be View objects that inherit from NetworkX Graph. So you might also need to overwrite the subgraph method. Try just adding ```fresh_copy``` first though.  If that doesn't work, try overwriting the subgraph method in your MyGraph class.  The current code is something like:      def subgraph(self, nodes):         induced_nodes = nx.filters.show_nodes(self.nbunch_iter(nodes))         SubGraph = nx.graphviews.SubGraph         # if already a subgraph, don't make a chain         if hasattr(self, '_NODE_OK'):             return SubGraph(self._graph, induced_nodes, self._EDGE_OK)         return SubGraph(self, induced_nodes)  The new code will require a subgraph class and a tweaked subgraph method of MyGraph.      class MySubGraph(nx.graphviews.SubGraph, MyGraph):         pass      # then inside MyGraph create a subgraph method that uses MySubGraph instead of SubGraph     ...     def subgraph(self, nodes):         induced_nodes = nx.filters.show_nodes(self.nbunch_iter(nodes))         SubGraph = MySubGraph         # if already a subgraph, don't make a chain         if hasattr(self, '_NODE_OK'):             return SubGraph(self._graph, induced_nodes, self._EDGE_OK)         return SubGraph(self, induced_nodes)   Please let me know how this works for you. This is an unanticipated complication for which we might need to create a workaround. Thanks!
comment
If you will call MyGraph on the data anyway, you don't need the copy of the subgraph. You can use ```sg = MyGraph(self.subgraph(cc))``` and then add the customdata.  Also, just to be clear -- the ```customdata``` was not copied to the subgraph under pre-2.0 subgraphs.  Only the methods were in the new subgraph.  Finally, your class' ```copy``` method will create a networkx Graph unless you add the method ```fresh_copy``` (or overwrite the copy method).    
comment
Are you suggesting an optional keyword ```max_weight```?  I think if you have a normalized weight between 0 and 1 then the max_weight will be 1.  Perhaps you are choosing random weights between 0 and 1 and haven't normalized the results to be between 0 and 1. In any case, I can certainly see why you would not want ```max_weight``` to be used.  My inclination is to remove the max_weight code. Is there a use-case where it is helpful? Can't people use ```set_edge_attributes``` to scale all the edge weights?  Also, the ```wij``` is computed before entering the loop because it doesn't change as we loop over ```k```.
comment
From my naive perspective it is probably best to make all of these iterators (unless there's a good reason not to for a specific case).  For example, if you really have to construct the list to get the iterator then let's return the list. But usually we would want an iterator.  shortest_path and friends could be quite a project if only because there are so many different versions. :) 
comment
Users do analyze dynamic graphs so we want to avoid surprises due to changing the graph. It is easy to avoid the problem by storing the iterator in a list/tuple before moving to code that changes the graph. But the documentation should make it clear that this is needed.  e.g. for topological_sort(), perhaps the doc_string can include an example where the graph is changed based on the topological sort so we use list(topological_sort(G)) in the example to avoid errors.  As for creating a new exception type for this case, There exists a python error that says the object being iterated was changed during iteration.  What is the error they get with this code? Is it just a KeyError? 
comment
@MridulS yes I think it makes sense to have path_length become an iterator of (node, length) 2-tuples. 
comment
I think of `get_node_attribute` as supplying this feature... but you are correct that to sum all the attribute values is another step. Am I correct in summarizing that your suggestion is to create a function that returns:  `sum(nx.get_node_attributes(g, attribute).values())`  With `size()` there is a literature that commonly defines this quantity and algorithms which refer to it. Do you know of anything like that for the sum of node attributes?  Thanks 
comment
There is interest here (judging from the comments over the years). NetworkX is not primarily a drawing package, but it is nice to have these tools available and easy to use. Ask here if you need help putting together your pull requests.  And Thanks, Dan  On Sat, Dec 27, 2014 at 1:56 PM, Yu Feng notifications@github.com wrote:  > Is a direct SVG renderer (not via graphviz) of any interest to the > upstream development? >  > I was to visualize some data flow charts represented with networkx in a > IPython notebook. Data flow chart nodes can have very fat text labels. This > scenario exposes a few drawbacks with the matplotlib rendering > (networkx.draw_xxxx ) functions ( > https://github.com/rainwoodman/nxsvg/blob/master/src/mpla.svg ): > - no proper arrows on edges > - marker representation of nodes are too small > - large text labels covers tails and heads of edges > - no multiline support for labels > - parallel edges are overlapping >  > I tried to fix the matplotlib renderer, but failed. pygraphviz is a > solution that I haven't explored since it requires graphviz which cannot be > installed via pip/easy_install (I imagine that's part of why we have a > builtin visualization in networkx) >  > In the end I wrote a simple direct SVG renderer skipping the matplotlib > layer completely. It depends on 'svgwrite', which can be piped. The new > renderer has specifically fixed all of these issues I listed above. >  > This also opens the possibility of embedding some nice javascripts that > allowing the user to pan/zoom or move the nodes around. Unfortunately I > don't know how to make this happen. >  > The SVG renderer is at >  > https://github.com/rainwoodman/nxsvg/blob/master/src/nxsvg.py >  > Currently it has support for directed edges, parallel edges and > self-looping edges. I am planning to add support for colors, > sub/sup-scripts, and font-sizes in labels, with a mini-language similar to > Pango markups. >  > Here is the same example rendered with the SVG renderer: >  > https://github.com/rainwoodman/nxsvg/blob/master/src/a.svg >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1312. 
comment
I think you should create a new module in the traversal directory. Name it something like depth_limited_search.py.  You can use the other modules in there as a guide for how to structure it. The tests can be put in a new module in tests/ again modeled after the files in that directory.   In general, we try to follow the [PEP8](https://www.python.org/dev/peps/pep-0008/) guidelines for Python style. It would be helpful to have `nose` installed for tests, and maybe sphinx to be able to build the docs locally to make sure they look good. 
comment
We did have it that way in some iteration..... Anyone recall why we didn't make the Graph and MultiGraph method arguments match?  On Tue, Jan 12, 2016 at 4:32 AM, jfinkels notifications@github.com wrote:  > There are many places in the code where we have >  > if G.is_multigraph(): >     edges = G.edges(keys=True) > else: >     edges = G.edges() >  > It would be simpler to just allow the Graph.edges() method to have a > keyword argument keys and just ignore it (or have **kw). >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1934. 
comment
Yes, I think edge objects were a large part of it.   I also vaguely recall it might be confusing to have `G.edges(keys=True)` return a 2-tuple for Graph and 3-tuple for MultiGraph.  Currently the output of all the variants of `G.edges(data=..., keys=...)` are the same tuple for both Graph and MultiGraph (when they are allowed). So you know how large a tuple to expect. This proposal would make it easier to create a bug where `keys=True` is used without an output check for multiedges.  The code idiom in the original post serves as a reminder that we have to handle the output carefully.    We could have `G.edges(keys=True)` for a Graph return a 3-tuple (u,v,0) for each edge... Would that simplify code in these places that use the code idiom in OP? 
comment
It would be nice to make a smooth difference between multiedge 3-tuples and edges 2-tuples. I would say its a NetworkX "wart". Been looking at it again with the edge views idea. Suggestions are welcome. 
comment
We certainly wont discourage a pull request. I don't know of any reasons this would not work. If you get it working for node labels the update for edge labels would be similar. :) 
comment
I like this discussion--we've talked about this some before of course but it'd be nice to figure out how to move forward with it. In the past I have been focused on performance issues with the backends.  But maybe they aren't so important.  Anyway, here are some issues I've tried to deal with when making test versions of matrix backends. - Integer nodes or not.  We can enforce integer nodes or we can use a dict to map nodes to indexes in the matrix. The translation from general node to index number and back gives a small performance hit with each access to the data. It also makes code a little messy to translate back and forth. So my preference is for the class to use integer nodes.  - adding nodes expands the martrix.  As @chebee7i mentioned you might need to expand the matrix each time a node is added.  But I think there are good rules of thumb for expanding in chunks that are mostly unused at first.  The Python dict class does this. So instead of adding one row/column to the matrix, add many at once and keep track of how many you have used. Then adding a new node usually just increments the number of rows/columns you actually use and no copying or space allocation is needed most of the time.  Occasionally when you add a node you have to expand the matrix too. I also think it is a good idea to allow the initial size to be specified. We should be able to do this fairly efficiently I think. - API issues: The NetworkX API is set up to return dictionaries in many cases e.g. G[n] and        G[n][nbr].  To strictly follow that API a matrix backend would need to create dictionaries for each of these calls. At some point it may be faster to convert to the dict-of-dict structure to run some algorithms and back to the matrix for other algorithms.  But some small changes to API in NetworkX 2.0 might help enable alternate such data structures. For example G(n) could return a full dict while G[n] returns a node iterator (which can be but doesn't have to be a dict). Note that there isn't an official NetworkX API so much as a set of examples of usage from the algorithms code.  So what Id like to see is the algorithms code work easily with graph classes using other data structures. 
comment
I'd prefer to see graph code housed in NetworkX and a (already occurring) stronger link to scipy/numpy code in networkx. If having cython required for installing networkx is a problem it doesn't seem difficult to provide a setup option that removes any such code (or even provides a pure python version in parallel for testing). But there may be aspects of this I am not aware of. Still, I think we should try it and see what happens. 
comment
@bjedwards Do I understand correctly that you are thinking the separate modules for nx_draw and nx_other_graphs would still be part of the networkx package? So in addition to directories for examples and networkx and docs we would add two more?  Or are you thinking of separate packages.  Maybe I should look at scikits more to understand.  On Sun, Mar 16, 2014 at 8:53 PM, Ben Edwards notifications@github.comwrote:  > @hagberg https://github.com/hagberg No problem! I'll try to paraphrase > my comment. I am +1 for having alternative graph implementations in a > separate module. I think this has organizational value, and we could > provide the suggested (and partly implemented) big graph types in this > library. NetworkX 2.0 might be a good time to do this. Additionally, at the > same time we could think about moving all the drawing functions out as > well. This would start a small, but hopefully useful scikits eco-system > for networkx. I know the scipy guys have moved away from the idea of > scikits, for example statsmodels no longer bears that name. >  > ##  >  > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/1076#issuecomment-37777541 > . 
comment
So, what applications/algorithms are we thinking of for the G.query() interface? If we can come up with a good network science problem that this would help with  it will help us design the interface.  If we can't, maybe we should hold off. 
comment
Take a look at:   https://stackoverflow.com/questions/35991403/python-pip-install-gives-command-python-setup-py-egg-info-failed-with-error-c  Reopen this if that doesn't solve the issue.
comment
Thanks @daefresh for this report!  The example helps quite a bit too.   I've isolated the problem to subgraph's iteration when there are many nodes in the graph and few in the subgraph.  It looks like this requires a replication to FilterAdjacency of what #2687 did for FilterAtlas.  We can check for a "nodes" attribute on the NODE_OK attribute of the filter. If it is there (usually the case) then iterate over those nodes checking that they are in the graph rather than iterating over the graph and checking if the nodes are in the filter.  I hope to have a PR by the weekend that would fix it.  If you need it quickly, look for the ```__iter__``` method in ```FilterAdjacency``` (about line 325 of coreviews.py).  Add the following two lines before the currently one-line body of that method.              if hasattr(self.NODE_OK, 'nodes'):                 return (n for n in self.NODE_OK.nodes if n in self._atlas)  This may also help #2716 and the fix I'm working on is shown in #2687 for FilterAtlas.  Thanks again for this report!
comment
There are so many things that affect timing. Your other program may be a compiled program -- Python in interpreted. The advantage of Python is flexible and understandable code. If you need speed, you should run your python code with PyPy or with Cython.  A brief look at your code tells me that there are some things you can do to speed it up. Perhaps the first would be to time portions of the code to find the bottlenecks. Your single line: ```p = str(p)``` could be taking a huge amount of time since you have 2.5 million keys and values to make into a string. Each of those keys is many characters and you have values associated with each. So you are probably talking about a 30-50 million character string.  You don't ever use the string version of p, so why create it?  I'm going to guess that most of your time is spent reading the file, creating an graph and calculating betweenness_centrality. Perhaps conversion to and from strings is a fair amount too. If it is reading the file and creating the graph, then you should do just that once and store the graph in a different format. If it is betweenness_centrality then you should consider making your nodes into integers rather than strings, and using Cython or PyPy to run the same code.
comment
To do what v1.x ```G.subgraph(nodes)``` did, you could use:      H=G.subgraph(nodes).copy()  That's what I think of as the best generic way to create a non-view version of a node induced subgraph. It does go through all the induced nodes (and filter them) when creating the copy. But it only does that once (same as the older-style subgraph).  Creating the subgraph view is actually quite fast.  And we do put in some shortcuts for iterating over the nodes.  For example, you mention iterating over ```G.nodes```. The code for ```FilterAtlas.__iter__``` is actually      def __iter__(self):         if hasattr(self.NODE_OK, 'nodes'):             return iter(self.NODE_OK.nodes & set(self._atlas.keys()))         return (n for n in self._atlas if self.NODE_OK(n))   For a node induced subgraph the first ```if``` condition holds True,  so, we iterate over a set created by the intersection of the induced nodes and the graph nodes. Looking at the guts of intersection, we actually only iterate over the 2-3 induced nodes -- not the 50,000 nodes.  Still, it is true that every node/edge access requires lookups of some sort. The subgraph view construct has slower lookup and vastly faster creation time than the older-style subgraphs.  But the slower lookup isn't too bad either -- it's looking up in a hash (dict or set).  We didn't make subgraph views with the idea of creating a live view of a graph -- though that is a nice benefit. The goal for subgraph views was to have a fast/light way to make induced subgraphs using the data already in the Graph. This was motivated by Python 3's ```dict``` methods ```items/values/keys```.  They also create live views of the dict. But most people don't use them that way. The big benefit is that we don't create a list every time we call items/values/keys. We create a fast/light object that uses the dict data. Python 2 has methods for iterators iteritems, et.al. But those can only iterate. The views provide many other features beyond iteration. The Subgraph Views (and the edge/node/adj views) of NetworkX similarly don't copy any data. They report what is in the original graph.  For cases where you must lookup nodes often... and ```G.subgraph(nodes).copy()``` feels too slow, you could always do:      nodes=set(nodes)     H=nx.Graph((u, v, dd) for u in nodes for v,dd in G[v].items() if v in nodes)     H.add_nodes_from((n, G.node[n]) for n in nodes)  It doesn't look at any nodes other than neighbors of the induced nodes.  If you don't need data attributes, or don't have isolated nodes you can make it even faster.     But in the end, I would suggest ```G.subgraph(nodes).copy()```
comment
Thanks!
comment
As far as I can tell this method never transferred the edge data to the new graph.  Can you verify that you see the problem with the dev version, but that you don't know whether the v1.11 release has the problem?    It also looks like tis code will not work with multigraphs.  It might be that transferring edge data to the new graph should be done with ```set_edge_attributes``` or similar. But if its easy to do in the routine, we should probably put it in the code. The first attempt should replace the cited lines of code with ```TC = G.copy()``` 
comment
This looks great -- Thanks!!   
comment
Thanks @md0000 !! This can be reopened if something else is wrong.
comment
This is great!  Thanks!
comment
This is very helpful. Thanks!  It looks like our tests never add nodes directly -- only via paths. When I try to add a test for nodes that aren't part of an edge I get       lyr.CreateFeature(feat)     RuntimeError: Attempt to write non-linestring (POINT) geometry to ARC type shapefile.  Can you give me a short example to test with? Even just a network with a single node.
comment
Yes -- that's a bug. Your fix looks like a good one. Do you want to make a PR or should I fix it?
comment
Fixed in #2693 
comment
Thanks very much!! 
comment
Can you provide a short example to demonstrate this? I will be helpful to know more precisely what the trouble is. Thanks!
comment
Which version of NetworkX are you using? It looks like the offending code was removed in NX2.0, but maybe there's more offending code. :}
comment
You might want/need to start a fresh pull request with just the new file. Also you should add tests for any new functions.   Thanks!
comment
Pareto_sequence and create_degree_sequence were deprecated with a warning 4 years ago -- and have now been removed.  They can be replaced by:      import random     max_deg = n     try=0     while try < max_tries:         raw_seq = (random.paretovariate(exponent) for i in range(n))         seq = [min(max_deg, max(int(round(s)), 0) for s in raw_seq]         if networkx.is_valid_degree_sequence(seq):             break         try += 1  Configuration_model has not been changed so you can pass it the valid sequence as before.     
comment
Go for it!   Yes -- I think that's it -- try it to make sure of course...
comment
Fixed in #2503
comment
It's a bug.  The line with g.add_edge should declare the dict as an attr_dict because multigraphs have different positional arguments to the add_edge method.  I'll put in a PR (or you can).  Line 211 of convert_matrix.py should add the text `attr_dict=` as in:  ``` g.add_edge(row[src_i], row[tar_i], attr_dict={i:row[j] for i, j in edge_i}) ```  You can probably edit it by hand in   ``` /usr/lib/python2.7/site-packages/networkx-1.10-py2.7.egg/networkx/convert_matrix.py``` if you want a quick fix.  ``` 
comment
To be able to help we need much more/better information... but first you need to narrow down what's going on, follow up on the error messages to see if files exist etc.
comment
The lowest_common_ancestor code has been merged in #2597  Rebase to use it.
comment
@erotemic please go through the docstrings and add ```Parameters``` sections. I think it would also be helpful to add a summary sentence for what each function does -- along with definitions. You have some text at the module level to define your terms, but even that doesn't define k-edge-connected. We need definitions at the function level too -- remember that in the online docs, each function has their own page without the viewer being able to see the module level docs.  I'm not saying these should be long -- they shouldn't. But one or two sentence definitions of what the function does would be helpful. 
comment
Looks great!   Thanks very much!
comment
You are the second person to ask about this. So clearly we should change something in the docs or the code. Here are some suggestions:      deg = G.in_degree   # sets up a view -- can access all nodes, but doesn't until you use it.     [d for n, d in deg]   # gets all nodes' degree values     (d for n, d in deg)    # iterator over degree values     [deg[n] for n in [0, 1]]   # using lookup for only some nodes      dict(G.in_degree([0, 1])).values()    # works for nx-1 and nx-2     # G.in_degree(nlist) creates a restricted view that only allows lookup of those nodes.     # but see the fourth option above for using lookup instead.     list(d for n, d in G.in_degree([0, 1]))      [len(nbrs) for n, nbrs in G.pred.items()]    # probably slightly fastest for all nodes     [len(G.pred[n]) for n in [0,1]]               # probably slightly faster for only some nodes
comment
You might try asking on the NetworkX discussion list.  That has a broader audience and someone might have tried something similar.
comment
You might try asking at the networkx discussion list. That gets a broader audience.
comment
How long would you expect it to take?  Replacing your last line with:      n=0     for c in networkx.simple_cycles(G):         n += 1         if n % 100000 == 0: print(n)  runs to at least 55million (I stopped it). I imagine storing all those cycles in a list would use a lot of RAM which would slow it down if your machine starts swapping. You may be running into the combinatorics headache of large numbers.
comment
If clockwise and counterclockwise does not matter then you really have an undirected graph. It looks like you want a "cycle basis".  We have an algorithm in NetworkX called ```cycle_basis``` which works on undirected graphs. Take a look online for definitions of cycle basis and see if it is what you want. 
comment
Your code is not wrong. You are using the function correctly.  Just as a vector space has many choices for basis, and a coordinate system has many choices for coordinate axes, graphs have many choices for cycle basis.  Your picture makes it clear to us humans how to choose the cycles to be as short as possible. But many/most graphs don't look so symmetric when drawn.  From the algorithm's perspective it is not easy to pick which cycles to be in the basis.  In your example, the 45 cycles are a basis for all cycles. You can add them together (going across an edge in opposite directions removes that edge from the sum) to get other cycles. You can also subtract them to get them going the other direction. They are a basis because you can get all cycles by adding and subtracting these 45 (they span the cycles of the graph) and you can't write any one of them as sums/differences of the others (they are linearly independent).  The 9 big ones the function found COULD be replaced with the 9 that you found -- that would give a different basis. But each of the small ones you found could also be constructed as a sum/difference of the 45 that the function returned.  So the ```cycle_basis`` function did return a correct basis for the cycles of the graph. It was not the basis you were expecting (which I think is a basis with minimal number of nodes in the basis cycles).  But it is a valid basis. I don't know any algorithm to find the basis with minimal number of nodes.  Also- it is no accident that the number of long cycles reported in the basis is the same as the number of short cycles not reported. The size of each possible basis is the same (45). Just as the number of coordinate axes needed is the same when changing coordinate systems. It's like the dimension of the cycle space of the graph.
comment
The difference from what you are expecting is due to self-loops. In your example, node 0 is connected to itself, node 1 and node 2 in an undirected graph. The self-loop represents two connections to node 0. So the number of connections to node 0 is four. Similarly for the other nodes.  In general is you connect all N nodes to all nodes, the self-loops will make the degree of each node N+1.  This could/should be better described in the docs.
comment
The usual [definition for degree](https://en.wikipedia.org/wiki/Degree_(graph_theory)) of a node is the number of edges incident to the node with loops counted twice. Your characterization using the sum of the adjacency matrix works for graphs without loops. But self-loops are counted twice in [the degree matrix](https://en.wikipedia.org/wiki/Degree_matrix).
comment
closed via #2678 
comment
Thanks!    I'm on it. :)
comment
The nodes of the minimum spanning tree should be the same as those from the original graph. We do add the edges we've found for the tree after adding the nodes from the original graph, so it is possible that a "bug" would create extra nodes in ```mst0``` that are not in ```q```. But you are getting fewer nodes in ```mst0``` and I don't see how that could happen with the current code.  Which version of NetworkX are you using? Is it helpful to  find out the extra nodes?      ```set(mst0) ^ set(q)```
comment
I mean:  what are the nodes that get added?  You can see that by using the command: ```set(mst0) ^ set(q)          #  provides the set elements that are in either set but not both```     Hopefully it will just give you two nodes.
comment
Thank you for that simple example!  This weakness is partially fixed in NetworkX 2.0 (that is, with the current Github development branch OR the pypi version 2.0rc1 using ```pip install networkx==2.0rc1```).  Only partially fixed because the selfloop edges still aren't part of the mst forest, but the nodes are.
comment
It looks like the minimum spanning tree of a single node graph with a self-loop is just the node (That node is itself a spanning tree by all definitions I have found; and including the edge will only increase the weight of the tree).  So it looks like the v2.0 reporting of the MST is correct (it includes isolated nodes but not self-loops on isolated nodes).  I will add a note to the documentation that this is the case.  V1.11 has a bug where it doesn't report any isolated nodes which have self-loops.  Thanks for catching and reporting this!
comment
This looks like a bug --  Thanks for the detailed report! I think its just a matter of switching order, but I'll check to make sure. 
comment
I understand the change matplotlib is making toward ```isinstance``` over duck-typing (the duck-typing doesn't work with e.g. ```open```). But instead of spreading lots of basestring cruft throughout our codebase, could we continue to use ```is_string_like``` with the basestring cruft built into it. That would contain the py2/py3 checking to ```utils/misc.py```.  [eventually py2 checking will need to be deprecated too -- can we contain the impact?]
comment
This looks quite promising. I think we should go ahead and merge it and try it out.  If I understand correctly you are suggesting that we manually upload the tagged repository docs as part of the release process. This updates the dev version of the docs.  When I first looked at this I got lost in trying to figure out how it differs from e.g. SciPy. They have a documentation repository too with automated travis-ci push to github. I could understand how this proposal works more easily than I could understand how their's works.    Your work separating the commits helped considerably.  I'm fine with merging.
comment
In thinking about where to put the older documentation, it seems like ```networkx/documentation``` is a reasonable repository. While that repository is very large, it doesn't need to be accessed often -- (only when a new release is made?).   Are there advantages to having the most recent version docs in a new repository? -- you mentioned maybe ```networkx/docs```. That would separate v1.10 and earlier from v1.11 and later. The cutoff that exists was created when we switched to RTD, but I don't think there is any aesthetic reason we should split at that release point.  I like the idea of having it all in one place, but maybe I don't see advantages to having new versions in a separate repo.  If we use ```networkx/documentation``` we should add the v1.11 docs and then the v2.0 docs once that is official.  
comment
I have redirected the landing page of all versions from ReadTheDocs to the NetworkX landing page. I was not able to redirect each doc page to its corresponding docpage on github because RTD doesn't have ```.htaccess``` available.   So:   people looking for the docs of a specific function will get a 404 error.  I did some test searches and for whatever reason it seems like Google wants to show the networkx-1.10 docs on the github site ahead of the ReadTheDocs stable docs. I've seen this before too.  Hopefully the bots will be able to find the latest and stable versions with the new config.  
comment
hmm... I need some more information. I can't reproduce the error with either python3.6 or 2.7 (with the master github branch of nx).  The G.copy() doesn't actually create new node instances. It just copies the pointer to each node, so H.nodes and G.nodes show the same memory locations in my tests.   Also it seems that your code didn't include the graph in your call to ```contracted_nodes```. Is that a typo though? Because that should create a TypeError, not a KeyError...  What versions are you using?
comment
I'm going to close this issue -- if you can provide more information, reopen it.
comment
Thanks very much for the report!  I'm looking into it.
comment
It definitely depends on version. And the error messages change too...   But pickle problems are indeed caused by pickling iterators, lambda function, and other objects that require information outside of what is held in that object's __dict__.  I think the way to eliminate these errors is to have getstate/setstate avoid pickling these objects.  I can get rid of the reported error regarding G.edges (where G is a base graph class) by changing the __getstate__ and __setstate__ methods for EdgeView et al. But I am still chasing similar errors for pickling edges on a GraphView -- That is, for when G is a view of a graph.  I'll try to post a PR soon for people to look at.
comment
I've added a bunch of tests for pickling and fixed any bugs they identified (including the one posted here :).  It would be great to have people check this by running their code against it. I'll merge which will close this Issue, but you can either reopen or just comment on the closed issue to provide feedback.  Thanks @glennehrlich for finding and reporting this.
comment
I agree that RTD is not as smooth as travis. And it would be nice to track the sphinx errors rather than have to go clean it up every N months. Building the docs and pushing them to github.io would be great so long as it is automated in a low-maintenance way.  I'm interested to see what options you come up with.
comment
Thank you! That is a bug -- the ```__class__``` method is no longer effective when "self" is a view.  We need to change that to something like:      H = self.fresh_copy()     H.name = "Reverse of (%s)" % self.name
comment
This is implemented in #2523
comment
This seems to be set up correctly and tests fine. I don't get any deprecation warning, but that is probably my setup somehow.  I would prefer to make the ```_iter``` routines return an iterator instead of a list. Also, I have a slight preference to have the properties return ```self.nodes``` instead of ```self._node``` and ```self.adj``` instead of ```self._adj```.  The biggest issue I have with this is that after v2.1 we still have no way to allow people to easily produce code that works for both v1.x and v2.  So the reason for putting it back in with deprecated status will still be there with v2.1.  Don't we need to make a long term decision here?  Here's my proposal: 1) We leave out the ```edges_iter``` and ```nodes_iter``` -- All  they need to do is change their code to ```edges``` and ```nodes``` and it will work in both v1.x and v2.x  2) We put back the ```G.node``` simply duplicating the code for ```G.nodes```.  That gives two names for the same thing -- small pain for backward compatibility.  No deprecation...  We commit to having this available for the long haul --  maybe we can only advertise one of them in the docs.  We'll eventually remove ```G.node``` but not until v1.x is basically no longer used.  Maybe v3.x?  3) We should not provide ```G.edge``` any more.  Have them change their code to ```G.adj``` which is what ```G.edge``` used to point to anyway.  4) Don't provide ```neighbors_iter```, ```adjacency_iter```, ```adjacency_list```, etc.  They should change their code to ```neighbors```, ```adjacency```, etc.  There's still another Issue #2634 about add_path, etc. that should be addressed too. But I think that is a different sort of issue.  So to summarize, my proposal is to make this PR only include code for the ```node``` property -- identical to the ```nodes``` property without the doc_string and without any deprecation notice.  What do you think?
comment
Oops... I forgot to address your first point about ```rag.copy```.  Instead of switching to ```deepcopy```, we should leave the ```rag.copy``` code the same but include a new method ```rag.fresh_copy()``` which is just ```return rag()```.  The ```Graph.copy``` method uses that to set up the new object to hold the graph.   I guess we should add docs to our discussion on subclassing that includes this wart.  We have to expect a lot of subclasses of Graph now that views are subclasses. ```fresh_copy``` was the best solution I came up with.  Maybe there's some OO magic that could make it so we can just use ```G.__class__``` like we used to, but I don't think so.
comment
You need to replace ```G.edge``` with ```G.adj```   then it works for both. So:  ```G.node``` equates to ```G.nodes```   but ```G.edge``` should switch to ```G.adj``` (which is what it actually is in v1.x).    As you point out: the interface for ```G.edges[u, v]``` is different from ```G.edge[u][v]```  and they are NOT interchangeable. 
comment
Said another way --   In code that works for both v1.x and v2.x we should have ```G.node```, ```G.nodes()```, ```G.adj``` and ```G.edges()```.   We should not have ```G.edge```, ```G.*_iter()```.
comment
I put a comment in https://github.com/scikit-image/scikit-image/pull/2766/files#r134671549 about subgraph.  I think we don't need an updated subgraph method for scikit-image. We can remove it completely and just let it use the inherited version. It is true that the v1.x subgraph creates a read-write graph while v2 creates a read-all/write-attributes view.  RAG actually relies heavily on the fact that both v1.x and v2 subgraph's attributes ARE the original graph's attribute dicts.  So changes to the subgraph attributes change the original graph.  The RAG class never changes the graph structure of the subgraphs. In my tests, everything works for RAG if we leave ```subgraph()``` as inheriting from the base graph class.  In v1.11 the docs say that people who want to change the subgraph (but don't want to change the original graph) should use ```nx.Graph(G.subgraph(nbunch))``` or ```G.subgraph(nbunch).copy()```.  Both of those work for v2 too. If anyone wants to change the graph structure of subgraphs of RAG outside of the scikit-image code, they will need to use one of these methods.  But the scikit-image code itself should be fine.  (I think it is highly unlikely that someone will want to change the graph structure of subgraphs of RAG. And they would probably already be using one of the two methods above to copy the attributes to the new graph anyway so would not have issues.)   If someone wants to do: ```H=RAG.subgraph(nodes);H.add_edge(e)```, they will need to change their code (in either v2 or v1.x) to ```H=RAG.subgraph(nodes).copy();H.add_edge(e)```.  We need to keep G.subgraph as a shallow copy for it to work for scikit-image. A view does that.  Finally, I don't think the subgraph code from v1.11 actually works for v2.0. It uses ```self.__class__()``` instead of ```self.fresh_copy()``` to create the new graph structure to fill -- so won't work when ```G``` is a subgraph/reverse/graphview.  Also the code assigns to ```H.node[n]``` when in v2.0 there is no ```H.node```.  Can you try it without that method in RAG and see what you think?
comment
> However, I am not sure how to handle G.node in downstream projects, which are assigning to it. For example, take a look at def _rename_node(graph, node_id, copy_id) in https://github.com/scikit-image/scikit-image/blob/master/skimage/future/graph/graph_merge.py What is the recommended way to modify this code so that it runs on v1 and v2.0b1?  Line 45 of the linked code assigns to ```graph.node``` when it doesn't need to.  This is an example of why we made views read-only.  To make this work for v1.x and v2.x, the code should replace      graph.node[copy_id] = graph.node[node_id]   with      graph.node[copy_id].update(graph.node[node_id])  Then the attrdict for node ```copy_id``` which was just created in line 44 is used to hold the node attributes.  In this case, they don't want/need the two attribute dicts to be the same object.  If someone really wants to make two nodes have the same attribute dicts they are breaking our data structure and they will have to muck with making their code work for both versions via their own trickery.  :)
comment
The printgraph example should be OK for v1.x and v2.x with a change of ```self.node.items()``` to ```self.nodes(data=True)```.   I'm guessing the example code is older than the ```data=True``` functionality. 
comment
Q: Can we remove ```G.node``` (as we have done) and still have people create code that works for both v1 and v2?  The iterator parts should be fine:  ```G.node.items()``` can be ```G.nodes(data=True)``` in both v2 and v1. Getting and setting node attributes are harder.  Old code v1:      G.node[n]['weight'] = 1.2     # v2.x => no attribute "node"     G.node[n]['weight'] == 1.2  New code v2:      G.nodes[n]['weight'] = 1.2     # v1.x => 'instancemethod' object has no attribute '__getitem__'     G.nodes[n]['weight'] == 1.2  What can we do to create a single code idiom that does this kind of operation in both v2 and v1? I guess the first can be ```G.add_node(n, weight=1.2)```.  But for the second, there is no "get_node" method in v1.x other than ```G.node[n]```.  You could use:       [data for u, data in G.nodes(data=True) if u == n][0]['weight'] == 1.2  but that makes me cringe.  ===========================  Notice that this is not an issue with ```G.edge```.  We can always replace ```G.edge``` with ```G.adj``` and it will work in both v1 and v2.  That's because ```G.edge``` was ```G.adj``` in v1.   So, unless we can come up with a way to write node attribute code that works in both v2 and v1, we will need to put ```G.node``` back in.  I think it should be a property that returns the same thing as ```G.nodes``` instead of ```G._node```.  This issue will persist to v2.1 unless people stop using v1.x.  I'm fine with committing to make ```G.node``` remain in the code base until people stop supporting v1.x.
comment
It looks like the test is actually passing.  It is taking FOREVER to run though.  I've run it with v1.11 and v2.0b1 and the ```cut_normalized()``` function has a doctest that takes much longer in v2 than v1.  It does eventually pass though and the sequence of subgraph sizes for all its cuts are the same in both versions.  I hypothesize that it is the recursive subgraph that's killing the speed. I'm running it now and it looks like once we get above 20 recursive subgraphs the speed slows to a crawl.  Every single node that is reported must be checked 20 times with full function lookups and calls.  I recall that the networkx.algorithm.approximation.ramsey code used recursion and subgraphs too, and I ended up not using views there because of slow speed from too many views-of-views-of-views.  So:   1) ~~I can work on a way for cut_normalized to not use views.~~ 2) ~~We may want to add an optional keyword on subgraph that makes a shallow copy of the graph -- shallow meaning that it keeps the same node/edge data dicts as the original graph, but isn't a nestable view.~~  Perhaps there is a way to have the subgraph views detect nesting and bypass it with a single view that involves multiple filters. I'll think about that too. [EDIT: This is the way to go... see #2635]  Any other ideas?
comment
A change to the subgraph method that eliminates nested subgraphs made the skimage doctest go from 43:11 wall time to 0:05.8 wall time.  I'll try to put together a pull request tonight. 
comment
This looks good -- Thanks!
comment
I think if we leave it for v2.0 it will be hard to change it for v2.1 where people don't expect strong changes in API.  But, I am fine with leaving ```G.node``` and ```G.edge``` until v?.? determined at a later date.  I suppose we could keep ```nodes_iter/edges_iter``` also pointing to ```nodes/edges``` but I'd prefer to have them change to the non-```_iter``` version (which works with v1.x code too).  Either way we might want to advertise a way to make graphs from v2.0 look like v1.x graphs. That way people can use code that will work for either. I guess that should be put into the migration document.  The other major change is that copy/subgraph/reverse/edge_subgraph have changed the level of deep-ness of their copying.  I'm not sure what the best way of handling that is.  It depends whether their code needs a certain level of deep-nees or not. Most of the time the v2 deep-nees works: a simple ```.copy()``` should be enough. But if their graph has edge data that is a container and they change it on the subgraph, they will need a ```deepcopy```. Note too that using ```deepcopy``` on a graphview will copy both the view object and the underlying graph object.   Here is some code to make v2.x look like v1.x except for deep-ness of copies. ```     if not hasattr(G, "nodes_iter"):         G.nodes_iter = G.nodes    # or make them remove _iter from their code.         G.edges_iter = G.edges   # or make them remove _iter from their code.         # same for G.degree_iter, G.adjacency_iter, G.adjacency_list, G.neighbors_iter          G.node = G.nodes         G.edge = G.adj         def subgraph(self, nbunch):             induced_nodes = nx.filters.show_nodes(self.nbunch_iter(nbunch))             return nx.graphviews.SubGraph(self, induced_nodes).copy()               # Notice the .copy()         G.subgraph = subgraph ```  If we re-introduce ```G.node``` and ```G.edge``` and tell people to remove the ```_iter``` from their code I think we don't need the above conversion. The only compatibility issue left is the level of deep-ness of copy/subgraph/reverse.  We can say: if in doubt, deepcopy.  But I suspect most of the time they shouldn't deepcopy.
comment
I don't know a way to make deprecation warnings on use of an attribute.   Perhaps we should just leave them in the API long term...
comment
Nice use of @property.    :) There are other ```_iter``` functions too. If these work we could do the same for them.  Q: will the scikit-image code then produce deprecation warnings when they use code that works for both v1.x and v2?  How do we best serve packages which want to support old and new versions of our code?
comment
Thanks @chebee7i I figured something like that was happening. :)
comment
Thank you for this.  It looks like the doc_string for this function could also be improved to make this more clear. Would you like to take a stab at it? If not, I'll do that.
comment
Thanks
comment
In addition to ```G.get_edge_data(1, 2)``` you can use ```G[1][2]``` (though it provides a read-only version. To be specific -- I mean a read-only dict keyed by edge-key to values that are read-write dicts of edge attributes.)  We could put this functionality into ```G.edges[1,2]```, but I wonder if you expect to be able to change the resulting dict to add or remove keys/edges or are you only using it for reporting those edges? If we go this route I think it should report the read-only version just as ```G[1][2]``` does.  
comment
I'm not sure why I didn't think of this earlier, but the straightforward way to get code to work with both v1.x and v2 is to replace ```G.edge[1][2]```   with ```G.adj[1][2]```.   That should give what you want.  The ```get_edge_data``` and ```G[1][2]``` will work too, but a simply replace of the name might be easiest.
comment
Yes, in both v1.x and v2.x they are equivalent.    In v1.x they are both equivalent to ```G.edge[1][2]```. But the ```edge``` attribute has been removed for v2.x.  
comment
Can you give a simple example which shows the problem? Also, which function are you sending the eccentricity to? If I understand what you are saying correctly, he infinite path length error should only occur if you remove nodes which make the rest of the graph disconnected.
comment
The original purpose was to avoid recalculating paths on the same graph more than once. If you've already got the paths then you can compute the eccentricity cheaply.   But if you change the graph (move to a subgraph) you change the shortest paths.  Unless you have a special case, there isn't a nice way to know the shortest paths of a subgraph from the shortest paths of a graph.
comment
I have un-hidden the stable release v1.11.  Apparently all older versions automatically get hidden by default when a new version is uploaded. I've un-checked that box on the admin page. I assume that means we will need to manually do it in the future.   I've tested installing networkx via pip in a conda virtual environment and it pulled down v1.11 as desired.  I can then install the new beta version using ```pip install networkx==2.0b1```. 
comment
Thanks @vladsaveliev !!
comment
I think the issue with shadow version modules is that release.py adds and removes the main networkx directory to sys.path.   I don't think it is a problem to do this in setup.py. And I don't think it is necessary to do it in release.py.  If I am right, then the fix is simply to remove the lines in release.py where we add basedir to sys.path and then pop it. This happens twice in release.py.  I've been able to recreate the symptoms on all versions using networkx packages installed with pip. I then change the release.py file in the site-packages directory. I remove the sys.path changes before and after importing version. I do this in two places in the file.   After making those changes, the import test passes -- no shadow module appears.      
comment
This looks good.  Could you adjust the docstring so it is fewer than 80 chars wide? (pep8 style). Thanks!  
comment
If you are up for it, adding a test to tests/test_efficiency.py would be great! 
comment
These tests look good. But they should go in the ```networkx/algorithms/tests/test_efficiency.py``` file rather than the ```networkx/tests/``` directory.   Don't worry about the Codecov errors... those are fixed in another PR.
comment
This should be pretty straightforward to add. It's very similar to the methods you mention only using ```single_source_dijkstra``` instead of the path and length varieties of these functions.   Note that if you have the paths it is fast to find the lengths, so maybe you don't need both.  In any case, would anyone put forward a PR for this?
comment
implemented in #2510 and #2612 
comment
Thanks for this PR!  At the moment, it doesn't give any way to figure out which source is attached to each dict. Also, I think we don't need the keys 'distance' and 'path' if we just return 2-tuples where the first entry is always distance and the second always path.  I thought about yielding the 3-tuple  (n, distance, path) but I think it is better to yield (n, (distance, path)) because then someone could pipe it directly to ```dict``` and create a dict keyed by source to a 2-tuple of dicts keyed by target.  Perhaps the spirit of iterators over data storage would say distance and path should themselves be iterators rather than dicts. I'm open to new ideas about what should be returned.  
comment
Good questions.  It has been a long time since I went through all the different shortest_path functions looking for consistency.  Which two do you see returning a dict and yielding a ```(source,dict)``` tuple? (you say ```all_pairs_dijkstra_path``` for both)  I think a typical use of all_pairs dijkstra is to go through each source looking at the distances/paths to each node from that source. That's the idea behind yielding for all_pairs.  Also these structures can be large and yielding provides good memory savings in the case where only one source is viewed at a time.  The output can of course be made into a ```dict``` using ```dict(all_pairs_dijkstra...)```. The single_source functions tend to return dicts because a single source doesn't create so large outputs. In general we are coming from code written to return ```dict``` and trying to modernize by yielding instead where-ever it makes sense to do so.
comment
I think the single_source and multi_source versions should return a dict (they are presumably small enough to manipulate) while all_pairs/all_ (functions that collect the data for each source) should yield a (source, info) tuple.
comment
No...   They should just call the single/multi over and over again. So yielding is easy:      for s in G:         yield (s, multi_source_...(G, [s], ...))     #  you could do single_source_... if you prefer  This is done, for example in ```all_pairs_dijkstra_path_length```.   Some will need to be changed from:      return {n: path(G, n, cutoff=cutoff, weight=weight) for n in G}   # from all_pairs_dijkstra_path  Some of the ```single_source``` functions currently create the dict and then make an iterator from it which is silly really, so let's have the ```single_source``` and ```multi_source``` return dicts while the ```all_pairs``` yield 2-tuples.  The next confusing thing is that for those functions with an optional ```target```, the output should be different when ```target``` is specified.  So ```single_source_dijkstra(G,0)``` should return two dicts while ```single_source_dijkstra(G,0,1)``` should return a length and a path.  This should already be true so you just need to check  that it is true for all those routines.   Here is what I think needs doing from the ```weighted.py``` module.  Your mileage may vary. :)  Should return a dict instead of current treatment      multi_source_dijkstra_path_length     single_source_bellman_ford_path_length  Should yield ```(source, stuff)``` 2-tuples      all_pairs_dijkstra_path     NEW:  all_pairs_dijkstra     all_pairs_bellman_ford_path  Similarly, from ```unweighted.py```:   Should yield ```(source, stuff)``` 2-tuples      all_pairs_shortest_path  Should yield dict of what they yield: (Note: this leaves ```_single_shortest_path_length``` as an iterator. But it is a private function and it is nice to have it written this way. We're putting the others into dicts for consistency. Perhaps we should make all of the single_source be iterators, but not for now.)      single_source_shortest_path_length     single_target_shortest_path_length  Do you agree?   Anyway, it might be best to change the tests first and then make sure the code passes the tests.  This is turning into more than I thought it would.
comment
The tests are in ```tests/test_weighted.py``` and ```tests/test_unweighted.py```. If the tests expect iterators and get dicts or vice versa we will know about it. My comment was just that it might make sense to change the tests to what we want and then make the code match that.
comment
@markhkim Have you made any progress on this PR? Any chance of progress in the next few days?  If not, I will take a stab at it.  Thanks!
comment
That would be cool -- what would you like to take on?
comment
Sounds great!...   Thanks!
comment
It looks like it will not be too hard to add the all_pairs routines to #2612. It includes your commits. I will go ahead and do that unless I hear from you. Thanks for your help getting this in!
comment
merged via #2612 
comment
Unfortunately, ipython and probably other systems use ```repr``` to show values of the expression. So without the ```print``` the more detailed output appears.       >>> print(G.edges())     [(1, 2), (2, 3)]     >>> G.edges()     EdgeView([(1, 2), (2, 3)])  The repr output does not fulfill the Python doc request that ```repr``` provide sufficient information to recreate the object "in an appropriate environment".  That would require  even more output.  Should we make ```repr``` produce list-like output and forego the view name? That allows ```repr``` and ```str``` to be the same. We gain from simple output. We lose the hint that this is more than a list.
comment
addressed by #2613 
comment
Hmmm...  Why does ```to_pandas_edgelist``` add the edge attribute ```weight```? I would expect that       >>> Gtrue = nx.Graph([(1, 1), (1, 2)])     >>> df = nx.to_pandas_edgelist(Gtrue, dtype=int)      would create a pandas frame without any ```weight``` column.  Just ```source``` and ```target```.  I understand that this is not the API that was in place before this PR, but I think it would be better.  Ideally you could specify a list of edge attributes that would each create a column in the dataframe. If the list is empty (or maybe None?) then no edge attribute columns are written. This is how we read the dataframes so we should write them that way too.  I would be fine with that as a separate PR though.  I think the way you are testing the round trip is good.
comment
As far as I can tell from the discussions on github this change may not have been intentional.   It looks like we might need functions for pandas adjacency matrix as well as pandas edge list. Can anyone involved in the previous commits comment on the best way to fix this?
comment
Yes, have a go at it!  :)  At the moment we have one written for adjacency and one written for edgelist. You can find the other edgelist in the history, but it might be just as well to rewrite it.   Have fun!
comment
Awesome @jarrodmillman !  Thanks!
comment
Hello @calmofthestorm  Sorry for the big delay. I'm afraid there is a backlog of good code waiting to come in. I can try to get this merged before V2.0 is released (very soon).  If it takes a while to get approval we might need to wait until v2.1.  I don't think you should split this code. The DAG function relies on the tree version and it fits nicely together in the module you have provided.  Looking through the code and starting to get a feel for the algorithm I notice many small changes will be needed for v2.0. Things like ```G.nodes_iter()``` doesn't exist anymore.   If I rewrite the existing module for v2.0 and merge it, will you still need to get approval? What restrictions are there on your end?
comment
Great!  Thanks very much. I will work on it.
comment
merged in #2597
comment
Thanks @baharev for tracking this down, creating a very nice example of the problem, and putting together the code to check @hagberg's fix.   I agree that there doesn't seem to be a better way to create the subgraph while matching the order of both successors and predecessors in the subgraph to the order in the graph.   As far as how best to implement this, it looks like this version of subgraph takes twice as long as the existing version for simple (unordered, no attributes, etc) example graphs (which makes sense since we are essentially traversing the edges twice). So I guess we shouldn't put it into the standard `DiGraph` class.  We definitely should add it to the new `OrderedDiGraph` class provided in the classes/ordered.py module.  We should probably also rewrite the docs about classes to include pointers to the new `OrderedGraph`. As it is, the docs suggest building your own--but evidently that is more tricky than we thought. I wonder if there are other pitfalls lurking for ordered graphs in e.g. `G.reversed` or `G.to_undirected`, `G.copy` etc.  Probably good to go through those to at least check what the expected result should be. 
comment
I did some checking on whether OrderedDiGraph adjdict order is preserved with methods.   ```G.copy``` works OK with ```OrderedDiGraph``` ```G.reverse``` does not work- The new order is that of ```G.edges()``` not ```G.succ``` or ```G.pred``` ```G.subgraph``` does not work (as described above). Order of ```G.succ``` is Ok but ```G.pred``` changes. ```G.to_undirected``` does not work - Returns a ```Graph``` instead of an ```OrderedGraph```  I also re-noticed that the edges are reported in a different order than the order they are added. They are reported in the order of the adjacency dict-of-dict. Even if the dicts are ordered, the edges may lead to differences between edge-add-order and edge-report-order.        G.add_nodes_from([3, 2, 1])     G.add_edges_from([(1, 3), (2, 3)])     G.edges()    # -> [(2, 3), (1, 3)]     because the node order dominates the edge reporting order.  Even if we don't use ```add_nodes_from``` to specify the order, earlier edges can create a node order that misaligns add_order from report_order. So OrderedGraph and friends don't preserve edge-add-order. They only preserve node-add-order but at least they preserve SOME order for the edges.  But it is NOT the add-order.  Let's make this clear in the docs.
comment
The ordering works as you expect for the subgraph filters proposed in #2523
comment
#2595 should address this concern.
comment
The general API looks good.  You should check the github wiki page for style of the header for the module. (copyright and author in comments now instead of author variable -- then docs then imports.  I'm not sure about names for ```general_k_edge_subgraphs```. You would probably know the terms better than I.  I have also not looked at the references nor check for accuracy.   This looks on the right track.
comment
Including more information in the module docstring is encouraged. :)    It looks like you still have some profiling code in the testing module (```if __name__ == ```...)  Thanks!
comment
This looks ready for merge as far as i can tell.  Are you planning more for this PR?  There is one comment just before the ```construct``` method about not_implemented_for('multi'). I'm fine with that being there as a comment, but just thought I would verify that there is some issue with actually using the decorator.
comment
It also looks like it doesn't work for multigraphs. The code also uses ```G.edges(v)``` to find the neighbors of ```v```, which could be updated using the new views.
comment
The intent is indeed to normalize relative to the minimum distance possible within the component. I think the documentation is fairly clear about this. But there are other issues going on.  In the cited article, inverse closeness is the sum of the distances divided by (number of nodes -1). Closeness is then (number of nodes -1) divided by sum of distances.  Let N be the number of nodes and n be the number of nodes in the component.  Questions: - without normalization we have `C = (n-1)/sum_of_dict`.  So normalization already occurs even when `normalized is False`. That should be made clear in docs. - with normalization we have `C = (n-1)/(N-1) * (n-1)/sum_of_dict` which looks like the reciprocal of what we want to multiply by. Shouldn't the scale factor `s=(n-1)/N-1)` be instead `s=(N-1)/(n-1)`?  The tests don't check normalization so its hard to tell what was intended. 
comment
Following up on the previous discussion, Wasserman and Faust do indeed suggest an "improved" closeness centrality measure that scales as we scale it.  The result is "a ratio of the fraction of actors in the group who are reachable, to the average distance" to the reachable actors [W&F pg 201].  So we've got the right scaling and it matches our documentation.  The only issue I see here is the word "normalized" and whether the documentation sufficiently stresses what that means. I think @shongololo was able to figure out what was going on... but was concerned that it might not be what was intended. I'll change the docs...   Should I change the name "normalized"?  BTW, to get the sum of the distances on can use the shortest_path routines directly:  ``` dict((node, sum(d for _,d in nx.single_source_shortest_path_length(G, node))) for node in nodes) ``` 
comment
PR #2595 fixes this. 
comment
The timingclasses are there to allow comparison between the old API and the new API. I envision them being removed as soon as v2.0 is put in place.  So I don't think you should put time into them unless you think you might use them for some other purpose later.
comment
I haven't used coveralls very much -- my loss... not anything against coveralls.  Does codecov post the results to the conversation as it did in this conversation? Or is that something you put here to show us what is possible?  I think it might crowd out some conversations to have each push result in a report...  especially if they stay in the conversation when we squash commits.  I like the fact that it handles ```if```` statements better by noting whether they test both branches -- especially when one branch doesn't have code to run, so is effectively invisible.
comment
These commits look good -- just need to correct the import of nx, I suspect. You might like to add an ```# Author: Name <email>``` line, and update the copyright to 2015-2017. The only other thing is to add a short description to ```doc/source/reference/release_2.0.rst``` but I can do that if you prefer.  Thanks very much!
comment
My attempt to make the test work for all versions also failed. @aryamccarthy do you have any ideas for how to do this?  I suppose we can just remove the ```test_five_clique_ring``` test.  What is best here?
comment
A new small PR would be great -- And if you include a reference to #2420 in the commit comments it will link to the original PR automatically.  Thanks for spotting this.
comment
Hi, I'm finally taking a look at this. It seems like the two modules should be split into different PRs. If you take the augmentation into a new PR, then I think all the test failures go away too. They are almost all import errors.   If you are up for it, the new modules should be added to __init__.py appropriately and probably given .rst files in  doc/source/reference too.    Thanks! 
comment
Interesting...  Thanks for the report.   Setting the initial position to (1, 1) works.  The strange behavior occurs when setting one node to the origin. That occurs because we find the domain_size by taking the maximum value of all initial positions. Unfortunately, if that maximum value is ```0``` then the domain size is set to zero and all the points are initially put at the same point. That leads to a division by zero exception.   I think a straightforward fix would be to check if there is only one initial point provided. If so, then set the domain_size to be 1.    Would someone like to provide a pull request for this?  Thanks!  
comment
The miniconda includes numpy/scipy, etc.  With them turned off, lots of tests get skipped. To find out what the actual error was, click on one of the red (bad) reports from the Travis page. It iwll give you the complete output of the nosetests command.  (You can run that on your own machine if you install nose). Search that output for "fail" or "error" and you should find out what the error was.  Usually its something silly and then you submit another commit.  :)
comment
This looks pretty good.  The tests is clear and the way you've solved the issue makes the code slightly easier to read than what was there before too.   Could you check for PEP8 issues? The typical ones are spaces after commas and operators and line length of <80 chars.  You can do ```pep8 graphml.py``` at the command line to check a file once the pep8 program is installed (I use ```pip install pep8```). I suppose you can read the [PEP8 guidelines](https://www.python.org/dev/peps/pep-0008/) and check the code yourself too, but I wouldn't advise it. :)   If that's too much for you now I will get to cleaning it up at some point fairly soon.   Thanks again!
comment
Thanks very much! 
comment
Oops... sorry about that. I am guilty of merging changes faster than I'm documenting them. I am planning to shift to doc mode soon.   Meanwhile:  would it be better to revert to the old argument ordering and make BOTH ```name``` and ```values``` have default ```None```?  If ```values is None```, we could raise an error, or better yet, check if ```name``` is a dict and raise an error explaining to use the keywords ```values=```.   Hmmm...  This doesn't sound very smooth either.    gnash -- gnash....   :)
comment
I agree with all you say...   I always get the order of ```hasattr(obj, name)``` backwards.  But luckily getting the order wrong results in an obvious error -- not a secret behind the scene bug. As long as we're moving to v2.0 people will expect some changes.  Thanks for the PR! 
comment
I've got it -- started by trying to merge with release notes -- :)
comment
When does the KeyError occur? The code only sets the attributes with keys in the passed in dict. So it should not matter whether the graph nodes have the attribute key present or not.  This routine could create a KeyError if a node in the passed in dict is not a node in the Graph. Perhaps the confusion is whether ```values``` should be a dict-of-attribute-dicts (e.g. {node:{key:value}}or an attribute-dict (e.g. {key:value}).   On Wed, Dec 28, 2016 at 11:58 AM, Michael E. Rose <notifications@github.com> wrote:  > When the dictionary that contains node attributes in set_node_attributes() > misses a key present in the graph, there will be a KeyError. I find this > annoying and would like to add a few lines to either skip those entries > (i.e. do not pass a value at all) or to set a standard value. > > I thought I ask first how you guys feel about. > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/2343>, or mute the thread > <https://github.com/notifications/unsubscribe-auth/AA32XasCxXvQGvEx_6SpEaOUKg0RxhpOks5rMpUrgaJpZM4LXDN0> > . > 
comment
Yes, it seems better to me also that we simply ignore nodes in the incoming dict that don't exist in the graph.  Thanks!
comment
Which others that are active will delay this one?
comment
#2344 and #2326 are now merged so we could take on set_node_attributes now. Or we could bump it to networkx-2.1 instead. Its not a new feature so much as an enhancement.
comment
I'm inclined not to do option 2 because defaults can so easily be created by sending in a constant first. I'd really like to train people to make sure they are dealing with all nodes unless they really want not to.  I like the first option and the third option.  And I think we should simplify the case when values is a constant so that it just sets every node to that value rather than creating a potentially large dict in hard to read manner just to make it fit with the next lines of code.  So:   I see three cases:  - name is None: ```for n,d in values.items(): G.node[n].update(d)```  - values is dict/mapping:  ```for n,v in values.items(): G.node[n][name]=v```     can try/except this looking for attributeError to see if values is dict/mapping   - values is not a dict/mapping:  ```for n in G: G.node[n][name]=values```   I'm slightly concerned about making this too complicated to manage (for users and maintainers). What do you think?
comment
If I understand you correctly the example of ```.for_dict()``` isn't actually passing a keyword based on which incoming container is being handled. It is a separate method for each incoming container type.  Ideally we would provide simple operations that can easily be connected together to do complicated things. I think your example of summing the attributes should be easy to construct with the current ```set_node_attributes``` and ```get_node_attributes```, right?  And the changes we're discussing here ought to help that -- allowing entire dicts lets us use ```G.node``` to update ```H.node```. And ignoring nodes in ```values``` let's us provide dicts with extra nodes not in G.  
comment
Yes, that looks good!   Go ahead and update/amend the tests.  Thanks!
comment
If we're changing the order of parameters we ought to make this v2.0  Yes we should change the ```set_edge...``` interface to match this.  Stick with the current requirement for tuples as edges.  Multi graph must include key.    I don't think these will become complex enough to warrant out factored code, but you can be the judge of that.  Whatever makes it easy to maintain and understand.
comment
These changes still do not have the full path in the `see also` section.  You need the module name at the end. e.g.:  ``` networkx.algorithms.components.weakly_connected.is_weakly_connected ```  These make the name of the reference quite long (and potentially confusing because the reader may expect to have to type that long string just to call the function).   I'm looking for a way to make the text of the link be just the function name while still telling Sphinx where to link to... 
comment
Thanks @jfinkels.   I found a description of that at:  http://www.sphinx-doc.org/en/stable/markup/inline.html  I've tested it and it seems to work. So I have for the "See Also" section of "connected_component_subgraphs".  ``` See Also -------- :func:`connected_components` :func:`~networkx.algorithms.components.strongly_connected.strongly_connected_components` :func:`~networkx.algorithms.components.weakly_connected.weakly_connected_components` ```  Note:  once I add the `:func:`name`` to any line, the other lines don't produce links without :func:`name` even if they are in the current module.  @Michael-E-Rose is this enough to go on?    
comment
This looks like it works! :) I'm going to merge it so we can check the readthedocs sphinx versions.  If all is good we can take stock of how much else needs to be converted. 
comment
This looks good as far as I can tell. 
comment
Thanks a lot!!  hopefully this will settle the pydot import trouble for a while. :) 
comment
Thanks! 
comment
Looks Good!   Thanks!! 
comment
While edge weights can be floats, they are slightly dangerous due to round-off error.  So, while we don't forced people to use integer weights, we suggest that they use integers (perhaps by multiplying the float by 10^8 and rounding) to avoid potential round-off trouble.  We don't make this clear in each doc-string however... I'm not terribly worried about the type listed being more restrictive than what could actually be used. But I'm open to other approaches.  I'm going to merge this as is... not worrying about the type in the doc-string or the conciseness  of the sum portion of the code. 
comment
Thanks! 
comment
Excellent --- Thanks! 
comment
This looks good! Thanks for the improvement. 
comment
Thanks @jfinkels for you work on this (and many other) PRs! And thanks @JamesClough for the PR!   :) 
comment
It looks like the paths for ```..._target_shortest_path``` are backwards. I also unified the code by creating helper functions for ```_path_length``` and ```_path```  Then added tests and pep8.  @bioglio can you [add permission](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) for me to commit to this PR? It is supposed to appear as an option in the right sidebar of this pull request. Something like: ```allow edits from maintainers``` If  that's any trouble at all I will just open a new PR to finish this one.
comment
This looks good. I tried to push some pep8 changes. The biggest changes are to shrink wording to 80 chars per line.  See if you agree with my rewording of comments. 
comment
I think this (#2355) is ready to go.  There were other GML fixes recently that made two small conflicts.   @Michael-E-Rose do you have time to fix them? If not I can do it. Thanks.
comment
Based on #2436 I think you need to remove the ```viz``` part of line 293.  You undid the change from #2436 when you merged. That change in a new commit might be easier than undoing the merge and redoing it.  But git allows you to revert and try again (google git revert).  You can also start a new branch from just before the merge with ```git checkout HEAD~1``` followed by ```git checkout -b new_name``` and try thr merge again...   still seems easier to change line 293 in ```test_gexf.py```
comment
I restarted the tests, but you should not worry about it -- we are trying to fix the issue with a different PR.  We can merge your PR  even if it fails the Appveyor tests. :)
comment
Can you mention in the doc_string that this uses KDTree when Scipy is available. Might also be good to mention that the algorithm changes from O(n^2) to O(n).
comment
Thanks very much!
comment
Thanks!!
comment
The current failure occurs sometimes when a single community (all nodes) is found. Is that still possible with this graph and this algorithm?  I'm not sure if this commit is making it less likely to occur or impossible to occur.
comment
It can still fail the test -- the algorithm chooses nodes at random and they "vote" (change their label) to the label that is the majority of their neighbors (or arbitrarily for ties). The order in which nodes are chosen determines the outcome. So I think we should include both possible outcomes in our test.  I'll try to put together a PR leaving the old test, but including both possible answers.  What do you think?      -    assert_equal(result, ground_truth)     +    # probabilitistic result could be all nodes in one community. So test result is either.     +    assert(result in [ground_truth, set(frozenset(range(16)))]) 
comment
I merged #2339 which is an alternate version of this.
comment
This does not pass the node object.  In fact it only passes the attribute dictionary for the node. I think your use case is easy to implement from the nodes themselves. Pass the nodes in -- then have the function get any attributes needed by looking them up using G.node[n]['attr_name'].
comment
I restarted the appveyor tests -- that test is one that sometimes fails... :{
comment
This looks good.  Can you move the example to just below the other example in the same docstring? Your language same "see the example" so people should be able to find it in the "Examples" section. Also it will allow the code to work because G will exist.  Thanks for this!
comment
Ack -- now the errors show a typo:  an extra closing brace {
comment
Take a look at #1108  If that takes care of the issue great -- if it doesn't then can you say how this is different? Also, it may be that some strategically placed documentation would help people a lot here. Suggestions are very welcome.
comment
Go ahead and put the changes into this PR. Let's see if we can get conf.py cleaned up.
comment
Let's go ahead and make the relabel files pep8 compliant.  Our general philosophy is to improve pep8 stuff only when we touch code for other reasons. Sometimes that means only part of a file and sometimes it means the whole file. The key is to make it so the pep8 changes don't swamp the actual changes and they won't here.  I don't change the tests for pep8 very often because it feels like I make the readability worse by adding so many line breaks to keep the code within 80 chars per line. But that's personal preference and depends on the test code your looking at. I'll leave that to you. Thanks!
comment
It looks like ```self.assertCountEqual``` isn't available in python2.  Your ```assert_count_equal``` looks good.  For the tests, can we use       assert_equal([(y, x)], list(G.reverse().edges()))     assert_count_equal(G.nodes(), G.reverse().nodes())  The current code should pass the edges test and fail the nodes.  Your fix should pass both.   It's basically the same as your proposal, but tests the edges for equality instead of one-way inclusion.  If you have other ideas that is fine; I'm just making suggestions.
comment
Thanks!!
comment
Yes, that seems like a good approach to me.
comment
It seems to work in Python3.6   I get the error only for python2.7 though I haven't tested in all the versions. I'm assuming it is a python2 vs python3 issue.   In the test you are processing all strings through ```literal_stringizer``` instead of letting str and unicode be unprocessed. I guess this fixes the int node name ```0``` to ```"0"``` but now the test fails on the attribute 'demo' having a unicode value because ```literal_stringizer``` in python2.7 doesn't convert the unicode symbol to HTML correctly.  In the old tests, strings were not processed by ```literal_stringizer```. So this wasn't a problem.  We might need a separate section just for labels.  Instead of calling literal_stringizer on all strings AND on labels, you would just call it on the labels. I think that will make this test work but is kind of a hack because its not clear how to handle labels (nodes) that are unicode strings. Perhaps just send them through without processing and apply ```literal_stringizer``` to the numeric types.  You could hardcode the stringizer to use or make literal_stringizer the default.  Perhaps we should add a test where a node is a unicode string.    Does this make any sense?
comment
OK... clearer head -- hopefully clearer answer...  The problem arises due to using literal_stringizer on the strings that don't need it.  Instead of excluding 'label' from the numeric handling routines and relying on the stringizer to fix them, I added code to handle the numeric labels specially. Then we don't have to send all the strings through the stringizer just to get the numerics through the stringizer.  I also added a test for a node that is a string with unicode present.  These changes passed python2.7 and python3.6 on my machine. We'll see what Travis and Appveyor say.
comment
Yes, LogGraph was a reference to a graph that logged each mutation. That's what PrintGraph does. It's certainly fine to remove the confusing reference.  ThinGraph only appears as you see it in the docstrings. But OrderedGraph eventually evolved to being included (along with Di/Multi versions) as you point out.  I think removing the OrderedGraph examples and pointing readers to the ```ordered.py``` code would be good. It might even be time to add docs to the ```OrderedGraph``` classes. I think another Issue asks for it to improve the subgraph and reverse behavior for ```OrderedDiGraph``` so that code may expand soon.  Finally, it should be noted somewhere that ```OrderedGraph``` give a consistent order for reporting of nodes and edges, but for edges, the order is not necessarily the order that the edges were added.  The dict's are ordered, not the edges.
comment
OK... I looked through these changes and while there are a lot due to pep8, I've looked at the others and think there are two more steps needed.  - remove the examples for OrderedGraph and friends and replace with pointers to the actual classes.  - change the language to not claim that OrderedGraph and friends maintain the order of adding edges.  While it is true that the order of node reporting agrees with node adding, that is not true for edges. The adjacency dicts are ordered, and edges will be reported in a consistent order.  But the edge order is not typically the order that edges were added. (that's because we pass through the adjacency dicts to get the edge order, the edge reporting exhausted one nodes neighbors before moving to the next node.  Thanks for all this!
comment
These are important files to work through and get right. You should also take a look at #2054 which takes a stab at restructuring the files in the /doc directory to make it easier for people to find and hopefully contribute to them.  Another focus of  that effort was to move the examples into the documentation so that they would be automatically built without a separate script. It looks like we are moving to have them housed in a separate repository, so maybe that isn't necessary. Anyway, it might be helpful to look through the list of commits made for #2054. They are a good framework for improving the docs.
comment
I don't think #2054 will be merged without a lot of work -- and then probably in a different PR. It has been a long time and there are many conflicts at this point. So I think you should proceed to make changes as you are. But there are some good ideas in those old commits so incorporate them wherever you think they are helpful.   I think the examples don't need to be moved to rst files -- so long as we get the notebooks repository to automatically hook in with the testing system we should keep the examples there. But restructuring the current ```/doc``` would be good. 
comment
I agree with both your comments.  Let's go with Sphinx-gallery and also get the notebooks repository going.
comment
Let's go ahead and merge and open another PR for tweaks and probably another for moving the files.   The way to test on ReadTheDocs is to create an account and start a project for your repository. When you go to your project site click on the "Admin" link (with a settings gear next to it) and then on "Advanced Settings" on the left sidebar.      - Check the box for: "Install your project inside a virtualenv using setup.py install"   - Add "doc/requirements.txt" to the requirements file textbox   - scroll down to uncheck the "enable epub build"   - Check the box for "Give the virtual environment access to the global site-packages dir."   - So far we have left the python version for the build as "CPython 2.x"  you can try other options there.  Submit the configuration using the button at the bottom.  Then go to "Versions" in the left sidebar. It lists every tag and branch for your repository. It also has "stable" and "latest" which automatically pick master and the "most recent" tag.  Those are the only two selected by default. You can uncheck them and check your **Fix-sphinx** branch to build that branch...  Press submit when done selecting which branches to build  The Builds tab (at the top) shows the most recent builds and you can click on them to see what happened.
comment
I've set up a project on readthedocs to build your branch. I have read-only access so can't add the hooks to make it automatic, but you can at least see what they look like. http://testfixsphinx.readthedocs.io/en/fix-sphinx/
comment
Thanks for this!  Is the code the same as for to_numpy_matrix, only at the end you convert it to array?  If so, you should just call the existing function, convert the result, and return that.  But I think the argument was that ```numpy.matrix``` was going the way of the dinosaur, in which case perhaps we shouldn't rely on it here. Would it be easy to change this function so it doesn't use the numpy.matrix class?  Also, could you take a look at the docstrings and make sure it says "matrix" and "array" appropriately?  In my quick first-read it seemed like it said matrix more often than it should given that we are working with arrays.   
comment
Yes, I like the idea of making ```to_numpy_array``` be the old function without the conversion and then ```to_numpy_matrix``` adding the conversion t the output of the array function.  For ```from_numpy_array``` should we just monkeypatch: ```to_numpy_array = to_numpy_matrix```?  I guess that makes the docstring identical too... which is OK by me.  On the docstrings, I understand and agree that ```adjacency array``` isn't a recognized term. So long as it is clear that ```matrix``` means ```adjacency matrix``` and not ```numpy matrix``` its great. And that might just mean that the first time you use the term you give the adjective.  I'll leave that judgement up to you.  Thanks!
comment
The fails on the Travis site are just the docstring tests looking for an array instead of a matrix or vice versa.  [You can test the docstrings on your machine using ```nodetests --with-doctest```.] 
comment
Looks great -- Thanks very much!!
comment
This sounds good to me. 
comment
This is a good idea.  Thanks!
comment
Closed by #2099
comment
Fixed by #2378 
comment
Closed by #1740.  But it isn't a view.  It is a shallow copy however.  When we get a view version we can update it. 
comment
Fixed by #2378
comment
This looks good to me.  Is it ready to merge @jtorrents ? 
comment
Yes, I agree with @jfinkels.  This is so similar to `dfs` that it should just be implemented as an optional argument to the existing `dfs` routines.  We do this already for `single_source_shortest_path` with an argument called "cutoff". I like `depth_limit` here as @jfinkels suggested.  I hope you have solved the proxy issues. :) 
comment
1. How hard is it to compare the time of the original dfs code with the    code that supports depth_limit?  Using ipython and the `%timeit` magic    try timing each method for a single graph (maybe binomial_graph(2000,    0.3)?). If its within 20-30% then make the changes to original dfs. We    probably want it in there even if the timing slows more than that but we    might want to talk about it first. :} 2. The tests can go in the existing file.  On Tue, Jan 12, 2016 at 2:23 AM, Arafat notifications@github.com wrote:  > @dschult https://github.com/dschult I think you are right, depth should > be an optional argument to the existing dfs routines.And in case if the > optional argument is not provided then the function will take the depth as > the number of nodes in the graph which is the maximum possible depth, This > will function exactly like the original dfs. > I have two questions > 1. Should i make the changes such that the original depth_first_search > supports depth_limit ? > 2. Where should i keep the tests?( a new file or in the existing test_dfs > file) >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1928#issuecomment-170822190. 
comment
blank lines are needed at lines 5 and 19 (and maybe elsewhere).   That type of white space is restricted by the formatting software we use for documentation (sphinx). Each doc_string starts with a single-line description of the object, a blank line and then an optional paragraph with more details. This is true even at the top of the file. Generally try to follow what is already present in terms of blank lines.   The white space issues we often need to update are: -   spaces at the end of lines, and -   spaces after most commas and around operators/assignments.  
comment
Would it be more efficient to put the check on depth_limit before the node goes on the stack instead of after it comes off the stack?  It seems like the stack would be much smaller and the number of times you hit a `continue` would be smaller too.  
comment
Looks like the running time changes for dfs_edges should be put in place for dfs_labeled_edges. 
comment
If you've still got the code for a time check, it'd be worth trying it with the newer version. Python performance is not obvious -- sometimes small changes make a large difference. 
comment
I think the Depth Limited Search code is good, but it now has conflicts with the current master. @Arafatk are you able to fix those? If you don't have time I'll try and put the result into a new PR. 
comment
Yes -- new PR should be fine. But be aware that you may be overwriting recent changes to the repo if you simply use the same files as from this PR.  That's a place to start, but then use the "diffs" to make sure your changes didn't erase someone else's work.   OR   You can update your local repository's master branch with `git pull` and then switch to your depth_limited_search branch and try `git merge master`. The conflicts will be flagged in the file `depth_first_search.py`.   You can search for `=====` to find where the conflicts are.    In this case, they are all in the doc_string examples because `G=nx.Graph();G.add_path([0,1,2,3])` is now `G=nx.path_graph(range(4))` and similar.  I think it might be pretty easy to do the merge, fix those 4 examples and then `git commit`. 
comment
Can you do this without copying the graph? For many graphs copying will require a large amount of memory. (see suggestion at bottom).  The doc_string is not explicit about how weights are computed. And there is more than one way to handle node weights--so you may need to be explicit. The original poster provided two possible node weight methods. How can we best do this? You could ask the poster for the use case to find out which method makes most sense, or whether we need to provide a flexible way to handle it.  You should probably let the user pick different attribute names for node and edge weights.  By using edge[0] and edge[1] you are unpacking the tuple many times. It might be better to unpack it once with `for u,v in G.edges():` because then you can use u and v.  Using G[u][v][weight] does not catch errors in a  user friendly way. For example, if an edge does not have the attribute what should we do?  Try copying the dijkstra code and morphing it to allow node weights. It handles special cases so you will see how it does that.  Plus you will get the benefit of learning the algorithm more deeply. :)  
comment
Those look like sorting issues. The order items are reported from dicts and sets is not specified in Python so different versions and different machines may give different orders. Sounds like this might be another pull request--figure out how other tests have handled potential order differences (search for "sorted" in the tests) and implement them for these tests.  Ah, the joys of testing. :) 
comment
The original Dijkstra code does not copy the graph. So start with that code... make it work for node weights.  Your form of adding vertex_weight(v) to the edge weight(u,v) is not the only way to add node weights. See the original post. 
comment
Can you look at the original posting?   We need to figure out what the appropriate way to handle node weights is.  The PR method is not what was suggested. But what is best? We also need to check for timing impact. This might need to be a separate routine if it slows down the no-node-weight case by a large factor. 
comment
I think we still need different weight names to be possible for the node weight and the edge weight.  Also, this is a large number of new functions. We could just provide the underlying routine(s) and let people make their own version that e.g. only returns the path or only returns the length? Perhaps @hagberg has a feel for that.  To get travis to like it better you could rebase to the current master. 
comment
Can you make it accept a different attribute name for node_weights than for edge_weights?  On Tue, Jun 16, 2015 at 7:45 AM, Mridul Seth notifications@github.com wrote:  > @dschult https://github.com/dschult @hagberg > https://github.com/hagberg Any final word on this? >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1350#issuecomment-112394458. 
comment
Fixed in #1945 
comment
It looks like we have a stand-alone version of Dijkstra here because it returns more than just distance or paths.  It keeps track of multiple paths as well as the order in which the nodes are traversed.  This makes it easier to accumulate the paths into betweenness measures.  Two comments on the current pull request.   1) I get different results for sigma.  It looks like the PR version gives sigma values that are half the original, but that may only be the networks I looked at.  Is there some reason this doesn't matter in the end? 2) A quick speed test shows the PR version is slower.  The original takes about 70% the time of the new for random graphs with density 0.2.  I also have a question about the replacement of dicts "D" and "seen" with the dict "dist".  The difference between "D" and "seen" is that "seen" gets updated when a node is first put on the stack and "D" is updated when the node is taken off the stack.  The PR doesn't distinguish and this reduces some "if" statements.  My question is can you explain why this gives the same results?  Also the if conditions may account for the speedup.  Summary: Geo-Piskas, can you verify that my observations on sigma and timing are done correctly?  And is there a way to see that the removal of the if conditions (and thus one dict) gives the same results? Thanks 
comment
It looks like this version is slower.  The original takes about 70% the time.  But perhaps more important is that it isn't clear to me that this version is correct.  The logic has changed in order to reduce the number of dicts.  With two dicts, the "seen" dict gets updated when the node is first connected to, and the "D" dict is updated when its neighbors are first accessed.  With the new code, the "dist" dict is updated when the node is first connected to.  It is likely that the logic of the if statements (the "continue" based on D and the creating/updating seen/dist if's) works out to be the same, but I haven't worked through all possibilities.  But even if the logic is the same, the routine is slower.  It looks like saving one dict loses some speed.    I'm inclined to leave the current code unless the new code is faster or demonstrably thinner (less RAM).   
comment
Try Q.remove( ... ) ?   maybe? 
comment
You can leave it in the queue and ignore it when it comes to the top of the queue.  But that's what the old code did. 
comment
Thanks for that!   I'm getting closer to understanding.  Two more questions though:  You treat sigma differently than the original code does.  You set sigma[w] = sigma[v] when w is first seen.  The old code sets sigma[w] = 0.0 when first seen and then adds sigma[pred](which is just sigma[v]) when the neighbors of w are first examined.  At first the two methods look the same.  But how do you know that sigma[v] hasn't changed between these times?  By adding sigma[v] too early you may not get all the paths to v and thus the number of paths to w will be incorrect.    The timing is still slower for the new routine.  I think the test "if v in S" takes longer than checking "if v in D", especially for large graphs, because S is a list and D is a dict.  I used gnp_random_graph(100,0.5,seed=0) and it was 2 times slower, but with 1000 nodes it was 10 times slower.    It seems like a tradeoff between space (an extra dict D keyed by node to distance) and time (looking up nodes in a dict rather than a list). 
comment
This looks good--I don't see any issues with it.  It's very helpful to have someone dig through the code for these common routines.  Actually looking at the diff from the current "master", we're almost back to the original codebase!   In fact the only changes I can see are the name seen becoming dist and the default weights are int whereas in the codebase they are float.  dist is probably a better name than seen for this method.  But we have a number of other routines that use the name seen so I'd like to keep the name the same as those routines.  For int vs float, it shouldn't matter.  But Python 2 has the crazy int-division-truncates problem so I'd like to keep that as a float.  So I'm going to close this PR.    Thanks for the submission and please submit other routines when they're ready.   Dan 
comment
It looks like the travis build couldn't connect to an ubuntu site (about line 460 in output) but I'm not sure that caused the trouble. I've restarted the tests and they look better. 
comment
Maybe Ii don't understand completely the distinction between bipartite and general graph covering. It looks like your code for the bipartite case simply calls the general algorithm. Why do you have the bipartite version of these functions? 
comment
Fixed by #2378 
comment
There is no simple way to do this, no...  You can look into matplotlib curves for more info.
comment
Hmmm...   What would you expect from G.edges(0, 1)?   The nbunch is `0`.  But the data requested is named `1`.  This is the same for v1.11 and v2.0.  The calling form is ```G.edges(nbunch, data, default)```  Perhaps you want ```G.edges([0, 1])```?  Also, I haven't written the docs for the views, but it goes something like this:      G.edges      # is what you will use most of the time.  Note: No parentheses!           #  G.edges is a set-like object but it also has dict-like lookup to get edge data.     G.edges()    # backward compatible and returns exactly the same as G.edges without ()     G.edges(data='weight')    # get edges as tuple with (u, v, weight)     G.edges(data=True)    # get edges as tuple with full data dict  (u, v, datadict)     G.edges(data=False)    # get edges as tuple with only nodes (same as G.edges)  These all act like sets of edge tuples.  For multigraph, we have to worry about the ```keys```.  So      MG.edges      # set of (u, v, key)     MG.edges()    # backward compatible so set of (u, v)     MG.edges(data='weight')    # get edges as tuple with (u, v, weight)     MG.edges(data=True)    # get edges as tuple with full data dict  (u, v, datadict)     MG.edges(data=False)    # get edges as tuple with only nodes (same as G.edges())     MG.edges(data='weight', keys=True)    # get edges as tuple with (u, v, key, weight)     MG.edges(data=True, keys=True)    # get edges as tuple with full data dict  (u, v, key, datadict)     MG.edges(data=False, keys=True)    # get edges as tuple (u, v, key) (same as G.edges)   The EdgeView and EdgeViewData objects are set-like, but EdgeView also itself allows lookup of the edge data.  So it's a set with dict-like lookup.      G.edges[u, v]    # provides the edge data dict  (what G.edge[u][v] == G.adj[u][v] used to provide)     MG.edges[u, v, k]    # multigraph edge lookup requires a 3-tuple at the moment  Neighbor lookup should be done with the ```G.adj/G.succ/G.pred``` objects instead of the functions ```G.neighbors(n)```, and friends.   I wonder if ```G.adj``` should be renamed ```G.nbrs```...  Hope this helps.
comment
Another way to summarize the ```G.edges``` migration path is that it is backward compatible for most uses.  Iteration should work the same as before. ```list(G.edges())``` should still be the same. ```(u, v) in G.edges()``` should still be the same. ```G.edges(data='weight')``` should iterate and check contains the same as it did before.  The features are almost all backward compatible. But we get extra features if we leave off the parentheses. We get data lookup ```G.edges[u,v]['weight']```. We get set operations ```G.edges & H.edges```.  Python ```dict``` has methods ```.keys()```, ```.values()``` and ```.items()```.   To those options we add ```G.edges.data()```  This is like ```.items()``` but can return one attribute from the datadict if you use it as ```G.edges.data('weight')```.  There is a tricky bit that ```G.edges(data=True)``` is still set-like, AND it does NOT allow lookup.  It is actually ```G.edges.data(True)```.  And just like dicts don't allow lookup on ```keys/values/items```, the ```EdgeView``` family doesn't allow lookup on keys/values/items/data views. So the ```EdgeDataView``` family is set-like while the ```EdgeView``` family is set-like and dict-like.
comment
Those are good questions. This is going to take me a bit to figure out though as I am offline today. Hopefully tomorrow.  I seem to recall that we did some one-off changes to v1.11 to make it work while deciding the longer term future had a different route for the layout algorithms. If that recollection is correct then we completed those changes to dev in master and we shouldn't include the v1.11 commits. But let me make sure I'm remembering correctly.   See #1759   #1760   #2096 
comment
- The commit c5234e8 for v1.10.1 should NOT be included. There never was a release 1.10.1.  That was a number I assigned to the release when I was hoping it would be smaller. It eventually grew to be v1.11.   - The layout scaling and center should not be the same for v2.0 as it was for v1.11. That was a design choice when releasing v1.11. It will take a little bit for me to go through the conversation and make sure that we actually did commit the v2.0 layout center and scaling options. I've opened #2537 to do that. So I think we should include changes related to a35b7ea in a PR addressing that issue.  All the rest of this looks good. Thanks!
comment
I don't get the error with ```parallel_betweenness``` on python2.7 or python3.6.  The antigraph error is because ```A.degree``` returns a generator while ```G.degree``` returns a singleton degree value.  I think it is an oversight bug due to changing the base classes. Even if both returned generators the code wouldn't work.  I'll look at that one.  
comment
I added a PR to your branch to fix the AntiGraph example.  Are you still getting the error for the parallel_betweenness.py code?
comment
Which python version?
comment
Ack -- I was sure I tested with Python2.7 but apparently I didn't.  Anyway, it doesn't work for me in python2.7.13 either.  It does work in 3.6, 3.5, 3.4 ,3.3.  So maybe it is a python2 vs 3 issue.    Looks like its a known issue for 2.7: https://stackoverflow.com/questions/1816958/cant-pickle-type-instancemethod-when-using-multiprocessing-pool-map  
comment
shortest_path should work on DAGs. This issue only talks about making it more efficient for DAGs.
comment
I'd like to get views instead of iterators for edges/nodes/degree/etc. I will have some time in May to work on that. I guess it could also be in 2.1 if needed.
comment
Could we switch the order in the calling sequence when we change the names? That should take care of backward compatibility when people haven't used keywords. But it makes the parameter order out of alphabetic order: ```waxman_graph(100, beta, alpha)```  What do you think?
comment
The theme on nxtestweb.github.io looks wonky in my browser. The top of the sidebar is cutting off some of the first line of text.  Does it look that way to you?  Also the links in the sidebar are not consistent in that some bring you to a page about that topic while others perform an action directly. For example "Download" initiates a download rather than bringing you to a page about downloading.  I know this kind of comment is about content rather than theme, but its hard for me to separate the two.  I haven't looked at the output of [networkx/networkx-website#11](networkx/networkx-website#11) to make a comparison. But it seems to be closer to our current landing page, only with a new theme.
comment
You can avoid copying the graph and also avoid removing the edge by "hiding" the edge using a weight function in the shortest path routine.  The weight function would be something like:  ``` enodes=set((u,v)) weight = lambda n, nbr, d: 1 if (n not in enodes or nbr not in enodes) else None ```  Then replace `shortest_path_length(G,u,v)` with `shortest_path_length(G,u,v,weight)` 
comment
NetworkX uses matrix and sparse_matrix for consistency and because the SciPy sparse matrix collection has  many good tools associated with it. The sparse matrix mimics the matrix structure though I have read that there are a few differences. Also, to change from a matrix object to an array is fast and easy with ```numpy.asarray(M)```, ```M.A```, ```M.getA()``` and perhaps other methods.  I would be Ok with adding a conversion function ```to_numpy_array``` so long as it is easy to maintain.
comment
Yes - if you could create PR I'll work to get it in. Thanks
comment
Since the tutorial will be a notebook, maybe we can house it in the notebooks repository instead of its own repository?  https://github.com/networkx/notebooks/blob/master/tutorial.ipynb
comment
**What does having the examples in a separate repo gain for us?**   (This might be of interest to people at #2462 SciPy working on the docs)  With the examples in a separate repository ```networkx/notebooks``` is there an easy way to still include them as part of the Travis testing?  Appveyor?  Is there an easy way to have PRs to ```networkx/notebooks``` be tested in Travis?  Appveyor?  I'm concerned that moving to separate repositories makes at least the testing harder to maintain?  I know that the examples being Jupyter notebooks is nice. I think github.com displays the notebooks whether they are in their own repository or in an examples folder. So jupyter notebooks is a separate issue from moving them to a separate repository.  
comment
This has been a real help!   Other PRs and issues have taken over for the ideas here so I'm closing it now.
comment
These ideas are great! Any one of them would be really helpful --  I won't be there this year -- but will be "lurking" remotely and helping out as best I can.
comment
Let's shift any discussion from SciPy sprints to  Notebooks: #2518     and Website landing page:  #1657 
comment
You might get a quicker response from the [mailing list](https://groups.google.com/forum/#!forum/networkx-discuss) as this is for developer issues.   The [docstring for the function](http://networkx.readthedocs.io/en/stable/reference/generated/networkx.algorithms.flow.max_flow_min_cost.html?highlight=max_flow_min_cost) says that the flow is reported by ```FlowDict[9506][9505]```  I'm going to close this issue, but feel free to post to the mail-list or reopen this issue. 
comment
Is there a good way to tie in the notebooks repository with our documentation? How do people know to look there? 
comment
When you say "regularly update" the repository, could that be done by a hook based on commits to a branch on networkx/notebooks?  Then it wouldn't be very difficult to keep it synced with our repo. 
comment
Maybe we don't need to sync two github repos. The notebooks in networkx/notebooks are accessible in many ways. Github displays them of course. But ipython now provides [nbviewer](http://nbviewer.ipython.org) with direct urls to our github repository [notebooks](http://nbviewer.ipython.org/github/networkx/notebooks).  The resulting URLs are even similar: Github:  https://github.com/networkx/notebooks/blob/master/tutorial.ipynb NBViewer:  http://nbviewer.ipython.org/github/networkx/notebooks/blob/master/tutorial.ipynb  The surrounding graphics are slightly different, but both provide static views of our notebook examples.  Let's update the main README and maybe the main website page to have a link to our example ipynb files.  And we should make those example notebooks as good as we can. 
comment
I think I misunderstood... try.jupyter.org gives a dynamic view of the notebooks. nbviewer and github give static views of the notebooks.  Maybe static with a download link will be enough for now?  Also, we should probably have tags and/or branches of the notebook examples for the different versions of networkx. We'll have to be careful to be using the correct version of networkx when we update the ipynb files. And we should find a way to automate their updating since they currently don't get built during our `make doc` sequence. 
comment
After a lot of searching and playing with possible solutions, it looks like @hagberg solution is the best one:  make sure that strings you want protected are already unicode when they enter the ```write_graphml``` function.  This is probably good practice for Python2 code with many libraries.  It is possible that a better way exists for this, but the python3 code works very well and this working solution seems sufficient.  I'm going to close this and put a comment in the docstring as part of #2515 
comment
It looks like the conversion from integers to strings is the culprit here.  Note that if the conversion had been made in both the keys and the list of neighbors like ```{"0": ["1"], ...``` then the graph would have been as expected.    Are you requesting that "0" be treated like 0 when creating nodes?  Or maybe you are suggesting a doc change...  what should it say and where should it be? I think I'm not quite clear yet on how to improve this. 
comment
The problem is that we can't know ahead of time whether the user is looking for a string node or an int node. And some graphs may have both string nodes and int nodes in the same graph.   Your results are interesting. The first two are, of course isomorphic to each other as they should be. The order of the nodes differs but that is not surprising. The combination graphs have more nodes because strings are not equal to ints and so we have nodes: ```0,1,2,3,"0","1","2","3"```.  Do you have any suggestions for how to allow people to use whatever nodes they like while helping them find issues like you found?
comment
This might be related to #2118 . That asks for read_gml to handle bools
comment
Fixed by #2513 
comment
Fixed by #2513 
comment
I agree that we shouldn't write non-GML files even if that means we can read a file we can't produce. In this case underscores can be read, but can't be written to a GML file. I'm going to close this issue.
comment
It seems this could be done with a subclass of Graph which overwrites ```add_node``` and ```add_edge``` so that any change in the node attribute triggers a flow of corrections to all children nodes. I don't see this becoming a major part of NetworkX any time soon but it would sure make for a cool example!
comment
Take a look at: [shortest_simple_paths](http://networkx.readthedocs.io/en/stable/reference/generated/networkx.algorithms.simple_paths.shortest_simple_paths.html) Also #1447 and connections from there for discussion.
comment
The print statement for a graph prints the name of the graph. That is G.name It is hard to know ahead of time what the user would expect here. So we provided one alternative and rely on you printing ```G.nodes``` and ```G.edges``` if you want to know about those sets. Other options allow printing the node and edge data. Printing ```G.graph```, ```G.node``` and ```G.adj``` give you essentially the entire data structure (including the name as that is stored in ```G.graph```.
comment
The behavior of edge weights under contraction of the nodes is not universally agreed upon. And not as well specified in the documentation as it could be. When you contract two nodes the second node copies its edges to the first node. All the edge data also gets copied. So if they shared any common neighbors and those two edges have the same edge data attribute, the new edge to that common neighbor has the edge data attribute value of the edge from the newly deleted node to that neighbor.   I haven't played with this to see how it would work, but I think if you start with a multigraph and then contract two nodes, any common neighbors now have two edges from the new node to that neighbor. You could then compute the edge value you would want for that new edge.   The problem is that it is not defined what to do with edge data when you contract two nodes. It is likely to be different for different use cases. You could copy the code for the contract_nodes function.  It is pretty short and quite readable. It is likely that you could make it do what you want it to do.  If that is better than what we have think about justifying why it is better and submit a pull request to help us improve that function.
comment
Be careful here with ```data=True```.  That makes the edge return a dict. So your ```max_d = max(d, G[u][w])``` may not be calculating what you want. It compares dicts not weights. Try something like this:      nodes_u = G[u]     for w, dd in G[v].items():         d = dd.get('weight', 1)         if w not in nodes_u:             new_edges.append((u, w, d))         else:             max_d = max(d, G[u][w]['weight'])              new_edges.append((u, w, max_d))
comment
For anyone who has ideas about a better way to handle the node/edge data when contracting nodes please feel free to post to this issue or to open a new one.  I'm going to close this as I don't see an obvious way forward with it
comment
I am not very familiar with mapping applications on graphs, but it seems to me there are two natural implementations for storage in a NetworkX graph.  The first is to make edges represent roadway segments with nodes representing the intersections. In this storage system it is hard to restrict turns. But it is good for finding "shortest paths" because edges hold the lengths of the roadways.  The second is to make nodes represent an end of a roadway segment and edges represent two types of objects: the roadways themselves or a turn from one segment to a different segment. This seems like the natural way to restrict turns. But it does increase the number of nodes and edges quite a bit though. Each roadway segment is assigned two nodes -- one at each end. Each intersection is represented by (usually) 4 nodes and edges for each turn possible (usually 12 for all directions except u-turns).    On Sun, May 14, 2017 at 11:28 AM, sephib <notifications@github.com> wrote:  > Hi, > I trying to implement a MultiDiGraph street route network based on HERE > <https://here.com/en/products-services/data/here-map-data> data (taking > inspiration from the OSMNX <https://github.com/gboeing/osmnx> project. > I was wandering how I should implement the restricted turns for a car > network. Is there a way to implement a solution not using the weight > attribute (as suggested in this post > <https://gis.stackexchange.com/questions/16453/how-to-restrict-or-block-certain-paths-in-networkx-graphs> > ? > > Any directions would be appreciated. > > Sephi > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/2452>, or mute the thread > <https://github.com/notifications/unsubscribe-auth/AA32Xe1ElpfFz_qchh6hLIhIdQiSX9yPks5r5x2CgaJpZM4NaaMZ> > . > 
comment
Fixed by #2103
comment
See #2344   My quick take is that this is fixed in the latest version but not in the stable v1.11 release.
comment
@Michael-E-Rose can you be more specific about the fixes?  Is the link broken in the latest code (master from github)? Is it broken in the latest stable release (v1.11)?    My understanding is that it is fixed in the master branch of github but not in the v1.11 release.
comment
Hmmm... This is mixed up.   The readthedocs sites are consistent, but because the stable readthedocs pages point to the github.networkx.io pages we don't match up and it just doesn't work.  Got to find a simpler doc system to keep this working well.
comment
I attempted to fix this on: - the v1.11 stable readthedocs page (removing the inactive links from the front page and correcting the links on the Downloads page) fb8a8ca6ecd9c99ba3f5e3c2654fdf6df5b4e7c2  and ceadb923fe60c648c47bf4d0ede595822bdbfec0 - the master latest readthedocs page (changing the download page links from the stable version to the latest version)  54227d56880e389e8aea03f44d936c5d9a2dfc72  It appears to be working now -- you might have to reload the page.  Note: no links to pdfs from the front page any more -- links work from the Download page.  Thanks for persisting with this until we fixed it and feel free to reopen if needed...
comment
Appveyor has been failing occasionally at that step for a little while now. It is definitely not something you did.
comment
I haven't looked at this carefully, but some shortest_path algorithms choose whether to use Dijkstra or not based on the weight parameter. Does this make the default using Dijkstra instead of using an unweighted algorithm?  If so, how does the user specify using the unweighted algorithm?
comment
If there is no edge attribute ```"weight"``` for any edge then Dijkstra should return the same results as the unweighted methods, but would take longer.    If the ```"weight"``` attribute exists and differs from ```1``` for any edge then certainly the results could differ from the unweighted methods.  I suppose so long as it is clear which is the default and how to get both BFS and Dijkstra, people will find how to call it to get what they want.   Currently if we don't specify which algorithm and don't specify which attribute we get unweighted methods. If we use a method that implies a weight then the default weight attribute is ```"weight"``` and Dijkstra is used. So if no weight is needed, the whole idea of weight is not needed. I like the idea that users who don't care about weights don't have to think about weights when they call the functions.  What is the advantage of making Dijkstra the default algorithm? 
comment
As I look at this in more detail it seems that having ```weight=None``` for the dijkstra methods doesn't make sense.  Maybe those shouldn't even have a default.  We should at least make the private functions have no default for the weight parameter.
comment
I think you already implemented @jfinkels suggestions. This touched a lot of code.  Thanks for all your efforts. Is this ready for merge or do you have more you are looking to do?
comment
It's not easy to add to someone else's PR. But you can fork the PR, add/chage stuff and then submit it as a separate PR. Just make sure to say so somewhere in the PR comments.  On Mon, Apr 24, 2017 at 6:30 PM, Michael E. Rose <notifications@github.com> wrote:  > The problem still exists. Hence I would volunteer to finish the PR. Must I > start a new PR or can I somehow add to this one? > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/pull/1453#issuecomment-296840405>, > or mute the thread > <https://github.com/notifications/unsubscribe-auth/AA32Xe8hspy7t7m8sQ4D4aHpu1C2whsKks5rzSJygaJpZM4EBWAb> > . > 
comment
fixed in #2436
comment
see #2468
comment
I agree. This would bring that function into line with other graph copy/subgraph code. I suggest the order should be:      1. update the graph attribute dict.     2. add nodes and update node attributes.     3. add edges and edge attributes. 
comment
This just needs a PR with @hagberg fix and add @jfinkels test to the tests.
comment
The docs say:        The keyword setting copy=False modifies the graph in place. This is not always possible if the      mapping is circular. In that case use copy=True.  These seem accurate and maybe address your concern? Is there better wording?
comment
Thanks very much...
comment
All the issues you refer to have been closed. Can you state what your issue is and what symptoms/difficulties you are having?  And BTW, declaring that no work has been done for over a year when it has been and then implying that your trouble ought to be solved by someone else -- indeed that someone else should make your problem a priority is really against the whole spirit of open source software.   Let's start again -- open an issue.  Explain what you are trying to do and describe what problem you are having. Provide a simple example of code that does not work.  Oh yes, and read the issues that you refer to. Try to understand what the issue is in the code and how we have worked to fix it.  I would start with #2023 
comment
For anyone who finds this later:  I'm not sure which examples and documentation are not working. The issues referred to above have been resolved AFAIK and I can run the programs without error.  If you use pygraphviz:      pos = nx.drawing.nx_agraph.graphviz_layout(G)     nx.draw(G, pos=pos)  If you use pydotplus:      pos = nx.drawing.nx_pydot.graphviz_layout(G)     nx.draw(G, pos=pos)  More details can be found on the [layout docs](http://networkx.readthedocs.io/en/stable/reference/drawing.html)
comment
I think we should use views instead of generators for degree/edges/nodes.  Then this issue might work better. #1943 is one such attempt.
comment
Fixed by #2458 although the error message is about an invalid nbunch instead of a node because we expect an nbunch input to G.degree. Still, it raises an error now as it should.
comment
I've created a label named "Easy-fix or Beginner" and labeled a few issues as such.  It may take some effort to maintain the labels, especially adding labels to new issues. Let's try it and see how it works.
comment
It completely depends on what your nodes and edges are. By that I mean what objects represent your nodes and what type of data you use for edges.  Typical use of integers for nodes and without any edge data uses less memory than you report. In any case, it should not grow a huge amount when calling all pairs shortest path lengths.
comment
PR #2143 fixed this issue.
comment
I believe these generators will be of use to others. I think the best place to put them is in a separate module within the generators folder called duplication.py or similar. While they are mostly used for biological systems now they may find uses elsewhere later so I'd prefer a name that describes the generator method instead of the application for which it works well. Thanks! 
comment
Fixed in #1939 with both methods, removal of a similar function in random_graphs.py and tests added...
comment
Fixed in #1601
comment
Revisiting this to see if Views handles this Issue.  With the views interface, we have:      # Instead of G.neighbors(n, data=True) or even G.neighbors(n, data='foo', default=1)      # Edge data with neighbors      for nbr, edict in G[n].items():      # keydict returned for multigraph -- not desired     for _, nbr, edict in G.edges(n, data=True)):  # plays nicely with multigraph      # Node data with neighbors     (nbr, G.node[nbr] for nbr in G[n])  # doesn't duplicate for multiedges     (nbr, G.node[nbr] for _, nbr in G.edges(n))    # does duplicate for multiedges  I think these are sufficient so that we don't need to modify G.neighbors()...   So....  Do we get rid of G.neighbors() and tell people to just use G.edges()?
comment
Shift conversation to #2467 with a different emphasis...
comment
It looks like bfs_predecessor was made into an iterator in #1825  I'm closing this. If that PR doesn't do what you want we can open it again.
comment
Yes, that looks right.  I think ```H.add_nodes_from(G)``` is a good way.
comment
for n,val in sset_dict.items():     if n in G:         G.node[n][attribute]=val ??  On Tue, Apr 25, 2017 at 4:53 PM, Alexander Lenail <notifications@github.com> wrote:  > In my case, the node may or may not exist in my graph. I'd like to > > set_node_attributes(graph, attribute, dictionary_with_potential_superset_of_nodes_in_my_graph) > > It would be nice if such a method existed. =) > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/2434>, or mute the thread > <https://github.com/notifications/unsubscribe-auth/AA32Xc0vig8P_6jbD7SggePBfn7mywNqks5rzl07gaJpZM4NICph> > . > 
comment
That is a short bit of code (untested) that I think does what you are requesting. Does it do what you are thinking about?   On Tue, Apr 25, 2017 at 7:34 PM, Alexander Lenail <notifications@github.com> wrote:  > Are you suggesting a way of working around this or citing the existing > networkx code @dschult <https://github.com/dschult> ? > > — > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/2434#issuecomment-297195165>, > or mute the thread > <https://github.com/notifications/unsubscribe-auth/AA32XbKMVgaD_6CcvFHaRpWjDVU7tsqaks5rzoL7gaJpZM4NICph> > . > 
comment
Take a look at the ordered dict graph class: https://github.com/networkx/networkx/blob/v1.11/networkx/classes/ordered.py NetworkX uses Python dicts by default, but we put code into the base classes that allows other dict-like objects to be inserted in place of the default dict class. The most common request was for ordered dicts. But it sounds like your interest would be exactly served by the same treatment using randomdict instead of OrderedDict.    Also note, to make this work you'll want to access the neighbor dict directly instead of through self.g.neighbors.  self.g.adj[n1] provides a dict-like object with keys being the neighbors of n1.   
comment
This looks good...  Anything else coming or should I merge?
comment
I agree...  We should just have info for how to create a PR on github. I believe the current instructions have them use github to create the repos and then create a patch and by that time a PR is quite straightforward.
comment
I'm confused... There are similar readthedocs webpages with that method for V1.10 and V1.11. Why do you think the method is only available for version 2.0?
comment
Perhaps I am confused, but for a DiGraph adding the edge (u,v) should only add to succ[u] and pred[v]. If we add both directions then we have a Graph, not a DiGraph.
comment
Yes-- that's a bug all right....  Adding a node and only adding half of the pred/succ structure is a problem.   Thanks for identifying and reporting it!  
comment
The function to_agraph is no longer exposed at the nx. level.   Now you must call nx.nx_agraph.to_agraph() 
comment
My understanding is that shortest_path works fine on DiGraphs (and thus on DAGs).  It may not be as efficient as a shortest path algorithm that is designed specifically for DAGs, but it is the standard algorithm for general DiGraphs.  In what sense do you mean that shortest_path "won't work"?  Can you provide a simple example?  The reason we have dijkstra versions separate from the algorithms for unweighted graphs is efficiency. If all weights are 1 (a common case) we use the more efficient algorithm for that case.  
comment
That looks like a bug to me. And it is not just for sources.  With `self.D` in the second tests, it creates a spurious self-loop when it starts each connected component.  
comment
It seems like raising an exception is the way to go here... Then users can handle it explicitly. 
comment
You need to figure out how best to compute the flow between two nodes with more than one edge between them (this depends on the problem). Once you do this, create a new Graph from the MultiGraph with the edges acting as a combined version of the multiedges. Run max_fow_min_cost on the Graph. 
comment
The generic answer is "a long time". :)  Finding cliques is hard.  Do you have suggestions on how to estimate the progression? The time remaining depends on things like ram and cpu speed. Maybe there's another measure other than time?   On Thu, Dec 29, 2016 at 5:03 AM, DonBeo <notifications@github.com> wrote:  > I am wondering if it is possible to add a sort of progression bar to the > find_cliques algorithm. > > I think it would be useful to give the user an idea of how long the > algorithm will take. > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/2345>, or mute the thread > <https://github.com/notifications/unsubscribe-auth/AA32XVqmI5fCijOV5mxVcTLcTw5u7XdNks5rM4VsgaJpZM4LXeYS> > . > 
comment
It looks like topological_sort_recursive was replaced in pull request #1756  It is not in the latest development version.
comment
Nice --  :)  We moved to pydotplus because pydot was not maintained.... in particular, it had never moved to be compatible with python3.  There were a number of forks of the pydot repo and we picked pydotplus.  I haven't kept up with the social interplay between the pydot options. Perhaps pydot is now ready to take over from the people who made forks of it work during its low activity period.   Can you either 1- find the backstory.... did the person who took over pydot include the improvements made by the others --  or 2- change and test NetworkX's dev version to use pydot and make sure it passes the tests?   We'd love a PR. :) 
comment
Are the groups of nodes in this case 'blocks'?  I mean, aren't they maximally connected subgraphs?  I'm not sure what the potential confusion would be. 
comment
Yes, you are correct. And a PR would be great. Thanks! 
comment
Efficiency was added after v1.11, so that's why the module was not included in your network package. 
comment
And you dont have to send a new pull request--if you push commits to the branch you already sent the pull request from it automatically updates the original pull request. 
comment
@morenobonaventura the first implementation you refer to didn't compute the harmonic centrality for only one node. It computed for all nodes and then returned the result for only `u`.  Removing that argument was done to AVOID having people mistakenly call this function for a set of nodes.  Better to call it once and then pull the desired nodes from the resulting dict. 
comment
I have seen articles which cite each other -- they may be written at the same time but published at different times. Still the second can cite the first (either in anticipatory form, or in a preliminary form which the citation databases correctly match to the actual article).  On Sun, Sep 4, 2016 at 10:40 AM, michaelkkim notifications@github.com wrote:  > Hello, >  > I'm currently working on a big citation network data ('older' nodes cannot > cite 'newer' nodes, whereas 'newer' nodes can cite 'older' nodes). >  > When I convert my graph to an undirected version using > 'Graph.to_undirected()', the number of nodes stay the same, but the number > of edges decreases for some reason. Why is this so? >  > The only reason I can think of is that if two nodes 'cite' each other > (have two directed edges pointing to each other), 'Graph.to_undirected()' > forms this as one edge. However, this should not be possible in a citation > network data... >  > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > https://github.com/networkx/networkx/issues/2242, or mute the thread > https://github.com/notifications/unsubscribe-auth/AA32XQpWQiqMn0U2XHC9Lq8knuSawccWks5qmthxgaJpZM4J0jKr > . 
comment
Looks Good!   Thanks! 
comment
It would be good for them all to have that feature. Thanks! 
comment
This error occurs in both nx 1.11 and dev version. The problem seems to be with `scipy.linalg.expm` when calling a `matrix` object instead of an `array` object.  The size 199x199 woks, but no 200x200.  ``` import scipy.linalg as LA import numpy as np a = np.ones((199,199)) - np.eye(199) LA.expm(a)  # works fine A = np.matrix(a) LA.expm(A)  # works fine b = np.ones((200,200)) - np.eye(200) LA.expm(b)  # works fine B = np.matrix(b) LA.expm(B)  #  gives the error ```  I'm not sure of the consequences of using an array vs a matrix in this routine.  Can someone closer to that code check the impact of np.matrix vs np.array there?  If no difference, we could use  ``` A = np.array(nx.to_numpy_matrix(G, nodelist)) ``` 
comment
This TypeError only occurs if `sources` is a string.  But sources should be a set of sources.  Perhaps the fix is to check that sources is a set rather than checking that target is None. (At least the doc_string says it should be a set.  Would it be Ok to have sources be a container? An iterable?   The previous if statement checks `if not sources"` already.  Is there a good check to replace that one which would give a better error when someone uses a string for sources? 
comment
The trouble this fix is intended to address occurs when the input arguments are not used as the documentation says they should be.  More specifically, when "sources" is a string instead of a set of nodes. The proposed fix actually hides this error from the user which could lead to surprising results. I'm going to close this PR.  
comment
This might be better sent to the email list discussion group https://groups.google.com/forum/#!forum/networkx-discuss  Briefly, if order of the nodes matters, you might want to use DiGraph instead of Graph.  Or if you really need a graph (edges are bidirectional), you can try the OrderedGraph class. Dan 
comment
Also, with read_weighted_edgelist() the attribute is named 'weight'. To name it 'time' use:  G = nx.read_edgelist(ss, data=(('time', int), ))  # int could be float or datetime type  On Mon, Jun 20, 2016 at 9:34 AM, Mridul Seth notifications@github.com wrote:  > Storing time stamps as edge attributes is not a problem. >  > If you are working with actual date/time (something like "13/04/2015 10:20 > AM") I would recommend using > https://docs.python.org/3/library/datetime.html objects as edge > attributes, easier to manipulate and play with. >  > Hope this helps :) >  > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > https://github.com/networkx/networkx/issues/2169#issuecomment-227143193, > or mute the thread > https://github.com/notifications/unsubscribe/AA32XSB3_LoE6AFo5CvfvkSm-t4LbW9Nks5qNpbhgaJpZM4I5oMe > . 
comment
DiGraph has successors and predecessors. But Graph only has neighbors. This basic tutorial has some more details: http://networkx.readthedocs.io/en/stable/tutorial/index.html  On Sat, Jun 18, 2016 at 8:21 AM, Filip90 notifications@github.com wrote:  > Hi. I don't know why but when I run my script show this problem: > 'Graph' object has no attribute 'successors' >  > I installed the package networkx 2.0 from GitHub so I don't know how > resolve it. Thank you >  > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > https://github.com/networkx/networkx/issues/2165, or mute the thread > https://github.com/notifications/unsubscribe/AA32XVnaKGXzbKoY4uBd0ffE-0tXLAWyks5qM-KxgaJpZM4I473h > . 
comment
Dijkstra considers edges with a weight and finds the lowest weighted path. BFS treats all edges the same (essentially using weight=1 for every edge). The routines give the same result if every edge weight is 1.  Note though that both routines use an arbitrary ordering of neighbors when traversing the graph.  That order can affect how ties are resolved when finding shortest paths. If two paths are the same length you may get either of these paths depending on the order in which neighbors are reported. So it is possible to have BFS report different results for the same graph.  Thus BFS can differ from Dijkstra in this way also.  On Mon, Jun 20, 2016 at 11:53 AM, maq123 notifications@github.com wrote:  > Will it be safe to state that BFS would _always_ give the _same_ result > as Dijkstra? > Spent some time reading about it and I got quite confused. >  > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > https://github.com/networkx/networkx/issues/2166#issuecomment-227184367, > or mute the thread > https://github.com/notifications/unsubscribe/AA32XSzaIBRcZHrpBnnQHU0Zwavan7laks5qNrdzgaJpZM4I49ma > . 
comment
An edge from a node to itself is often called a self-loop. The ability to create self-loops as part of a path is a "feature" not a "bug", though I must admit it does not get used very often.  If you can figure out different wording in the documentation that would have helped you avoid this issue please let us know. 
comment
If your nodes are custom objects, you may be able to save a little time in the instantiation (but pay for it with lookups later) by using integers for nodes and keeping a list of the custom objects.  When you get graph results with integers as nodes and need to use the custom objects you'd look up the custom object in the list using the node (an integer) as the index.   Not sure it would speed up the overall processing--depends how much you would have to do lookups.   
comment
In v1.11 we still used a default formatting for sphinx single-ticks of math mode. The error is that in combination with this doc_string using single ticks around that word. In the latest docs the trouble has been fixed (by changing the formatting for single ticks to be `'obj'`) http://networkx.readthedocs.io/en/latest/reference/drawing.html (or https://github.com/networkx/networkx/blob/master/doc/source/reference/drawing.rst) 
comment
The cycle basis of a graph is not unique. Just as a basis for a euclidean space is not unique.  One can "add" two cycles and "subtract" them and use the results to replace the two original cycles and you'll still have a cycle basis. So any computation that relies on the number of cycle **basis** entries longer than 3 is suspect.  I suspect your betti number algorithm needs to be looking at something other than a cycle basis.  Presumably the cycle basis returned by nx.cycle_basis depends on the order of nodes and edges as produced by the dict data structure. The order of reporting from dicts is not consistent in Python by design.  If order matters to you you can try OrderedGraph, but special care is needed even there for any algorithms which store intermediate results in dicts.   In your case though, I think you need to focus on your algorithm for finding the betti number. It can't be correct that looking at lengths in a single choice of cycle basis is enough to compute the betti number. 
comment
I agree that this handles the 'name' attribute incorrectly. Fix is on the way. 
comment
Just to verify.... can you print what @hagberg suggests? We did change the circular layout from [0,1] to [-1,1] and while they might both be 1.11 they might have different version numbers.  That would be confusing so hopefully not the case, but I just want to make sure. Dan 
comment
I can see that this needs more explicit documentation...   Here is what I recall from our original thinking: Sometimes we want attr_dict to actually BE the object used to store attributes. If you have a group of nodes that should all have the same attributes, it might be useful to have a single object to update all those attributes. Also, if you have multiple graphs using the same node, it would be useful to be able to change attributes for a node in one graph and have it change that node's attributes in all the graphs.   Other times we only want the information held in attr_dict to be stored. You might set all nodes to have color "blue" initially, but still want to allow the color to change for each node separately later. Then we don't want to use the same object to set the initial colors. We could use a single dict to set the initial attribute values without having them all use the same object to store those attributes.  We also had to decide the relative order of updates for attr_dict and the keyword dict called attr. And they aren't the same for `add_nodes_from` and `add_node`.  I should also remark that we paid much more attention to this for edges than for nodes. It is possible that node attributes have been relatively neglected in this regard. 
comment
**Summary of Existing Code** It looks like for new nodes:  ``` add_node(n, attr_dict, **attr)   use the object attr_dict updated by attr add_nodes_from( [(n,adict)], **attr)  use a copy of attr updated by adict ```  For updating existing nodes, the existing attribute dictionary is updated as:  ``` add_node(n, attr_dict, **attr)   update by attr_dict then by attr add_nodes_from( [(n,adict)], **attr)    update by attr then by adict ```  For edges, there is the option of a custom `edge_attr_dict_factory`, but let's assume that is `dict`. For new edges:  ``` add_edge(u, v, adict, **attr)  new dict updated by adict then by attr add_edges_from( [(u, v, dd)], adict, **attr)  new dict updated by adict then by attr then by dd ```  For existing edges:  ``` add_edge(u, v, adict, **attr)  update by adict then by attr  add_edges_from( [(u, v, dd)], adict, **attr)  update by adict then by attr then by dd ``` 
comment
I think this PR is a good one. It reduces surprise mutations of input `attr_dict` when calling `add_node` and/or `add_edge`.  The downside is that users of `add_node` cannot reuse the same attribute dict for multiple nodes or for the same node in multiple graphs.  But I don't think that is a common usage. It is also still possible to do so via direct assignment to `G.node`.   I'll go through the PR and look for other possible issues/trouble.  Thanks for bringing this up! 
comment
+1 for eliminating `attr_dict` argument.    Users can use `G.add_node(1, color="b", **attr_dict)`  without trouble so long as keys don't collide. 
comment
I'm afraid that removing the argument `attr_dict` will introduce errors all over the codebase. You can try, but I'm pretty sure there will be many places where you have to add `**`. I also think there are a few places where we use an attr_dict that has numbers for keys.  With **attr the dict has to have strings as keys.  I'd be interested to hear about attempts to remove the u and v positional terms.  
comment
Nice work -- I think you are right that the line from `convert_matrix.py` would potentially cause trouble when integers show up as an edge attribute key.  Perhaps that one should add the edge and then update:  ``` g.add_edge(row[src_i], row[tar_i]) g[row[src_i]][row[tar_i]].update((i, row[j]) for i, j in edge_i) ```  The function declaration and doc_string for the function `add_edges_from` in `graph.py` seems to be duplicated.  Also It doesn't look like you have removed the argument `attr_dict`, though you have laid the groundwork for doing so.  I'll look more later. 
comment
I think the best way to change `convert.py` is to use:  ``` G.add_edge(u,v,key=0)   # I added the fix from @hagberg below G.edge[u][v][0].update(data)  # use update instead of add_edge to allow non-string keys ```  Also -- I think we're not as concerned about backward compatibility because this is a major release (2.0). We'll have to flag it pretty strongly in the change log (and maybe even put something in the doc_string for the short term to help those upgrade=ing their code to 2.0) 
comment
Another way to alleviate some of the multigraph pain would be to require an explicit edge key for each edge.  I think there was a ticket suggesting that `add_edge` return the edge key some time ago. Not sue whether it is still active -- it was part of another discussion. 
comment
The problem stems from using `key=0` for all edges. If the dataframe has multiple rows with the same source and target we don't get multiedges this way.  Luckily `from_pandas_dataframe` doesn't have a way to specify keys, so we KNOW the default is being used. That means the most recent edge key is the maximum of all edge keys for that pair of nodes.  We can figure out the `key` without having it returned by `add_edge`.  Try this:  ```    for row in df.values:         s, t = row[src_i], row[tar_i]         if g.is_multigraph():             g.add_edge(s, t)             key = max(g[s][t])  # default keys just count, so max is most recent              g[s][t][key].update((i, row[j]) for i, j in edge_i)         else:             g.add_edge(s, t)             g[s][t].update((i, row[j]) for i, j in edge_i) ``` 
comment
As for timingclasses.py, that file was created by me to ease testing the impact on speed of release v2.  It probably shouldn't even be there, but is a convenience for me (stored on github).  We shouldn't make changes to the API in that file or we won't be able to compare the speed of the two APIs.  Could you please take out the changes you make to timingclasses.py?  Sorry about the confusion. 
comment
I'll open another issue to discuss multigraph `key` I think @hagberg is correct that it will come back to bite us someday. 
comment
The Travis CI error you are seeing is occurring because the maxflow problem seems to have two solutions and the algorithm is now finding the one that isn't checked for. I believe this is a problem with the test not accounting for the arbitrary ordering of dict.  This test was touched recently (in May) and we might not have taken into account this potential test failure.  I'm not sure why it would show up on the commit where you changed timingclasses.py because it doesn't use that code at all. Perhaps something else (like a different version of some package on Travis) changed the order of dict reporting for this particular case.  Anyway, don't worry about that for this PR.  I'll put together something for the release.  I'm also going to take one last pass through the PR before merging.  
comment
Ahhh.... I spoke too soon (as I often do :)  In test_match_helpers.py the `setup` method uses `attr_dict=attr_dict` instead of `**attr_dict`.  So we were ending up with an attribute called `attr_dict`.  Still not sure why that passed previous tests.  I also notice that algorithms/tree/branchings.py creates a class with methods `add_edge` and `add_edges_from`. Those methods should remove the `attr_dict` keyword to match the main classes.  
comment
In classes/multidigraph.py the method `add_edge` should remove the parameter `attr_dict` from the doc_string. 
comment
I'll add to this PR and merge shortly. 
comment
I can imagine users being confused by this if they use MultiGraphs without setting keys (which many don't).  On the other hand, changing it as you suggest could be confusing to users who do pay attention to the edge keys. At the very least a note should be included in the docs.  Perhaps there should be an argument to describe how to treat the edge keys. 
comment
See #2100  
comment
Cycle_basis_matrix can use features from #1740. Also the add_path changes seem to have caused conflicts.  
comment
This has been superceded by #2069  
comment
Is this still a [WIP] ?    Also, you should update the docs at doc/source/reference/exceptions.rst 
comment
This has definitions and an algorithm...  hope it helps. https://en.wikipedia.org/wiki/Dominator_(graph_theory) 
comment
I don't think we should push too hard on making the parameters the same for all the layout functions. In particular the `random_layout` was specifically chosen to be different. `circular_` and `shell_` naturally surround the origin, while `random_` and `spring_` are more natural on the unit square. I'm not sure `spectral_` should even have a "center" argument at all.  The `random_layout` is especially important to be clear that the size affects the range of "possible values" and not the range of actual results. If you choose uniformly random values on `[0,1)` and then rescale them so the max value is "size", then it's not easy to figure out the random distribution you chose from. Much better to choose uniformly random on `[0, size)`.  I think this ticket is really about aligning expectations of users with the code.  Users expect the arguments to be the same -- and we should be more clear that the layout range is different for different layouts.   Perhaps the differences would be more clear if we used different names for the arguments in layouts which return circular results from those with square.  For example, "scale" and "center" for circular domains vs "size" and "shift" for square domains.  ``` circular_layout(G, scale, center]) shell_layout(G, nlist, scale, center) random_layout(G, size, shift) spring_layout(G, weight, k, pos, fixed, iterations, size, shift) ```  I don't particularly care about the order of the parameters. But if we're going to go there then `dim` should not be the first argument. The default is almost always used. Perhaps the last three should be `scale, center, dim)` so that the layout specific arguments can be used without keywords: e.g. `spectral_layout(G, "weight")`.  I would propose (leaving spectral's return values as they are because I don't have a better suggestion):  ``` circular_layout(G[, scale, center, dim])     --> [-scale, scale] + center shell_layout(G[, nlist, scale, center, dim])  random_layout(G[, size, shift, dim])         ---> [0, size] + shift spring_layout(G[, weight, k, pos, fixed, iterations, size, shift, dim])  spectral_layout(G[, weight, scale, center, dim])    ---> [-scale, scale] + center ```  @AgostinoSturaro what do you think? 
comment
#1760 was merged back in Sept to v1.11 to fix this issue for that version. It added scale and center for all but random_layout which did not get the scale parameter (because scaling the points to be in a specific range makes the random process wonky--scaling the potential domain makes sense but differs from the other usage of the keyword `scale`). I plan to revisit this for v2, but thought we should not do the other changes on a minor release.   Discussion came back to this issue when #1892 was submitted. but I think of it as being for v2 not for v1.11  The proposal for v1.11 above (two comments up) suggests changes to `circular_layout()` that have already been made and then to add `scale` to `random_layout()` which I had decided not to do because it is potentially confusing. 
comment
You may need to prefix the see-also functions with the full path: networkx.algorithms.components.connected.is_connected 
comment
I notice that the centrality code has `__init__.py` use relative imports instead of full path. So, apparently sphinx could find the routines inside the centrality folder even if not inside the same module. But I can't reproduce that with the current version of sphinx used on ReadTheDocs, so maybe something has changed with the way sphinx looks for links.  I'll investigate further...  Why doesn't the sphinx documentation build for you? It should not work for Python3.4 (because make_gallery.py uses `execfile` -- also not worth fixing because examples are moving anyway).  But using Python2.7 with the PYTHONPATH variable pointing to your local git development copy of networkx should work. 
comment
To check whether this works, you'll need to build it (ReadTheDocs only builds the master branch).  With pip, use `pip install sphinx sphinx-rtd-theme`  (It is possible I am forgetting others, but you'll get an error if you don't have what sphinx needs.)    Then go to the `./doc` directory and use `make html`.  The results are built and put in ./doc/build/html where you can try them locally.   BTW I see that Errors in the ReadTheDocs documentation builds were introduced in one of the merges this morning. You can avoid that by using python3+ when you do the `make html`.  I will correct those errors in another PR soon. 
comment
Yes, it looks like make_gallery won't work with python3 due to the `execfile` in make_gallery.py  I think the error you are getting with python2.7 means you are using an older version of networkx. Perhaps you need to set your PYTHONPATH environment variable?  But even with that I am getting an error on the dergee_histogram example because of namespace collision for `nx.generators`.  I'll open an issue about that.  
comment
Looks like it worked! 
comment
Or maybe it didn't work.....  can you check it?  Thanks... http://networkx.readthedocs.org/en/latest/reference/algorithms.component.html 
comment
Should the new repository be called `networkx-examples`?  (Should we move the examples there too?) Or would it be better to call it `networkx-ipynb` an/or just have notebooks there? 
comment
I've created a repository for notebooks at https://github.com/networkx/notebooks I think @MridulS examples will fit there nicely. 
comment
I don't see the harm in keeping them public. Is pollution of the namespace the concern? How many such functions are we talking about? 
comment
Do #2073 and #2074 implement the ideas from this PR? Is there anything else to do here? 
comment
Those two PRs have been merged into the master branch on github.  So you can clone the networkx github repository and run code against that.  If you already have that code set up with git you can use `git pull` to update it. 
comment
Fixed in #2075  
comment
It looks like the `__all__` variable in hybrid.py is `_all__` (missing one underbar) so the automatic generation of docs is not working.  The source for docs that are not autogenerated is doc/source/reference 
comment
Ugh -- you are correct... it was an oversight on the rename/relocation process. Unfortunately it is more tricky than your fix covers because graphviz_layout is supplied by both the nx_pydot and nx_agraph modules.  So the "nx_pydot" text you added will only work if they have pydot installed. If they only have pygraphviz then you will need to put nx_agraph there.  Maybe it's best to try one and when it fails go to the other... 
comment
Maybe @Michael-E-Rose or @jfinkels (or others) could put together some tests of the graphviz_layout function in nx_pylab.py (and non-test code in the function itself). Anybody looking for some coding fun?  We need to make sure the function works in each installation case: 1) pydot and no pygraphviz, 2) pygraphviz and no pydot, 3) both, 4) neither. 
comment
Removing it sounds good to me. :) 
comment
Can you update the docstring accordingly too? Also, are there other options that might be helpful? 
comment
One node graph needs check that node is in G. 
comment
I think `is_simple_cycle` should wait until it is needed somewhere. (YAGNI principle) 
comment
This looks good. What about corner case of 0 or 1 node?  Currently they return True--is that the desired outcome? 
comment
The PR dealing with is_path and is_simple_path has conflicts with master. @jfinkels are you able to get those conflicts resolved? If not, I will try and put the result in a new PR.  
comment
If edge_subgraph can help here, it was implemented in #1740 and now merged. 
comment
This generally looks good. Thanks! The travis CI errors look related to updating calls to bellman_ford within the package. I hope those are due to rearranging of arguments. I think its ready to move forward with the items you suggest.  I would prefer not to check for negative weights in shortest_path. But we should add a note in the doc_string that negative weights require special methods and point them to bellman_ford and maybe to johnson (which then uses bellman_ford).  We are also discussing a change in the API for these weighted algorithms to make them return iterators instead of dicts. But you probably shouldn't worry about that and get this in place before that work starts. However that is decided we'll just do the same for bellman_ford as we do for Dijkstra. 
comment
Your solution for initial values for predecessor when we are tracking multiple shortest paths seems good to me. Why don't you look into dijkstra for similar issues in a separate PR though. I think this one just needs an update to remove the conflicts for a merge. hopefully not much.  Could you also make sure the files end with a newline on the last line? Also, instead of `== None` you should use `is None`. And instead of `!= None` it should be `is not None`.  Thanks 
comment
Thanks very much!   I know all too well how merging messes up subtle things. I appreciate your effort. 
comment
The code `edge in g.edges()` doesn't check if the edge is in the graph.  It only checks that the edge is in the list of tuples creates by the g.edges() function. Since the graph is undirected, the edges are created with one orientation in g.edges() and if the other orientation is in `edge` then you will get false even though the edge is in g.  Perhaps you can use `g.has_edge(*edge)` or `edge[1] in g[edge[0]]`  On Wed, Mar 23, 2016 at 9:25 PM, paulmorio notifications@github.com wrote:  > I recently ran into a problem creating subgraphs as I believe it creates > edges between nodes in the subgraph that do not exist in the original graph > it came from. To illustrate this I have written a small program using graph > generators. >  > import networkx as nx >  > g = nx.barabasi_albert_graph(150,6) > group = g.nodes()[100:140] > sub_g = nx.subgraph(g, group) >  > print "\n Edges of g \n" > print g.edges() >  > print "\n Edges of sub_g \n" > print sub_g.edges() >  > print "\n Edge check \n" > for edge in sub_g.edges(): >     print (edge in g.edges()) >     print edge >  > Running this code gave me this. I apologize for the large output. >  >  Edges of g >  > [(0, 128), (0, 35), (0, 47), (0, 6), (0, 7), (0, 8), (0, 41), (0, 10), (0, 39), (0, 108), (0, 13), (0, 46), (0, 15), (0, 48), (0, 123), (0, 53), (0, 23), (0, 27), (0, 60), (0, 138), (1, 97), (1, 34), (1, 131), (1, 147), (1, 86), (1, 6), (1, 135), (1, 8), (1, 9), (1, 12), (1, 98), (1, 44), (1, 16), (1, 19), (1, 22), (1, 90), (1, 27), (1, 61), (1, 30), (2, 35), (2, 38), (2, 6), (2, 7), (2, 8), (2, 9), (2, 11), (2, 110), (2, 72), (2, 117), (2, 118), (2, 24), (2, 104), (2, 26), (2, 91), (2, 60), (2, 94), (3, 128), (3, 133), (3, 6), (3, 7), (3, 9), (3, 14), (3, 17), (3, 21), (3, 23), (3, 26), (3, 30), (3, 33), (3, 36), (3, 39), (3, 53), (3, 54), (3, 63), (3, 64), (3, 65), (3, 76), (3, 77), (3, 83), (3, 89), (3, 94), (3, 101), (3, 108), (4, 6), (4, 7), (4, 10), (4, 11), (4, 12), (4, 15), (4, 16), (4, 17), (4, 18), (4, 25), (4, 31), (4, 32), (4, 40), (4, 41), (4, 44), (4, 52), (4, 54), (4, 61), (4, 67), (4, 68), (4, 69), (4, 70), (4, 80), (4, 83), (4, 96), (4, 144), (4, 114), (4, 116), (5, >  128), (5, 6), (5, 7), (5, 8), (5, 10), (5, 12), (5, 13), (5, 14), (5, 141), (5, 17), (5, 20), (5, 24), (5, 25), (5, 26), (5, 29), (5, 35), (5, 37), (5, 42), (5, 48), (5, 49), (5, 51), (5, 57), (5, 66), (5, 67), (5, 70), (5, 79), (5, 84), (5, 86), (5, 87), (5, 88), (5, 99), (5, 100), (5, 102), (5, 103), (5, 127), (6, 134), (6, 7), (6, 8), (6, 9), (6, 10), (6, 12), (6, 13), (6, 14), (6, 15), (6, 17), (6, 18), (6, 131), (6, 20), (6, 22), (6, 26), (6, 28), (6, 29), (6, 46), (6, 52), (6, 58), (6, 60), (6, 64), (6, 66), (6, 75), (6, 78), (6, 82), (6, 86), (6, 88), (6, 143), (6, 98), (6, 101), (6, 105), (6, 106), (6, 108), (6, 115), (6, 122), (6, 124), (7, 129), (7, 135), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 15), (7, 16), (7, 20), (7, 22), (7, 29), (7, 30), (7, 31), (7, 34), (7, 35), (7, 40), (7, 42), (7, 51), (7, 55), (7, 68), (7, 130), (7, 71), (7, 87), (7, 143), (7, 93), (7, 144), (7, 101), (7, 103), (7, 105), (7, 106), (7, 120), (7, 127), (8, 131), (8, 9), (8, 11), ( >  8, 130), (8, 142), (8, 15), (8, 18), (8, 19), (8, 20), (8, 28), (8, 33), (8, 34), (8, 39), (8, 42), (8, 43), (8, 49), (8, 54), (8, 56), (8, 57), (8, 61), (8, 62), (8, 79), (8, 88), (8, 100), (8, 110), (8, 119), (8, 148), (8, 122), (8, 123), (9, 10), (9, 11), (9, 14), (9, 19), (9, 32), (9, 37), (9, 135), (9, 51), (9, 54), (9, 55), (9, 65), (9, 68), (9, 73), (9, 74), (9, 75), (9, 80), (9, 142), (9, 87), (9, 95), (9, 104), (9, 107), (9, 110), (9, 113), (10, 11), (10, 12), (10, 14), (10, 16), (10, 17), (10, 18), (10, 19), (10, 21), (10, 22), (10, 23), (10, 24), (10, 30), (10, 31), (10, 32), (10, 40), (10, 45), (10, 50), (10, 53), (10, 62), (10, 65), (10, 67), (10, 75), (10, 82), (10, 83), (10, 99), (10, 110), (10, 117), (10, 118), (11, 129), (11, 13), (11, 142), (11, 144), (11, 18), (11, 25), (11, 31), (11, 36), (11, 37), (11, 38), (11, 39), (11, 42), (11, 43), (11, 46), (11, 48), (11, 73), (11, 78), (11, 87), (11, 135), (11, 97), (11, 99), (11, 102), (11, 106), (12, 139), (12, 140), (1 >  2, 13), (12, 14), (12, 15), (12, 16), (12, 19), (12, 21), (12, 23), (12, 25), (12, 26), (12, 28), (12, 33), (12, 36), (12, 37), (12, 47), (12, 49), (12, 138), (12, 62), (12, 70), (12, 71), (12, 74), (12, 84), (12, 90), (12, 144), (12, 102), (12, 103), (12, 111), (12, 114), (12, 115), (12, 126), (13, 145), (13, 148), (13, 21), (13, 24), (13, 25), (13, 31), (13, 35), (13, 37), (13, 39), (13, 40), (13, 41), (13, 52), (13, 60), (13, 84), (13, 94), (13, 97), (13, 98), (13, 107), (13, 109), (13, 113), (13, 114), (13, 119), (13, 125), (14, 39), (14, 40), (14, 16), (14, 17), (14, 21), (14, 108), (14, 92), (14, 94), (15, 137), (15, 18), (15, 33), (15, 36), (15, 38), (15, 43), (15, 46), (15, 50), (15, 56), (15, 66), (15, 67), (15, 73), (15, 79), (15, 89), (15, 91), (15, 92), (15, 104), (15, 115), (16, 34), (16, 72), (16, 20), (16, 86), (16, 120), (16, 25), (16, 125), (16, 94), (17, 130), (17, 146), (17, 19), (17, 22), (17, 28), (17, 33), (17, 34), (17, 41), (17, 47), (17, 49), (17, 51), (17, >  60), (17, 61), (17, 64), (17, 147), (17, 69), (17, 76), (17, 96), (17, 109), (17, 115), (17, 116), (17, 117), (18, 33), (18, 37), (18, 71), (18, 20), (18, 58), (18, 28), (18, 29), (19, 34), (19, 35), (19, 113), (19, 81), (19, 43), (19, 109), (19, 112), (19, 21), (19, 24), (19, 132), (19, 58), (19, 27), (19, 149), (20, 67), (20, 41), (20, 61), (20, 56), (20, 53), (20, 24), (20, 143), (20, 29), (21, 64), (21, 79), (21, 130), (21, 101), (21, 70), (21, 119), (21, 96), (21, 107), (21, 45), (21, 145), (21, 146), (21, 148), (21, 22), (21, 23), (21, 59), (22, 144), (22, 147), (22, 23), (22, 26), (22, 27), (22, 31), (22, 44), (22, 51), (22, 52), (22, 55), (22, 58), (22, 61), (22, 62), (22, 70), (22, 81), (22, 85), (22, 93), (22, 107), (22, 111), (22, 115), (22, 123), (22, 124), (23, 32), (23, 66), (23, 132), (23, 41), (23, 78), (23, 44), (23, 48), (23, 105), (24, 137), (24, 146), (24, 149), (24, 27), (24, 29), (24, 30), (24, 53), (24, 58), (24, 67), (24, 77), (24, 78), (24, 82), (24, 85), (2 >  4, 87), (24, 93), (24, 105), (24, 109), (24, 120), (25, 32), (25, 97), (25, 59), (25, 142), (25, 131), (25, 27), (25, 133), (26, 132), (26, 106), (26, 43), (26, 98), (26, 46), (26, 80), (26, 59), (26, 83), (26, 148), (26, 120), (26, 38), (27, 73), (27, 43), (27, 84), (27, 28), (28, 76), (28, 30), (28, 127), (29, 96), (29, 97), (29, 130), (29, 144), (29, 38), (29, 63), (30, 32), (30, 36), (30, 40), (30, 60), (30, 139), (30, 46), (30, 48), (30, 88), (30, 51), (30, 54), (30, 121), (30, 42), (30, 126), (31, 99), (31, 69), (31, 38), (31, 108), (31, 111), (31, 113), (31, 83), (31, 68), (31, 90), (31, 95), (32, 75), (32, 44), (32, 45), (32, 47), (32, 50), (32, 126), (32, 124), (32, 42), (32, 63), (33, 45), (33, 77), (33, 50), (33, 123), (34, 36), (34, 49), (34, 143), (35, 139), (35, 77), (35, 45), (35, 80), (35, 50), (35, 86), (36, 73), (36, 49), (36, 62), (36, 92), (37, 65), (37, 98), (37, 136), (37, 105), (37, 59), (38, 69), (38, 135), (38, 72), (38, 101), (38, 120), (38, 82), (38, 52), >  (38, 55), (38, 56), (38, 57), (38, 92), (39, 128), (39, 134), (39, 76), (39, 111), (39, 50), (39, 54), (39, 58), (39, 123), (39, 93), (39, 146), (40, 47), (40, 112), (40, 118), (40, 91), (40, 63), (41, 118), (41, 138), (41, 109), (41, 47), (41, 112), (41, 117), (41, 89), (41, 68), (41, 59), (41, 141), (41, 133), (42, 132), (42, 136), (42, 85), (42, 118), (42, 89), (43, 99), (43, 44), (43, 45), (43, 111), (43, 92), (44, 102), (44, 143), (44, 149), (44, 56), (44, 100), (45, 66), (45, 149), (45, 64), (45, 130), (45, 79), (45, 55), (45, 89), (45, 124), (45, 63), (46, 149), (46, 75), (46, 100), (46, 138), (46, 80), (46, 62), (46, 55), (46, 57), (46, 85), (47, 129), (47, 134), (47, 81), (47, 65), (47, 64), (47, 48), (47, 83), (47, 89), (47, 91), (48, 66), (48, 102), (48, 136), (48, 81), (48, 52), (48, 121), (49, 80), (49, 57), (50, 133), (50, 77), (50, 113), (50, 115), (50, 53), (50, 87), (50, 88), (50, 120), (50, 91), (51, 84), (51, 69), (52, 72), (52, 140), (52, 100), (52, 86), (53, 96) >  , (53, 101), (53, 104), (53, 77), (53, 93), (53, 56), (53, 121), (53, 84), (53, 74), (54, 74), (54, 90), (55, 65), (55, 98), (55, 75), (55, 114), (55, 71), (55, 112), (55, 82), (55, 78), (55, 88), (55, 57), (55, 59), (55, 110), (56, 99), (56, 122), (57, 79), (57, 136), (57, 111), (57, 94), (57, 63), (58, 72), (58, 126), (59, 129), (59, 70), (59, 95), (59, 141), (59, 82), (59, 116), (59, 137), (59, 91), (59, 125), (59, 69), (60, 134), (60, 81), (60, 122), (61, 133), (61, 74), (61, 132), (61, 90), (61, 126), (62, 103), (62, 141), (63, 131), (64, 114), (65, 100), (65, 71), (65, 131), (65, 73), (65, 121), (65, 124), (66, 68), (66, 71), (66, 74), (66, 76), (66, 117), (66, 125), (67, 85), (68, 76), (68, 114), (69, 102), (69, 78), (69, 122), (70, 96), (70, 95), (71, 72), (71, 116), (71, 148), (72, 97), (72, 145), (72, 85), (73, 95), (75, 139), (75, 109), (75, 116), (76, 135), (77, 145), (77, 107), (77, 140), (77, 81), (77, 119), (77, 123), (78, 129), (78, 92), (78, 125), (79, 107), (79, 11 >  2), (79, 119), (79, 126), (79, 117), (80, 128), (80, 103), (80, 124), (80, 125), (81, 104), (81, 137), (81, 119), (81, 93), (82, 108), (82, 149), (82, 127), (83, 142), (83, 121), (84, 136), (86, 147), (86, 127), (87, 122), (88, 105), (88, 140), (88, 90), (88, 95), (89, 139), (90, 113), (93, 112), (93, 147), (94, 128), (94, 103), (95, 106), (95, 116), (96, 106), (96, 118), (96, 127), (99, 134), (102, 129), (102, 146), (103, 104), (103, 137), (103, 138), (103, 140), (104, 139), (104, 110), (106, 148), (109, 147), (110, 132), (111, 133), (114, 146), (116, 136), (116, 121), (119, 143), (120, 137), (121, 145), (123, 134), (123, 141), (125, 138), (125, 142), (131, 141), (134, 140), (140, 145)] >  >  Edges of sub_g >  > [(129, 102), (132, 110), (133, 111), (134, 123), (136, 116), (137, 120), (137, 103), (138, 125), (138, 103), (139, 104), (103, 104), (104, 110), (116, 121)] >  >  Edge check >  > False > (129, 102) > False > (132, 110) > False > (133, 111) > False > (134, 123) > False > (136, 116) > False > (137, 120) > False > (137, 103) > False > (138, 125) > False > (138, 103) > False > (139, 104) > True > (103, 104) > True > (104, 110) > True > (116, 121) >  > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/2045 
comment
This caught a bug and provides a feature not advertised--but perhaps worth expanding. - Squashed an unfound bug at line 729 for a multigraph self-loop where previous `.get(weight,1)` didn't check for multi-edges. - Enables screen of edges (essentially edge-subgraph for traversal) if the supplied weight function returns `None` for edges that are not allowed.  Thus by using `_dijkstra` with the following provides the shortest_path using only "blue" edges.  (see #1946)  ```     def weight(u, v, edata):         return 1 if edata['color'] == "blue" else None ```  I'm going to merge this and separately commit the few small changes for this file I noticed on the way.  Take up discussion of expanding this idea for edge restriction at #1946   
comment
I am looking at this and thinking about the best implementation in terms of views vs new graph structures, etc.  This implementation looks pretty good if we go the route of creating a shallow copy of the graph.  I'm playing with views generally. Aric has been a proponent for a long time of a subgraph view as well.  Something like:  ``` restrict_to_edges = lambda u,v: return u != v G.edge_subgraph(restrict_to_edges) ```  And it should be lightweight (low ram) and fast.  :}  On Thu, Feb 4, 2016 at 1:31 AM, jfinkels notifications@github.com wrote:  > I think this is an extremely useful function for implementing other > algorithms, I'd like to push for its inclusion! >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1740#issuecomment-179669261. 
comment
I was hoping to find a nice light-weight "view approach" to edge-subgraph.  But this is a good implementation of creating a new shallow graph copy with only those edges.  I'm going to merge and if/when we come up with a "view" version we can morph this at that time.  Thanks @jfinkels  
comment
The edge data is "shallow copied" in that the subgraph uses the same datadict as the graph. I guess this method violates the "one way to do it" principle. 
comment
I disagree with removing documentation from helper functions and also with banning them from the public API. It's also a little disappointing to see essentially arbitrary rules stated with such rigidity. Rules of thumb are fine but don't follow them for the sake of following them.   The overarching principles we look for are ease for new users to do simple things and learn more while they do it, and ease for experienced users to do complicated things without a lot of trouble.  I think a quote from early on was: make simple tasks easy and difficult tasks possible.  This style of admittedly vague goals allows you to avoid getting locked into rules of thumb for their own sake.  I prefer making helper functions public and documenting them. It makes them easy to find and people can get ideas from them (and maybe even USE them...horrors...) I also prefer absolute imports because a casual reader (who say followed a search to get to the source code) does not have to figure out the file structure of the package to know where to look. Ease for the user trumps ease for the developer in my view. 
comment
I'm not seeing the advantage of the separation. I guess we distribute a 12MB package instead of an 18MB package. Is that the positive impact of doing this? Others I am missing?  I am not so sure that users don't care about the test code. Even if 10% of them do, isn't it worth keeping it in the package?   Finally, looking at the other packages in the scientific python community they all seem to keep the tests close to the code. In fact they use the same structure we are using.  So I'm seeing 1 small reason to move (less space in Pypi package), 1 small reason to keep (users who want tests) and 1 larger reason to keep (same as other packages in scientific python ecosystem). Overall (-1) without more reason to move. 
comment
Reading source code is definitely not a last resort for many of our users. We're providing tools and code; and some users won't want to look at the code. But some users are quite adept at reading code and tests. I don't think we should make it harder for either users or developers UNLESS there is a good reason for doing so. I still haven't seen (include the blog @midnighter pointed to) a justification for separating tests from code that resonates for me. 
comment
I believe the osgeo (and thus ogr) come with gdal.  At least with conda I install gdal to get these features.  Also pyparsing needs to be a version before 2.0 for pydot to work. (not sure why neither of these came up in your search). pyparsing=1.5 seems to work. 
comment
I was using networkx-1.10 instead of dev.... so ignore my comment about pyparsing.   Still true that gdal is the package that gives osgeo and ogr (I think) 
comment
I think the follow should do it.  At least with these installed, no tests get "skipped" and docs build.  decorator numpy scipy  matplotlib nose pandas gdal pyyaml pygraphviz sphinx sphinxcontrib-bibtex sphinx_rtd_theme  For python2 we need pydot pyparsing=1.5  For python3 instead we need pydotplus 
comment
I don't actually know how pip handles that...   Can you work through their help page? Or maybe @OrkoHunter or @chebee7i know?  
comment
The [pydotplus webpage](https://pypi.python.org/pypi/pydotplus) says that pydotplus is compatible with python2.7-3.5.  Since we don't support python2.6 any more maybe we can get by with pydotplus for all our configurations. Has anybody tried pydotplus with python2.7?  It'd be nice to jettison pydot if we can use a single package to replace it (as @SanketDG notes). 
comment
See #1898 for a switch to using pydotplus instead of both pydot and pydotplus. Its a maintained version and works for all versions of python that we support.  So @karrtikr can your switch that name and remove the pyparsing line?  I think this means we have the same file for both python2 and python3. 
comment
Fixed in #2033  
comment
1) Look at the `make_str` code. It is supposed to help with python2/python3 unicode/str handling.  It's goal is not to make code shorter. It is to centralize and simplify handling of strings/unicode/etc.  2) We don't strive for compression of code. We strive for readability of code. Follow PEP8 guidelines (including the idea that exceptions to the guidelines are sometimes better than the guidelines).  We want readable, maintainable code.  3) Author attributes are NOT being removed. They are being replaced by comment lines. See the section of the NetworkX GitHub Wiki page for how to replace the `__author__` code with `# Authors:` comments  Thanks 
comment
`sorted(data1.items())` seems good to me. Also, the Travis-CI tests should give the same results as tests using `nosetests` on your own machine. So it really isn't a Travis-CI issue.   You can install `nose` and then run `nosetests -v --with-doctest` to check your work before committing. (you need to use environment variable PYTHONPATH or similar to make sure python will find your networkx directory) 
comment
Perhaps I'm confused. On a directed acyclic graph, I thought ancestors based on shortest path would be the same as ancestors on any path.  If there is a path then there is a shortest path, right? I don't think the fact that we look at shortest paths matters.   
comment
But that IS what nx.ancestors(G, 4) gives for your example graph.  In [_1_]: import networkx as nx  In [_2_]: G=nx.DiGraph()  In [_3_]: G.add_edges_from([(1,2), (2,3), (3,4), (1,5),(5,4)])  In [_4_]: nx.ancestors(G, 4)  Out[_4_]: {1, 2, 3, 5}  It doesn't matter whether the path is a shortest_path or another path.  If there is ANY path from a node to 4, then that node will show up as an ancestor.  We get away with using shortest_paths because if there is any path, then there must be a shortest_path. So by looking at shortest_paths we get the same nodes as using any path.  Your original understanding of the function is correct. You don't need to write a new function. Dan  On Mon, Feb 15, 2016 at 12:05 PM, dorkaerdos notifications@github.com wrote:  > I want to output all node ids that are on ANY directed path between the > root and my node. In a DAG there could be multiple paths from the source to > the node with different length, not just the shortest path. Remember, a DAG > is not necessary a tree. >  > Anyway, I'm just going to implement the function that I need for myself, > I just wanted to make you aware that maybe adding more information to the > documentation would make it more clear for some people (me). >  > Thanks for your time! >  > Here is an example for what I mean: > I will index my graph nodes by numbers. Let 1 be the root. Let us assume > directed edges are (1,2), (2,3), (3,4), (1,5),(5,4). In this case I would > like to have DiGraph.all_ancestors(4) = [1,2,3,5] >  > On Mon, Feb 15, 2016 at 11:48 AM, Dan Schult notifications@github.com > wrote: >  > > Perhaps I'm confused. On a directed acyclic graph, I thought ancestors > > based on shortest path would be the same as ancestors on any path. If > > there > > is a path then there is a shortest path, right? I don't think the fact > > that > > we look at shortest paths matters. > >  > > — > > Reply to this email directly or view it on GitHub > > <https://github.com/networkx/networkx/issues/1993#issuecomment-184295198 > > . >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1993#issuecomment-184303415. 
comment
Looks like they should be added to the "functions" autosummary entries as well.  There are quite a few other errors in the [sphinx doc builds](http://readthedocs.org/projects/networkx/builds/3764367/) too...  mostly with communicability and cuts.  I'll try to clean that up. 
comment
Issues #1997 correctly points out this bug also.  The short term solution is to not use `nx.pylab.drawing_graphviz`.  That function created confusion because you could be using pygraphviz or pydotplus to access the graphviz libraries and this function doesn't tell you which and its fairly tricky to find out which because it depended on import order.  All the function did was get the layout positions and then call `nx.draw`.  So the fix is to replace your last line with two lines.    If you use pygraphviz:  ``` pos = nx.drawing.nx_agraph.graphviz_layout(G) nx.draw(G, pos=pos) ```  If you use pydotplus:   ``` pos = nx.drawing.nx_pydot.graphviz_layout(G) nx.draw(G, pos=pos) ```  More details can be found on the [layout docs](http://networkx.readthedocs.org/en/stable/reference/drawing.html) 
comment
Would you be willing to make the exact same change for DiGraph, MultiGraph and MultiDiGraph? It looks good to me.  Thanks! 
comment
Thanks @jfinkels !! 
comment
I think this corrupts the data structure of the new graph for multigraphs. The test works fine for the Graph class, and passes for multigraphs, but the resulting copy is only a dict-of-dict-of-dict...  no key level.  
comment
One approach is (like for subgraph) use a different constructor in the `copy` routine itself.  ``` G = self.__class__() ```  For unit tests we should add a method to the multigraph tests that creates a multiedge and then apply copy to that. The current tests may be missing many tests like that for the other methods, but we can focus on `copy` for now.  
comment
The graph class unit tests use `self.K3` as a prebuilt graph. It is actually built in the `SetUp` method which is different for each graph class.  Along with that graph are data structure variables like `self.k3edges` which are created in `SetUp` so that we can check the data structure. So I think the code you had in there might work if you tested the data structure using those variables instead of building the dict-of-dict directly.   ``` assert_equal(G.nodes(), self.k3nodes) assert_equal(G.edges(), self.k3edges) assert_equal(G.adj, self.k3adj) ```  We created a bunch of methods to help determine if a shallow_copy had been made, but I don't think they are helpful here because it's not really a copy...    hmmmm    is this just a shallow copy?  Can we simply do: `G = nx.Graph(self)` or better: `G = self.__class__(self)` and get the same thing as `self.copy(with_data=False)`?  I'll think about this more.  
comment
The testing methods `is_shallow` and `is_shallow_copy` and `is_deep` were developed with this type of issue in mind. I just haven't wrapped my head around whether any of them test what we intend to happen here. Can you clarify the intent? Do you want to create new empty dicts for edge attributes? Is it sufficient to ignore edge attributes but be allowed to add and remove edges without affecting the old data structure? Lots of possibilities. 
comment
But this isn't a new exception. The NetworkXPointlessConcept already exists. 
comment
There has not been a v1.11 release yet. The release candidate v1.11rc1 is a release candidate and it does include the fix, but you will have to point pip to it explicitly.  pip will install 1.10 by default until there is a v1.11 release.  
comment
In the meantime, you can use:  ``` H = nx.Graph((u, v) for u, v, d in G.edges(data='color') if d == 'red') H.add_nodes_from([x, y]) # in case x or y has no red edge nx.shortest_path(H, x, y) ``` 
comment
My guess is that creating the new graph will be more efficient/effective than having to look at edges which default to `inf`.  I'm not opposed to exposing the default value, but I don't think this issue is a compelling case for doing so.  Perhaps it would be good to develop a subgraph view of graphs that restricts nodes and/or edges.  
comment
Due to #1690 (now merged), a feature in Dijkstra allows the weight to be a function. So, you can get shortest paths based on edge properties like color.  ``` def color_blue(u, v, edata):     return 1 if edata['color'] == "blue" else None G=nx.Graph([(1, 2, dict(color="blue")), (2, 3, dict(color="blue")), (1, 3, dict(color="red"))]) shortest_path(G, 1, 3, weight=color_blue)  # [1, 2, 3] ``` 
comment
To use the output of `G.edges(data=True)` you should just use `H.add_edges_from()`. The method `add_weighted_edges_from()` was created for the case when you have numeric data coming in that isn't in a dict form yet.   ``` [(u, v, {'wt':2.3})]   -> add_edges_from [(u, v, 2.3)]     -> add_weighted_edges_from ```  It is true that you could use this to add nonnumeric data, but it was originally intended for numeric data. 
comment
It looks like a bug. You can change line 425 to `node, (length, _) = max(dist.items(), key=lambda x: x[1][0])`   (I just added `[0]`near the end).    the problem is that dist is a dict with node keys and values with form (length, node).  We tell `max` to look at the value to sort. That will usually work because (1, process)<(2,process).  But if they have the same length tuple compares go to the second argument of the tuple and that's not orderable.  The fix is to make the `key` argument for the `max` function strip off the node and just use the length.  If making that change works for you, the latest code has the same problem on line 448.  You could submit a pull request to fix it. Or, I will do so if that's too much to learn how to do.  Sorry for the bug--thanks for the report.   
comment
Could you add a test to tests/test_convert_pandas.py?  Something like:  ``` --- a/networkx/tests/test_convert_pandas.py +++ b/networkx/tests/test_convert_pandas.py @@ -24,6 +24,9 @@ class TestConvertPandas(object):          df[0] = a # Column label 0 (int)          df['b'] = b # Column label 'b' (str)          self.df = df +        mdf = pd.DataFrame([[4, 16, 'A', 'D']], +                           columns=['weight', 'cost', 0, 'b']) +        self.mdf = df.append(mdf)       def assert_equal(self, G1, G2):          assert_true( nx.is_isomorphic(G1, G2, edge_match=lambda x, y: x == y )) @@ -34,6 +37,12 @@ class TestConvertPandas(object):                                 ('A', 'D', {'cost': 7, 'weight': 4})])          G=nx.from_pandas_dataframe(self.df, 0, 'b', True)          self.assert_equal(G, Gtrue) +        # MultiGraph +        MGtrue = nx.MultiGraph(Gtrue) +        MGtrue.add_edge('A', 'D', attr_dict={'cost': 16, 'weight': 4}) +        MG=nx.from_pandas_dataframe(self.mdf, 0, 'b', True, nx.MultiGraph()) +        self.assert_equal(MG, MGtrue) +       def test_from_dataframe_multi_attr(self, ): ``` 
comment
Great!   Thanks so much! 
comment
The second commit is not needed (and has a huge diff--irrelevant to this pr). If you want to move this branch back one commit I'll take the PR otherwise I'll just cherry pick the first commit. :)  
comment
Does this change anything of substance in algorithms/matching.py and related tests file? Or is it only spacing/pep8 changes? 
comment
I think the best way to do that is to create a separate function (perhaps starting by copying bellman_ford) rather than providing an option on an existing function. Then we can see how it works, compare them and decide if its better to combine the functions.  I don't work with negative edge weights much so I don't know about interest--but you sounds interested enough. :) Go for it. :) 
comment
Pickle doesn't seem to work with instancemethods. Something about infinitely recursive inspection. Here is a [discussion on stackoverflow](http://stackoverflow.com/questions/1816958/cant-pickle-type-instancemethod-when-using-pythons-multiprocessing-pool-ma) which points to a fix.   The [fix involves creating a simple handler](https://bytes.com/topic/python/answers/552476-why-cant-you-pickle-instancemethods#edit2155350) and registering it using copy_reg. I can't verify that this is a complete fix, but it works for me.  ``` def pm(f):      func_name=f.im_func.__name__      obj=f.im_self      cls=f.im_class      return upm, (func_name, obj, cls)  def upm(fn,obj,cls):     for c in cls.mro():         try:             func=c.__dict__[fn]         except KeyError:             pass         else:             break         return func.__get__(obj,cls) import copy_reg import types copy_reg.pickle(types.MethodType, pm, upm)  # now pickle should work nx.write_gpickle(graph, 'test.gpickle')     ``` 
comment
How can a subgraph of 12 nodes have the node degree in that subgraph be >=12? The largest degree they can have in the subgraph is 11.  Please look at the referred paper for the definition of k-core.   I'm not saying that there is not a bug -- there might be.  But you haven't demonstrated a bug. 
comment
Yes, you are correct about the degree... I was mistaken. Still, the returned 12-core is bigger than your proposed subgraph (24 nodes vs 12 nodes). Is the returned subgraph not a 12-core?   The definition of core number seems to be, for each node, the largest k for which that node is in a k-core. Then to form the returned subgraph, we take all nodes with e.g. core number >= 12. The paper claims that the k-cores nest inside each other as k increases. So this way of choosing nodes seems to make sense from that perspective. What I don't understand is that in your example the returned 12-core subgraph has nodes with degree 11 (within that subgraph). For example, 'NOR' has core number 12, but has degree 11 in the returned subgraph. I'm not an expert in k-cores and it has been years since I even looked at this part of the code. Do you have any perspective on how core numbers relate to the degree of the nodes in the k-core subgraph? 
comment
Your investigation seems correct to me. It makes sense.  Also, the paper only provides the algorithm for undirected graphs with a paragraph describing how to change it to work for directed graphs. The paragraph does not mention changing the decrement size depending on number of edges between (u,v), and the algorithm details only state: `dec(deg[u])` So I see nothing in the paper to contradict what you are saying -- and it is the treatment that makes sense.  Clearly directed graphs are a poor sibling in this implementation of the algorithm (and in the original paper). My initial response was also indicative of neglect of directed situations. :{ Thank you for being persistent and for your detailed investigation!  If you would like to submit a pull request that would be great -- if you prefer that I do the coding I will take it on. I will try to create a test on a small graph as well that shows this issue. 
comment
OK... Thank you @mikk-c I'll take a look at it, try to come up with a test too and send a shout out to you when its ready.  @hagberg I agree that both the `inc(bin[du])` and `dec(deg[u])` are suspect (perhaps all the `inc` and `dec`).  The paper explains that those two terms act to shift the node u to the next lower bin. But if the degree is reduced by 2 maybe it should shift by 2 bins. What if it leapfrogs the current vertex.  I wonder if the author worked through the directed `deg = in+out` case carefully.  
comment
I'll look some more... we could make k_core `NotImplemented` for directed with docs for both making it undirected and pointing to d_core. 
comment
OK... here's a summary of what I found. The [referenced 2003 paper](http://arxiv.org/pdf/cs/0310049v1.pdf) has a [precursor 2002 paper](http://arxiv.org/pdf/cs/0202039.pdf) both by Vladimir Batagelj and Matjaˇz Zaverˇsnik. There they give a more general theoretical discussion which shows that this algorithm should work with any monotonic measure of centrality--not just degree. This includes in+out degree as we are pursuing for directed graphs (and is explicitly discussed there). So we should not rule out directed graphs for k_core--though we may want to implement D-cores too.  Furthermore, digging through the code and algorithm some more I see that we form a neighbor `set` rather than a neighbor `list`. That removes duplicates -- which is precisely what occurs if you use a DiGraph and have edges in both directions. By handling such a neighbor twice, we decrement `degree` and increment `bin` correctly.  I have tested this change on the @mikk-c example graph and it corrects that example. If anyone wants to try it, change `set` to `list` on line 84 of core.py.  I will update the code, docs and tests.  Thank you to @mikk-c !! Longstanding Bug Finder.  :) 
comment
If we are to get rid of add_path we will have to change quite a few tests and doc_strings. It seems that is a common way to make a simple connected graph.  @hagberg what is your suggested replacement for quickly adding a bunch of connected nodes.  Something like  `G.add_edges_from(zip(nlist[:-1], nlist[1:]))`. But that's a little cumbersome.  Another way?  add_star and add_cycle are used less in tests and doc_strings, but they are used. Suggestions for replacement idioms?  We could also move them from the base classes to the function.py module, but I think if we want them gone we should do it in a way that we don't have to fix again later. 
comment
@jfinkels wrote:  > If I learned that path_graph accepted a list of nodes, I might expect cycle_graph, complete_graph, grid_graph, etc. to accept a list of nodes.  The other functions in the generators package could also accept either a list of nodes or a number of nodes. I think a decorator could make this reasonable to implement (though a fair amount of work).   ``` @arg_is_nodes_or_number([0,1]) def grid_2d_graph(m1, m2, ...):     # If m1 and m2 are numbers they become range(m) via the decorator     ... ```  I think it would be a good feature to add.  e.g. `grid_2d_graph(range(4,7), range(15,18))`. There aren't too many functions we'd want this feature for.  ``` empty_graph complete_graph path_graph cycle_graph star_graph wheel_graph grid_2d_graph random_geometric_graph geographical_threshold_graph waxman_graph grid_graph lollipop_graph ``` 
comment
See #1913 for overloading path_graph See #1905 for extending pairwise to cycles. 
comment
Should we name the new function `nx.add_path(G, [2,3,4])` or `nx.make_path(G, [2,3,4])` or something else? 
comment
See #1970  
comment
Try here: https://networkx.github.io/documentation/latest/reference/algorithms.simple_paths.html 
comment
I'm having trouble seeing what has changed. Can we separate the pep8 stuff?  In current release a shallow copy is done with H=Graph(G) and a deep copy with G.copy().  The conversion to directed can be done using DiGraph(G) for shallow and G.to_directed() for deepcopy. It sounds like this PR adds a third option to copy the dict-of-dict without copying the attributes.    Do I understand this correctly? Are there benefits I am missing?  The movement of multigraph specific code to the base graph class worries me some. It is true that it collects all the copy code to one module. But it spreads out the multigraph code. If we later change MultiGraph, we have to remember to go to the Graph class and check all the multigraph code there as well. I'd rather keep multigraph stuff separate (or perhaps combine the four classes into a single class--but it used to be much slower to do that).  But I may not understand the advantages of putting the copy code into a single module. It'd be nice to see it without the pep8 stuff. 
comment
@ysitu Thanks for the pointer to the commit with essential changes. I think I'll avoid the discussion about where to put code. That swings like a pendulum over time and there are advantages to having all the copy code together and advantages to having all the multigraph code together. We can probably handle the code wherever we put it.  Let's look at the interface/functionality first.   For the functionality, this adds a "structure only" feature to the copy/to_directed/to_undirected landscape. It also combines the choice of shallow/deep copy into a keyword argument instead of using different calls (Graph(G) for shallow and G.copy() for deep). I like both. Are there other changes in capability that I'm missing?  
comment
I like the added features here.  Any idea on performance? Especially the part where we create a copy that then gets deep-copied? It would be nice to end up G.copy(data="shallow") being faster than Graph(G) since we know the input is a graph it ought to do better. 
comment
Shallow copies/conversions should be looked at again given #1876 
comment
I think an example demonstrating how to work with a given circuit could go into examples/algorithms.   
comment
here are some questions... feel free to answer "yes, I think this is best". I'm not saying it isn't, I'm just asking.    Does chords() require finding a spanning tree or a minimum spanning tree? Which would be better?    There are many e[:2] and e[:2][::-1] used in cycle_basis_matrix. I think I see why -- to handle multigraphs which have 3-tuples for edges. The code returns the node-tuple or its reverse. Would it be cleaner to split "e" into the nodes, or does that require more convolutions?  You had some tests at the end of your original script. Did all those tests get transferred over to test_cycles?  In the docs for chords() you should specify that the output objects are networkx graphs.  Is that what you want or would it be better to return sets of edges?  Looks good!  Thanks, Dan    
comment
I believe the pull request is just a pointer to your repository. If you push to the branch in your github repository that you made the request from it adds that commit to the pull request automatically.  It is also true that others cannot change the pull request directly.  We can make pull requests to your branch on your fork. Then you would have to merge them to have them show up in the pull request. So that's why I think of it as a pointer to your branch on your fork. But I'm still learning about github....  My question about the e[:2][::-1] notation is this:  Could this be done with something like: try:    u,v=e except ValueError:   e,v,k=e e_nodes=(u,v) e_nodes_reversed=(v,u) and then you wouldn't have to use double subscripts.  But looking at what I just wrote, it seems ugly too. Is there a better way?  Dan  On Sun, Mar 2, 2014 at 4:31 PM, Juan Pablo Carbajal < notifications@github.com> wrote:  > Does chords() require finding a spanning tree or a minimum spanning tree? > Which would be better? > I do not remember exactly why I used minimum spanning tree. Let me check > ... >  > There are many e[:2] and e[:2][::-1] used in cycle_basis_matrix. I think I > see why -- to handle multigraphs which have 3-tuples for edges. The code > returns the node-tuple or its reverse. Would it be cleaner to split "e" > into the nodes, or does that require more convolutions? > I am not sure I understand what you mean. Indeed I am checking the order > of the edge because I assumed directed graphs are allowed. >  > You had some tests at the end of your original script. Did all those tests > get transferred over to test_cycles? > No, some of my test where equivalent to the ones already in place in the > tests for cycle_basis. So I skipped those. I did not have tests for chrods > cause I alsways thought of it as an private function...if it is not useful > for the user one could rename it to _chords_ >  > In the docs for chords() you should specify that the output objects are > networkx graphs. Is that what you want or would it be better to return sets > of edges? > Since the two graphs are then used as networkx graphs I prefer to return > that. I will improve the docs (in my repository? I do not understand how > this thing works...is my repo synchronized with the repo of networkx?) >  > ##  >  > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/1067#issuecomment-36467749 > . 
comment
pip installs it Ok on my machine and the tests pass. 
comment
Can you try uninstalling python-pydot and and install pydotplus instead? pydot says on its webpage that it has been tested with python2.6.  So, while you have it installed, I don't believe it is working.  For python3 we support pydotplus instead.  Also: We may be trying to replace pydot completely with pydotplus (see #1898)  
comment
Yes, pydotplus is typically installed with pip at this point. It would be great to get one of the pydot descendants into debian. I am very surprised that python-pydot even installs on py3k for debian. It doesn't work with py3k so it really shouldn't install with py3k.  NetworkX supports python3, but you have to have a working version of pydot to get the pydot functionality. In python2.6-7 pydot itself provided that functionality. But in python3 there are a couple of descendants of pydot that are trying to support pydot features. One of them is pydotplus. So our code tries to find a working version of pydot by importing pydot, pydotplus or pydot-ng. If any is available we assume it works. If none of the versions import then we disable that functionality in NetworkX.   Is there a better way to support py3k?   NetworkX does not "depend" on pydot. It add features if you have a working version of pydot.  To my mind the debian python-pydot distribution is broken if it installs to py3k. Should we be checking that pydot actually works, or is it enough to assume that if it imports we have a working version?  
comment
@sandrotosi,  I didn't realize you are the packager for debian's python-pydot and python3-pydot. Has there been any development on the underlying code? The error messages you report above could in some sense be considered bug reports for python-pydot.   It would be great to have python3-pydot actually work...  we're considering moving to pydotplus instead. Do you have any news as to which version of python3-compatbile pydot works best?  Thanks! 
comment
As far as I can tell we've got two functions called graphviz_layout. One is in nx_pydot.py and one is in nx_agraph.py (which relies on pygraphviz). It's definitely a bug.  Calling graphviz_layout from within nx_agraph.py is trying to run graphviz_layout from nx_pydot.py. 
comment
Do you get these errors on v1.10?  Because I don't think the fundamentals have changed for either pygraphviz or pydot. You should have had trouble using pydot in python3 for v1.10.  And if you decided not to have pydot installed then this bug would have shown up.  Does v1.10 work on debian stable? 
comment
@hagberg do you have a suggestion on how to resolve calls to graphviz_layout?   It looks like this bug only shows up when pygraphviz is installed but pydot is not. In v1.9 and before it should show up when someone has pydot installed but not pygraphviz.  The order of the imports is switched, but the problem should be there either way. Luckily its a low probability event--maybe nobody has noticed before.  Maybe we should create a single `graphviz_layout` that tries `pygraphviz_layout` and if that doesn't work it tries `pydot_layout`? 
comment
@sandrotosi,  There is definitely a bug, and we'll fix it for v1.11.  Thanks for digging into it with us. I'm just surprised that it didn't get exposed before. But it wouldn't show up unless the pygraphviz was installed and pydot was not. Perhaps the testing done for v1.10 was not as thorough. Good job. :) 
comment
well, it should have showed up in python3 for v1.10...  On Sat, Jan 9, 2016 at 10:04 PM, Sandro Tosi notifications@github.com wrote:  > it showed up just know because just now I tried to remove pydot and see > what happens :) >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1882#issuecomment-170305389. 
comment
@sandrotosi,  We merged PR #1930 which should take care of the import difficulties.  Thanks for helping us find that.  Can you test again? Do you need us to do anything for that? Dan 
comment
That makes sense to me. I have an old file with a checklist for releasing. Does this look up-too-date? Should it be put in the repo somewhere?  # Networkx Release HOWTO  ## Pre-release - Open pull request with release.py modifications.  - Assign PR to milestone. This will keep milestone open until that PR is merged - Resolve open issues for milestone at https://github.com/networkx/networkx - Update Readme.txt, News.txt, Credits.txt - Check tests and fix any that are failing  ## Release - Merge release.py update PR (on github.com) - git clone https://github.com/networkx/networkx.git - Create tag for release   - git tag -a networkx-x.x -m "networkx-x.x release"   - git push origin networkx-x.x - Build documentation   - cd doc   - make clean; make gh-pages networkx-x.x   - cd gh-pages; git push   - go to readthedocs.org and under "Setup" / "Versions"      set the release tag as the one to be built - Upload to pypy   - python setup.py register   - python setup.py sdist --formats=gztar,zip upload   - python2.7 setup_egg.py bdist_egg upload   - python3.3 setup_egg.py bdist_egg upload   - python3.4 setup_egg.py bdist_egg upload   - python3.5 setup_egg.py bdist_egg upload - Update webpage   - git clone https://github.com/networkx/networkx-website.git   - Update _templates/sidebar_versions.html to latest release number   - Change documentation links (and add) to point to correct versions   - make gh-pages   - cd gh-pages; git push  ## Post release - Send email to networkx-discuss list - change release.py to set up next version with dev=True on master 
comment
Unless another import issue is going on (which it isn't :)  it looks like pydot is fine and pygraphviz is causing these errors. So, yes it is probably important to have the specific version of pygraphviz to be able to find what's wrong. It was Ok on the Travis-CI with pygraphviz 1.3.1. Hopefully that's what we need. 
comment
Good news! -- a testing failure instead of an algorithm failure. :) 
comment
I'm for releasing... :) 
comment
A maintainer needs to tell readthedocs to rebuild the stable branch. That should automatically pick up the latest tagged repo and build everything.  I've done that and its looks like its working. But someone who isn't a maintainer should check it. There are two versions available "latest" and "stable". Currently the latest version is the default, but we could change that. We can also turn on versions of older docs if that's what we want.   http://networkx.readthedocs.com/en/latest http://networkx.readthedocs.com/en/stable http://networkx.readthedocs.com/           goes to the default  Could turn on: http://networkx.readthedocs.com/en/networkx-v1.10 
comment
OK..  I've turned on the 4 versions @hagberg suggests and made the default be the "stable" version as per @mriduls suggestion.  Looking at other projects on RTD there is a lot of variability in how this is done. The label "latest" seems to be the default in the ones I have checked -- but some groups (e.g. sphinx) change "latest" to be the same as stable using "master" as the label for the dev version. Sphinx provides previous versions by number, but numpy does not.  So the setup we have chosen is reasonable and we probably should revisit in a year or two as use of the site evolves. 
comment
I think you can use `assert_equals(G.edge, self.k3adj)` and the tests should work for all classes. (maybe `G.adj` instead of `G.edge` though they should be equal)  
comment
This isn't actually testing the `without_data` feature. A deepcopy would pass this test. Think about using the helper functions `is_shallow_copy` and `is_deep_copy`. It should probably also use the self.k3edge and self.k3nodes created in `SetUp`.  Also keep in mind that the resulting dict for G[u][v] must be the same object as the dict for G[v][u], so the constructions to compare to aren't quite right.  This brings up a bigger question though:  When would you want to ignore the current edge attributes, yet still create new dicts for them? It saves memory (and probably cpu time) to use a shallow copy which simply points to the already existing attribute dicts in the original graph. We've got (at least) three types of graph_copy possible:  a deepcopy, a new graph structure with same attribute dicts (shallow copy), and a new graph structure with new attribute dicts. In the existing codebase `G.copy()` provides a deepcopy and `nx.Graph(G)` provides a shallow copy. At some point when we last discussed this we decided not to implement the "new graph with new attribute dicts" version because it isn't a common need (and isn't efficient). It's use-case is only when you've got existing data attributes that you want to throw out and start over fresh with new data attributes.  Let's figure out what we want here before we jump in too far. I think #1876 actually takes longer and uses more memory than a shallow copy... when its intention (in my understanding) is to save time and memory. Perhaps we could solve these issues (#1126, #1152, #1164, #1876, #1916, #1917) by figuring out what it is that we actually need in terms of:  1) Do we copy existing data attributes to the new graph? 2) Do we provide independent data attribute dicts in the new graph?  Yes/Yes -> deepcopy     G.copy()  Yes/No -> shallow copy      `nx.Graph(G)`  No/No   -> doesn't matter but shallow copy is most efficient in terms of memory and cpu.  No/Yes   ->  not provided.... (and not needed?)  The fourth combination is to ignore existing data attr and create new data attribute dicts for the new graph. Do we need that? I don't think we do... 
comment
Yes, that provides the fourth combination.  Should we change `.copy()` to provide these 3 options? They are not documented well.  We could put your recipe for new datadicts in the copy() docstring and only provide `G.copy(deepdata=True/False)` in the method.  Or similar to #1164 we could use:  `G.copy(with_data=True/'shallow'/False)` 
comment
I don't recall a discussion of `G.deepcopy()`.  I think that would tend to make `G.copy()` used more than `G.deepcopy()` (similarly we could create two methods `G.copy()` and `G.shallow_copy()` if we want more people to use the deep version.)  I think people will find the `copy()` more easily. Maybe its better to have it in one place with a good docstring that lays it all out.  As far as the code goes it sounds like we want:  ``` def copy(self, deepcopy=True):     if deepcopy is True:         return deepcopy(self)     return self.__class__(self) ```  With the docstring providing the idiom:  ``` H = nx.Graph() H.add_nodes_from(G) H.add_edges_from(G.edges()) ``` 
comment
@jfinkels do you want to take this on or should I open a different PR? 
comment
That was my intent in #1957.  Reopen if there are other issues. 
comment
You'll need to change the `__init__.py` files in both affected directories to load the appropriate modules due to the name change.  
comment
:)  Perseverance!!  You can clan up the commit history using a git feature called "squash"ing the commits. Basically you use `git rebase -i origin master` and an editor lists all your commits--you can choose which to squash by switching the first word from "pick" to "squash".  Then when you save the editor reopens and you can combine/edit the commit messages.  When you save the messages, your history has been rewritten! Don't overuse this feature. :)  Since tis rewrites the history you have to use `git push --force` to update the branch that the PR is on.  Here's a post [describing the process](http://stackoverflow.com/questions/14534397/squash-all-my-commits-into-one-for-github-pull-request). As always it's possible to create a small test repo to play it with before actually doing it, but so long as you don't push you can mess it all up and then re-clone it.  
comment
I'm going to merge this. Its a step in the right direction. If someone wants to put in installation instructions or improve the Download section open another PR. 
comment
This looks much better... Note: Both the old code and this code treat all edges as undirected.  [(0,1)] will equate with [(1,0)]  Note: Duplicates in the edges1 and edges2 containers are not identified. Maybe they should be?  The node routine looks fine. A 2-tuple node like (1,3) might get stored as `{1:3}` instead of `{(1,3):None}`, but it will happen in both lists so they still equate when they should.  For the edges_equal routine:   [(0,1), (1,0)] will test equal to [(0,1)]. To fix that, you should test how many elements are in edges1 and edges2 rather than d1 and d2. Perhaps use enumerate in the loops, or compare lists at the start.  Also, it might be faster to compare the d1 and d2 dicts using `assert_equal(d1, d2)`. Is there an advantage to looping over k? 
comment
Realized today that duplicates are the same idea as multiedges.   Also I agree with you that we can't test (u,v) vs (v,u) without specifying directed/undirected.  Perhaps we are simply coming back to 4-types of edges.  (un)directed and (multi)edge.  We could provide 4 assert_<type>_edges_equal... :}  I think what you've got is also good. 
comment
Can you find the connected components (and thus their size N_k) and then use len(G) and N_k? Probably cheaper than recomputing has_path many times. 
comment
Yes, it's true.... getting the arithmetic right is necessary for counting undirected pairs.    :) 
comment
Try `del G` after you are done computing `nx.connected_components`. That will clear out memory for your next graph. 
comment
I think we can close this issue then..... 
comment
Fixed in #1961 
comment
I've got it working on my fork, but thought it strange to createa hook from the main repo to my personal account on appveyor.  Is there a way to get a globally viewable account on appveyor so we can all look at the build results?  I'll look into it...  On Sat, Jan 16, 2016 at 5:59 AM, Himanshu Mishra notifications@github.com wrote:  > We have the code for appveyor already merged (#1699 > https://github.com/networkx/networkx/pull/1699), but it's not being > used right now because it hasn't been activated yet. _One of the owners_ > need to do look into this and set up a hook over appveyor > https://www.appveyor.com/. >  > Ping @hagberg https://github.com/hagberg @dschult > https://github.com/dschult >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1650#issuecomment-172182817. 
comment
It looks like appveyor is set up as a webhook service for networkx. It lists the payload URL as https://ci.appveyor.com/api/github/webhook?id=y09ce4gwjxamp5m4  I can connect to it from my appveyor account and start a build. But I'm not sure that's helpful for anyone else and I can't find a nice way to have it report back to github the way travis-ci does.  Any help would be appreciated. 
comment
@orkohunter, I set up the hook for appveyor and it is working. The results show up on the PR page like for travis-ci (see #1950). But the ["details" link](https://ci.appveyor.com/project/dschult/networkx-pqott/build/1.0.10) goes to my personal appveyor account--can you see it?  If so, maybe this is enough for now and we can move it to a more communal account on appveyor later if needed. 
comment
`On the graph, number of nodes which degree is 4 = 1, 2, 3, 4, 5 : which isn't true logically)`  This is a degree rank plot.  That is the rank order of the degree values. So, the rank 1 degree is 4. So is the rank 2, 3, 4 and 5.  The the 6th ranked degree value is 3.  That's what is actually plotted. Your code plots the histogram. We should change the name... You could also add your code as an example--then we'd have a histogram too. 
comment
I think the CI errors are not related to this PR. I'll look into that.  Nice catch on the `::` to `:`  Could you move the ndict line inside the `if` structure so it doesn't get executed when `data=False`?  Could you make the part of the doc_string which explains the parameter `data` match that for `edges()`?  Since their treatment is the same, it is probably good if the docstring wording matches where reasonable.  
comment
It wasn't working in the docs before... perhaps because the blank line was not there. The code was formatted as a single paragraph. In any case a single colon and blank line puts the examples in a [nice box in sphinx](http://networkx.readthedocs.org/en/latest/reference/generated/networkx.Graph.nodes.html).  I thought we were all over to sphinx for docs.  Is numpydoc used somewhere? 
comment
I would suggest adding a test to tests/test_cuts.py that checks for correct output in the problematic case.  While you are there, change the test at line 244 to use `G = nx.path_graph(5)` and add a comment something like `# auxiliary should be a mapping not a graph`.  This will fix the travis-CI fails and you can move your new code back to the top of the function (don't want to compute the auxiliary if we don't need it.) 
comment
I might be wrong on the definition here, but I think the edge_cut routines should not change... If two nodes are directly connected then that edge must be in the edge_cut. And the edge_cut does exist. The problem arises for node_cuts because you can't remove the source and target nodes, so an edge between them can't be cut.  So I think you'll have to undo some of your changes. 
comment
OK... now we're getting to the silly details at the end... Could you adjust spacing in the new code to conform to PEP8 standards and then "squash your commits".   It looks like spacing is the only PEP8 issue... need spaces around `=`, `==` and after each comma. So `G.has_edge(u,v)` becomes `G.has_edge(u, v)`. Lots of commas...:) If that ends up making the line longer than 80 characters you should split the line onto the next line, or create a variable to hold an intermediate result, or in your case, simplifying the example by removing an edge.   To [squash your commits](https://github.com/ginatrapani/todo.txt-android/wiki/Squash-All-Commits-Related-to-a-Single-Issue-into-a-Single-Commit), use `git rebase -i HEAD~6`, change the `pick` commands for all commits after the first so they are squashed to the commit above them, save, edit the resulting commit messages as appropriate and save again. In your commit messages the first line's first 50 chars become the title of the commit. Skip a line and then put any details that you want in the description of the commit.  There may be better help on squashing if you search for it. 
comment
I think you should be able to use git via the `https` remote addresses. If you can see it on a browser you should be able to get git to interact with it. It'll be worth spending some time getting this to work. I don't have a lot of experience with it but @OrkoHunter can help. The theory is to `git clone` and `git remote add` using the https://github.com/networkx/networkx.git addresses.   
comment
Thanks! 
comment
#1895 added the docs 
comment
This looks good on first pass--though I haven't delved into it in detail yet.  I guess that the doc_strings need to be updated. Also might be good to add a comment line in _dijkstra just before returning to remind us developers that pred and paths don't need to be returned because they came from the caller.  
comment
Actually the single calculation for both ints and floats breaks the tests here... Looks like the tests were imposing that we return `int` when `weight is None` and `float` when `weight is not None`.  The `from future` stuff isn't needed because integer sums when `weight is None` always divide an even number by 2.  So we can remove that.  But.... now we need to decide whether to change the API as defined by the tests. Should `G.size()` return an int? Should `G.size(weight='weight')` always return a float?  Thoughts? 
comment
The original implementation (way back) allowed size to sum anything. Let me look through the logs to see why it was changed to be int or float.  The int/float difference for unweighted size is a separate issue. Clearly the number of edges is an integer. But python3 will return a float because we use division (not tested). The code is smoother if we let python choose to return a float.   Question: do we care if the size returns a float even though the value returned will be an integer? 
comment
It is more than using integer division in the correct place. For weighted edges, `sum(weights) / 2` in python 2 can create ints OR floats whereas in python 3 we always get floats. The python 2 case that gives ints occurs if all weights are ints.  Allowing sums of arbitrary weight objects requires that the result of their sum allow division by 2.  So weights that are lists or strings won't work. Your complex value case would allow division, but does it provide anything sensible to sum complex edge weights?  I'm fine merging this with: 1) `from __future__ import division` back in  2) splitting based on whether `weight is None` and using // 2 for the unweighted case.  Then we get int and float in the same cases we had before and we allow custom weights that can sum with the result of the sum divisible by 2 (which I think was your point).  Is this what you were thinking @jfinkels?   
comment
Revisit in light of #1876 
comment
I'm going to close this and refer to #1164 for this feature. 
comment
Should we add a line to the client webchat.freenode.net for those who don't know/want to install a client? 
comment
Regarding @hagberg first comment about using the term adjacency list: 1a) We use the term `adjacency list` and `adjlist` in the docs and we store the adjacency info in a dict-like structure created by `adjlist_dict_factory`. The term `adjacency list` can be replaced with `adjacency information` easily. The term `adjlist` could be changed to `adjlist_dict`.  But: Would it be better to change the name from `adjlist_dict_factory` to `adj_dict_factory`? It's not backward compatible, but adjlist_dict_factory has only been around for about a year. I'm for leaving it as adjlist_dict, but am willing to change if desired.  1b) I think the section heading `subclasses` is appropriate here. Do I understand that terminology wrong? The examples make a new class which inherits from the original class. 
comment
As far as I know we are done here... @MridulS ? 
comment
I'm not sure what your question is, but the variable `err` equals a sum over all nodes, so it will increase as N increases. Normalizing the tolerance by N seems reasonable. 
comment
We've never had add_clique before...  I think @hagberg is pointing out that it is not so clear what it would actually do. Would it require an iterable of nodes to put into the clique? etc... I think there is probably a good way to approach this.... somehow...  but I do remember @hagberg eventually advocating for removing add_star/path/etc :) 
comment
Sorry for the late response... I'd be happy to be a mentor this summer. I still need to finalize summer timing, but I'm hopeful that I'll have more time for NetworkX than in the past few years.  
comment
30 lines shorter does not correspond to simpler or clearer. Besides, how many lines of six are there that we are effectively adding?  One power of python is its indentation structure which tends to use more lines, but makes code more readable.  On Sat, Aug 8, 2015 at 1:16 PM, Neil notifications@github.com wrote:  > I have to disagree that the changes in #1709 > https://github.com/networkx/networkx/pull/1709 are "no simpler or > clearer". At the very least they are 30 lines fewer. >  > I agree with you that we should "be thinking in Python3", which means that > things like basestring should not appear in the code. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1702#issuecomment-129037570. 
comment
The intended change for v1.10 is to center at (0,0) and scale so that the box is [-scale, scale] rather than [0, scale]. Looks like there are two issues here:    1) the docstring didn't get updated.    2) the scaling routine only looks at the maximum coordinate, not the max of the absolute value.  That is a bug.  Essentially the box is now from [-something, scale].  Thanks for reporting this with good, helpful examples. 
comment
There are other bugs/issues I am finding with the layout routines in v1.10.  (see #1759) I haven't come up with any good work-arounds yet. I'm proposing we make a v1.10.1 release.  Thanks for pointing this out! 
comment
@AgostinoSturaro -- Can you take a look at PR #1760 and see whether it solves the problems for you? It only changes the one file layout.py   
comment
Yes, v1.11 resolves this issue. 
comment
It seems to me this is a case where we don't need six or lib2to3. lib2to3 is used to get rid of u before unicode strings (for python3.2) and to label exceptions that don't really need to be labeled.  It is especially easy to remove these dependencies if we stop supporting python3.2. 
comment
The log information isn't very helpful to know where it is getting this address from.  It is the old address for networkx and it currently redirects to our github webpage https://networkx.github.io  I have looked through all the information at https://pypi.python.org/pypi/networkx and don't see any links to networkx.lanl.gov. Any information you can give about  why it is trying to connect there would be helpful. 
comment
This question might be better placed on the [networkx-discuss email list](https://groups.google.com/forum/#!forum/networkx-discuss) to get a wider audience.  The green `source` link on the page you cite shows you the code for diameter and friends. But I suspect you will want to compute using [`all_pairs_shortest_path`](http://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.shortest_paths.unweighted.all_pairs_shortest_path.html#networkx.algorithms.shortest_paths.unweighted.all_pairs_shortest_path)  You would have to compute the maximum path length using your own code, but the path would be there when you find the maximum. 
comment
Also, `len(g)` is the best way to calculate how many nodes are in `g`. `len(list(g.nodes()))` is not the best way--though it does explain the iterator nature of g.nodes(). 
comment
Strange... the degree_histogram example is working on reathedocs.org and on my machine without an error. 
comment
I do get that error if I import networkx 1.8 instead of the latest.   Are you sure you got the dev version loading? 
comment
> Also, I was trying to run `make gh-pages` from within the `doc` directory > and I had jumped through all of those hoops to make sure I was up to date > with the `upstream/master` branch, so I don't know how I could have even > had access to `nx` version 1.8. >  > Sidenote: I wasn't running the `make gh-pages` command after importing but > just inside the directory itself in the command line. As far as I knew, > `make` is usually run from the command line and you import things into a > python interpreter and therefore there's no meaning to importing to the > command line. But I readily acknowledge that I could be totally mistaken. > What set of commands did you use to build the docs? Why did you need to > import networkx at all to build them? I might be doing this incorrectly, > and would love to know how to do it better (or at least to know if there > are multiple ways available for building docs). Thanks! >  > Yes, the make command is run from the command line. It then runs commands > like sphinx to create the docs based on files in the docs directory as well > as information from the python objects created when sphinx runs python and > imports networkx.  Given that your "by hand" import of networkx provides the development version, it might be that you have some old files in your doc directory. To try a fresh build, you can cd to the doc directory and try  ``` python -c "import networkx;print networkx.__version__" make clean make gh-pages ```  hope this helps... Dan 
comment
Everything looks OK at [readthedocs](https://networkx.readthedocs.org/en/latest/reference/generated/networkx.DiGraph.edges.html#networkx.DiGraph.edges)  Not that it helps you much except to know that it can work.  For readthedocs we use the sphinx 1.3.1 from github and NOT the sphinx 1.3.1 from pip.  The newer version has a few commits that fix autosummary, and while I don't know whether they do anything for links to course codes, maybe there is a fix in those commits there too.  So that's another potential source of trouble--the version of sphinx should be the stable release 1.3.1 from github and not the 1.3.1 from pip.  Maybe it'd be good to revisit the goal here... I understand the source link is not working on latest or on the [v1.10 docs](https://networkx.github.io/documentation/latest/reference/generated/networkx.DiGraph.edges.html) at networkx.github.io.  At networkx.readthedocs.org I believe they are working for the [latest docs](https://networkx.readthedocs.org/en/latest/reference/generated/networkx.DiGraph.edges.html) but not for the [v1.10 docs](https://networkx.readthedocs.org/en/networkx-1.10/reference/generated/networkx.DiGraph.edges.html).  Would you agree? Do you see other problems that need fixing regarding this issue (other than some documentation on how to create the documentation. :) 
comment
The viewcode extension to sphinx puts links to the source code in the documentation.  Unfortunately the links are only added to methods that are not inherited from another class. So, when `DiGraph` inherits a method from `Graph`, that method has no link to the source from the `DiGraph` documentation.  I've created an issue at sphinx-dox/sphinx#2196 but in the meantime I don't see a fix for this issue.  
comment
Thanks! 
comment
I'm not worried about future divisions and the range(-n,0) gives only the last n values.  But I am worried that there are other places in the code where we compare to 0 using ==. This PR's changes will allow the procedure to continue but will it give correct results if it can't compare to 0 precisely?  Also, do you really need float? or could you multiply all capacities by 10^9 and round to integers? My understanding is that these algorithms do strange things for floats, but I haven't checked this particular algorithm.  
comment
I think the only solution is to use integers.  Maybe the docs should explain how to handle it....  multiply all weights by 1e9 and round   seems like a good way to proceed. Thanks for digging into this!  On Tue, Dec 8, 2015 at 1:37 PM, mortonjt notifications@github.com wrote:  > The only solution to this that I can think of is to log scale all of the > intermediate computations and perform approximate log addition. Any > thoughts on this? >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1848#issuecomment-162974491. 
comment
I don't know the algorithm well but it looks like that could be a problem.  The doc_string says:  ``` This algorithm is not guaranteed to work if edge weights are floating point numbers (overflows and roundoff errors can cause problems). ``` 
comment
I agree. And I can't see why anyone would need the with_nodes=False option. Not sure why we put it in there.  The documentation should also "see also" the empty_graph function.  So:  - remove with_nodes argument - add a with_data argument   ??default value??  I would guess "True"... other opinions? - add a "see also" in the doc_string - update tests accordingly (There is only one test function now besides the test of the doc_string.) 
comment
Yes, please. :) 
comment
Which do you think is better. I like the simplicity of with_data, but the flexibility is also nice. How likely is it for someone to want node data without graph data?   I'm open to suggestion, but have slight preference for `with_data`  
comment
Can you please test changing a node attribute in G and seeing that it doesn't change the attribute in self.G? That's the most common bug I've run into with copying data attributes--that the containers end up being the same rather than copies. As currently written I suspect that the G.graph and self.G.graph are actually pointing to the same dict.  It looks like the nodes get added twice...  but I think there is a way to add nodes with data using `G.nodes()`  ``` H.add_nodes_from(G.nodes(data = with_data)) if with_data:     H.graph.update(G.graph) ``` 
comment
Did you try a version with the methods I suggested above? It avoids creating any lists or dicts explicitly (so might be faster) and it avoids re-assigning the H.graph attribute by updating the existing one. These suggestions might be worse... but I think you should at least try them.  ``` def create_empty_copy3(G, with_data= True):     H = G.__class__()     H.add_nodes_from(G.nodes(data = with_data))     if with_data:         H.graph.update(G.graph)     return H ``` 
comment
That example works with the latest dev version.  It also works with the v1.10 stable version.  But between those two the behavior of G.nodes() changed from returning a list to returning an iterator and random_sample was broken for a short time.  I suspect you got a version from github in summer that hasn't been updated to the latest dev. Can you check that? If so, update to latest master and try again.  
comment
This is looking good. Two minor things.... We should remove the comment line about connecting the middle node.  Could we add a one line (or two) definition of a ring of cliques placed after the blank line at the top of the doc_string.  I mean: 1st line description, then blank line, then definition, then blank line, then parameters. [hope this is clear-if not ask]  Thanks! 
comment
I don't think this function makes any sense with clique size 1. I prefer the NetworkXError in that case as is currently coded. Then the user can deal with it if they really want to generate a path. So I think you should not change the inner loop.  The force directed layout algorithms will certainly treat the connected nodes differently from the nodes in the clique not connected outside the clique. But it will not care whether the nodes are numbered 1 and 2 or 1 and 10. If all goes well, the nodes connected outside the clique will end up on opposite sides of the clique however they are labeled. If I'm not correct help me to understand better.  Actually you can try drawing it with NetworkX. The default layout is force directed.  How good is it?  
comment
The code as it is looks good to me.  Could you change the NetworkXError text for cliques size to say "at least two nodes" instead of "at least one node". Then I think it is ready to merge. 
comment
The docs at readthedocs.org are not creating useful links. It's there, just basically impossible to find: https://networkx.readthedocs.org/en/latest/reference/generated/networkx.algorithms.isomorphism.categorical_multiedge_match.html  There are also older (but still mostly the same) docs for v1.9.1 at: http://networkx.github.io/documentation/networkx-1.9.1/ 
comment
Fixed in #1820 and #1821 
comment
This is a strange github quirk-- maybe a bug.  When I look at the diff for your PR I see old code as the supposed code from networkx's master branch.  It should be:  ```     s = sum(dict(self.degree(weight=weight)).values()) / 2     if weight is None:         return int(s)     else:         return float(s) ```  I think your idea for getting rid of the coercion to float or int is fine, but your code won't work with the latest version of Graph.degree() because  it has been changed to an iterator.   Perhaps you could start from a fresh branch off the latest master. If you prefer, I can make your submitted changes in a different PR. 
comment
See #1798  
comment
Thank you!!  ...  I'll get that fixed.   Looks like it was changed in 1.8 without changing documentation. oops.   `Each cycle is represented by a list of nodes along the cycle.` 
comment
Thanks! 
comment
Fixed in #1819  
comment
Looks like this fixes errors where the intended node is a tuple containing an unhashable. Thanks! 
comment
Do we really need to be doing this? Can't we spend our time on some new graph algorithms?  On Tue, Aug 4, 2015 at 2:02 PM, jfinkels notifications@github.com wrote:  > There are several functions whose first line rejects empty graphs. This > can be abstracted and placed in a decorator, for example, > not_implemented_for('null'). The exception raised would be > NetworkXPointlessConcept. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1706. 
comment
https://networkx.github.io/documentation/latest/reference/algorithms.shortest_paths.html 
comment
I've almost got this working. The main trouble is that readthedocs sphinx 1.3 introduced an error so that autosummary won't work correctly. Sphinx fixed it in their development version but readthedocs won't change until a new release of sphinx comes out.  Sphinx is close to a new release, but I'm not sure what that means.  https://github.com/rtfd/readthedocs.org/issues/1290 https://github.com/sphinx-doc/sphinx/issues/1822  I've managed to get rid of all the other errors, and if I put the (one-line) fix into my local sphinx, the errors go away.  I'll try to put the changes into a PR   
comment
I'm going to close this issue since we have readthedocs turned on now. But I'm going to open another issue about what to do until readthedocs updates to a version of sphinx that allows autosummary to work. 
comment
It looks like c is used in the tuple that gets pushed onto the heap. So presumably the position in the heap will be different if we remove the `c` variable. Currently we push 3-tuples (uv-dist, next(c), u). If we push (uv-dist, u), and uv-dist is the same for two nodes, then the heap comparison will depend on comparison of nodes `u`.  We want to allow nodes that can't be compared.  I think this is the reason we include the value c here.  Maybe a comment in the code to that effect would be helpful.  
comment
There's no separation here between pip install and conda install. If we add this information we need some structure to make it easy to find/follow. 
comment
There are currently two headings: `quick install` and `installing from source`. Maybe they should be changed: `Installing with pip` then `Installing with conda` then `Installing from source`.  I'm not sure what you mean by take inspiration from...    They seem to have very different approaches. Only the first uses INSTALL.rst.txt, but maybe I'm missing something.  Anyway, this addition is good -- just need some headings/structure. 
comment
Perhaps this PR should add more content to INSTALL.txt  (also note that the file is called INSTALL.txt, not INSTALLATION.txt) You can make changes to the branch of your fork and it will update the PR.  
comment
Yes, there would be support for such a PR. Check out the suggestion from @hagberg...  
comment
see #1778 
comment
These two methods were designed for different activities. The successors method was intended to give information about nodes -- specifically, which nodes are successors. The edges method is really about edges.  So I feel the edges method should return a multi-edge as multiple copies of a 2-tuple. But successors is asking for the nodes, so we should not repeat that information.  If we make them return the same information, we might as well have one method.  I see them as having different uses. 
comment
I believe this issue is now fixed with #1756 merged.   Anything else here? 
comment
Here's a draft(untested) of some code that could create a graph from a neighbors function and a starting state.  :)  G=nx.Graph() states= {initial_state}   # or a set of starting states seen={} while states:     node = states.pop()     if node in seen: continue     for s in neighbors(node):         G.add_edge(node, s)         states.add(s)     seen[node] = 1  On Mon, Aug 24, 2015 at 12:15 PM, Tim Vergenz notifications@github.com wrote:  > So for example a starting state could be a dictionary of locations to > block types. (The puzzles are small, on the order of there only existing a > few thousand states total.) >  > Pseudocode for the neighbor function could then look something like: >  > def neighbors(state): >   for (pos, block) in state if moveable(block): >     for direction in which block can be moved: >       # yield state with block moved in direction >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1745#issuecomment-134275082. 
comment
This looks ready to merge (leaving #1736 to deal with self-loops). Any other issues I don't know about? 
comment
Sorry this took so long to get to. It looks good -- Thank you...  Could you update the "Note" at line 87 to reflect the new use of 'weight'? Then I think it is ready to go. 
comment
Probably a better place (more viewers with broader perspective) is the NetworkX-discuss https://groups.google.com/forum/#!forum/networkx-discuss email list. This tends to be just developers.  That said, I imagine many people are interested in mixing times. Enjoy Python and NetworkX. If you come up with a useful function/module this would be a good place to propose it and get feedback on the code. :)  On Tue, Aug 25, 2015 at 2:15 PM, Sebastian Bordt notifications@github.com wrote:  > Hi guys, > I'm writing a masters thesis concerned with the mixing times of Markov > chains. My Markov chains often result from simple- or non-backtracking > random walks on simple (random) graphs. So I started writing python code > that allows to compute mixing times using your networkx package to > represent my graphs (I am a newbie to python). With some additional effort, > I could write a Python package having methods like >  > srw_mixing_time(G, node) > plot_nbrw_mixing(G, node) >  > Before doing this I'd like to know if anyone here is interested in the > mixing times of random walks on graphs? Hope this is the right place to ask > the question :) >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1746. 
comment
Yes, the bug was fixed...   but @kshitij10496 was offering to add functionality to allow the user to choose which attribute takes precedence.  With regard to that, couldn't the user pick by specifying the order of the inputs? `compose(G,H)` vs `compose(H,G)`?  Are you suggesting something more than that? 
comment
Yes, I will close this issue now...   It looks like #1714 fixed the bug that caused attributes in G to be dropped. It works as the python dict update method: Effectively that is a union of the keys with duplicate keys given the value from the H graph.  
comment
Thanks! 
comment
It is stated in the documentation that None should not be used as a node. But currently there is no check when you add a node that it is not None. That was a choice we made, but should it be revisited? Do you have a use case for None as a node? Dan  On Tue, Mar 3, 2015 at 5:56 AM, abeld notifications@github.com wrote:  > As per the included shell example, None is accepted as a node identifier, > but then calling degree() without an argument will return a number instead > of a dictionary. This breaks code that assumes that degree() will return a > dict in this case. >  > The bug is due to the the check at the beginning of degree(): "if nbunch > in self: # return a single node" see: > https://github.com/networkx/networkx/blob/master/networkx/classes/graph.py#L1351 > since in this case 'if None in g' will return True, since there is such a > node in g. >  > I am not sure what is the right fix -- even an explicit "if nbunch is > None" won't be a full solution, since in that case "for n in g.nodes(): > g.degree(n)" would break. > Possibly None has to be prohibited to be used as a node? But that might > also break various codes that use networkx. >  > Python 2.7.6 (default, Mar 22 2014, 22:59:56) > [GCC 4.8.2] on linux2 > Type "help", "copyright", "credits" or "license" for more information. >  > > > > import networkx > > > > g = networkx.Graph() > > > > g.add_node(None) > > > > g.degree() > > > > 0 > > > > gg = networkx.Graph() > > > > gg.degree() > > > > {} > > > > networkx.**version** > > > > '1.9.1' >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1381. 
comment
Renaming the `_iter` functions along with what has already been committed is already a fairly large set of changes.  So I think limiting 2.0 to that is a good way forward. We don't want to wait too long between releases anyway.   
comment
I think we have more changes already for a new release than we usually do. And presumably there will be many more between now and Sept. Maybe its a good idea to get these changes released before we start messing with API issues.   
comment
That sounds good to me. 
comment
Is there an difference/advantage of G.adjacency().items() over G.adj.items()?  
comment
Am I right that exposing the internal data structure and providing an interface that returns it are the same thing? Now if G.adjacency() could provide an interface that gave an immutable (maybe view?) version of G.adj then I see nice advantages.  As for iterators vs iterable views:  Right now the changes are creating iterators. For python 3 we create an iterator of a view like:  ``` iter(self.node.items())```  Is there an easy way to create views of dicts in python 2.7?  If so we could think about shifting to returning views instead of iterators.  As part of this dicsussion, can you lay out what the trade-offs are for views vs iterators?​ ``` 
comment
Is there a way to get viewkeys() without using version speciifc syntax?  We currently use e.g. `iter(self.adj.items())` to get an iterator. Is there a version agnostic way to get a view?  
comment
Using pypy more than Python 3 at this point, right?      (ok... almost always python2.7 here...:)  I think I prefer an iterator for G.edges() but perhaps I don't really grok the advantage of a view. For G.edges() wouldn't a view be a consistently updated no memory edgelist? Each time it is accessed we must compute all the edges?  What is a view of graph edges? 
comment
View vs iterator:   skip the blogs/stackoverflow, etc and go straight to the docs and python introspection.  A view is a low-memory object that provides three methods: `__iter__`   -- returns the iterator we are comparing views with `__len__`  -- returns the number of objects in the view `__contains__`   -- allows testing for membership in the view  dict.keys() is "set-like" and provides `__and__`, `__or__`, `__xor__`, `__sub__` But this is not the case for dict.values() or dict.items().  So:  what about G.edges() and G.degree() and G.adjacency()? Should they return "views" that provide `__len__` and `__contains__`?  Default input:   G.edges() `__len__` would return the number of edges in G `__contains__` looks up a 2-tuple edge and/or checks 3-tuple for multigraph.  nbunch input:   G.edges([2,3]) The view object would store both set(nbunch) and G. `__contains__` could look up the edge and then check that one node is in nbunch. `__len__` could compute the edges and count them...  ``` count=0 for e in iter(self):     count += 1 return count ```  I think it is possible... probably a nightmare to get all the in/out_edges, data/key options working conveniently. What do people think? Is it worth that effort to provide `__len__` and `__contains__` for  the edges() methods? 
comment
I  think `__len__` and `__contains__` have issues for G.edges()...     Maybe edges() is not a good example.  The view is of a subgraph induced on all nodes in nbunch and their neighbors...    That's a strange beast.  OK Then, what is a good example...  Even if G.adjacency() returns G.adj.items() so that it is a view, it doesn't seem like a good interface for anything but iterating.    Come to think of it, how often do you use len() or `__contains__` on a dict view? You can just check the dict itself...   ...ok, maybe `n in d.values()` I'm not convinced.  Other views on views?  
comment
So, if I understand correctly you are proposing:   [not sure what Yes and No mean :] 1. G.adjacency()  should return G.adj.items() 2. G.nodes(data=True) should return iter(G.node.items()); G.nodes() should return iter(G.node) 
comment
Yes, let's implement adjacency() to be what adjacency_iter used to be and remove adjacency_list. Not sure how much this is used in the code so it will be interesting to see. 
comment
Thanks... this is a bug that has been around for quite a while apparently. Nice fix too. 
comment
As you know, the power iteration method will not work for general networks/matrices.  In cases where degenerate eigenvalues are dominant, convergence is not guaranteed. In the example here (odd order path graphs) a negative eigenvalue has the same magnitude as the largest positive eigenvalue.   Using A^2 creates a 2-D space of eigenvectors with largest eigenvalues: any linear combination of the dominant eigenvectors of A. Not all of them are eigenvectors of A however. Only the original eigenvectors of A work. All the non-trivial linear combinations of them are eigenvectors of A^2 but not of A. For example, if A_v1 = v1 and A_v2 = -v2, then A^2_(v1+v2) = (v1+v2).  So (v1+v2) is an eigenvector of A^2. But A_(v1+v2) = v1 - v2.  I cannot find this written up anywhere, but it is true:  while eigenvectors of A must be eigenvectors of A^2, eigenvectors of A^2 do not need to be eigenvectors of A.  Luckily, Perron-Frobenius comes to the rescue (as it often seems to). So long as G is strongly connected (connected for an undirected network) the adjacency matrix is non-negative and irreducible. Thus the dominant eigenvalues include a positive real value r with one dimensional eigenspace represented by a non-negative eigenvector. Also, all other dominant eigenvalues are evenly spread about the circle in the complex plane centered at zero with radius r. So the entire spectrum of A lies on or in that circle, and the eigenvalue we care about is on the positive real axis.  If we shift the spectrum to the right by considering the matrix (A+I), we obtain a single dominant eigenvalue r+1 with the same eigenvector that corresponds to the eigenvalue r of A. So, to find the eigenvector centrality using the power method we should multiply by A+I.  (Adding other multiples of the identity to A will also work, but A+I is cheap to implement and sufficient for strongly connected networks.)  I can't find this written anywhere, so maybe it should be. If anyone has seen this please let me know.  This change expands the usefulness of our simple power iteration method from strongly connected networks with non-degenerate adj matrices to all strongly connected networks. I'll put together a PR to implement it.  Thanks @tantto for suggesting it and @hagberg for nugding me into what turned out to be a productive rabbit hole. :} 
comment
I like the original submission better @jfinkels because the indentation structure shows its process. Also, helper functions that only get used once are usually better in-line. Finally the original code is closer to the paper's explanation. But I'm fine with changing it too if @smopucilowski likes the suggested approach.   
comment
Only other idea/suggestion for efficiency I have is to consider whether storing the reduced degrees in a dict might be more efficient than repeatedly calling G.degree(node). Something like:  ``` reduced_degree = {n:d-1 for n,d in G.degree()}  # Note: github master has G.degree() an iterator ```  Then later `reduced_degree[node]` replaces reduced_degree(node). Also you would have to remove the `map()` by putting `reduced_degree[node]` inside the frontier_nodes set comprehension. If you move the `sum()` to the line with the set comprehension you can save having to create the set.  ``` frontier_sum = sum(reduced_degree[node] for node, path_length in path_lengths.items()                                 if path_length == distance ) ```  Not a big deal if this is too much--I'm just in the mode of working with iterators lately. It's your PR.  Finally, you should add this change to the doc/source/reference/release_2.0.rst and consider adding yourself to the credits.rst file in that directory.   
comment
I think you have to compute the reduced degree of all nodes when you find the influence of all nodes. But when computing for only one node you are right--we only need it for local nodes.  The G.degree(node) vs reduced_degree[node] question is best solved by testing... evaluating G.degree itself takes time, and then inside the function there's a lookup and a function call to len. Could you try changing the reduced_degree function to look it up in a dict? I use IPython for timing with the %timeit "magic" command.  ``` rdeg = {n:d-1 for n, d in G.degree()} reduced_degree = lambda node: rdeg[node] ```  This should be slower for individual nodes, but might be faster for full graphs.  Again, if this is too much let me know and we'll mess with that issue as part of the iterator switch over of the shortest_path routines. 
comment
This looks like a bug.... Thanks for reporting it. I'll look into it. 
comment
The method `Graph.neighbors` has two doc sections for `Notes`. Would it be better to combine them into one with appropriate rearranging of the examples they refer to?  `DiGraph.edges` has an empty `See Also` section.  `in_edges` (for both DiGraph and MultiDiGraph) could include both `edges` and `out_edges` in its `See Also` section.  For the `degree` docstrings (and friends) the word `and` is used instead of `or`` when specifying what is returned when a single node is requested. This appears twice in each docstring:  ```     Return an iterator for (node, out-degree) and out-degree for single node.      The node out-degree is the number of edges pointing out of the node.     This function returns the out-degree for a single node and an iterator     for a bunch of nodes or if nothing is passed as argument. ```  Should become:  ```     Return an iterator for (node, out-degree) or out-degree for single node.      The node out-degree is the number of edges pointing out of the node.     This function returns the out-degree for a single node or an iterator     for a bunch of nodes or if nothing is passed as argument. ```  In the main docstring for each class, there is a `Reporting` section that has a sentence that should be updated. The sentence `Iterator versions of many reporting methods exist for efficiency.` is now slightly misleading and could be replaced with something like `Reporting methods usually return iterators instead of containers to reduce memory usage.` 
comment
For the `degree` set of methods, we state what they return but the logic of when they return an iterator is not completely described. I'm not sure if it is better to just add `if multiple nodes requested` to the `nd_iter` section, or perhaps the Returns section should put the `if` structure more explicitly like:  ```     Returns     -------     if a single node is requested     deg: numeric value         Degree of the node.     OR if multiple nodes are requested     nd_iter : an iterator         An iterator of two-tuples with form (node, degree). ``` 
comment
Does this show up on readthedocs.org somewhere?  
comment
The formatting of the release2.0 file is strange in a few places. Lists aren't showing as lists, etc. I think the migration formatting looks ok.  When we merge these into master will there be links to them on the readthedocs pages? How will people find these pages?  Also, the name release_2.0 is different from the name we've been using api1.10.rst. What is the history there--should we make it match or is there a choice being made for a new name? 
comment
Thanks @SanketDG that looks good and I missed it. Let's put it in release_2.0.rst  The formatting is a little strange even on the github page with *AntiGraph *OrderedGraph   Would a link from the release_2.0.rst to the migration page make sense? Also, instead of a link from the homepage to the migration page we could put the migration page in the history section with the api/release notes. 
comment
I think the single_source_... is good... certainly what I was envisioning. The all_pairs has a question.... which is probably why you included it. Building the dict for each single source takes memory that people may not want to use.  You could `yield (n, single_source_shortest_path_length(G, n, cutoff=cutoff))` so the usage would look like e.g. diameter:  ``` path_length = nx.all_pairs_shortest_path_length(G) diameter = max( max(length for v,length in spl) for u,spl in path_length) # or for a dict of dicts all_path_lengths = dict( (v,dict(spl)) for u,spl in path_length) ```  This makes the default be lower memory, but is it too tricky for users to easily grok? what do you think is better? 
comment
Yes, I think sticking with (node, dict) for all_pairs is a reasonable way forward. :)  On Wed, Aug 5, 2015 at 10:07 AM, Mridul Seth notifications@github.com wrote:  > @hagberg https://github.com/hagberg @dschult > https://github.com/dschult @jfinkels https://github.com/jfinkels > @chebee7i https://github.com/chebee7i @ysitu https://github.com/ysitu > Should I stick with (node, dict) and move forward? >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1692#issuecomment-128009870. 
comment
Do you want to revamp docstrings in this PR or in a separate PR? 
comment
I'm fine either way -- what are the advantages of a separate repository over a separate directory as the examples are now? 
comment
Sounds like a lot of good reasons to put them in separate repositories. 
comment
This might be better proposed to Python.set() as a new method. But also see [stackoverflow](http://stackoverflow.com/questions/59825/how-to-retrieve-an-element-from-a-set-without-removing-it) on this issue. 
comment
I prefer next(iter(S)) because it uses standard python names (peek is only familiar to those who were taught peek as a method name for stacks).  I would argue that the loop/break idiom is harder for humans to parse and not much faster. 
comment
The Graph protocol does not have a **next** method. (And that's all you need to check for to see if it is an iterator.  The **iter** method returns an iterator, and any iterator object should just return itself, but any iterable will have an **iter** method.  Only an iterator will have a **next** method.)  On Mon, Jun 15, 2015 at 11:05 PM, jfinkels notifications@github.com wrote:  > The only thing I could think of for testing if an object is an iterator is > if there is an **iter** and a **next** method, the two methods that make > up the iterator protocol > https://docs.python.org/3/library/stdtypes.html#iterator-types. > However, this doesn't make sense because, for example, a Graph instance > (or a list, or a set) also has these methods, and I want to allow peeking > at them. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1534#issuecomment-112268955. 
comment
Thanks very much! 
comment
I might be wrong on this (or out of date), but I think when merging branches with git, we should identify the conflicts and change them in the branch to be merged. This looks like you are rebasing to pull the branch up to master before merging.  Again, I'm not a git expert (still spend too much time figuring out how to do things with git than I should). But what is the best way to get ready for such a merge?  It looks to me like the only conflicts are with mst.py and test_mst.py.  The old files need to be removed and the new file needs `list(G.nodes()) -> list(G)` and `G.degree_iter() -> G.degree()`.  With this single commit to prepare for the merge I think the merge would then be a clean merge and we wouldn't have to check through this very large PR.   comments welcome. 
comment
I guess this is a question of whether we should merge master into iter_refactor or rebase iter_refactor so that its commits apply to the head of master. Rebasing gives a "linear" history.  One description is at https://www.atlassian.com/git/tutorials/merging-vs-rebasing  I think we are close to being ready to merge iter_refactor into master. It seems like it might be messy if we include a bunch of commits in this branch that are already on the master branch. Then when we merge, we have double commits in some sense...  What are the disadvantages of rebasing the iter_refactor branch?  
comment
@MridulS can you open a new PR which rebases to master and fixes any conflicts. Then I think a separate PR to apply changes from the last 3 commits that aren't related to fixing conflicts.  Thanks  
comment
To make the nbunch treatment closer to the typical NX treatment for nbunch, we should allow an iterator.  This certainly wasn't in the code before, but if we're touching nbunch it probably should be added...  Also what happens when nbunch includes nodes that are not in G? Currently it will lead to a KeyError. If we wish to quietly ignore such nodes (the usual behavior in much of NX), then we should run nbunch through the G.nbunch_iter(nbunch) filter. In this case that would add a line before line 147 something like:  ``` nbunch = list(G.nbunch_iter(nbunch)) ```  Similarly the same line just before line 247 in the recursive version allows iterator nbunch and quietly ignores nodes not in G.  Do we want to quietly ignore nodes not in G? Do we need nbunch to include all nodes in G? 
comment
You state that one motivation for this change is that you need topological_sort to give consistent results. In what sense is the old code giving inconsistent results? (consistent meaning compared to another network with the same nodes? or multiple calls on the same network?)  An OrderedGraph (see #1356) has nodes and neighbors/edges always returned in the order they were added to the graph. Would this provide what you need? 
comment
Yes...  but if they were disconnected paths, then the order in nbunch (the starting nodes) should be enough. So maybe it's if you have two crossing paths and you want to pick which branch gets ordered before the others.  But he suggests that he just needs the order to be consistent. (??over multiple calls, or over multiple graphs using the same nodes?)  On Mon, Jun 29, 2015 at 11:58 AM, Aric Hagberg notifications@github.com wrote:  > I was wondering about the purpose above too. I was guessing that the > reason was to try to impose some order - e.g. if you had two disconnected > paths and wanted the nodes of one to appear before the other. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1625#issuecomment-116741482. 
comment
Can you try that with the following? og=nx.OrderedDiGraph(g) nx.topological_sort(og)  in place of the top sort... I'm not saying this is a final solution, I just want to know what we're shooting for here.  On Mon, Jun 29, 2015 at 6:52 PM, Neil notifications@github.com wrote:  > Hi, my application is that I'm generating TikZ (LaTeX diagram) code from > Python. I have a dag representing dependencies between commands. So I need > to output the commands in topologically sorted order. The compilation of > the LaTeX code is elided when the outputted code doesn't change. To > mitigate recompilation, I call nx.topological_sort(g, sorted(g, > reverse=True)). >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1625#issuecomment-116873777. 
comment
This all looks good. We've been discussing how to handle sort order in tests and in graph reporting methods. Suggestions are welcome. Python dicts are inherently troublesome in that way for unittests.  Maybe the doc_string here should include an example use of edge_key and source_nodes.  On Mon, Jun 29, 2015 at 9:21 PM, Neil notifications@github.com wrote:  > I have implemented my suggestion. By the way, I think the previous tests > relied on undefined behaviour, namely that small hashes in CPython are not > reallocated and were being traversed consistently between runs. You need to > impose a sort order if you want this. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1625#issuecomment-116901522. 
comment
I'd prefer the separated functions.  Could be in the same module clearly.  
comment
It might be best to use the doi:10.1016/0378-8733(78)90021-7 http://dx.doi.org/10.1016/0378-8733(78)90021-7  (which ends up pointing to the sciencedirect page). That is likely to be longer lasting, and from information there people can search for a free pdf version if they want to.  On Sat, Jul 4, 2015 at 10:26 AM, joel miller notifications@github.com wrote:  > The link given for >  > R174 http://1,%202 Freeman, L.C., 1979. Centrality in networks: I. > Conceptual clarification. Social Networks > 1, 215–239. > http://www.soc.ucsb.edu/faculty/friedkin/Syllabi/Soc146/Freeman78.PDF >  > is broken. I can find the paper at > http://leonidzhukov.net/hse/2014/socialnetworks/papers/freeman79-centrality.pdf, > or with a paywall at > http://www.sciencedirect.com/science/article/pii/0378873378900217 >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1641. 
comment
Yes, I think the 2-tuple is a good choice. This will come up again for functions which return node properties (or even edge properties). triangles, clustering coefficient, betweenness_centrality, etc... So we should look at it for degree() because it should probably be done similarly for the others. 
comment
@MridulS, how are you thinking about handling G.degree(n) vs G.degree()? Is G.degree(n) going to return a single value while G.degree() returns an iterator? That seem the closest to the current API, but it is sometimes tricky to check all cases correctly. 
comment
What do people think about  `G.degree(3) # where 3 is a node in G`  Should it return a single integer value? (as it currently does) Or should it return an iterator for which you need to use next to get the integer value?  The first makes it more backward compatible and probably easier for users who want degree of individual nodes. The second is more consistent with asking for the degree of many nodes, easier to code in the classes. 
comment
Im worried that the idiom `d=G.degree(n)` is popular and common enough to make changing it troublesome.  Also I don't like `d=next(G.degree(n))` as a replacement.  So, I would propose that G.degree() return an iterator of (node,degree) 2-tuples when more than one node is in nbunch and that it return a single integer value -- the degree -- when nbunch is a single node.  
comment
@MridulS can you combine degree and degree_iter to make it treat single valued input differently?  That will be more work in the classes, but much less work elsewhere. Also the inconsistency is actually a feature in this case... :) 
comment
For a single argument, return the single value. For a multiple node argument (including None) then define a generator (function with yield) and return that. Something like:  ``` def degree(nbunch):     if nbunch in self:         return len(self.adj[nbunch])+(nbunch in self.adj[nbunch])     def degree_iter(nodes_nbrs):          #all that stuff          yield stuff     nodes_nbrs =      return degree_iter(nodes_nbrs) ``` 
comment
I think the indentation is too deep for the function and return statements. They should be the same as the first "if" statement.  As for speed, you can start up ipython and use %timeit <statement> to test the speed interactively. But be careful--you often can't just re-import networkx--might be easier to create a number of versions in the same file and then run them all in a single ipython session. True interactivity would involve writing the function at the command line and the editor isn't very good IMHO.  I suspect you will do better speedwise if you make two different functions depending on whether weight is None. Might be fastest with 4 different functions weight/None and nbunch/None. But:  optimizing without knowing where the bottenecks are is a tricky business.  The slow-down due to creating a function should not be much if we do it once per call of degree(). Notice that we aren't defining the function is nbunch in a single node.   
comment
Maybe you can make a commit with just the graph.py module changed. That way we can look at it and if something larger needs to be changed you don't have to revise the entire package.  It will cause travis to go crazy, but that's OK... We can ignore the errors outside that one module for now...  OR if you prefer, just commit it to the branch on your account and dont make it a PR yet. Then we can look at that before you go through changing the whole package.  Whatever works best for you really. :) 
comment
I did a small time test comparing defining one function which includes the if statements with two functions and also with four functions. I also tested whether its better to use function arguments or just let the value of the variable be used when defining the function. And I compared with the speed of the current functions. There isn't much speed difference between any of these. The only exception is that the current code for getting the degree of a single node is about 4 times slower than any of these. (it creates the iterator and then picks off the only value from that iterator).  So pretty much any version is about the same time.  The following is slightly better than the others in the test I did.  ``` def degree(self, nbunch=None, weight=None):     if nbunch in self:         nbrs=self.adj[nbunch]         if weight is None:             return len(nbrs) + (1 if nbunch in nbrs else 0) # handle self-loops         return sum(dd.get(weight, 1) for nbr,dd in nbrs.items()) +\                 (nbrs[nbunch].get(weight, 1) if nbunch in nbrs else 0)      if nbunch is None:         nodes_nbrs = self.adj.items()     else:         nodes_nbrs = ((n, self.adj[n]) for n in self.nbunch_iter(nbunch))     if weight is None:         def d_iter():             for n, nbrs in nodes_nbrs:                 yield (n, len(nbrs) + (1 if n in nbrs else 0))  # return tuple (n,degree)     else:         def d_iter():             for n, nbrs in nodes_nbrs:                 yield (n, sum((nbrs[nbr].get(weight, 1) for nbr in nbrs)) +                     (nbrs[n].get(weight, 1) if n in nbrs else 0))     return d_iter() ``` 
comment
Try it with a bigger graph:    G=binomial_graph(1000,0.3)  or something similar.   I don't trust results that include a warning about cached results. 
comment
The `nbunch in self` and not discouraging `None` are the same as in the current degree(). (Sometimes hidden in nbunch_iter().)   @MridulS, You time tests match my results so that degree_new and degree_v2 are essentially the same. You should pick between them based on which is easier to read.  I'm glad removing degree_iter() speeds up single node lookups. :) 
comment
consistency is the bane of ......     oh never mind....  I don't think Python is consistent on this.  I can make a list-like class (a mutable object) with no hash method and still use it as a key in a dict.  It's only the built-in unhashables that create a TypeError.  I think Pieter first argued that **contains** should just return False instead of raising an error. But I don't think he got much resistance.  It's such a common use case that writing try/except all the time gets tedious, and since python isn't consistent anyway...   Anyway, this way is at least self-consistent and natural:   return True if n is in the graph else False. No need for a "third truth value" which really means "is not and could not be in the graph".  Besides... it lets us test whether something is a node by simply using  `if n in self`         :) 
comment
Yes, I agree a comment would be helpful. The problem is that nodes CAN be iterables. So we have to figure out if nbunch is a node or an iterable of nodes. We assume that if nbunch is a node, we do not interpret it as an iterable of nodes.  It is possible to cause trouble with this:    ``` G=nx.Graph() G.add_edge( (1,2), 1) G.add_edge(1,2) G.degree( (1,2) )  # returns the degree of the node (1,2) # we can get the degree of both nodes 1 and 2 without something fancy  # (though G.degree( (2,1) ) would work in this case) ```  How about this for a comment:  ``` # Test to see if nbunch is a single node, an iterator of nodes or None(indicating all nodes) # (nbunch in self) is True when nbunch is a single node.  ``` 
comment
"Postmature Optimization" often breaks backward compatibility -- and if you include readability as part of optimization then the review process is all about optimization.  The question is really about whether the code is "mature"...    It looks like you've got the code up and working. You've put in a PR. Before we merge and walk away, lets check it for readability and basic performance issues. A number of pairs of eyes have looked at it for readability. It's worth taking ten minutes to run the unweighted case on a reasonable size graph and compare to the old code. Maybe it won't be so different -- then we leave it as is. But if it is 3-10 times slower then we take probably want to change something. 
comment
Thanks @MridulS !  Looks like the tests don't like finding max of tuples with None as the indicator of a source node. I think simply reverting back to the older source indicator should work for that. That should also take care of the long line.  Two other things and I think we're ready... - could you switch the return statement to   `return path.reverse()`  - in the docstring, could you move the text indicating default up one line. Like      weight : string (default= 'weight')  Also a blank line after each parameter would be nice.  Are there other items we need to resolve here? 
comment
Yes, nodes are not necessarily comparable. Maybe you can use the `key` argument to pick off just the length when doing comparisons. That is really what we want anyway. We're only comparing the 2-tuples out of convenience. We really just want to compare the first entry... right?  On Tue, Jun 30, 2015 at 1:42 PM, chebee7i notifications@github.com wrote:  > I'm not even sure we guarantee that other vertices are comparable. You can > have an integer node and also a string node---that comparison is invalid. > None is not supposed to be a valid node, but we do not enforce it. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1628#issuecomment-117275996. 
comment
Yes, let's go ahead and merge this and start another PR for fixing node comparisons. Thanks @sjackman ! 
comment
Do you specify the file name as a string (the name of the file), or do you open the file yourself and pass the file object into write_gml?  If you are using a string to specify the name of the file then the decorator should close the file object before returning and your symptoms indicate a bug in the decorator.  If you specify a file object/handler, (or anything not string-like) then the file object is not closed during this call and you will have to close it manually after the call to write_gml. The idea is that if you opened the file yourself then you might want to write more after the function call, so we put you in charge of closing the file. 
comment
Let's go ahead and close this. If it doesn't work feel free to reopen this issue. 
comment
In general I like to be able to find functions without having to mess with which module they are in. These names are unlikely to collide with other routines, so I'm for putting them in the top-level namespace. But I'm fine if others want to avoid polluting top-level. I sort of like pollution I guess.  On Fri, Jun 19, 2015 at 11:46 AM, chebee7i notifications@github.com wrote:  > Any other comments on this? @hagberg https://github.com/hagberg @dschult > https://github.com/dschult @ysitu https://github.com/ysitu @jfinkels > https://github.com/jfinkels. Should the *_edges functions be in the > top-level namespace? Or should we consider these to be "advanced" functions > and force the user to fetch them at networkx.algorithms.tree. Note that > we are already recommending users to call the *_tree functions from the > tree submodule, but they are still available in the top-level namespace. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1577#issuecomment-113553672. 
comment
I don't think we're opposite.  I don't like importing \* either. Explicit is good.  Easy to get to is good. They don't compete with each other. You just need to import lots of things explicitly. 
comment
@chebee7i, I'm not really the one to ask about how users would use these functions. If a number of them would use `T.edges()` then maybe its better to have it at the top level. If the overwhelming use-case wants a tree then its probably fine to hide the edges routines. If its a borderline case, I don't see any harm to having it exposed at the top level. These names are pretty clear and shouldn't cause confusion.  But that's just my bias toward exposing functions. 
comment
At the top of the PR it says which branch you will be merging to when the merge is done. It looks like these PRs will merge with the iter_refactor branch. 
comment
Yes, I misunderstood your question about whether these PRs could be merged. I thought you were asking if they could merge with the correct branch. Now I see that you were asking if anything else needed to be done before they are merged.  My bad... :}  I haven't checked all the changes--only read through the conversation.  If you have looked through it @chebee7i then we should go ahead and merge.  
comment
I think the point of the timing classes is to see how timing changes over the revisions, so I naively think they should not be changed. Once the changes are clear and we're comfortable with the timing of them we can update the timingclasses. 
comment
I think the easiest way to skip the doctests in timingclasses.py is to add a few lines at the bottom of the module.  ``` # Only to make the doctest feature of nosetests skip testing doctests def setup_module(module):     from nose import SkipTest     raise SkipTest("Skip nose doctests for timingclasses module") ``` 
comment
How does:  ``` python for u, e in G_succ[v].items():             cost = get_weight(u, v, e) ```  compare to:  ``` python for _, u, cost in G.edges(v, data=weight): ```  It's probably better for the long term to avoid exposing the data structure by using G.succ[v].items and friends unless there are performance issues. 
comment
I think this has been reported and subsequently fixed in #1544  Are you using those changes? Do they fix it? 
comment
I'm going to close this...  You can reopen if the recent changes don't fix the problem. 
comment
See #1508 
comment
You can look at the source code... in this case at : https://github.com/networkx/networkx/blob/master/networkx/classes/digraph.py The general version handles isolated nodes and edge attributes as well as graph and node attributes. 
comment
Looks like a good start...  Needs tests and needs to pass existing tests (import of izip_longest) 
comment
I think the `future.moves.itertools import zip_longest` is a typical way to handle that. We still support python2.7 so we need to make the code work for both (as you lobby for).  On Wed, Mar 11, 2015 at 2:03 AM, Mridul Seth notifications@github.com wrote:  > @dschult https://github.com/dschult izip_longest has changed to > zip_longest for python 3(travis fail). I could do something like from > future.moves.itertools import zip_longest, but we should discuss about > the code compatibility for python 2/3. > @hagberg https://github.com/hagberg suggested that we move towards > python 3 and ultimately drop support of python 2. > I think we should still keep our code python 2/3 compatible. People are > moving to python 3 but the transition is slow. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1399#issuecomment-78210079. 
comment
Actually, I think there is an easy way to avoid itertools here: `H.add_edges_from(  (n,nbr) for nbr in seen )` 
comment
This looks good to me... :) 
comment
You could try converting to the coo format (which easily gives edge lists).  A=ppm.tocoo() G=nx.Graph() G.add_weighted_edges_from(zip(A.row,A.col,A.data))  For my quick trials using your ppm generation code I get a speed-up of about 10 times over your method 2.  It might be even faster if you create coo format to create the sparse matrix. Note: there may be other sparse formats that are even faster for matrix creation than coo for your matrix. I didn't look into it. Dan  On Fri, Jul 11, 2014 at 2:28 AM, Erwin Rossen notifications@github.com wrote:  > Method 1: >  > g = nx.from_scipy_sparse_matrix(ppm, edge_attribute='weight') >  > Currently implemented as default when using the Graph constructor. For > 1,000 nodes and 39,000 nonzero elements, this takes 1.96 seconds. >  > Method 2: >  > nr_persons = ppm.shape[0] > g = nx.Graph() > g.add_nodes_from(range(nr_persons)) > for i in range(nr_persons): >     k = [l for l in ppm.indices[ppm.indptr[i]:ppm.indptr[i+1]] if l>=i] >     for j in k: >         g.add_edge(i,j, weight = ppm[i,j]) >  > For 1,000 nodes and 39,000 nonzero elements, this takes 0.90 seconds. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1214. 
comment
What does this do for layout algorithms that actually use the graph structure? If you replace G with a list of nodes, you have lost the graph structure. That works for a few of the layout algorithms that don't use the structure. But beyond those very simplistic layouts this interface won't work. Am I missing something? 
comment
For the "center" keyword, we have some FIXME comments in the code about adding normalization. Perhaps we should provide a separate function to normalize positions with specified center and specified width (std dev?  radius?).  ``` pos = nx.spring_layout(G.subgraph(nodelist)) pos = nx.normalize_layout(pos, center, width) ``` 
comment
This looks good and travis only tripped on the test_petersen test which should be fixed now. Id merge but dont know if more is planned here. 
comment
Can you provide a (hopefully small) example that reproduces the error?  On Thu, Mar 12, 2015 at 3:20 PM, beyssac notifications@github.com wrote:  > Hi, >  > I was trying to write a .Net (Pajek) file with generate_pajek() and I got > an error when passed MultiDiGraph as parameter: >  > 'int' object has no attribute 'copy' >  > It does not happen when I pass DiGraphs >  > PS: Both cases the graphs had edges having attributes >  > Thanks, > Pedro Henrique >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1403. 
comment
average in degree and average out degree should be the same... right?  (also, your output has 20 nodes for some reason---should be 10?) 
comment
Thanks very much!! 
comment
@AgostinoSturaro if you read the fine print, the igraph docs say there are strong finite size effects. The issues we are talking about are in that category. You'd have to look at the code to be sure, but it is likely they are using a similar algorithm. Self-loops and parallel edges are easy to remove if you don't want them. `G.remove_edges_from(G.selfloop_edges())` 
comment
Yup--that's a bug.  Thanks! 
comment
I didn't rewrite each unit test of Graph. I set SpecialGraph up to run each unit test of Graph making sure the output is as expected with OrderedGraph and ThinGraph. But there is only one unit test that hits this line of the code and OrderedGraph, ThinGraph, and Graph all return the same results with the existing unit test.  This will be relevant for matrix data structure backends too.  I suppose unit tests should be created which check the data structure directly rather than the output from reporting methods.  On Fri, Mar 6, 2015 at 12:49 PM, chebee7i notifications@github.com wrote:  > Nice catch. @dschult https://github.com/dschult, why didn't the unit > tests pick this up? >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1394#issuecomment-77602984. 
comment
This looks good!  Thanks...  You have put the k= keyword argument before the others. For backward compatibility would it be better to have it added at the end of the keyword args?  That's the only comment I have--nice job. 
comment
Yes, it is consistent this way. Let's do it...   They are keywords with defaults so order is unlikely to be a problem. 
comment
Can you change the doc_string in your PR to make it: "The node sets of G and H do not need to be disjoint." 
comment
I think this should be a separate method: Just as edges() returns edge info and neighbors() returns neighbor info, remove_edge() should remove individual edges and remove_neighbor() [or maybe disconnect_neighbor()] should remove all edges between two nodes.  I like the distinction between edges and neighbors. 
comment
Yes-- your solution with `remove_edges_from(G.edges([u,v]))` is sufficient. If not obvious enough we should add it to the doc_strings.   
comment
Python is filled with arbitrary outcomes to the same code on different architectures. Many stem from the dict and set data structures. You can't even predict what order a dict's keys/values are printed.  You can't predict whether a lookup in a dict will require handling a collision. Arbitrary outcomes are not always a bad thing.  Removing an arbitrary edge between u and v leaves one less edge between u and v. That's pretty clear in many/most cases. 
comment
@MridulS  It looks like you should make sure nosetests is set up and working on your machine to run the tests before creating the pull request. On the pull request page you can see what the automated testing service Travis-CI found for failures.  But these are often easier to find on your own machine.  The command: nosetests test_file.py can let you run a single test file at a time.  You change a number of test files where we use assert_raises(). I think these changes break the test code. Look at the original code to see how the function should be used. We need the function to be called followed by the arguments to be given to that function. By putting the arguments into the function, you are running the function before calling assert_raises. The interpreter calls the function with the given argument, gets the result and sends the result to assert_raises() as if it is a function to be tested.  Also while we are talking about testing, these methods are core routines for the graph classes so you should do some performance testing to see the difference in speed. With all the for loop construction in the new code, I suspect it is much slower than the previous code. But Python speed is often surprising.  The tests and speed checks are more demanding for code in the core graph classes. It might be easier to start contributing with less critical code. But if you want to learn about nosetests and Timer, this feature will need it. :) Dan  On Wed, Feb 4, 2015 at 7:44 PM, Mridul Seth notifications@github.com wrote:  > Hey, I am new to networkx and interested in contributing to it. > As mentioned in #1246 https://github.com/networkx/networkx/issues/1246. > I have tried implementing a API 2.0 feature on G.neighbor >  > neighbors(n, data=None) - iterator over neighbors of n. > If data is True: iterator over (nbr, eattr_dict) > if data not in (None,True): iterator over (nbr, eattr_dict[data]) >  > ## Please have a look. >  > You can view, comment on, or merge this pull request online at: >  >   https://github.com/networkx/networkx/pull/1344 > Commit Summary > - Added data keyword in G.neighbors >  > File Changes > - _M_ networkx/classes/digraph.py >   https://github.com/networkx/networkx/pull/1344/files#diff-0 (36) > - _M_ networkx/classes/graph.py >   https://github.com/networkx/networkx/pull/1344/files#diff-1 (18) > - _M_ networkx/classes/tests/historical_tests.py >   https://github.com/networkx/networkx/pull/1344/files#diff-2 (4) > - _M_ networkx/classes/tests/test_digraph.py >   https://github.com/networkx/networkx/pull/1344/files#diff-3 (8) > - _M_ networkx/classes/tests/test_digraph_historical.py >   https://github.com/networkx/networkx/pull/1344/files#diff-4 (14) > - _M_ networkx/classes/tests/test_graph.py >   https://github.com/networkx/networkx/pull/1344/files#diff-5 (13) > - _M_ networkx/classes/tests/test_multidigraph.py >   https://github.com/networkx/networkx/pull/1344/files#diff-6 (8) >  > Patch Links: > - https://github.com/networkx/networkx/pull/1344.patch > - https://github.com/networkx/networkx/pull/1344.diff >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1344. 
comment
The PR now changes 7 files, but only 2 files have substantive changes. The changes to the test files don't seem to affect behavior. I guess you should either remove those changes or explain why you request them. I think #1345 would help you learn the codebase and is more likely to be feasible. 
comment
Actually, I did mean `if data is True:`  The data we return for `neighbors()` when multiedges are present has to be combined if there is more than one edge between the same pair of nodes. 
comment
What do you mean by "common consensus"? It's not clear we have one. 
comment
Returning more than one edge per neighbor should be the job of G.edges() For G.neighbors() we should combine the multiedges between the neighbors. I think we should include an argument G.neighbors(n, data=False, combine_edges=min)  The networkx codebase seems to use min and sum to combine multiedges depending on whether the weight represents a length or a capacity. I like the default of min.  It's not clear to me whether we should allow different arguments for MultiGraph from Graph. Of course Graph doesn't have multiedges, so why include that argument? But including the argument and ignoring it would allow algorithm code to call the method with the same syntax for both Graph and Multigraph. 
comment
@hagberg, are you really saying we should use one method for all graph queries? To get G[u] we can use edges() in a 1-liner. To get G.neighbors(n) its a 1-liner with edges(). To get G.self_loop_edges() its a one liner with edges(). To get G.degree() its a 1-liner with edges(). I don't think you really mean we should reduce the interface to one method (or maybe 2--one for nodes() and one for edges()).   What criteria can we use for when to: 1) add a keyword to change the output behavior 2) add a new method that provides that output behavior 3) only output general data that requires the user to post-process with a 1-liner 
comment
I see the confusion for asking for edge data with a method called neighbors. And I think the idea of keeping methods close to the graph theory terminology is also powerful. Functions outside the class are currently present for get_edge_attributes and so maybe that is a good way to go. But these functions will depend on the data structure -- that's why I prefer they be methods.  Maybe the way to proceed is to look through the algorithms and find common query paradigms that can be packaged into functions. I'm sure one of them is to return (nbr, edge_weight) as it is used in the shortest_path routines. There are quite likely to be others. 
comment
Thanks for pointing this out. I found the correct link and updated the credits. https://www.python.org/doc/essays/graphs 
comment
Sorry for not checking the Travis error output for #1314.   So--do I understand correctly that inside the try/except clause CPython throws a TypeError for the "if n not in self.succ" while IronPython throws a TypeError on the next line: "self.succ[n]={}"???  Wow--- that's totally off my radar (and a good example of why try-clauses should be kept as short as possible.  You can't tell what error is being caught for which reason. :)  Would a quick fix in add_nodes_from along with some comments to describe why we have so much in the try-clause be the solution?  
comment
Yes, that makes good sense.  I usually don't like to catch two different exceptions with the same exception type, but I'm not used to protecting against two different pythons. :)  Welcome back!  And Congratulations on you new job!  On Fri, Jan 9, 2015 at 8:40 PM, ysitu notifications@github.com wrote:  > Supposedly that bug (CodePlex issue > https://ironpython.codeplex.com/workitem/35348) has been fixed ( > IronLanguages/main#221 https://github.com/IronLanguages/main/pull/221, > more specifically IronLanguages/main@2e9b42b > https://github.com/IronLanguages/main/commit/2e9b42b667f94f247d54f912230d4e83968d05e6) > and released in IronPython 2.7.5. .travis.yml apparently is picking up that > release. >  > You can use the following to work around the issue: >  > ``` >     for n in nodes: >         try: >             if n not in self.succ: >                 self.succ[n] = self.adjlist_dict_factory() >                 self.pred[n] = self.adjlist_dict_factory() >                 self.node[n] = attr.copy() >             else: >                 self.node[n].update(attr) >         except TypeError: >             nn,ndict = n >             if nn not in self.succ: >                 self.succ[nn] = self.adjlist_dict_factory() >                 self.pred[nn] = self.adjlist_dict_factory() >                 newdict = attr.copy() >                 newdict.update(ndict) >                 self.node[nn] = newdict >             else: >                 olddict = self.node[nn] >                 olddict.update(attr) >                 olddict.update(ndict) > ``` >  > Even if you manage to get past n not in self.succ you will still be > caught by self.succ[n]. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1327#issuecomment-69432211. 
comment
The f.geometry() returned None in python3 because the shpfile had not been created correctly. Unrolling the map() and xrange() calls allows the shpfile to be correctly written and then f.geometry() works.  PR #1317 holds the changes.  
comment
A cycle basis (http://en.wikipedia.org/wiki/Cycle_basis) is a set of cycles for which you can construct any cycle by forming a weighted sum (linear combination) of the basis cycles. Here addition of cycles means appending edges and eliminating any that "flow" in the opposite direction.  That is, each cycle has an implied orientation and if two cycles contain the same edge in opposite directions the edges cancel when the cycle adds.  I guess the important thing is that there are MANY choices for a basis. Just as there are many choices for a coordinate system in the plane even though the "standard basis" is the unit vectors along the x and y axes. A cycle basis is a coordinate system for the cycles of a network. There are many choices. And the algorithms don't have a good way to say which basis is better than another. So the choice for basis returned in your example is correct---just not the basis you expected. Your brain likes small cycles to be in the basis. But the algorithm doesn't always find the smallest cycles for its basis.  I believe [3,9,8,4] is obtained from the basis as:  [3,9,6,5] - [3,4,1] - [4,8,9,6,5,1] + [3,5,1] Dan  On Sun, Dec 21, 2014 at 9:52 AM, gregjfisher notifications@github.com wrote:  > Hello there, >  > thank you for all your work on networkx - it's helping a great deal in my > phd! >  > To cut a long story short, I stumbled across some instances where > nx.basis_cycles() was giving me basis cycles which didn't seem right (it > works most of the time). I had a look though this github to see if others > had seen this problem, and if it had been resolved, but I couldn't see > anything. >  > For example, for this graph: >  > [image: figure_6966443] > https://cloud.githubusercontent.com/assets/5628136/5518421/0c337b0c-891f-11e4-97fa-c339ab305692.png >  > I get this array returned: [[3, 5, 1], [3, 9, 6, 5], [4, 8, 9, 6, 5, 1], > [3, 4, 1]] >  > The array [4, 8, 9, 6, 5, 1] isn't a basis cycle and the basis cycle [3, > 9, 8, 4] is missing. >  > I had a look at the underlying code, which I took from this webpage - > http://networkx.lanl.gov/_modules/networkx/algorithms/cycles.html#cycle_basis > . >  > It looks like the code searches in a way which, for the above graph, meant > that [4, 8, 9, 6, 5, 1] was 'found' before [3, 9, 8, 4] was found; and > nodes were removed from 'gnodes' before [3, 9, 8, 4] could be found at all. >  > To replicate this problem, consider the following code (which replicates > the above graph via its adjacency matrix and uses the code above): >  > def basis_cycles(): >  > Adj_Matrix = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # 0 >                        [0, 0, 1, 1, 1, 1, 0, 0, 0, 0],  # 1 >                        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],  # 2 >                        [0, 1, 0, 0, 1, 1, 0, 0, 0, 1],  # 3 >                        [0, 1, 0, 1, 0, 0, 0, 0, 1, 0],  # 4 >                        [0, 1, 0, 1, 0, 0, 1, 0, 0, 0],  # 5 >                        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],  # 6 >                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # 7 >                        [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],  # 8 >                        [0, 0, 0, 1, 0, 0, 1, 0, 1, 0]]) # 9 > G = nx.from_numpy_matrix(Adj_Matrix) > gnodes=set(G.nodes()) > cycles=[] > while gnodes:  # loop over connected components >     if root is None: >         root=gnodes.pop() >     stack=[root] >     pred={root:root} >     used={root:set()} >     while stack:  # walk the spanning tree finding cycles >         z=stack.pop()  # use last-in so cycles easier to find >         zused=used[z] >         for nbr in G[z]: >             if nbr not in used:   # new node >                 pred[nbr]=z >                 stack.append(nbr) >                 used[nbr]=set([z]) >             elif nbr is z:        # self loops >                 cycles.append([z]) >             elif nbr not in zused:# found a cycle >                 pn=used[nbr] >                 cycle=[nbr,z] >                 p=pred[z] >                 while p not in pn: >                     cycle.append(p) >                     p=pred[p] >                 cycle.append(p) >                 cycles.append(cycle) >                 used[nbr].add(z) >     gnodes-=set(pred) >     root=None > return cycles >  > Has anyone ever raised this before? I am using Python v2.7 via Anaconda on > a mac and uploaded networkx about 2 months ago. >  > I might have a look to see if I can develop the code to make it work all > of the time but, for now, I thought I'd raise this issue. >  > Best wishes, >  > Greg Fisher >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1311. 
comment
One way to reduce the impact of hash order on only the networkx dictionaries would be to follow @chebee7i and use a class which has OrderedDict dictionaries.  See #980.  But that's not quite ready to go. 
comment
A clique is a graph where every node is connected to every other node. The find_cliques routine finds the maximal cliques--that is, the cliques for which we cannot add any other nodes and still have it be a clique.  https://networkx.github.io/documentation/latest/_modules/networkx/algorithms/clique.html  On Wed, Sep 10, 2014 at 8:21 AM, xiaoshuyimei notifications@github.com wrote:  > but i still don't understand what is clique and why the output like this >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1251#issuecomment-55107031. 
comment
I'm looking through the code, but am getting bogged down in some details.  Can you clarify why you changed how mincomp is computed?  I think it should be the component that contains the smallest ordering node.  Now I think you just take the first component that includes s.  There might be some way those are the same, but I can't see it easily.  Also, why is the "next subgraph" the same as previous with the previous node removed?  Finally, I assume you have tested speed... do you have some code that does that?  And did you use any additional tests for correctness (other than those in the tests directory already)? Thanks, Dan 
comment
OK... I finally grok your approach to use the ordering to build the sccs.  It doesn't impact the main part of the algorithm, but helps with removing SCC calculations. In fact I think none of your changes affect the algorithm as presented in the paper--- they just implement it more efficiently.  That's good!  Your comments 3.1) and 3.2) are related.  The paper has a theorem which claims that the algorithm is O((n+e)(c+1)), but it seems to me the "proof" only considers finding the SCC for the original graph once--and doesn't consider having to compute the "adjacency structure of strong component K with least vertex in subgraph of G induced by" nodes ordered s or larger.  If each SCC computation is really O(n+e) then don't we get O((c+n)(n+e)) and not O(n(n+e)(c+1)).   Anyway, I don't believe the proof of the articles clim on complexity--though maybe I am missing something.  The key to reducing the (c+n) to (c+1) is to explore your note about computing the SCC of a subgraph with one node without recomputing from scratch.  But for now, let's get this part into NX as it should improve speed quite a bit.  I'll looks through again for potential changes.  One possible improvement (from overstack: http://stackoverflow.com/questions/9868653/find-first-list-item-that-matches-criteria): In two places (one with a filter) you compute a list only to use its first element: e.g. [c for c in sccs if s in c][0]    Could that be replaced by an iterator that stops after the first is found?:  e.g. next(c for c in sccs if s in c)  My understanding is that creating the generator and calling next on it should stop after it found the component.  Perhaps it doesn't matter much though.  Can the "filter" also be replaced in this way?  Does it make sense to do so? 
comment
I'm not getting any consistent improvement from the current code--even though it seems like there ought to be.  I am getting the feeling that most of the time is being spent in the recursive circuit routine, so that the SCC computation isn't making much difference.  But my tests are all small networks.  What size and character are the networks you used to get the speedup?  Here's a quick test script:  (I started with the new code in an alias folder named networkx_new and old code in networkx)  ``` import timeit statement="nx.simple_cycles(G)"  setup="import networkx as nx;G=nx.fast_gnp_random_graph(20,0.1,seed=%i,directed=True)" setupnew="import networkx_new as nx;G=nx.fast_gnp_random_graph(20,0.1,seed=%i,directed=True)" print "Starting computation" for i in range(10):     t=timeit.Timer(statement,setup%i)     tnew=timeit.Timer(statement,setupnew%i)     print "starting first",     tt=t.timeit(1)     print "done with first.... now doing second",     ttnew=tnew.timeit(1)     print "done with second"     print "seed=%i:  increase: %8.2f %%"%(i, (ttnew/tt)*100) ```  About half were faster with the new code, half with the old.  I played around with the number of nodes and edges. 
comment
Testing on acyclic digraphs would return no cycles... ?....   It's becoming clear that I don't understand the use-case for this function.  Are you using it to check if an "almost-acyclic tree" is actually acyclic?  It would be really helpful is I knew what type of graphs to test this on. 
comment
Would it be possible to try it using the code in the pull request on your 55 node example?   Actually, I think if the list of cycles is going to be 10^95 long, we're not going to be able to help.    Maybe you should think about cycle_basis which gives a basis for all cycles and then all possible cycles can be created from that basis.  It certainly could be true that the recursive nature of this algorithm demands memory.  But I think the issue is bigger than that.  Listing all elementary cycles for most graphs is not practical.  Maybe we should provide more warnings of this fact in the docs. 
comment
Hello erikasantos, If you are still willing to plug in to this, now would be a good time. Can you test whether there is a speed difference (for the network you got a good speedup) between your submitted code and the non-recursive version in pull request #890 ?  I tried to include all the improvements you put in and getting rid of recursion may also speed it up.  Friedsoap, any kind of speed or memory tests you can put together would be great.  You are looking at dense networks while erikasantos is looking at sparse, so I think the results could be different.  I ran the code with a complete graph with 12 nodes and it didn't blow out memory like the recursive version, but more testing would be helpful. Thanks Dan 
comment
Actually, sympy objects are a good example of non-numeric data that would be useful in adjacency matrices. The current nx.adjacency_matrix doesn't have an option for that.  But it is a wrapper around nx.to_scipy_sparse_matrix() which creates the adjacency matrix and takes an argument dtype to determine the dtype of the data (see numpy or scipy for how to set up dtype).  If you don't want a scipy sparse matrix you can get a numpy (dense) matrix using nx.to_numpy_matrix() which also takes a dtype argument. 
comment
I made a pull request to your branch that has this pull request.  I'll comment here just to keep the suggestions all together on this page.  I reformatted the doc_strings a little removed some tests that didn't seem to belong and made some simplifications in the code.  I also switched the output from a dict of two dicts to a tuple of dicts which is how other functions do it in our library.  Now people can use:  influences,dependencies = heat_kernel(G,0)  Ideas/questions still to think about: 1) should these all be in one file instead of three separate files? 2) the doc strings should describe what the parameters are.  It would also be nice to know what the names refer to.  The Diaz article which seems to introduce PWP doesn't say why it is named that or what it stands for.  Similarly there is no description for why MIMAC is called that.  The heat kernel has a healthy literature that connects it to other topics.  I couldn't find that for PWP or Mimac. 
comment
The wikipedia article on adjacency matrix states that "often" the convention for handling loops differs for graphs and digraphs.  For graphs it is often to double the value on the diagonal.  [This allows the degree to be obtained from summing a row.]  For digraphs the convention is often to put the edge value on the diagonal (undoubled). There are no citations listed for these claims about conventions "often" using one or the other.  The NetworkX routines to_numpy_matrix() and from_numpy_matrix() leave the diagonal entry as 1, but adjacency_matrix() doubles the diagonal entry for loops.  In version 1.7 the diagonal entries were not doubled.  Note that nx.from_numpy_matrix(A) effectively ignores half the entries in matrix A by writing each value in A as a separate edge. So an asymmetric matrix A with conflicting edge weights ends up creating an undirected graph with edges (i,j,wt) where wt=A[j,i] when j>i and A[i,j] when i>j.  So this is consistent with not doubling the loops.  Also to_numpy_matrix() does not double the diagonal while adjacency_matrix() does.  The docs don't seem to mention what happens to self-loops.  I see the reason for doubling loops in the adjacency_matrix (degree is the row sum).  But I think it is probably best in terms of avoiding surprises to not double it. Or, to get the best of both worlds we could let adjacency_matrix double it while to_numpy_matrix does not and adjust the doc_strings accordingly. 
comment
Putting a 2 on the diagonal of an adjacency_matrix makes sense to me too.  How about for to_numpy_matrix? 
comment
Maybe I should be asking what the behavior for nx.from_numpy_matrix(A) should be.  It seems reasonable to have nx.to_numpy_matrix(nx.from_numpy_matrix(A)) return A.... at least if it is symmetric.  That suggests maybe we should make the self-loop weight be one half the entry in A when we are creating an undirected graph.  Ack...  This discussion may be relevant to the matrix backend project (#1076) being discussed. 
comment
How about this: - to_numpy_matrix() and adjacency_matrix() put twice the self-loop weight into the matrix by default when the graph is undirected. - from_numpy_matrix() puts half the diagonal entry on the edge weight for self-loops when create_using is undirected. - We can add an optional argument to allow adjustment of this if that 1) makes it more clear and 2)  allows flexibility.  Something like undirected_self_loop_factor=2.0 
comment
Documentation sounds like the way to go. But it doesn't help the issue of numpy types like strings and bools unless the convention we pick is to assign the diagonal element to be the value of the self-loop edge weight.  That is what we used to do as of version 1.7 so I'm OK with that. Multiplying by 2 (and 0.5) works for me too if we have a good way to handle the types. I think as long as the docs say how to change the diagonal elements we're OK either way.  On Wed, Mar 19, 2014 at 7:43 PM, Aric Hagberg notifications@github.comwrote:  > I took at look at implementing this. Two things came up >  >    - >  >    I don't like the coding complexity of adding a keyword. It is fairly >    simple to scale the diagonal by 2 - e.g. various one-liners like A[np.diag_indices_from(A)] >    *= 1 . But the extra logic makes the functions a little bit uglier. >    - >  >    We allow various numpy types like strings and bools as matrix entries >    (this is not an issue for scipy sparse matrices that only hold numbers). So >    dividing or multiplying by 2 is also kind of dodgy. Requires some extra >    logic. >  > Is there a compelling reason to design the interface with a keyword like > undirected_self_loop_multiplication_factor (hi @chebee7ihttps://github.com/chebee7i)? > Or can we more simply just pick one convention and document how users can > adjust the diagonal to suit any other needs? >  > ##  >  > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/1078#issuecomment-38121112 > . 
comment
The shortest_path with a cutoff of 2 can be quickly found using a list comprehension:  u_nbrs=G[u] for v in G_u:     mutual = [(u,n,v) for n in G[v] if n in u_nbrs]  Other potential speedups-->  use G[t] instead of list(G.neighbors_iter(t)) because G[t] is a dict and dict lookups are much faster than list lookups.  Also, the creation of Q looks like a check for whether two sets are disjoint.  If you can set up the sets as Python sets, the set.isdisjoint() method may speed up a lot.  Something like:  total=0 for s in ST:      Gs=set([n for n in G[s] if n!=u and n!=v])     for t in ST-Gs:  # set difference so t is not neighbor of s         if Gs.isdisjoint(G[t]):             total +=1   I haven't checked all the special cases here (can s be in ST-Gs?) but even with adding extra checks this is likely to be faster.  Finally, be careful of strange cases like embeded being zero in the normalization.  Thanks-- this looks interesting. 
comment
I don't know that isdisjoint is the best way to go, I'm only suggesting a general principle.  The idea is to use the builtin methods of the set datatype where possible to speed things up.  I haven't read the article in depth, but it says:  "we deﬁne dv(s; t) to be the function equal to 1 when s and t are not directly linked and also have no common neighbors in Gu other than u and v, and equal to 0 otherwise."  So, it looks like you need to restrict neighbors to only being those neighbors in Gu.  And then you count how many pairs of nodes in ST satisfy 1) not connected and 2) no common neighbors other than u and v.  Gu_nodes=set(G[u]) set_uv=set([u,v]) for s in ST:   nbrs_s = Gu_nodes.intersection(G[s]) - set_uv  # nbrs of s in Gu that are not u or v   for t in ST-G[s]:   # mutual neighbors of u and v who are not directly linked to s     if nbrs_s.isdisjoint(G[t]):  # no common neighbors in Gu other than u and v                                          # G[t] includes nbrs of t not in Gu, but thats ok because nbrs_s doesn't.  [This is untested and I didn't read the article carefully--it just demonstrates the idea of using set methods to speed things up.]  Dan 
comment
Thanks for the note and question-- I have used this routine for analyzing circuits and I get the edges from the list of nodes.  But all my work has involved Graphs, not MultiGraphs.  If there are multiple edges between two nodes, then isn't that equivalent to a single edge with appropriately combined properties?  The current code doesn't do any of that combining.  Other thoughts: If you have two nodes with two edges between them, couldn't you add two new nodes part-way along each edge? Adding multiedges would create 2-node cycles...  not sure the algorithm works in this case. Dan  On Fri, Sep 20, 2013 at 8:41 PM, Ken Kundert notifications@github.comwrote:  > I was very excited when I found cycle_basis() because it seemed to be > exactly what I wanted. I am analyzing circuits and I needed the fundamental > loops of the circuit's graph. Then I discovered two issues. First, it only > works with Graph and not MultiGraph, and in general circuits have parallel > edges and so require MultiGraph. Second, cycle_basis() returns the nodes of > the loop rather than the edges, and I need the edges. If it were a Graph, I > guess I could derive the edges from the nodes, but that seems like a > tedious process, and is much more tricky when using a MultiGraph. >  > These issues seem to dramatically reduce the usefulness of cycle_basis() > in its stated area of applicability, so I am thinking I am missing > something important. This would not be surprising since I am new to > NetworkX. If that is the case, could you point me in the right direction? >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/958 > . 
comment
In the docs where it describes what subgraph-isomorphic http://networkx.github.io/documentation/latest/reference/algorithms.isomorphism.vf2.html it states that in this context subgraph means "node-induced subgraph".  So adding edges can change the isomorphic nature. In your example, the node-induced subgraph includes edges (like 4-6) that don't appear in the graph H.  I'm no expert in this VF2 formalism, but hope this helps. Dan  On Sat, Feb 15, 2014 at 7:20 PM, zzazzdsa notifications@github.com wrote:  > With certain graphs, subgraph isomorphism returns False even when it > should return True. For example, here is an example: >  > import networkx > import re > ingraph = "0 5 0 8 1 4 1 6 3 7 2 7 | 3 5 3 8 4 6 4 7 4 8 1 8 0 3 2 6" > H = networkx.Graph() > H.add_edge(0,1) > H.add_edge(1,2) > H.add_edge(3,4) > H.add_edge(4,5) >  > G = networkx.Graph() > a = re.findall(r'\d+', ingraph) > G.add_edge(int(a[0]),int(a[1])) > G.add_edge(int(a[2]),int(a[3])) > G.add_edge(int(a[4]),int(a[5])) > G.add_edge(int(a[6]),int(a[7])) > G.add_edge(int(a[8]),int(a[9])) > G.add_edge(int(a[10]),int(a[11])) > G.add_edge(int(a[12]),int(a[13])) > G.add_edge(int(a[14]),int(a[15])) > G.add_edge(int(a[16]),int(a[17])) > G.add_edge(int(a[18]),int(a[19])) > G.add_edge(int(a[20]),int(a[21])) > G.add_edge(int(a[22]),int(a[23])) > G.add_edge(int(a[24]),int(a[25])) > G.add_edge(int(a[26]),int(a[27])) > print G.edges() > isotest = networkx.algorithms.isomorphism.GraphMatcher(G,H) > t = isotest.subgraph_is_isomorphic() > print t >  > This returns False, even though the first six edges in ingraph form a copy > of H. Removing all but these six edges (by deleting the G.add_edge commands > after G.add_edge(int(a[10]),int(a[11])) gives G as an isomorphic copy of H, > which networkx rightly recognizes as true. However, according to > subgraph_is_isomorphic(), adding edges to a graph means isomorphic > subgraphs can disappear. >  > ##  >  > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/1062 > . 
comment
I think we should switch to using all sparse format matrices.  My reasons are similar to the desire to use iterators wherever we can.  It is easy to switch to dense matrices if needed and for large networks a dense matrix doesn't make sense.   I think all our routines that require input of a matrix work with either sparse or dense, but I'm not sure of that.  So  +1 for sparse matrix.   (Would changing them all to being sparse have to wait for version 2.0?  If so, maybe a short-term keyword argument would be worthwhile.)  Dan  On Sun, Jul 28, 2013 at 10:13 AM, Aric Hagberg notifications@github.comwrote:  > This is a good idea. >  > In fact we could have sparse matrix versions of other functions too e.g. > adjacency_matrix() and laplacian_matrix(). >  > There are some design choices for the names and arguments for those > functions and I don't think we have come to a decision on how to do it. We > could do as you suggest and add a keyword argument whether you want a > sparse or dense matrix format. Or we could use two different functions like > we do in convert.py for generating adjacency matrices. Once you have a > sparse matrix S you can easily get a dense matrix by calling D = > S.todense() so that is another option (only have sparse matrix formats). >  > So before we incorporate this code I'd like to come to a consensus on how > to proceed with sparse matrices. We can comment here or maybe the mailing > list too. >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/913#issuecomment-21684094 > . 
comment
Sometimes it is better to use sparse and sometimes it is better to use dense.  We could try to include code that chooses, but perhaps its better to have the user decide.  Also, in looking at the PR, there is a function smat used.  What is that? or did I read it wrong. Dan  On Sun, Oct 6, 2013 at 8:35 PM, chebee7i notifications@github.com wrote:  > Also, since we don't have a hard dependency on scipy, maybe it would be > better to make another function? >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/969#issuecomment-25780390 > . 
comment
The recursive version is still there because it can be used to test the newer version. The only reasons to move away from the recursive version are speed and memory. I attempted to do that last June but it looks like the logic in the unblocking routine is incorrect allowing nodes to be unblocked when they shouldn't be.  Short term fix is to use the recursive routine. Ill take a look and see how long it takes to get my head back into this. 
comment
Thanks for the bug report.  I found an error in the logic for unblocking nodes and put in a fix in pull request 1042. I'll close this, but reopen (or create a new one) if you have trouble or just questions. 
comment
I'm intrigued by your suggestion for namespace rearrangement   according to type of graph object.  We could for example, have the   current code structure (organized by class/algorithm/etc) and still   present an alternative namespace structure so that someone who knows   they will use only undirected multigraphs could do:  import  networkx.multigraph_only as nx  Then only the functionality valid for multigraphs would be available   via nx.    This would pollute the usual namespace with 4 extra   names:  graph_only, digraph_only, multigraph_only,   multidigraph_only.  But it would provide a nice way to limit   confusion over what functions work with which classes.  I believe the   sphinx docs would also automagically provide a branch of the docs   listing functions valid for each class of graph.  To set this up it   might take only four new modules that simply import functions from   the existing modules.  I might be missing other implications--  especially to what sphinx would do. 
comment
I use benchmark,py to check relative timing of different base classes, and its convenient to have it versioned and all, but I suspect no one else uses it and I can certainly keep it in my personal versioned repos if it doesn't make sense to keep it in networkx.testing :) Dan  On Sat, Oct 26, 2013 at 12:03 PM, chebee7i notifications@github.com wrote:  > So should I update .travis.yml to only report coverage for 2.7? nose will > still output a coverage report for each build, but its our choice as to > whether to submit it or not. One point: I've noticed that coveralls > sometimes takes a while to show the coverage level. Does coveralls.iohave a rule such as "report coverage level only after 50% of all builds > have passed"? If so, then we might need to report more than just 2.7. We > can certainly try it out. >  > Also, I saw benchmark.py in networkx.testing. Do we need this file any > more? >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/993#issuecomment-27149220 > . 
comment
I think your recollection about the dangers of tuples was our thinking long ago.  But I don't recall that we specifically discussed it in convert.py--maybe it was a broader discussion (about nbunch?).  The danger: If someone uses a tuple as a node (e.g. for a grid-graph), how can we distinguish between a tuple edge_list and a tuple node?  That issue doesn't arise with someone using lists as nodes because lists aren't hashable so can't be nodes.  In the most dangerous case, someone builds a graph of relationships between edgelists of another graph.  The nodes are edgelists (turned into tuples so they are hashable) so it is hard to distinguish nodes from edgelists.  Convert: But I think in convert.py it is Ok to assume the input is an edgelist if it can be.  We aren't looking for nodes as input.  It can be a NX-Graph, AGraph, dict-of-dict, edgelist, or numpy/scipy array.  Since we're not looking for nodes, we can treat it like an edgelist if it looks like one.  We can still allow edgelists to be nodes and that shouldn't impact this function.  Actually, inputing nx.Graph is similar to inputting an edgelist.  Both are allowed to be nodes.  But in this context we don't accept nodes as input.  So we can treat the input as "what it looks like".  In a future version it might be good to have arguments like: nx.Graph(nodes=None, edges=None, data=None)   At least that was proposed once a few years ago.  That would give convert.py more info about the type of input.    Summary:  It looks like this PR is a reasonable approach and would not introduce unwanted "surprises". Dan  On Oct 17, 2013, at 9:50 AM, Aric Hagberg wrote:  > That seems like a reasonable suggestion. I thought we forbid using a tuple for a reason but I couldn't come up with that right now or from looking through the code revisions. It may be a corner case because we using 2- and 3-tuples for edges. Or e.g. this behavior >  > In [36]: list((0,1),) > Out[36]: [0, 1] > @dschult do you have comments or ideas on this? >  > — > Reply to this email directly or view it on GitHub. 
comment
So, you've got ordered adjlists, but not ordered edges.  Am I understanding correctly?  To get ordered edges you'd have to keep track across the dicts which was added first (perhaps with a list of edge-tuples).  What application uses ordered adjlists? Dan  On Sat, Oct 12, 2013 at 11:18 PM, Pontus Stenetorp <notifications@github.com  > wrote: >  > Since having ordered edges for a DiGraph is pretty much a deal-breaker for > me, I hacked it in. I am leaving my quick hack here, it may be useful for > someone: >  > class OrderedEdgesDiGraph(DiGraph): >     def add_node(self, n, attr_dict=None, **attr): >         raise NotImplementedError >  > ``` > def add_nodes_from(self, nodes, **attr): >     raise NotImplementedError >  > def add_edge(self, u, v, attr_dict=None, **attr): >     if attr_dict is None: >         attr_dict=attr >     else: >         try: >             attr_dict.update(attr) >         except AttributeError: >             raise NetworkXError(\ >                 "The attr_dict argument must be a dictionary.") >     if u not in self.succ: >         self.succ[u] = OrderedDict() >         self.pred[u] = OrderedDict() >         self.node[u] = {} >     if v not in self.succ: >         self.succ[v] = OrderedDict() >         self.pred[v] = OrderedDict() >         self.node[v] = {} >     datadict=self.adj[u].get(v,{}) >     datadict.update(attr_dict) >     self.succ[u][v]=datadict >     self.pred[v][u]=datadict >  > def add_edges_from(self, ebunch, attr_dict=None, **attr): >     raise NotImplementedError > ``` >  > This isn't the pretty, but since the vanilla dictionary type is explicitly > assigned in so many places I simply don't know of a better way. I left > place-holders for add_node, add_nodes_from and add_edges_from to reduce > the code footprint, these can of course trivially be added in the same way > that I did for add_edge. Side-note, I am currently using NetworkX 1.7. >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/439#issuecomment-26210706 > . 
comment
That will mess with other routines that expect the capitalization.  Looks like make_str should just be defined for the gexf case explicitly in gexf.py instead of importing from utils.  Seems to me that when the format has a specified need for lower case we should handle it specifically in that readwrite module. 
comment
You should ask this on the networkx-discuss email list.   https://groups.google.com/forum/#!forum/networkx-discuss It would probably also help to give a little more context for your question.  For example, weight usually refers to edge weights but an independent set has no edges. 
comment
easy_install matplotlib  should do the trick.  NetworkX has many parts that don't require any other libraries, so importing it doesn't require installation of the other libraries.  But depending on what you plan to do it is helpful to have matplotlib, numpy and scipy.  On Fri, Jul 19, 2013 at 7:22 PM, mh602653 notifications@github.com wrote:  > Hi everyone, >  > I used easy_install to install networkx on python 2.7. I think everything > went well but now that I am trying to use the module I am getting the > folliwing error: >  > "ImportError: No module named matplotlib.pyplot" >  > Note that I am importing "import networkx as nx" with now problem! >  > Any help is much appreciated here. >  > Thanks, >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/904 > . 
comment
I'm going to close this.... If you get a small example that produces the error, reopen this or open a new issue. 
comment
I'm going to close this since #899 should have fixed it. 
comment
I think you get this behavior if one of your nodes is the special value:   None It's true for Graph and DiGraph.  We have Docs saying that None should not be used as a node, but it is not enforced due to performance issues.  We could revisit those performance issues, but the fix is to not use None as a node.  g=nx.Graph() G.add_node(None) G.size()  Traceback (most recent call last):   File "<stdin>", line 1, in <module>   File "/Users/dschult/NXHG/networkx/classes/graph.py", line 1624, in size     s=sum(self.degree(weight=weight).values())/2 AttributeError: 'int' object has no attribute 'values' 
comment
Hi and Thanks for the pull request!  I think we set up the NetworkXError to avoid people accidentally giving an undirected graph and having them think it is acyclic.  I guess returning False is helpful if you have a collection of graphs some directed and some undirected that you want to look through for acyclic DAGs.  Could you describe your intended use-case for the function?  How would it help you to return False for undirected graphs? Dan  On Sat, Jul 13, 2013 at 5:00 PM, Alex Roper notifications@github.comwrote:  > ...than raising NetworkXError. Tests updated to expose old behavior and >  > ## verify change. >  > You can merge this Pull Request by running >  >   git pull https://github.com/calmofthestorm/networkx is_directed_acyclic_fix >  > Or view, comment on, or merge it at: >  >   https://github.com/networkx/networkx/pull/898 > Commit Summary > - is_directed_acyclic_graph now returns false on undirected graphs >   rather than raising NetworkXError. >  > File Changes > - _M_ networkx/algorithms/dag.pyhttps://github.com/networkx/networkx/pull/898/files#diff-0(2) > - _M_ networkx/algorithms/tests/test_dag.pyhttps://github.com/networkx/networkx/pull/898/files#diff-1(7) >  > Patch Links: > - https://github.com/networkx/networkx/pull/898.patch > - https://github.com/networkx/networkx/pull/898.diff 
comment
Thanks for the clear explanation.  I misspoke-- I meant that a user might think an undirected graph is cyclic, not that they might think it is acyclic.  You are correct that something like not_implemented_for might be more appropriate.  I'll look into it more.  On Tue, Jul 16, 2013 at 9:30 AM, Alex Roper notifications@github.comwrote:  > I'm working on a project where I need to find shortest paths in DAGs, > digraphs, and graphs (with various domain knowledge). In the dags, I want > to use the simple linear algorithm based on topological sort. Since I'm > also working with undirected graphs and cyclic digraphs, I wrote myself a > wrapper that will just find the shortest path by the most efficient method > possible. If it's not a DAG I just want to use the networkx dijkstra, but > if it's a DAG I want to use my topological sort one. I also want to add > clear, specific assertions to my code because this is somewhat messy > research code and I want to catch type errors ASAP. >  > I was confused when I was getting NetworkXErrors from it as well, as I > expected it to simply return False for a undirected graph. >  > To work around the behavior in question, I used the conditional > G.is_directed() and nx.is_directed_acyclic_graph(G) which does work but > seems redundant to me; the other option would be to catch NetworkXError, > but this is such a broad error that I'd need to test the string. >  > I'm not sure I follow what you mean about people thinking a undirected > graph is acyclic if they give it to the function: wouldn't either returning > False or throwing an error prevent this confusion? Or if it's only intended > to be applied to digraphs, would not_implemented_for (or something similar) > be more appropriate? >  > I agree it's easy enough to work around if you feel this isn't an > appropriate change, it did not occur to me that the throw might be desired > behavior, and given it's just a few lines I figured a pull request was just > as easy as opening an issue. >  > Thanks, > Alex >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/898#issuecomment-21041694 > . 
comment
That sounds good...   I've checked the other functions in dag.py.   descendants() and ancestors() work for the undirected case. Other than topological_sort, that leaves only is_aperiodic() which currently raises NetworkXError("is_aperiodic not defined for undirected graphs")  So I'm going merging this PR. 
comment
Here is a stackoverflow conversation with some reasonable links discussing how to rewrite a recursive algorithm as a generator/iterator function.  http://stackoverflow.com/questions/1549943/design-patterns-for-converting-recursive-algorithms-to-iterative-ones The need for it with this function is also discussed in https://github.com/networkx/networkx/pull/874 
comment
As long as you don't change the graph while you call those two   iterators, they will give the same ordering of the nodes.  The order   that the nodes are retrieved is the same as for two dicts (pred and   succ) that have the same keys added in the same order.  So, unless   you're doing wacky things directly to the data structure, the order   will be the same for those iterators and izip should work fine. Dan 
comment
Wow-- hash randomization sounds like a fairly subtle yet important   change.  Makes me wonder if we rely on dict order anywhere in   networkx.  I don't think so, but not sure.  It doesn't sound easy to   test for either.  Right now hash randomization is only used for str, bytes and datetime   objects.  If anyone is interested in avoiding it, you can set an   environment variable "HASHRANDOMIZATIONSEED" to an integer to make it   consistent from run to run.  See  http://docs.python.org/3/using/  cmdline.html#envvar-PYTHONHASHSEED 
comment
Hi,  I started up python, imported networkx (1.7), created a Graph and   add_edges_from your list of edges. The cycle_basis routine returned without issues.  3 cycles total.  I've got python 2.7 but I don't think that would be the issue. Any hints as to what else might be going on? Can you try just creating the graph from those edges and calling   cycle_basis immediately? Dan  On Mar 5, 2013, at 8:07 AM, alxtoth wrote:  > Hi, >  > Using NetworkX 1.7 with built-in python 2.6 on OS X 10.6.8 /   > Intel . Same happens with python 2.7 built from HomeBrew. >  > The edges of my undirected graph (called g) are : [('cc', 'cc'),   > ('cc', 'per'), ('cc', 'cat'), ('cl', 'per'), ('ctr', 'per'),   > ('per', 'cat'), ('cat', 'cat')] >  > When calling networkx.cycle_basis(g) the script gets stuck in   > infinite loop. >  > Please note that this happens in rare cases, and in general   > networkx.cycle_basis(g) works fine on other undirected graphs with   > cycles and with edges from- and to- same node. >  > Alex >  > — > Reply to this email directly or view it on GitHub. 
comment
Hi, I'm not sure what all else your safety code is doing.  Can you just run the following?  [in a new python session] import networkx as nx g=nx.Graph() g.add_edges_from([('cc', 'cc'), ('cc', 'per'), ('cc', 'cat'), ('cl', 'per'), ('ctr', 'per'), ('per', 'cat'), ('cat', 'cat')]) print nx.cycle_basis(g)  On Mar 5, 2013, at 9:59 AM, alxtoth wrote:  > Hi, >  > Thanks for the prompt reply. >  > I have a larger piece of Python code, and a watchdog kill after 5 seconds to handle similar issues in other parts of code. The watchdog produces a stacktrace. Please find the bellow the stacktrace, and the piece of code printing the graph's edges, nodes, and the message that is never reached. >  > The code fragment never reaches the sys.exit(), rather gets stuck in the loop and is killed by watchdog timeout. I believe printing the graph's edges, nodes adorned with 10 star chars, and sys.exit() properly delimit the code fragment in question >  > [code] > def break_cycles(g): > print g.edges(), '_'_10 > print g.nodes(), '_'_10 > cycles = networkx.cycle_basis(g) # this is line nr 13 > sys.exit() > ... > [/code] >  > The output: > [code] > maboo:proper alex$ python n.py tests/test52.sql  > ########## tests/test52.sql > [('cc', 'cc'), ('cc', 'per'), ('cc', 'cat'), ('cl', 'per'), ('ctr', 'per'), ('per', 'cat'), ('cat', 'cat')] ********** > ['cc', 'cl', 'ctr', 'per', 'cat'] ********** >  > **\* STACKTRACE - START *** > ,tests/test52.sql >  > ThreadID: 140735073041600,tests/test52.sql >  > File: "n.py", line 51, in ,tests/test52.sql >  > res = sqlparser.process(sql),tests/test52.sql >  > File: "/Users/alex/Documents/transfer/revj/proper/sqlparser.py", line 556, in process,tests/test52.sql >  > return unsafe_process(sql),tests/test52.sql >  > File: "/Users/alex/Documents/transfer/revj/proper/sqlparser.py", line 547, in unsafe_process,tests/test52.sql >  > return output_dot(d, starts_ends, newCols, join_dict),tests/test52.sql >  > File: "/Users/alex/Documents/transfer/revj/proper/dotoutput.py", line 274, in output_dot,tests/test52.sql >  > aliases),tests/test52.sql >  > File: "/Users/alex/Documents/transfer/revj/proper/dotoutput.py", line 77, in order_joins,tests/test52.sql >  > graph_order = peel_graph(g),tests/test52.sql >  > File: "/Users/alex/Documents/transfer/revj/proper/dotoutput.py", line 30, in peel_graph,tests/test52.sql >  > g = break_cycles(g),tests/test52.sql >  > File: "/Users/alex/Documents/transfer/revj/proper/dotoutput.py", line 13, in break_cycles,tests/test52.sql >  > cycles = networkx.cycle_basis(g) # this is line nr 13,tests/test52.sql >  > File: "", line 2, in cycle_basis,tests/test52.sql >  > File: "/Library/Python/2.6/site-packages/networkx-1.7-py2.6.egg/networkx/utils/decorators.py", line 63, in _not_implemented_for,tests/test52.sql >  > return f(_args,_*kwargs),tests/test52.sql >  > File: "", line 2, in cycle_basis,tests/test52.sql >  > File: "/Library/Python/2.6/site-packages/networkx-1.7-py2.6.egg/networkx/utils/decorators.py", line 63, in _not_implemented_for,tests/test52.sql >  > return f(_args,_*kwargs),tests/test52.sql >  > File: "/Library/Python/2.6/site-packages/networkx-1.7-py2.6.egg/networkx/algorithms/cycles.py", line 96, in cycle_basis,tests/test52.sql >  > cycle.append(p),tests/test52.sql >  > File: "/Users/alex/Documents/transfer/revj/proper/watchdog.py", line 20, in handler,tests/test52.sql >  > self.trc(),tests/test52.sql >  > File: "/Users/alex/Documents/transfer/revj/proper/watchdog.py", line 28, in trc,tests/test52.sql >  > for filename, lineno, name, line in traceback.extract_stack(stack):,tests/test52.sql >  > **\* STACKTRACE - END *** > ,tests/test52.sql > watchdog kill > [/code] >  > — > Reply to this email directly or view it on GitHub. 
comment
Yes, I'm sure this dijkstra works for multigraphs.  You can look at   the code.  We treat multigraphs differently from graphs.  It is still true that G.**getitem** returns different things for   multigraphs and graphs.  If you want a single interface for both,   probably G.edges() is the way to go.  On Feb 28, 2013, at 12:17 AM, jamiefolson wrote:  > Are you sure this fixed the issue? **getitem** simply returns adj  > [n], which for multigraphs, is not a dict of edges but a dict of   > dicts of edges. So when the "weight" is retrieved by   > _single_source_dijkstra_path_basic it instead retrieves the edge   > data for the edge add with key weight. >  > — > Reply to this email directly or view it on GitHub. 
comment
I found the code you refer to.  That looks like a bug in the betweenness.py code which has included its own version of dijkstra.  I thought you were referring to the networkx single_source_dijkstra methods (and friends) which live in algorithms/shortest_path/weighted.py.   [That's what this ticket originally was about.]  So, this is a new issue and it affects the betweenness routines. I'm pretty sure that this basic version of dijkstra is there because it is/was significantly faster than checking for a multigraph.  It would be faster to change the multigraph to a graph before calling betweenness.  That way you get to decide how to handle the weights for paths across multiedges.  In any case, we should either flag these routines as not working with multigraphs or make them work for multigraphs.  Thanks!   You can open a new issue to fix mutigraphs on betweenness functions.  Or I'll open one eventually. Dan  On Feb 28, 2013, at 10:11 AM, jamiefolson wrote:  > In _single_source_dijkstra_path_basic, I see: >  > ``` >     for w,edgedata in G[v].items(): >         vw_dist = dist + edgedata.get(weight,1) > ``` >  > With no alternative for multigraphs. What am I missing? >  > — > Reply to this email directly or view it on GitHub. 
comment
Hi!   Thanks for the good idea.   2 questions: - could this work for non-DAGs?  undirected graphs?  multigraphs? - Is there a way to avoid the check for DAG at the beginning which slows it down a lot for large graphs? 
comment
This looks interesting.    Potential trouble with subclasses of Graph though. We need to think about DiGraph and be careful with MultiGraph and MultiDiGraph too. I'm not sure that itertools.combinations works for DiGraph.   (nor itertools.combinations_with_replacement due to self-loops)  Dan  On Oct 21, 2012, at 8:37 AM, Enrico Giampieri wrote:  > In several Network type you receive the list of cliques (collaboration networks, movie databases, etc). > Having a nice syntax to add the whole clique, using a good performing algorithm (the function combination from the itertools module ) could give a real boost in easiness of creation of such networks. >  > I will add some test for the function as soon as i get used to the nose testing structure. >  > You can merge this Pull Request by running: >  >   git pull https://github.com/EnricoGiampieri/networkx Graph-extend-big-data > Or view, comment on, or merge it at: >  >   https://github.com/networkx/networkx/pull/780 >  > Commit Summary >  >   • add a new function to the Graph class, add_clique > File Changes >  >   • M networkx/classes/graph.py (33) > Patch Links >  >   • https://github.com/networkx/networkx/pull/780.patch >   • https://github.com/networkx/networkx/pull/780.diff > — > Reply to this email directly or view it on GitHub. 
comment
Name Issue: The proposed nx.reverse() function accesses the data structure directly when copy=False and I think that qualifies it for placement in the class as a method.  For example, if we get a matrix representation into a graph class, it may not have a succ or pred attribute and we would want to "transpose" instead.  So the nx.reverse would need to be changed.  We could add the function though and have it call the class method (similar to subgraph).  That's what I would prefer to do.  As for the namespace problem where subclasses could be tempted to use the name "reverse", we could rename the method "reverse_edges", as it more precisely describes what happens.  Any change in API is difficult for backward compatibility, but can be managed.  Maybe we can start a list of name changes for version 2.0.  For the moment it is probably easier to keep it as "reverse" until 2.0.  Extra functionality: I like the doc_string proposed in the comment https://github.com/networkx/networkx/pull/744#issuecomment-7531770  better than in the pull request.  The code is fairly simple and the docstring is almost the same as we have, but more complete in that it handles the remaining input case. 
comment
I don't think there is anything holding this up except my inexperience with github.  I know I can just hit a button to merge the pull request, but am not comfortable doing so without doing the merge locally and playing with it a little.  But, I am working on getting the github workflow into my brain.    So, I think rather than having Aric and Jordi be in charge of all the pull requests it would be better for you to merge this request yourself.  I have looked through all the code/diffs and it all makes sense and seems in good order.  So a second pair of eyes have looked at it.   Thanks, Dan  On Sep 3, 2012, at 12:30 AM, chebee7i wrote:  > Is there anything holding this up? I'd like to get it merged it before other changes to shortest_path are brought in, such https://github.com/networkx/networkx/pull/762. I've purposely held off on merging it myself...but if we're okay with pull requesters merging their own requests, I'll do so. >  > — > Reply to this email directly or view it on GitHub. 
