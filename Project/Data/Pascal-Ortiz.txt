issue
Center(s), centroid(s) of a tree#TITLE_END#NetworkX has a function to find the [center](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.distance_measures.center.html?highlight=center#networkx.algorithms.distance_measures.center) of a graph. This function has quadratic complexity. The case where the graph is a tree is a special case where it is known that there exists a [linear algorithm](http://www.cs.kent.edu/~dragan/ST/papers/HEDETNIEMI-COCKAYNE-HEDETNIEMIR-83.pdf). AFAIK, NetworX doesn't provide a tree-dedicated function. Do you think this algorithm could be useful to add? In the same vein, could be useful to provide a function to compute the centroid(s) of a tree.
issue
Floyd-Warshall Optimization#TITLE_END#The actual implementation of the Floyd-Warshall algorithm can be made 33% faster with a very simple optimization inside the triple-nested loop :  change everywhere it is worth expression like `dist[u][v]` by `D[v]` where `D=dist[u]` to avoid multiple evaluations of `dist[u]` (`dict` is a dictionary).  Here is my complete test:  ``` from time import clock import networkx as nx  def random_edges(n, density, max_weight=100):     from random import randrange     M=n*(n-1)//2     m=int(density*M)     edges=set()     wedges=[]     while len(edges)<m:         L=(randrange(n),randrange(n))         if L[0]!=L[1] and L not in edges:             w=float(randrange(max_weight))             wedges.append(L+(w,))             edges.add(L)     return wedges   def floyd_warshall_predecessor_and_distance(G, weight='weight'):     # from NetworkX source-code     from collections import defaultdict     dist = defaultdict(lambda: defaultdict(lambda: float('inf')))     for u in G:         dist[u][u] = 0     pred = defaultdict(dict)     undirected = not G.is_directed()     for u, v, d in G.edges(data=True):         e_weight = d.get(weight, 1.0)         dist[u][v] = min(e_weight, dist[u][v])         pred[u][v] = u         if undirected:             dist[v][u] = min(e_weight, dist[v][u])             pred[v][u] = v     for w in G:         for u in G:             for v in G:                 if dist[u][v] > dist[u][w] + dist[w][v]:                     dist[u][v] = dist[u][w] + dist[w][v]                     pred[u][v] = pred[w][v]     return dict(pred), dict(dist)   def floyd_warshall_predecessor_and_distance_opt(G, weight='weight'):     # Optimised version     from collections import defaultdict     dist = defaultdict(lambda: defaultdict(lambda: float('inf')))     for u in G:         dist[u][u] = 0     pred = defaultdict(dict)     undirected = not G.is_directed()     for u, v, d in G.edges(data=True):         e_weight = d.get(weight, 1.0)         dist[u][v] = min(e_weight, dist[u][v])         pred[u][v] = u         if undirected:             dist[v][u] = min(e_weight, dist[v][u])             pred[v][u] = v     for w in G:         Dw=dist[w]         Pw=pred[w]         for u in G:             Du=dist[u]             Pu=pred[u]             for v in G:                 d=Du[w] + Dw[v]                 if Du[v] > d:                     Du[v] = d                     Pu[v] = Pw[v]     return dict(pred), dict(dist)  def test():     density=0.2     n=400     wedges=random_edges(n, density)     G = nx.DiGraph()     G.add_weighted_edges_from(wedges)     return G  G=test()  start=clock() D3, D4=floyd_warshall_predecessor_and_distance(G) end=clock() print("Current  \t: %.2f" %(end-start))  start=clock() D1, D2=floyd_warshall_predecessor_and_distance_opt(G) end=clock() print("Optimised\t: %.2f" %(end-start))  # checking print(D1==D3 and D2==D4)  ```  outputting:  ``` Current         : 9.15 Optimised       : 6.07 True ```
issue
Prim's MST implementation very slow#TITLE_END#     Prim's MST implementation in NetworkX is very slow in comparison to Kruskal's one. The performance ratio is pretty high: the higher is the number of nodes, the bigger is the ratio. For instance   ```python from random import randrange from time import perf_counter import networkx as nx  n = 4000 p = 0.25 G = nx.erdos_renyi_graph(n, p)  for e in G.edges():     G.edges[e]["weight"] = randrange(1, 10)  start = perf_counter() T = nx.minimum_spanning_tree(G, algorithm='prim') prim = sum(T.edges[e]["weight"] for e in T.edges()) prim_time=perf_counter() - start print("Prim : %.2fs" %prim_time)  start = perf_counter() T = nx.minimum_spanning_tree(G, algorithm='kruskal') kruskal = sum(T.edges[e]["weight"] for e in T.edges()) kruskal_time=perf_counter() - start print("Kruskal : %.2fs" %kruskal_time)  print("Ratio: %.1f" %(prim_time/kruskal_time))  # checking print("Check:", prim == kruskal) ```  outputting ```bash Prim : 123.26s Kruskal : 6.55s Ratio: 18.8 Check: True ```     Looking at the Prim's algorithm source code (`mst.py` file), I found 5 spots possibly causing slow execution (cf. the `prim_mst_edges` function).  Here is the first one  ```python     nodes = list(G)     c = count()      sign = 1 if minimum else -1      while nodes:         u = nodes.pop(0) ```  At the last line, the `pop(0)` remove is not free: `nodes` data structure is a list hence removing at the front has linear cost and since this operation is done inside a `while` loop over the whole graph, this leads to a **quadratic slowdown**.   Later in the code and still inside the `while` loop, we can spot the following:  ```python             nodes.remove(v) ```              A `remove` operation on a list has linear complexity so the above line leads again to quadratic slowdown.  And finally, we can spot:  ```python v in visited ```  (this occured 3 times in the code) and since `visited` is a list, this leads again to quadratic slowdown.  To fix the issue, it suffices to change every list to a **set** data structure. I have done some experiments and this causes a nice speedup,  execution is now close to Kruskal's (ratio around 1.2)          
issue
Prim from list to set#TITLE_END#Fixes issue #3502.   Made the lists into sets. Prim and Kruskal implementations have now equivalent time execution. 
issue
Floyd-Warshall Optimization#TITLE_END#This is a continuation of my unfinished PR #3400. It implements a low level and very simple optimization of the `floyd_warshall_predecessor_and_distance()` function in order to avoid multiple recomputations of dictionary references inside of the inner loops.    I abreviated `pred[w]` by `Pw`.  As noticed by Dan Schult, the variable's name is not very suggestive but the shortcut is only for performance and has no semantics and this kind of shortcut is commun, see for instance [this code](https://github.com/networkx/networkx/blob/master/networkx/algorithms/communicability_alg.py#L94) from NetworkX.  I have done some benchmarks against random graphs from 20 up to 4000 nodes and for various densities: performances are always better, and on my machine the gain is from 22% up to 38%, cf. benchmark code at the end of this message.  In the original code, predecessors are stored in a default dictionary `pred`. As a consequence, the shortcut `Pw = pred[w]` has sometimes an unexpected side effect: it creates an empty dictionary that doesn't appear in the return value of the function. Networkx test doesn't detect this error. So I added a test to cover this case.   ```python # Benchmark code from collections import defaultdict  from time import perf_counter import networkx as nx  def random_edges(n, density, max_weight=100):     from random import randrange     M=n*(n-1)//2     m=int(density*M)     edges=set()     wedges=[]     while len(edges)<m:         L=(randrange(n),randrange(n))         if L[0]!=L[1] and L not in edges:             w=float(randrange(max_weight))             wedges.append(L+(w,))             edges.add(L)     return wedges    def floyd_warshall_predecessor_and_distance_opt(G, weight='weight'):     # Optimised version     from collections import defaultdict     dist = defaultdict(lambda: defaultdict(lambda: float('inf')))     for u in G:         dist[u][u] = 0     pred = defaultdict(dict)     undirected = not G.is_directed()     for u, v, d in G.edges(data=True):         e_weight = d.get(weight, 1.0)         dist[u][v] = min(e_weight, dist[u][v])         pred[u][v] = u         if undirected:             dist[v][u] = min(e_weight, dist[v][u])             pred[v][u] = v     for w in G:         Dw=dist[w]         Pw=pred[w]         for u in G:             Du=dist[u]             Pu=pred[u]             for v in G:                 d=Du[w] + Dw[v]                 if Du[v] > d:                     Du[v] = d                     Pu[v] = Pw[v]     return {u:pred[u] for u in G if pred[u]}, dict(dist)   def read(filename):     lines=open(filename).read().strip().split('\n')     n=int(lines[0])     m=int(lines[1])     lines=lines[2:]     L=[line.split() for line in lines]     return n, [(int(a), int(b), float(w)) for (a,b, w) in L]  def wedges2graph(wedges):      G = nx.DiGraph()     G.add_weighted_edges_from(wedges)     return G  def perf(n, density, repeat=1):     cumul=cumul_opt=0     check=True     for _ in range(repeat):         wedges=random_edges(n, density)         G=wedges2graph(wedges)                  start= perf_counter()         pred, dist=nx.floyd_warshall_predecessor_and_distance(G)         cumul+= perf_counter()-start          start_opt= perf_counter()         pred_opt, dist_opt=floyd_warshall_predecessor_and_distance_opt(G)         cumul_opt+= perf_counter()-start_opt                    check=(pred_opt==pred) and (dist_opt==dist)         if not check:             raise Exception     return cumul/repeat, cumul_opt/repeat   nnodes=[400] density=[0.2]  durations=[] repeat=1  for n in nnodes:     for d in density:         print("n =", n, "d =", d)         duration, duration_opt=perf(n, d, repeat)         durations.append((n, d, duration, duration_opt))         print("%.2f, %.2f" %(duration, duration_opt))         print("Gain: %.0f %%" %((duration-duration_opt)/duration*100))             Output:   n = 400 d = 0.2 9.01, 6.15 Gain: 32 % ```
issue
Prufer schemes: quadratic instead of linear complexity#TITLE_END#A tree with set vertices `range(n)` can be encoded by a sequence of `n - 2` integer in `range(n)` known as the tree Prufer sequence and conservely, any such sequence can be decoded into a unique tree.   The NetworkX module `/networkx/algorithms/tree/coding.py` implements the encoding and decoding schemes by mean of resp. the `to_prufer_sequence` function and `from_prufer_sequence` function. The docstring for each function references a 2009 paper describing linear time algorithms (and not n*log(n) as the docstring claims cf. ยง2 and ยง3 in the paper).   The problem is that performances of the implemented algorithms are unsatisfactory and the time complexity seems to be more quadratic than linear. It results for instance that decoding a Prufer sequence of lenght about 20000 requires 32s.   On the other hand, based on another paper, I implemented Prufer encoding and decoding functions and the previous Prufer sequence need only 0.01s to be decoded: the NetworkX implementation has a time execution problem.   To be more accurate, I have tested the NetworkX decoding function successively on   - data input of size 2000 - on data input of size 10 x 2000=20000   and time execution raises from 0.32s seconds to about 31.83s: this is a strong indication that the algorithm has quadratic complexity. Here is the test code :    ```python from time import clock from random import randrange import networkx as nx  def test_speed(n):     sequence=[randrange(3) for i in range(n-2)]      d=clock()     G=nx.from_prufer_sequence(sequence)     print("%d: %.2fs" %(n, clock()-d))          test_speed(2000) test_speed(20000)  ```  ``` 2000: 0.32s 20000: 31.83s ```    I suspect that the following part of the source code has not linear complexity, cf. the last line:   ```python # from NetworkX sources def from_prufer_sequence(sequence):      # ...         # code snipped     # ...       for v in sequence:         # ...         # code snipped         # ...            # the min function executes a loop         # so we have 2 nested loops of size n (sometimes)                  index = u = min(k for k in range(index + 1, n) if degree[k] == 1) ```    The `to_prufer_sequence` function has more or less the same problem.  
issue
Prufer with linear complexity#TITLE_END#This pull request solves the issue #3521. The new implementation has linear complexity and his based on the paper [Linear-time Algorithms for Encoding Trees as Sequences of Node Labels](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.5052&rank=1).    Time execution has been tested. Some results for a tree with 50,000 vertices:  | Functions | Previous implementation | New implementation | | :---:         | :---      | :---      | | `to_prufer_sequence`  |  44.30s    |   0.29s       | | `from_prufer_sequence`  |   83.65s    |   0.21s       |
issue
DAG shortest path#TITLE_END#Does NetworX provide a specific algorithm to solve the single-source shortest-paths problem on a weighted DAG? There is a [well known algorithm](https://www.boost.org/doc/libs/1_37_0/libs/graph/doc/dag_shortest_paths.html) solving this problem and it has better complexity than Dijkstra algorithm.   According to issue  #1161, seems that NetworkX doesn't provide it. Do you think this algorithm could be useful to add?
comment
In some cases, NetworkX provides his own Python implementation **and** a Numpy based implementation. For instance, this occurs for the Floyd-Warshall algorithm with a very smart and efficient implementation (see timings below).   By the way, another very interesting option is Numba + Numpy I'm experimenting for the moment: it provides sometimes still faster execution than  C++. Some  Floyd-Warshall  algorithm timings (in seconds) for a graph with parameters n=800, m=100000:  ``` Networkx                        : 82.66  Python (no dict)                : 34.58 Python  (no dict and multiprocessing)   : 21 Numpy from Networkx             : 1.67 (no predecessors calculation)  Numba-Numpy-nx                  : 1.33 Graph-tool                      : 0.91  Numba-Numpy-nx-paral            : 0.5 Scipy (Cython)                  : 0.50  C++                             : 0.30 Numba                           : 0.24 C++ threads                     : 0.14 Numba parallel                  : 0.11 C++ openMP                      : 0.09 ```  I tried Numba under GPU but didn't get nice results (I'm new to GPU programming).   
comment
I suppose that your graph is a directed one. In this case, `isolates(G)` are vertices with no in-neighbors **and** no out-neighbors. This is not what you need. In your case, you need to remove the **sources** of the graph ie vertices with no predecessor because the function divides by `len(in_neighbors_u) * len(in_neighbors_v)`.
comment
> I created a function that checks if there are nodes with no predecessors.  And you remove them from the graph?  >  > Still getting the ZeroDivisonError  Could you provide a [minimal working example](https://stackoverflow.com/help/minimal-reproducible-example)? 
comment
> I created a function that checks if there are nodes with no predecessors. >  > Still getting the ZeroDivisonError  Perhaps because removing sources create another source.  Here is a minimal example:  ```python from itertools import product import networkx as nx from numpy import array     def _is_close(d1, d2, atolerance=0, rtolerance=0):     """Determines whether two adjacency matrices are within     a provided tolerance.          Parameters     ----------     d1 : dict         Adjacency dictionary          d2 : dict         Adjacency dictionary          atolerance : float         Some scalar tolerance value to determine closeness          rtolerance : float         A scalar tolerance value that will be some proportion         of ``d2``'s value          Returns     -------     closeness : bool         If all of the nodes within ``d1`` and ``d2`` are within         a predefined tolerance, they are considered "close" and         this method will return True. Otherwise, this method will         return False.          """     # Pre-condition: d1 and d2 have the same keys at each level if they     # are dictionaries.     if not isinstance(d1, dict) and not isinstance(d2, dict):         return abs(d1 - d2) <= atolerance + rtolerance * abs(d2)     return all(all(_is_close(d1[u][v], d2[u][v]) for v in d1[u]) for u in d1)  def simrank_similarity(G, source=None, target=None, importance_factor=0.9,                        max_iterations=100, tolerance=1e-4):     """Returns the SimRank similarity of nodes in the graph ``G``.      SimRank is a similarity metric that says "two objects are considered     to be similar if they are referenced by similar objects." [1]_.          The pseudo-code definition from the paper is:          def simrank(G, u, v):             in_neighbors_u = G.predecessors(u)             in_neighbors_v = G.predecessors(v)             scale = C / (len(in_neighbors_u) * len(in_neighbors_v))             return scale * sum(simrank(G, w, x)                                for w, x in product(in_neighbors_u,                                                    in_neighbors_v))          where ``G`` is the graph, ``u`` is the source, ``v`` is the target,     and ``C`` is a float decay or importance factor between 0 and 1.          The SimRank algorithm for determining node similarity is defined in     [2]_.      Parameters     ----------     G : NetworkX graph         A NetworkX graph      source : node         If this is specified, the returned dictionary maps each node         ``v`` in the graph to the similarity between ``source`` and         ``v``.      target : node         If both ``source`` and ``target`` are specified, the similarity         value between ``source`` and ``target`` is returned. If         ``target`` is specified but ``source`` is not, this argument is         ignored.      importance_factor : float         The relative importance of indirect neighbors with respect to         direct neighbors.      max_iterations : integer         Maximum number of iterations.      tolerance : float         Error tolerance used to check convergence. When an iteration of         the algorithm finds that no similarity value changes more than         this amount, the algorithm halts.      Returns     -------     similarity : dictionary or float         If ``source`` and ``target`` are both ``None``, this returns a         dictionary of dictionaries, where keys are node pairs and value         are similarity of the pair of nodes.          If ``source`` is not ``None`` but ``target`` is, this returns a         dictionary mapping node to the similarity of ``source`` and that         node.          If neither ``source`` nor ``target`` is ``None``, this returns         the similarity value for the given pair of nodes.      Examples     --------     If the nodes of the graph are numbered from zero to *n - 1*, where *n*     is the number of nodes in the graph, you can create a SimRank matrix     from the return value of this function where the node numbers are     the row and column indices of the matrix::          >>> import networkx as nx         >>> from numpy import array         >>> G = nx.cycle_graph(4)         >>> sim = nx.simrank_similarity(G)         >>> lol = [[sim[u][v] for v in sorted(sim[u])] for u in sorted(sim)]         >>> sim_array = array(lol)      References     ----------     .. [1] https://en.wikipedia.org/wiki/SimRank     .. [2] G. Jeh and J. Widom.            "SimRank: a measure of structural-context similarity",            In KDD'02: Proceedings of the Eighth ACM SIGKDD            International Conference on Knowledge Discovery and Data Mining,            pp. 538--543. ACM Press, 2002.     """     prevsim = None      # build up our similarity adjacency dictionary output     newsim = {u: {v: 1 if u == v else 0 for v in G} for u in G}      # These functions compute the update to the similarity value of the nodes     # `u` and `v` with respect to the previous similarity values.     avg_sim = lambda s: sum(newsim[w][x] for (w, x) in s) / len(s)     sim = lambda u, v: importance_factor * avg_sim(list(product(G[u], G[v])))      for _ in range(max_iterations):         if prevsim and _is_close(prevsim, newsim, tolerance):             break         prevsim = newsim         newsim = {u: {v: sim(u, v) if u is not v else 1                       for v in newsim[u]} for u in newsim}      if source is not None and target is not None:         return newsim[source][target]     if source is not None:         return newsim[source]     return newsim   G = nx.DiGraph()  edges=[("A", "B"), ("B", "C")] G.add_edges_from(edges) in_degrees=G.in_degree(G.nodes()) print(in_degrees)  sim = simrank_similarity(G)  print(sim) ```  outputting:  ``` [('A', 0), ('B', 1), ('C', 1)] Traceback (most recent call last):   File "sim.py", line 161, in <module>     sim = simrank_similarity(G)   File "sim.py", line 145, in simrank_similarity     for v in newsim[u]} for u in newsim}   File "sim.py", line 145, in <dictcomp>     for v in newsim[u]} for u in newsim}   File "sim.py", line 145, in <dictcomp>     for v in newsim[u]} for u in newsim}   File "sim.py", line 138, in <lambda>     sim = lambda u, v: importance_factor * avg_sim(list(product(G[u], G[v])))   File "sim.py", line 137, in <lambda>     avg_sim = lambda s: sum(newsim[w][x] for (w, x) in s) / len(s) ZeroDivisionError: division by zero  ``` 
