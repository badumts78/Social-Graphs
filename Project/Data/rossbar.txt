issue
DOC: Add warning about special GEXF reserved keywords.#TITLE_END#Closes #8339 
issue
Policy for docstring consistency among graph classes#TITLE_END#This discussion was originally brought up in #5529 in the context of updating the `degree` method docstring for the MultiGraph class. The question then becomes: what should be done about the `degree` method for the other classes (`Graph`, `DiGraph`, etc.)? What is the best approach for keeping the class+method docstrings in sync (and correct from a typing perspective) while taking into account the differences between the classes. Original discussion from @dschult below:  ---  The 4 graph classes have overlapping and intentionally repetitive doc_strings across the classes. Whenever we change one, we need to change -- or at least be aware of -- the others. If we improve one, we should make that change to the others. If we put special comments about one graph class into one doc_string, we should try to keep that comment easily identified so that readers can recognize the rest as being the same.  For example, adding a "Note" to describe the differences rather than just changing one or two words in a paragraph in the doc_string.  There is a tension between keeping the doc_strings in all graph classes close to being the same, and adding the tweaks to make them specialized for each class.  When the docs are the same for a method that belongs to e.g. Graph, DiGraph, MultiGraph and MultiDiGraph, there is a unified presentation. Maintainers have fewer doc_strings to parse (but more to remember to update), and users can more easily recognize the docs when they look at the same method in another class and understand that the methods do the same thing.  When the docs differ, the presentation diverges over time but can be tailored to each class. Maintainers don't have to remember to go and change all 4 copies of the doc_string when one is changed (though maybe they should because it is probably better for all the classes). And users get docs that are specialized for that class. But they lose the ability to see what is the same across the classes.  You can see our attempts to keep the doc_strings the same in the Examples section just below the changes in this PR.  ```>>> G = nx.Graph()  # or nx.DiGraph() or nx.MuiltiGraph() or nx.MultiDiGraph()```  appears in all 4 graph classes. This way the user can understand that they all would give this same output after just reading one doc_string.  We could move toward making each graph class use the examples with *that* graph class. But I would claim that the doc_string would be ever so slightly less useful.  Perhaps we should change the return type to ```MultiDegreeView or DegreeView or OutDegreeView or MultiOutDegreeView or int```.  But that certainly doesn't seem clear. And it might mess up typing. We could try to add another line to the doc_string stating that the return type varies from one class to another but that they are basically the same.  If we choose to make the doc_strings differ by graph class, then we should maybe make the examples differ by graph class as well. That would test all the graph classes for the example in this doc_string (which we currently don't do). But someone will have to keep track of the doc_strings in the 4 classes and make sure that improvements in one get updated in the others.  What is the better philosophy about the methods for doc_strings of similar subclasses? I am reminded of the 7 subclasses in  scipy.sparse for the different types of sparse representations. 
issue
`approximate_current_flow_betweenness_centrality` `kmax` parameter not directly used in algorithm#TITLE_END#`approximate_current_flow_betweenness_centrality` has a parameter `kmax` which is currently only used to check that `k`, which is computed from the number of nodes in the graph and the user-provided `epsilon` value, is not above a certain user-specified threshold:  https://github.com/networkx/networkx/blob/ddb1cb663d1c0be293aaa4d4284eab641255014f/networkx/algorithms/centrality/current_flow_betweenness.py#L121-L123  This struck me as a strange pattern: having an argument that is only used as a threshold above which an exception is raised. There's nothing wrong with this since `k` is a derived quantity, but it could be possible that better documenting `epsilon` would be sufficient to remove the explicit check.  See #6143 for further discussion.
issue
Review the `.update` method of graph classes#TITLE_END#This is an action-item extracted from the discussion in #4208.  The graph classes have an `update` method which is intended to add the nodes/edges from one graph to another (similar to dict.update). There is a comment in #4208 that the implementation is not particularly easy to understand, nor is it 100% clear that it will work for all graph types (e.g. are multigraph edge keys preserved?). It would be a good idea to review the method, see if it can't be refactored to be more readable, and to ensure that testing is sufficient for all use-cases.
issue
Proposal: Add sphinx.extlinks extension for Wikipedia#TITLE_END#I was looking through built-in sphinx extensions this weekend and came across [sphinx.ext.extlinks](https://www.sphinx-doc.org/en/master/usage/extensions/extlinks.html), which is designed to make linking to common external resources simpler. The immediate external resource that comes to mind for networkx is Wikipedia, which we link to over 100 times in the docstrings (`grep -rI "wikipedia.org" . | wc -l -> 122`).  Adding this extension and a simple configuration allows us to replace the full external link target (e.g. `<https://en.wikipedia.org/wiki/Plug-in_(computing)>`) with the target name like so: ``:wiki:`Plug-in_(computing)` ``. I updated a couple instances in the developer docs to show what the new pattern looks like, and to prove that the results are correct!  I intend to use this PR to gauge interest so will hold off on going through *all* the links until we've had a chance to discuss. If folks like the idea then I'll go ahead and update the usage pattern throughout the docs.
issue
Fix formatting for release docs.#TITLE_END#Improved HTML rendering for the developer docs specifying the release process.
issue
Minor documentation build improvements#TITLE_END#Two minor updates while looking at the latest doc build:  1. Updates the intersphinx timeout so that it doesn't block for an inordinately long time when other projects' `objects.inv` are unreachable. Generally not important, but was an issue during the recent AWS outage. Should have no impact at all for normal operation.  2. Fix sphinx warning related to underline length.
issue
MAINT: Remove unused sphinx extensions from conf.py#TITLE_END#I noticed this while investigating #8313   `conf.py` registers three extensions that are not used at all in the docs build process:  1. [sphinx.ext.doctest](https://www.sphinx-doc.org/en/master/usage/extensions/doctest.html)  2. [sphinx.ext.coverage](https://www.sphinx-doc.org/en/master/usage/extensions/coverage.html)  3. [sphinx.ext.todo](https://www.sphinx-doc.org/en/master/usage/extensions/todo.html)  The first two allow running the doctests (+coverage) *via sphinx*. Note that this is completely independent from `pytest --doctest-modules` and has no effect on the recommended testing workflow(s). What `sphinx.ext.doctest` allows you to do is run `make doctest` in the docs directory. However, we do not actually set up our docs fully enable this usage, so if users do so they will get failures (e.g. from not having the `skip_numpy` stuff set up for sphinx doctests). In principle this could be set up, but the contributor guide recommends using `pytest` for running doctests and I think that is definitely the right thing to do. Removing this (and the coverage) extension simply disables the already non-working `make doctest` sphinx-based testing workflow.  The latter extension adds `.. todo::` and `.. todolist::` directives. I grepped through the codebase to confirm that these are not used anywhere (nor should they be IMO - that's what the issue tracker is for!). Therefore I think it is safe to remove this as well.  Finally, I added a comment to the `suppress_warnings` configuration because I tried removing it and ended up reminding myself why it's there in the first place!
issue
Rm 3D layout and animation from greedy_color example.#TITLE_END#The plot_greedy_coloring.py example contains multiple different renderings of the graph, including a 3D rendering and a 3D rotation animation. These latter two contribute significantly to the doc build time on CI and largely overlap with the 3D drawing/animation examples in the gallery.
issue
Add/bump Python 3.14 to testing matrices.#TITLE_END#Thanks for the reminder @eriknw !  Now that 3.14 is out, I've added it (IMO judiciously) to several testing matrices. I largely targeted the default dependencies, though we could get more aggressive with the other workflows as well.  Let's see how it goes!
issue
Fix node attributes on lattice graphs#TITLE_END#There are two minor fixes in this PR, a result of me taking a closer look at some of the lattice graphs for a maze-making application I'm thinking about.  1. `hexagonal_lattice_graph` has a `with_positions` boolean kwarg. It is `True` by default, so the positions of the nodes are computed and stored on a node attr named "pos". It turns out that `with_positions` is being ignored in `hexagonal_lattice_graph` - it's a very easy fix though, just move the existing position calculation under an `if with_positions` condition (this was already in place for `triangular_lattice_graph`  2. The second is more subtle - both `hexagonal_lattice_graph` and `triangular_lattice_graph` use `contracted_nodes` under-the-hood to identify boundary nodes when `periodic=True`. By default, `contracted_nodes` stores information about the contraction on the nodes/edges in the resultant graph. In this particular case, the contractions are an implementation detail - the information about the contraction itself shouldn't be stored on the resulting lattice graph. This PR adds tests for this and uses the new `store_contraction_as` kwarg (see #7902) to suppress this info on the final returned lattice graph.  Finally, one stylistic change: I removed the explicit import of `contracted_nodes` from `algorithms.minors` within the functions and simply call it from the top-level `nx` namespace instead!
issue
CI,DOC: Only run one parallel betweenness example.#TITLE_END#Run only one example graph instance instead of looping over 3 different instances. Simplifies the example and lightens the multi-cpu load on CI. For these reasons I think it's a good idea on it's own, but I'm also curious to see if it has any impact on #8297.  This also explicitly sets the number of processes to 2 for the multiprocessing pool. The default `None` uses `cpu_count()`, which is likely higher (and may behave strangely in a container... we've seen similar issues with openMP).
issue
CI: Install ffmpeg in circleci docs pipeline.#TITLE_END#Something I noticed while looking through CI build logs trying to figure out what's going on with the pipelines for #8289. May improve matplotlib animation gallery example runtimes.
issue
CI: Add nicer rendering of env contents.#TITLE_END#Adds `pip list` to the end of the install steps for the circleci workflows. This will make it easier to see what is installed in any given circleci workflow.
issue
Add seealso crosslinks between lattice graphs.#TITLE_END#Very similar to #8307 - I was recently looking at the 2D grid for building mazes and thought it would be useful to cross-link to the other lattice graphs in each others' docstrings. For my particular use-case I was interested in all the available 2D lattice graphs so I grouped the See Also section. I don't feel strongly about this though so if folks prefer just a general listing (with no grouping) I'd be perfectly happy with that too!
issue
Cross-link Platonic graphs in See Also section#TITLE_END#Minor doc improvement based on my own inability to remember all 5 Platonic solids off the top of my head!  Adds a `See Also` section to each of the 5 Platonic graphs linking to the other 4.
issue
DOC: Move deprecation procedure from contributing->dev guide.#TITLE_END#A minor doc refactor that reorganizes where the deprecation procedure is found. Currently it lives in [step 7 of the development workflow in the contributor guide](https://networkx.org/documentation/latest/developer/contribute.html#development-workflow). I have trouble finding it every time I look for it.  This PR moves it to the [Deprecations](https://networkx.org/documentation/latest/developer/deprecations.html) article which has a top-level heading in the developer guide (see the left side-bar on the linked page). Placing it here will also provide an anchor link to the section, making it easier to refer/link to elsewhere.  Note I didn't change the text of the deprecation procedure at all, just where it lives!
issue
Fix sphinx build errors#TITLE_END#A couple new sphinx errors from a detailed look at the build logs.  The first is related to the whole mess of how rst handles headers. It didn't like `~~~~` as that hadn't been used yet (and therefore was assigned `h4`) while the preceding header was an `h1`. There are basically two options - force this to an `h1` (which I opted for in 8d28fe9), or change this to an `h3` (which maps to `^^^^^`) then modify the Example heading below to use `~~~~~`.  The second is related to missing references after the betweenness centrality docstring updates in #8256 .
issue
CI debug experiment - Revert "Add iplotx to network drawing documentation (#8289)"#TITLE_END#Trying to figure out what's going on with #8297. The original hypothesis of CI resource limits appears not to have been the case. IIRC this issue in CI appeared around the time of this PR, so I'm reverting just to see if the CI behavior changes.  Note this is a shot in the dark - this all runs just fine locally on multiple machines and the CI logs (as linked in #8297) really don't give any valuable debug info.
issue
CI: circleci credit limit#TITLE_END#I'm opening a meta-issue to raise awareness and track progress related to the CI issues. I believe we've exhausted our OSS credits on the circleci platform for the month of September, so circleci jobs (i.e. docs, pytest-mpl, and coverage) will likely fail for the remainder of the month until our credits reset.  This is almost certainly related to our transition of the coverage job from GH -> circleci #8178 which we subsequently improved (in terms of run time) in #8273 . My hope is that this is a one-time overage due to the transition.  I've opened a ticket on circleci to see what we can do about bringing CI online for the remainder of the month and will report back here with any progress!
issue
Expire deprecation of link kwarg in node_link fns.#TITLE_END#... and the 2nd of 2 deprication expirations for 3.6 - see also #8281   This one's a little trickier as it involves removing a kwarg named `link` from `node_link_graph` and `node_link_data`, and replacing it with the more NetworkX-y `edges`. Note that most of the changes to the test suite involve removing a context manager that was ensuring a `FutureWarning` was being raised - the underlying tested code remains the same.
issue
Rm unused dissuade_hubs kwarg from forceatlas2.#TITLE_END#Great catch @eriknw ! Closes #8290 
issue
Move coverage configuration to pyproject.toml#TITLE_END#Meta-proposal to move the `coverage` configuration options to `pyproject.toml`. The main benefit to my mind is removing one more hidden file from the root directory of the repo. Pyproject-based configuration is fully supported (at least for our relatively simple use-case) according to the [coverage docs](https://coverage.readthedocs.io/en/latest/config.html#sample-file).
issue
Update louvain test modularity comparison to leq.#TITLE_END#Closes #6823   I'm no expert in community detection, but from reading through the Louvain description (and thinking about the various floating-point issues associated with the modularity calculation) I think using `<=` for this comparison is correct, since equivalence is also "not increasing".
issue
Expire deprecation of compute_v_structures.#TITLE_END#With the NX 3.6 release on the horizon, I wanted to make sure we get the scheduled deprication expirations in. This is the first of 2 removals slated for 3.6: removing the `compute_v_structures` fn.
issue
CI: Move slow tests from coverage to dedicated run#TITLE_END#Follow-up to #8178  We recently moved away from codecov for publishing code coverage results instead relying on circleci artifact system.  As noted in the discussion in #8178, NX has traditionally run it's "slow" tests in the coverage job to ensure maximum coverage. The downside is that the slow tests take *extra* long with the coverage plugin, bumping the test runtimes up into the 20 min range.  This PR splits the slow tests out of the coverage job and moves them to their own dedicated job. Thus the slow tests are still being run with each workflow, but without adding the extra burden to the already expensive coverage job.  Note that this will likely lead to a drop in reported coverage for some functions, but this is being actively tracked/worked on in #8223. Note also that some of the "lost" coverage is actually misleading - many of the covered lines originate from the `test_all_random_functions` test module, which is really a collection of smoke tests and often doesn't probe the behavior of the underlying function. In other words, there are some cases where the coverage numbers are artificially inflated by the slow tests (namely the random seed test), so this split makes coverage issues easier to identify and fix properly!
issue
Rm outdated codecov badge from README#TITLE_END#Closes #8262 
issue
Set length threshold in FR and use np.clip#TITLE_END#Closes #8113   I have been convinced by @HirokiHamaguchi 's and @rudyarthur 's careful analysis of the thresholding in the FR layout that it's worth considering setting the length fill value to be equal to the threshold. While the change itself in source code is quite small, bear in mind this *will* break backward compatibility; i.e. `spring_layout` will no longer produce *identical* layouts to previous versions of NetworkX even when seeded identically. This is evidenced by the required change in the doctest.  IMO given how little the layouts will change (especially for the default number of iterations) and the minor improvements for some cases (see @HirokiHamaguchi 's analysis in #8113), I'm willing to accept this breakage. That's just my opinion though - if others feel different please share your perspective!  Switching to a fill value that matches the threshold also enables another minor improvement - we can use `np.clip` instead of `np.where` or any other mask-based approach, which is both faster and arguably more readable. I've included that change as well (and in another unrelated location where `np.where` could be replaced with `clip`).  Finally, I added a code comment explaining the clipping and referencing back to #8113. It may be a bit unorthodox to link to a GH discussion in source code, but the discussion there is so rich I feel it's worth pointing to.
issue
DOC: Add examples to contracted_nodes.#TITLE_END#Adds a more extensive examples section to contracted_nodes to highlight behavior in a myriad of cases:  - Highlights the handling of contracted nodes, the `"contraction"` attribute, and any edges that result from the contraction (and their attributes). Includes explicit examples for both non-multigraphs and multigraphs.  - More detail on `self_loops` and how self-loops in the original graph are handled by contraction  Closes #7781 Closes #7660
issue
Update links for broken testing badge in README.#TITLE_END#While we're looking at badges - it looks to me like the testing badge on the README has a broken link. This will hopefully fix it!
issue
Rm networkx.algorithms.threshold.swap_d.#TITLE_END#A proposal to remove the `swap_d` function from the `threshold` module.  I originally came across this via test coverage - according to coverage metrics, this function is "covered" by the `test_all_random_functions` test, but that just tests that it runs with a single set of parameters. While trying to design more tests for this function, I found some (IMO) bugs. For example:  ```python >>> from networkx.algorithms import threshold as nxt >>> cs = ['d', 'd', 'd', 'i', 'i', 'd'] >>> nxt.swap_d(cs) Traceback (most recent call last)    ... IndexError: Cannot choose from an empty sequence ```  This error originates from [this section](https://github.com/networkx/networkx/blob/5cfb44f7afa9b46a863e501f817e12a572eca94b/networkx/algorithms/threshold.py#L1008-L1012) in the code - in particular, the `dlist` leaves off the first and last element of the list, but doesn't account for this truncation in the enumeration. Therefore it's possible for `choice` to end up being `0`, which then causes the IndexError in L1012 when you try to `seed.choice(range(0))`  ---  Ultimately, I suspect that this function is neither very reliable nor widely used. Given that the `swap_d` function is not tested (aside from smoke tests - see the test removals in this PR which cover all instances in the test suite) nor used internally anywhere else in the library, that proposing to remove it is justified. Given the fact that this is in a non-auto-imported module and there have been (to my knowledge) no issues raised related to it (despite the bugs, though they are stochastic) I'm proposing to remove it without a deprecation.
issue
MAINT: Rm print from threshold_graph#TITLE_END#Minor maintenance for threshold_graph module. There's *a lot* that could be done to improve things here but I want to keep changes small and digestible.  The first proposal is to eliminate two instances of `print` from public code. They're both a bit nuanced:  - 971e849 replaces a `print/return None` with `raise ValueError`. Technically this is a backward-incompatible change, but the `threshold_graph` fn in which this lives is not in the `__all__` and very much seems like it is not intended for direct use by users.  - b796aaa removes a code branch entirely. My initial impulse was to simply update the `ValueError` with the printed message, but after [looking at the code](https://github.com/networkx/networkx/blob/5cfb44f7afa9b46a863e501f817e12a572eca94b/networkx/algorithms/threshold.py#L545-L551) I don't believe it's possible to ever hit this line. 
issue
STY: Variable rename proposal in bidirectional_dijkstra#TITLE_END#This is a completely cosmetic change, i.e. a proposal to rename some internal variables. Just something I noticed while reviewing #8206. There are two independent proposals in this PR:  1. Rename `dir` -> `direction` (32ba3ff). This is more descriptive *and* has the added benefit that it doesn't mask the builtin `dir` (which is extra annoying while reading due to syntax highlighting)  2. Rename `neighs` to `neighbors` (ee306b7). IMO this goes especially well with change 1, which transforms `neighs[dir]` to `neighbors[direction]`.  Please feel free to consider these separately - happy to back out the second if desired (I feel more strongly about `dir`!)
issue
Add GEXF 1.3 to the recognized GEXF versions#TITLE_END#Closes #8040 #7972   I went ahead and tried `read_gexf` using the [Example Graphs](https://gexf.net/schema.html) from the GEXF schema page and indeed they weren't working for v1.3. Fortunately the "fix" is just to add the version info to the `GEXFReader`. One thing to note - GEXF files prior to v1.3 appear to have `www.` prepended to all the schema URL's whereas v1.3 (at least the examples) do not. This may need more explicit handling depending on whether this change is intentional or not, but let's cross that bridge when (if) we come to it.
issue
Add benchmarks for multisrc_dijkstra over many small graphs.#TITLE_END#Adds another benchmark for #8023   One of the concerns I expressed there (after reviewing the original benchmark results) was degradation in performance when computing shortest paths over many small graphs. Therefore, I decided to add an explicit benchmark for this case! The idea is to compute dijkstra multisource over all connected graphs of 7 nodes (drawn from `graph_atlas_g`) with various weighting schemes.  The good news: according to these benchmarks, there is no performance degradation in #8023 for the looping-over-many-small-graphs case!
issue
Add benchmarks for is_regular.#TITLE_END#Benchmark cases included directed and undirected complete graphs (regular) as well as graphs with one edge removed (not regular) to test early termination.  Relevant for #8138
issue
`pos` kwarg not recognized by `display`#TITLE_END#Something I noticed while attempting to use `nx.display` with a non-default stored pos. Here's the MRE:  ```python >>> G = nx.star_graph(10) >>> pos = nx.bfs_layout(G, 0, store_pos_as="pos_bfs") >>> dict(G.nodes(data=True))  # Shows that the pos data is stored on the graph as expected {0: {'pos_bfs': array([-0.2020202,  0.       ])},  1: {'pos_bfs': array([ 0.02020202, -1.        ])},  2: {'pos_bfs': array([ 0.02020202, -0.77777778])},  3: {'pos_bfs': array([ 0.02020202, -0.55555556])},  4: {'pos_bfs': array([ 0.02020202, -0.33333333])},  5: {'pos_bfs': array([ 0.02020202, -0.11111111])},  6: {'pos_bfs': array([0.02020202, 0.11111111])},  7: {'pos_bfs': array([0.02020202, 0.33333333])},  8: {'pos_bfs': array([0.02020202, 0.55555556])},  9: {'pos_bfs': array([0.02020202, 0.77777778])},  10: {'pos_bfs': array([0.02020202, 1.        ])}} >>> fig, ax = plt.subplots() >>> nx.display(G, pos="pos_bfs", canvas=ax) Traceback (most recent call last)    ... NetworkXError: Unrecongized visualization keyword argument: pos ```
issue
DOC: Add docstring example count number of unique triangles#TITLE_END#A simple solution to the problem originally posed in #8135 - so simple (and nice in the sense that it uses `triangles` instead of other alternatives like `cycles` or `triads`) that I thought it was worth documenting! Thanks @dschult for pointing it out!
issue
Revert dict comprehensions -> dict.fromkeys accidentally introduced in #8017#TITLE_END#Reverts auto-conversion of dict comprehensions to `dict.fromkeys` introduced in networkx/networkx#8017.  The removal will not affect the pre-commit hook versions (they remain up-to-date) and the offending ruff rule is ignored to prevent this from auto-happening again!  
issue
Proposal: update semantics for nonisomorphic trees with order 0 or 1#TITLE_END#While putting together an NX guide, I noticed a few (what I believe to be) inconsistencies in `nonisomorphic_trees` and `number_of_nonisomorphic_trees`, specifically for `n = 0` and `n = 1`.  Currently, these functions simply raise a `ValueError` (with no exception message!) when `n < 2`. However, I think this is inconsistent with the definition of a tree, both generally and as used elsewhere in the library. For example, a graph with a single node and no edges is technically a tree:  ```python >>> nx.is_tree(nx.empty_graph(1)) True ```  `is_tree` raises a `PointlessConcept` exception for the null graph (i.e. no nodes or edges), which makes sense. However, I don't think `nx.number_of_nonisomorphic_trees` should raise in that case (nor `nx.nonisomorphic_trees`, though this is subjective - see [inline comment](https://github.com/networkx/networkx/pull/8083#discussion_r2118328142) for details).  ## Proposed semantics  I propose to change the behavior of `nx.nonisomorphic_trees` and `nx.number_of_nonisomorphic_trees` for `order=0` and `order=1`. I went about this in a test-driven manner, so the first commit 9de48f2 captures the proposed changes most clearly (the `order < 0` case is consistent with current behavior, albeit with an added message to the `ValueError`)
issue
DOC: Add missing params to bfs_layout docstring.#TITLE_END#Another minor thing I noticed while working on an nx guide: the `bfs_layout` docstring was missing a couple of parameters from the signature.
issue
Add Python 3.14 to testing matrix#TITLE_END#Inspired by #8091 - adds Python 3.14 to the testing matrix in CI. Specifically, it adds 3.14 testing *only* to the `base` jobs, i.e. the ones without dependencies. This is to avoid conflicts for default dependencies that do not yet supply 3.14 wheels (it looks like only NumPy does so at this point).  Also includes a fix for #8091 which fails on Pythoon 3.14. I'm happy to break this out in a follow-up PR if folks prefer to keep the CI config/code changes separate. Fizes #8091  Finally - I arbitrarily chose to include 3.14 both with and without free-threaded support (the `3.14t-dev`). In principle this shouldn't affect networkx at all (i.e. networkx without dependencies) so we could remove it.
issue
Refactor image comparison tests.#TITLE_END#Alternative to #8094 for closing #8090 . The main difference between the two is that this PR leaves the CI footprint (i.e. which workflows run the mpl tests) unchanged.  All this PR does is move the 6 `mpl_image_comparison` tests out of `nx_pylab` to their own test file. The "fixing CI" part is then just adding `pytest.importorskip("pytest_mpl")` to the new test file.  Even aside from the pytest/plugin dependency issues, I think moving these tests to their own file makes sense as they are quite different from all of the other nx_pylab tests.
issue
Rm extraneous print from nx.display.#TITLE_END#I'm trying out `nx.display` in an nx-guide I'm working on and noticed that some attributes are printed during drawing. 
issue
Benchmarking: graph atlas#TITLE_END#Adds a benchmark that uses `atlas6` (i.e. the set of all connected graphs with at most 6 nodes and at least one edge that are not isomorphic to one another) to provide an evaluation point for algorithm performance whose variance depends on the number of components.  Inspired by #7762 - note this PR doesn't implement the suggestion from that issue, but it should be useful in evaluating the performance gains!  I also took the opportunity to add a bit more detail to the benchmarking readme - specifically, I added the recipe for running only a subset of the full benchmarking suite.
issue
Bipartite layout nodes optional#TITLE_END#A feature proposal to make the `nodes` parameter of `bipartite_layout` optional, with the default behavior to compute the nodesets with `nx.bipartite.sets`.  This is intended to be a minor convenience enhancement so users don't have to explicitly create partitions just to visualize the graph. This is only a proposal to add a feature and should be completely backwards compatible - including preserving the order of the nodes in the `pos` dict. I wasn't sure how important that was, but it's straightforward not to break!  I also added a `draw_bipartite` convenience function in 9196501 to match the pattern used for the other layout functions. I did so only for consistency and am completely happy to leave it out if preferred!
issue
MAINT: use nx.layout instead of importing layouts.#TITLE_END#A purely cosmetic change: removes the importation of specific layout functions in `nx_pylab` and instead calls them from the `nx.` namespace. Essentially just an (opinionated) lint - the main motivation is getting rid of the big import statement.
issue
Improve square clustering test derived from Zhang paper (reference 2)#TITLE_END#The test originally only checks the square clustering coefficient for a single node - this PR expands the test to check the square clustering coefficient of all nodes in the example graph. I came across this while doing some background research for #7810 !
issue
Add test-extras to optional dependencies.#TITLE_END#I was getting set up on a new computer today and thought I'd take the opportunity to propose another optional requirement `test-extras` to automate the installation of pytest plugins that I always use when setting up a NetworkX development environment.  I think there's a strong case for adding `pytest-mpl` in this way, as it is required to run the mpl image comparison tests. `pytest-randomly` is used in at least one CI job and is useful to catch issues with the tests themselves. Finally, `xdist` is just for convenience - running tests in parallel is a joy (I'm down to 14 sec to run the entire test suite on this new machine!)
issue
Expire deprecation of create kwarg in nonisomorphic_trees.#TITLE_END#Expires the last deprecation scheduled for 3.5: removing the `create=` kwarg to `nonisomorphic_trees`.  During the removal it seemed to me there is also an opportunity to remove some redundancy in the conditionals by restructuring the loop, but that's for a separate PR!
issue
TST: Refactor example test case generation functions.#TITLE_END#A minor follow-up to #7770 . Refactors tests for a bit more code reuse. The main change is to use the `generate_close_cliques` (renamed `close_cliques_example`) to create the test case for `test_big_fista_dataset`. The remainder of the changes are cosmetic (i.e. putting comments above rather than at the end of lines, etc.)  I considered turning the fns that create the test cases into fixtures, but honestly I thought it was easier to read if they're left as normal functions. Parametrizing fixtures is a nice feature, but IMO harder to understand for this particular case.  @FaroukY if you could take a quick look and make sure I'm not unintentionally breaking something intended in the original test organization that'd be great!
issue
More `random_paths` docstring improvements#TITLE_END#Follow-up to #7832   Incorporates @Schefflera-Arboricola 's suggestions and then extends them further. The main changes are:  1. The original wording in the first example implied that `generate_random_paths` is a function. I've reworded to make it clear that it's a generator, and extended the example to illustrate this more clearly.  2. Seeded the second example per @Schefflera-Arboricola 's suggestion, but also changed the example so that there is at least one output path that *doesn't* contain node 0 to better illustrate the usage of `index_map`.
issue
DEP: Raise an exception for k_core functions with multigraphs.#TITLE_END#Another FutureWarning behavior changed scheduled for 3.5
issue
Expire d_separated and minimum_d_separator functions.#TITLE_END#Expire deprecations of `d_separated` and `minimum_d_separator`, which are scheduled for removal in 3.5
issue
Expire random_triad deprecation.#TITLE_END#Expires the `random_triad` deprecation, which is scheduled for removal in version 3.5
issue
Expire all_triplets deprecation.#TITLE_END#Expires the `all_triplets` deprecation, which is scheduled to be removed in 3.5
issue
Deprecate graph_could_be_isomorphic#TITLE_END#`graph_could_be_isomorphic` (and friends `fast_` and `faster_`) are aliases for `could_be_isomorphic` that is not exported outside of the `isomorph` module, so this should be a relatively undisruptive deprecation (the only way to access the alias is to import from `nx.algorithms.isomorphism.isomorph` - see e.g. the removed examples in the test suite).  There were some internal usages in the test suite which are easily replaced with the (newly) recommended function.
issue
Add a few more square clustering test cases.#TITLE_END#It took me longer than I'd like to admit to work out the correct value for the 2D grid example (I think the formulae in *both* cited references give the incorrect answer for the central node), but the good news is that networkx's implementation gives the correct answer!
issue
Add feature to make storing node contraction data optional#TITLE_END#Implements the feature discussed in #7781 - adds a `store_contraction` kwarg to `contracted_nodes` to allow the user to specify whether to store info about the contraction as node/edge attributes on the resulting graph.  The feature was relatively straightforward to implement - unfortunately the diff is kind of intimidating due to all the test suite changes. Despite the removals in the test suite, all changes are about beefing up the existing tests; particularly parametrizing over the boolean flags that already exist (`self_loops` and `copy`). However, I know how much I personally dislike changes to the test suite when reviewing, so I'm happy to split this up into two PRs where one covers the changes to the existing tests (without the `store_contraction` feature), and  a second which simply adds the coverage related to `store_contraction`.  Finally, I there's likely an API discussion to be had here. I went with a boolean flag to indicate whether to store data, but hard-coding attribute names is not very networkx-y. We could also take this opportunity to do something similar to the layouts, i.e. add `store_contraction_as="contraction"`. Happy to do so here or in follow-up PRs - LMK what you think! Either way, the addition will be backward-compatible! 
issue
Add examples to graph_atlas_g docstring.#TITLE_END#I often find myself searching for [the graph atlas gallery example](https://networkx.org/documentation/stable/auto_examples/graphviz_layout/plot_atlas.html#sphx-glr-auto-examples-graphviz-layout-plot-atlas-py) as a reminder for how to partition the graph atlas to get all graphs of *n* nodes. Therefore, I thought it might be worthwhile to put that info in the `graph_atlas_g` docstring itself.  I've elected to do so with `Examples`, though it could perhaps be done more concisely in a bulleted list somewhere in the docstring summary. I'm open to ideas/suggestions!
issue
Correct sphinx warnings from doc build#TITLE_END#Just a couple fixups for sphinx warnings and formatting issues when building the docs.
issue
Rm stray instances of sparse matrices from test suite.#TITLE_END#Caught a couple remaining instances of `csc_matrix` while combing over code for #7859 !
issue
Activate pycodestyle in linting pre-commit#TITLE_END#I fell down the linting rabbit hole while looking back at #7693 . I don't think `pycodestyle` actually has an effect on docstring lengths, but I took the plunge of getting it set up so I figured I'd push it up. I personally find some of the flake8 rules more nit-picky than helpful, so I'm not advocating strongly for this going in. Here's a quick breakdown of what the linter caught that I fixed:  - [E721](https://www.flake8rules.com/rules/E721.html) - don't use == for `type` comparisons, a minor improvement  - [E402](https://www.flake8rules.com/rules/E402.html) - imports at the top of the file. This one feels extra nitpicky to me, but since networkx is lazy-loaded I feel the performance hit of forcing the imports before any statements is negligible. The place where this rears it's head the most is the test suite (because of `pytest.importorskip`). I'd be just as happy to ignore this one instead.  - [E712](https://www.flake8rules.com/rules/E712.html) - get rid of the `== <boolean>` antipattern  - [E711](https://www.flake8rules.com/rules/E711.html) - Use `is` instead of `==` with `None` and other singletons  - [E731](https://www.flake8rules.com/rules/E731.html) - Don't bind names to lambda functions  There were a couple others ([E741](https://www.flake8rules.com/rules/E741.html) and [E722](https://www.flake8rules.com/rules/E722.html)) that I chose to ignore at this stage because they were extra nit-picky and trickier to resolve, respectively.  IMO none of these are really major improvements - the main payoff here would be the enabling of auto-checking at commit time to ensure these antipatterns don't sneak in via future contributions. I'm personally +0.1 .
issue
Add square_clustering to algorithm benchmarks.#TITLE_END#This one was easy - I just plugged it directly into the existing algorithm benchmarks. Note however that I chose the test cases that excluded the drug interaction network, as the existing implementation can be quite expensive!  Here's an example incantation and some results:  ```bash $ asv continuous --bench AlgorithmBenchmarksConnectedGraphsOnly.time_square_clustering 2d1ad6b1 main $ asv compare 2d1ad6b1 main ```  <details>   <summary>Benchmarking results</summary> <pre> All benchmarks:  | Change   | Before [2d1ad6b1] <faster_square_clustering>   | After [f87c3611] <main>   |   Ratio | Benchmark (Parameter)                                                                                        | |----------|------------------------------------------------|---------------------------|---------|--------------------------------------------------------------------------------------------------------------| | +        | 3.59±0.02ms                                    | 10.5±0.6ms                |    2.92 | benchmark_algorithms.AlgorithmBenchmarksConnectedGraphsOnly.time_square_clustering('Erdos Renyi (100, 0.1)') | | +        | 22.7±0.7ms                                     | 583±2ms                   |   25.68 | benchmark_algorithms.AlgorithmBenchmarksConnectedGraphsOnly.time_square_clustering('Erdos Renyi (100, 0.5)') | | +        | 26.9±0.8ms                                     | 2.14±0s                   |   79.38 | benchmark_algorithms.AlgorithmBenchmarksConnectedGraphsOnly.time_square_clustering('Erdos Renyi (100, 0.9)') | </pre> </details>  
issue
BUG: graph6 format invariant to trailing newline.#TITLE_END#Closes #7557
issue
Add linting for line length in docstrings and comments#TITLE_END#Activates [`W505`](https://docs.astral.sh/ruff/rules/doc-line-too-long/) in the linter to catch cases when docstring lines and comments exceed a configurable number of characters. I chose `110` for this value to start - happy to amend if others prefer other values.  Unfortunately this linter doesn't "autofix", but it will at least prevent the unwitting committing of long, unreadable lines to the codebase.  In terms of fixing the existing issues, I've broken them down by commit:  - beba169 contains the setup/configuration for this feature  - 5086efd contains purely cosmetic changes; i.e. places where I've adjusted the line breaking  - c09c542 contains actual code changes. For the most part, these are docstring examples where the outputs exceeded the allowed limit. I've generally chosen to get around this with `pprint`, which leads to more readable outputs while preserving the character-exact matching quality required for doctests.   - 40b8f9a contains a single instance of a sphinx external link -> internal: it doesn't actually use that many fewer characters, but hopefully makes the linking more robust!  It's probably worth looking at the individual diffs for the commits during review!  Closes #7693
issue
MAINT: rm debug print from similarity module.#TITLE_END#Something I came across while looking at more linting outputs. I think the `debug_print` statement should definitely be removed - if other folks want to keep the comments the commented asserts I think that's fine!
issue
Refactor tree_isomorphism to improve code reuse and readability#TITLE_END#A few more minor improvements while working my way through the tree_isomorphism module (fun!)  There are two main bits in this PR where I've attempted to replace custom code with built-in functions:  1. `tree_isomorphism` essentially implements the `faster_could_be_isomorphic` check before it actually begins searching for tree isomorphisms. I've replaced it with an explicit call to `faster_could_be_isomorphic`.  2. `rooted_tree_isomorphism` relied on two helper functions `assign_levels` and `group_by_levels` to set up the algorithm. Upon closer inspection, I think these two functions are equivalent to grouping nodes by their shortest path length to the root node - so I've replaced them with the 3-liner to do this grouping (using `defaultdict(list)`). Technically these functions are "public", but they are so specialized here that I doubt they see heavy use in user-code. In other words, I'm hoping we can get away with simply removing them without a deprecation cycle... however if we want to be safe I'm happy to do so!
issue
DOC: Update first docstring example and add a serialization example.#TITLE_END#Update the examples section for cytoscape_data  1. Modifies the original example so that it is not doctest-able (i.e. add `pprint` and remove the `doctest: +SKIP`) 2. Adds an example of using `json` (with StringIO) to simulate serializing to a file.  Closes #7913
issue
Some light refactoring to make the tree isomorphism tests more readable#TITLE_END#Something I noticed while reviewing other related work.  The `test_tree_isomorphism.py` bit of the test suite had a lot of helper functions that IMO made the tests harder to follow than they could be.  The biggest change hereis captured in the first commit 1f3b75f which replaces the custom isomorphism checking functionality with a simpler check: essential relabel the nodes with the given isomorphism and check `nx.utils.graphs_equal`.  The other big-ish change is refactoring the two tests which check *all* nonisomorphic trees for various numbers of nodes. There are two tests:  - one that tests that all nonisomorphic trees are isomorphic with themselves, with their nodes    relabeled, and  - one that tests that all pairs of nonisomorphic trees are not isomorphic with one another.  In practice I think these test cases may be overkill, but I'm not proposing to remove them here - I simply refactored them so that they are parametrized on the number of nodes `n`, which helps when the tests are run in parallel, as the maximum runtime decreases by about 30-40% for these cases.  My main intention though was to make things easier to read!
issue
Tree isomorphism input validation#TITLE_END#Replaces assertions with exceptions in `tree_isomorphism` input validation.
issue
Minor follow-up to gh-8002 tests#TITLE_END#I was late reviewing #8002 (apologies!) but did eventually get a chance to look at it and just turned my comments into a PR :). Not at all important, just a minor followup!  There are two main proposals:  - 98d5074 refactors the test added in #8002 to check properties of the generated paths (i.e. that they all start with `source`, rather than doing an exact-match on a seeded example. IMO this is a minor improvement as it's slightly more flexible and removes the dependence of the test on the random number system (in principle also increasing robustness). It's clear that the test was inspired by the one below it, so it made perfect sense but I thought it was a nice opportunity to add some flexibility!  - 558f4d2 adds an additional test for when the `source` is not in `G`. 
issue
Add nx-guides link to navbar without dropdown.#TITLE_END#Minor doc config tweak: this prevents the link to `nx-guides` from being hidden in a dropdown in the top-level navbar on the page. Related to a discussion at the community meeting several weeks ago!
issue
MAINT: Follow-up to 7945 - rm helper function#TITLE_END#Here are the additional changes I proposed in #7945   - Removed `generate_isomorphism` as a standalone function and moved implementation into the spot where it was used in `rooted_tree_isomorphism`  - Re-did how the `ordered_children` are `reversed` with the goal of maintaining the current order of the returned values  Sorry if I jumped the gun @amcandio , I just happened to be looking at this when #7945 was merged and wanted to get it out while it was fresh in my head!
issue
Minor refactor to cleanup/improve matching test suite#TITLE_END#A few test suite improvements from having read the code while reviewing #8062.  This PR essentially contains 3 unrelated proposals, and might be easier to digest commit-by-commit:  - 942193a is the big one: it removes a layer of indirection in the tests by getting rid of the consistent calls to `matching_dict_to_set` for munging the "expected" answers. IMO it's much more straightforward to compare to the expected edge sets explicitly. This is possible because `nx.utils.edges_equal` already handles arbitrary edge ordering, so there's no need to do so twice.  - af4aef3 converts all the instances of `pytest.raises` from function calls to the context manager pattern, and adds the `match=` kwarg better ensure that the expected exception is being raised.  - Finally, 2ba9698 and e01171d factor out all of the input-checking tests for the various functions into single parametrized tests. The main advantage of doing so is that, currently, all of the `is_*matching` functions have the same exception message for the input validation exceptions. By lumping them altogether in the test suite, it makes it less likely that individual function messages will diverge in the future.
issue
STY: Rm local variable remapping of heappush and heappop.#TITLE_END#There is a consistent pattern in the codebase where `heappush` and `heappop` are renamed to `push` and `pop` in the functions where they are used. IMO this indirection makes the code less readable; especially in diffs, where the renaming often occurs in one of the unchanged bits of code and is therefore not immediately obvious.  This PR replaces all of the `push = heappush` and `pop = heappop` with just calling `heappush` and `heappop` directly. IMO this is a (very minor) readability improvement[^1]; however, it's so minor that I'm also happy just to close if others think it's not worth the code churn!  [^1]: explicit is better than implicit
issue
TST: Minor improvements to layout test suite#TITLE_END#A few things I noticed while reading code for #8041, but unrelated to that PR so I split it off to a separate one.  There are two main changes:  1. c1441d4 makes the `test_spring_fixed_without_pos` test more specific by adding `match=` and also explicitly checking the expected output in the final case.  2. Adds a test to ensure that all layout functions that support `dim=` raise some form of exception when `dim` is not a valid number, e.g. negative. There is variability in both the support for dimensions and the exception messages, which the test handles by simply `|`-ing existing message text together.
issue
Fix all sphinx build warnings#TITLE_END#A little activity from my recent flight without internet. I'm loathe to add to the PR queue but it might be nice to have a clean doc build for the release (though certainly not critical).  In principle we could add a CI job to fail on sphinx warnings to prevent these from getting in. Personally, I don't think these should be blockers for PRs so my vote would be to keep things the way they are. Open to other opinions though!
issue
MAINT: Finicky test for `maybe_regular_expander`#TITLE_END#Just noticed a [failure on main](https://github.com/networkx/networkx/actions/runs/15054318833/job/42316500062) after a recent merge. The test failure is related to `maybe_regular_expander` hitting an iteration limit. This is a stochastic function and the test is un-seeded so this is not necessarily surprising, but it's probably worth looking into robust-ifying the test to prevent spurious failures in the future.
issue
CI Failures due to pydot 4.0 release#TITLE_END#Pydot v4.0 was [released on May 4th](https://pypi.org/project/pydot/#history) with API changes that affect `nx_pydot`. This is the cause of all the CI failures recently.  We'll need to investigate how exactly we want to support things moving forward, especially given this is so near the nx 3.5 release candidate. The simplest solution is to pin `pydot<4` but if we can get away with supporting 3 and 4 without a ton of boilerplate that's preferable IMO!
issue
Support both pydot v3 and pydot v4.#TITLE_END#Closes #8026   I think it's relatively straightforward to deal with the signature difference between pydot v3 and v4 for `get_strict`.  My personal vote would be to support both for nx 3.5, then bump the pydot minimum version to 4.0 in nx 3.6.
issue
Linting: enforce line lengths#TITLE_END#I was under the impression that the pre-commit auto-linters enforced a maximum line length both in code and documentation, and would automatically re-format inputs that violated those rules. However, it appears this isn't currently the case.  For example - if one follows the contributor guide to set up the development environment (including pre-commit) then creates a dummy commit with a line with 120+ characters in it, pre-commit won't complain or try to reformat it.  Was the removal of this feature intentional? I know we adopted black in the past for this very reason and IIRC `ruff format` was intended to supplant black, but it seems we've lost the autofix-line-len-violations feature that we once had. If the removal was intentional, I'd vote to bring it back - it's much more convenient than having to ask contributors to manually keep lines at a certain length.
issue
Consolidate could_be_isomorphic#TITLE_END#This is a left-over idea from the summit that I thought I'd write up for further discussion!  NetworkX has a function `could_be_isomorphic` to check various properties of two graphs to determine whether or not they, well, could be isomorphic. `could_be_isomorphic` checks three properties: 1) degree sequences, 2) sequences of triangles, and 3) sequences of number-of-cliques.  There also exist two other functions, `fast_could_be_isomorphic` and `faster_could_be_isomorphic` that check degree+triangles and just degree, respectively. IMO, the name of these functions doesn't do a whole lot to illuminate what they are checking. There is also a lot of code duplication between these three functions as all of them check degree, two check triangles, etc.  Therefore I thought it might be worth trying to consolidate things a bit. This is one proposal to do so: adding a `mode=` kwarg to `could_be_isomorphic` to determine which properties to check. For consistency, I've used the same designation as the functions (i.e. "fast" and "faster") but one could imagine different patterns here. For example, we could do something like `check="cdt"` where "c" indicates cliques, "d" degree and "t" triangles, to allow users to specify whichever combination of property checks they'd like (e.g. `check="cd"` for cliques and degree, `check="t"` for just triangles, etc.)  I'm mostly looking for opinions here, so LMK what you think!
issue
`effective_size` of nodes with only self-loop edges is undefined#TITLE_END#Handles the corner-case of isolated nodes that contain self-edges. Currently, isolated nodes are treated as having undefined `effective_size`:  ```python >>> G = nx.Graph() >>> G.add_node(0) >>> nx.effective_size(G) {0: nan} ```  However, the current behavior for isolated nodes with self-loops is inconsistent, resulting in `ZeroDivisionError` for the undirected case and `0.0` for directed graphs::  ```python >>> G = nx.Graph([(0, 0)]) >>> nx.effective_size(G) Traceback (most recent call last)    ... ZeroDivisionError: division by zero >>> G = nx.DiGraph([(0, 0)]) >>> nx.effective_size(G) {0: 0.0} ```  This discrepancy is due to the ambiguity of the direction of a self-edge.  Based on the current behavior and Borgatti's formula, I think the thing that makes the most sense is to have `effective_size` be undefined for *all* of these cases, making the behavior consistent across the board.  Closes #6916 
issue
Add missing usebounds param descr to distance docstrings.#TITLE_END#Adds missing `usebounds` parameter description to the docstrings for the relevant distance measures. Addresses at least the documentation part of #7687, though it may be worth keeping that open in case there's any other discussion.  Wording suggestions welcome!
issue
Refactor network_simplex test of faux_infinity.#TITLE_END#Follow-up to #7796   Just a refactor of the test for the `faux_infinity` bound that was added in #7796. The tests themselves should be the same, I've just modified the organization to:  1) move the modifications of weight/demand/capacity into the test itself to make it more clear how the base example is being modified, and  2) Use stacked fixtures to simplify the generation of test cases.
issue
Expire total_spanning_tree_weight deprecation#TITLE_END#Removes `total_spanning_tree_weight` and replaces internal usages with `number_of_spanning_trees`. `total_spanning_tree_weight` is scheduled for removal in v3.5
issue
Fix docstrings for generators#TITLE_END#Fixes #7258 and adds a regression test for all of the generators in NetworkX which had incorrect `Returns` sections in the docstrings.  These are just the ones caught by numpydoc (and ignores inherited methods), so there may be others!
issue
Intermittent failure of `test_pickle` dispatching test#TITLE_END#I just began noticing an intermittent failure of the `test_backends.test_pickle` test. Here's the testing log:  ``` $ pytest networkx/utils/tests/test_backends.py  ===================================================================== test session starts ===================================================================== platform linux -- Python 3.13.1, pytest-8.3.4, pluggy-1.5.0 Using --randomly-seed=1969482262 Matplotlib: 3.10.0 Freetype: 2.6.1 rootdir: /home/rossbar/repos/networkx configfile: pyproject.toml plugins: cov-6.0.0, xdist-3.6.1, randomly-3.16.0, mpl-0.17.0 collected 9 items                                                                                                                                               networkx/utils/tests/test_backends.py ....Fss.s                                                                                                         [100%]  ========================================================================== FAILURES =========================================================================== _________________________________________________________________________ test_pickle _________________________________________________________________________      def test_pickle():         count = 0         for name, func in nx.utils.backends._registered_algorithms.items():             pickled = pickle.dumps(func.__wrapped__)             assert pickle.loads(pickled) is func.__wrapped__             try:                 # Some functions can't be pickled, but it's not b/c of _dispatchable >               pickled = pickle.dumps(func) E               AttributeError: Can't get local object 'test_not_implemented_by_nx.<locals>._stub_func'  networkx/utils/tests/test_backends.py:26: AttributeError =================================================================== short test summary info =================================================================== FAILED networkx/utils/tests/test_backends.py::test_pickle - AttributeError: Can't get local object 'test_not_implemented_by_nx.<locals>._stub_func' =========================================================== 1 failed, 5 passed, 3 skipped in 0.12s ============================================================ ```  The fact that it's intermittent leads me to believe this has to do with test setup via randomized test order (via pytest-randomly) rather than a real bug.  Some basic environment info:  Python version: 3.13.1 networkx hash: f2c43bc6 pytest version: 8.3.4
issue
Add a subplot fixture to automate test cleanup.#TITLE_END#Follow up to #7783 - add a fixture for creating figure/axis objects for the pylab tests which uses `yield` statements to [automatically clean up the tests](https://docs.pytest.org/en/stable/how-to/fixtures.html#yield-fixtures-recommended) afterwards. This will hopefully help prevent issues like this with adding new visualization tests in the future! 
issue
Opaque exception message for degree when node not present#TITLE_END#I think there's an opportunity to improve the clarity of the exception raised when requesting the degree of a node not found in `G`:  ```python >>> G = nx.path_graph(5) >>> G.degree(100)  # Node "100" not in G Traceback (most recent call last):   File "/home/ross/repos/networkx/networkx/classes/graph.py", line 2040, in bunch_iter     for n in nlist:              ^^^^^ TypeError: 'int' object is not iterable  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File "<stdin>", line 1, in <module>   File "/home/ross/repos/networkx/networkx/classes/reportviews.py", line 440, in __call__     return self.__class__(self._graph, nbunch, weight)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "/home/ross/repos/networkx/networkx/classes/reportviews.py", line 425, in __init__     self._nodes = self._succ if nbunch is None else list(G.nbunch_iter(nbunch))                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "/home/ross/repos/networkx/networkx/classes/graph.py", line 2055, in bunch_iter     raise exc networkx.exception.NetworkXError: nbunch is not a node or a sequence of nodes. ```  There are multiple messages fromt he chained exceptions, but neither really points to the real issue: the node is not in `G`.
issue
Minor updates to `single_source_shortest_path_length` docstring#TITLE_END# * Wrap parameters in single quotes  * Simplify docstring example  * Make see-also section linkable and add dijkstra + bf  * Add 1-sentence description to each See-Also Entry
issue
Add blurb about pytest-mpl dependency to contributing guide.#TITLE_END#Closes #7738
issue
Parametrize edge_subgraph multigraph test.#TITLE_END#Extremely minor follow-up to #7724 - propose to parametrize that test over the different multigraph types.
issue
Gallery example: bipartite a/b-core motif#TITLE_END#An alternative to #7728 presenting the computation of "alpha-beta-core" motifs in bipartite graphs as a gallery example instead of a function.
issue
Fix sphinx warnings from numpydoc parsing.#TITLE_END#Very minor cleanup of recently-introduced docstring parsing warnings.
issue
Add Python fallback to random_k_out_graph + document dependencies#TITLE_END#Intended to address the first bullet of #7716   Adds a try-except-based fallback to the pure-Python implementation for `random_k_out_graph` when NumPy is not installed. Also adds some documentation on the default behavior, as well as recipes to select the specific implementation for niche use-cases. Also adds a prominent `.. note` near the top of the docstring to highlight this behavior.
issue
Deployed documentation failing to load javascript, hangs on views.scientific-python.org#TITLE_END#There is something about the anolytics setup in the documentation that causes the deployed documentation to bork when I visit the site.  Essentially, when I go to `networkx.org/documentation/latest` (or `stable`), I get a `Connecting to views.scientific-python.org...` in the lower left corner of the browser, and an endless loading wheel in the browser favicon. While this is happening, none of the javascript elements on the page work at all (e.g. the searchbar is unresponsive). Similarly, if I load this in a private window where I don't have the pages cached, the theme fails to render properly - though refreshing will cause the theme to load. Ultimately the javascript elements remain unresponsive until the attempted to connection with `views.scientific-python.org` times out (~30 seconds) at which point the banners load and the search bar becomes active.  I'm on Firefox 132 with the uBlock origin extension, though I've seen this behavior on other computers with Firefox without any extensions.  Any ideas @jarrodmillman @stefanv ? I'm happy to provide more detailed debug info!
issue
Refactor closeness centrality tests#TITLE_END#A proposal for minor cleanup of a test file - My main goal is to get rid of the `setup_class` and replace it with direct calls to graph creation functions (or fixtures, when things are used in multiple tests).  Commits 2-4 contain some additional minor (opinionated) tweaks, but I went ahead and left out a complete parametrization overhaul to try to keep this somewhat reviewable. The first commit (2400e66) has all the important stuff, so if the other changes are too much I'm happy to cut back down to the first commit.  Note also that this does not create, destroy, or change the tests themselves!
issue
Remove print statements and comments from test suite#TITLE_END#In #7712 I ran into some print statements (or commented print statements) in the test suite which have no effect. That inspired me to grep through the rest of the tests - this PR removes print statements as well as some comments from the test suite.  90% of this is just linting, though there were a few other removals that were a bit more involved. I've tried to break this down into self-consistent commits to make review easier. The first two commits contain the "linting" - i.e. the removal of comments + print statements from the test suite.  The other commits involve changes to the code in the test suite, though not in a way that affects the tests themselves:  - 9a3ac10 converts some prints to asserts to improve test_graphviews.  - 280ef57 removes a conditional that would print if the test didn't pass (which is redundant because it is `assert` ed in the following line)  - 15f3546 removes a try-except that would print additional info if a failure was caught before re-raising. Unfortunately there was a lot of code inside the `try`, so the diff is ugly.  - 47b7803 contains more changes to `test_edge_augmentation`, but I decided to break it into a separate commit. It removes unused "verbose" flags and functionality from some internal test suite helper functions.  - Finally, 6bdf7f5 removes both printing and global "progress tracking" from the `run_all_random_functions` test helper.
issue
Re-submission of gh-7087 with better file provenance#TITLE_END#The state of the files in this PR (as of fd27267) are identical to 8452290 in #7087 . The main difference is I've re-applied the changes in a different way to better preserve attribution and make the changes more easily reviewable in github. See [this comment](https://github.com/networkx/networkx/pull/7087#issuecomment-2418511909) for details.  Now, the big payoff of all this reorg is that the *content changes* are now diff-able in the GitHub UI by reviewing the diff for commit fd27267 .
issue
Mv changelist to release deps.#TITLE_END#Just a minor procedural proposal - AIUI `changelist` is not necessary for the standard developer/contributor workflow, so perhaps it can be moved to reduce the dependency surface and avoid (admittedly very minor) issues like #7706 
issue
Fix doc warnings from recently added docs#TITLE_END#Minor formatting touchups to get rid of sphinx warnings in the doc build
issue
3D Animation gallery followups#TITLE_END#Follow-up to #7025   The main things are:  - Splits the original example into 2 examples: one for view updating and a second for the random walk  - Refactor the random walk example to use attribute setting instead redrawing everything each frame  What do you think @lobpcg ?
issue
Add examples to docstrings of subgraph_(iso/monomorphism) methods#TITLE_END#Addresses #7615   Adds examples to the `subgraph_is_(iso/mono)morphic` and `subgraph_(iso/mono)morophism_iter` to highlight the role that argument order plays when constructing the GraphMatcher objects.  After some back and forth, I ultimately decided to do a separate docstring for each of the 8 methods. This means I added stubs for the `DiGraphMatcher` methods for the sole purpose of updating the docstrings. I had tried adding *both* a directed and undirected example to the `GraphMatcher` methods (which are then inherited and thus don't need to be duplicated), but decided against it for the subjective reason that I thought the examples section got too long when I did so. One solution would be to make shorter examples... I'm happy to give that a stab as well if it's preferred --- feedback welcome!  This PR also includes a few minor rst cleanups and comment removals.
issue
Add examples section to `to_scipy_sparse_array`#TITLE_END#This PR is largely motivated by issues like #7173 and #7580 - it's clear from questions like these that the mapping between the nodes in the networkx graphs and the row/col indices of the adjacency matrix is a common point of confusion.  This PR attempts to address this confusion by re-working and significantly expanding the examples section of the `to_scipy_sparse_array` docstring.  I've also included some minor rst formatting updates in 06321dd and made similar formatting changes to the `adjacency_matrix` docstring (see 3e87b53), which is nearly the same.  The main change is 50207dd, which adds the re-worked examples. For now, I only changed the `to_scipy_sparse_array` docstring so we can focus on the content changes in review. I would propose to add the examples section to the `adjacency_matrix` docstring as well after it's been thoroughly reviewed!
issue
Revert breaking change to `node_link_*` link defaults#TITLE_END#Follow-up to #7565   One of the changes in #7565 was to change the default value for the `edges` (cf. `link`) parameter from `"links"` to the more networkx-y `"edges"`. At the time of the discussion, this was recognized as a breaking change, but the consensus (including my own opinion) was that it was better to get all of the changes in now rather than have multiple deprecation cycles.  However, this change did lead to a break downstream in `nx-guides`, which can be interpreted as a proxy for how disruptive any change might be in user-code. Essentially, changing the default without warning will affect any user attempting to read saved graphs saved in `node_link` format with the default edge values. Given this, I *do* now think a deprecation cycle is a good idea - not only does it avoid breakage in all existing code that relies on default values, but there's also a way to provide users with a "fixable" warning; i.e. a recommended change that will make the code more explicit and be forward-compatible. Here's a quick example of what this proposal looks like:  ```python >>> G = nx.Graph([(1, 2)]) >>> data = nx.node_link_data(G)  # Relying on default value for `link` raises a FutureWarning The default value will be `edges="edges" in NetworkX 3.6.  To make this warning go away, explicitly set the edges kwarg, e.g.:    nx.node_link_data(G, edges="links") to preserve current behavior, or   nx.node_link_data(G, edges="edges") for forward compatibility.  >>> data = nx.node_link_data(G, edges="edges")  # Explicitly specifying `edges` silences the warning  ```  The downside here is that this warning is *a lot* more noisy than the DeprecationWarning, as it is emitted whenever the function is called with the default value for `link`. However, I think this noisiness is just a consequence of changing a default in the API - in other words I don't see a good way around it. We can either raise a noisy warning (with a proposed fix that will make the warning go away) or we just break user code.  The only other alternative I could think of was to continue with the current plan (i.e. don't warn) and add special-casing logic for certain values of `edges`/`links` to raise specific exceptions that clearly state what the issue is (right now, users get a generic KeyError). IMO though, if you're going to raise special exceptions for the same special cases, you might as well warn instead and give users an opportunity to avoid the exception. 
issue
Forceatlas2#TITLE_END#Continuation of #5392   Opening this one instead of #7521 to disable force-pushing by non-maintainers to ensure that changes made during the review process are not accidentally lost.  A quick summary of the discussion form #5392 - the addition of the forceatlas2 layout to networkx has been approved, all that remains is to ensure that API is "networkx-y"!
issue
Bump minimum pydot version to 3.0.#TITLE_END#Follow up to gh-7588.  I came across this while running the test suite in an old environment which had pydot 2.0 installed: It looks like the `pydot.quote_id_if_necessary` function is only available in pydot 3.0.
issue
DOC: Rm redundant module from autosummary.#TITLE_END#Autosummary started raising warnings in the sphinx 7.4.X series related to multiple specifications of the parent module in autosummary directives (xref sphinx-doc/sphinx#12589).  There were some such instances in the tutorial. In this case the `networkx.` was unnecessary because the `.. currentmodule:: networkx` was specified at the top of the tutorial.  These changes preserve correct linking and get rid of the sphinx warnings.
issue
Update sphinx gallery config to enable sphinx build caching#TITLE_END#Somewhere in the Sphinx 7.3.X series, sphinx-build began emitting warnings about unpickleable values in the sphinx env:  ``` pickling environment... WARNING: cannot cache unpickable configuration value: 'sphinx_gallery_conf' (because it contains a function, class, or module object) done ```  Fortunately this has already been fixed in sphinx-gallery, cf. sphinx-gallery/sphinx-gallery#1289. These changes were part of the sphinx-gallery 0.16 release, which we're already pinned above. Due to the sphinx-gallery devs' hard work, the fix is very simple :tada: . Note to review whether this patch worked, you'd have to view the sphinx build log from circleci and verify there are no more unpickleable warnings. FWIW I've built the docs and verified this locally.  Finally, I also snuck in a tiny tweak to the warnings filters to clean up some of my own sloppiness in #7539 ( d43d177 should get rid of a SyntaxWarning I accidentally introduced).  ### Additional context  In case anyone else is interested in reading a bit more about this, I found the following useful: sphinx-gallery/sphinx-gallery#1286, matplotlib/matplotlib#28103, and [the sphinx-gallery configuration docs](https://sphinx-gallery.github.io/dev/configuration.html#importing-callables).
issue
Minor updates to colliders v_structures tests#TITLE_END#A very minor update to the warnings filter for `compute_v_structures` to prevent it showing up in test logs.  I also took the liberty to parametrize the colliders/v_structures tests. These refactors are very subjective - IMO they make the test body more readable, but the test setup less so :shrug: . WDYT @Schefflera-Arboricola ? NBD either way - I'm happy to back those last two commits out!
issue
Minor touchups to node_link functions#TITLE_END#A quick follow-up to #7532 ...  There are two proposed changes:  - 6347e0f removes `_attrs` dict from the node_link module which is both private and no longer used internally  - 0b68182 adds `pprint`-ing to the node_link docstring examples to make the dictionaries easier to read.  The first should be uncontroversial; the second is subjective, so if anyone prefers to back it out I'm happy to do so!
issue
Expire deprecated nx.join in favor of join_trees.#TITLE_END#Completes removal of `join` in favor of `join_trees`.
issue
Update images used in docs build workflow.#TITLE_END#Bump the images used to build the docs from oldest supported Python (3.10) to 3.12.
issue
CI: Add timeout limit to coverage job#TITLE_END#The [actions log](https://github.com/networkx/networkx/actions/workflows/coverage.yml) shows we recently had a coverage workflow hang until it was killed at the maximum timeout limit, which defaults to 360 minutes. I propose to add a more stringent timeout limit so that if a coverage job goes awry, it will be terminated sooner and free up the GH runners for other jobs.  I also bumped the coverage workflow Python version from 3.10 -> 3.12, though if there is a preference for keeping it pinned to the minimum-supported Python version, please go ahead and ignore!
issue
Rm deprecated normalized param from s_metric.#TITLE_END#Removes the deprecated `normalized` parameter from `s_metric`. I also snuck in a cleanup of the one existing test for this function, but I'm happy to back that out if it complicates review (the test itself is unchanged, only formatting updates).  Like other dep PRs - draft for now, but this is ready for review. 
issue
Expire deprecated `sort_neighbors` param in `generic_bfs_edges`#TITLE_END#Removes the `sort_neighbors` param per the deprecation in #5925 .  This is ready for review, but will mark as draft until we're sure there's no need for a 3.3 patch release!
issue
Expire deprecation for strongly_connected_components_recursive.#TITLE_END#Completes removal of `strongly_connected_components_recursive`.
issue
Expires the `random_tree` deprecation#TITLE_END#Finalizes the removal of `random_tree`
issue
Minor doc/test tweaks for dorogovtsev_goltsev_mendes#TITLE_END#Quick leftovers from review of #7473 - nothing important!
issue
CI: cugraph installation errors in docs workflow#TITLE_END#The doc build workflow on circleci has [begun failing](https://app.circleci.com/pipelines/github/networkx/networkx/9544/workflows/ac431fee-19dc-4b71-8bfa-a2ba7612cd82/jobs/15838).  The failure seems to be originating here: https://github.com/networkx/networkx/blob/530392e2f604ced69dfbe16a6f2bd904eb4b8a02/.circleci/config.yml#L44  @eriknw @rlratzel any ideas what may have changed? Perhaps we can pin the cugraph target to a specific tag/commit?  
issue
Use intersphinx_registry to manage intersphinx mapping.#TITLE_END#Makes use of the centralized intersphinx db provided/maintained by [intersphinx_registry](https://pypi.org/project/intersphinx_registry/). IMO this is a nice convenience!
issue
DEV: Add files generated by benchmarking to .gitignore.#TITLE_END#Very minor improvements to the development workflow related to benchmarking.  Running `asv` locally to do comparison benchmarks ends up generating files/dirs in the `benchmarks/` folder. This PR adds those generated files to `.gitignore`. I've only captured the files generated from local workflows (e.g. `asv run`, `asv continuous` and `asv compare`). There may be others for other workflows, in which case please feel free to add them here (@MridulS ).
issue
Add docstring example for directed tree.#TITLE_END#Closes #7410
issue
Can't replicate directed functionality of deprecated `random_tree`#TITLE_END#The deprecated `random_tree` function supported the `create_using` kwarg to allow the creation of directed trees:  ```python >>> G = nx.random_tree(10, create_using=nx.DiGraph, seed=42) >>> G.edges OutEdgeView([(0, 6), (0, 4), (1, 5), (1, 8), (2, 1), (3, 7), (3, 2), (4, 3), (8, 9)]) ```  The `random_labeled_tree` function is the recommended replacement for the deprecated `random_tree` fn, but it doesn't support the `create_using` kwarg. AFAICT, the only way to replicate the current behavior of `random_tree` is to create a DiGraph from the generated edges manually:  ```python >>> H = nx.random_labeled_tree(10, seed=42)  # Undirected >>> DH = nx.DiGraph() >>> DH.add_edges_from(H) ```  I'm mostly just raising for visibility - I don't recall if this was an explicit decision to remove `create_using` support - @vigna . From a user perspective it'd be convenient to be able to replicate existing functionality with as little code modification as possible.
issue
Prioritize edgelist representations in `to_networkx_graph`#TITLE_END#This PR was originally motivated by #7401. The main issue there is the extraneous `ImportWarning` s that are raised by `to_networkx_graph` (often via the graph constructors) when creating graphs in an environment without the default dependencies (numpy, scipy, pandas). This is the case because converting from these data structures is actually attempted *before* attempting to interpret the input as an edgelist. However, creating a graph from an edge list (or edgelist-like, such as an EdgeView or generator of edges) is a very common operation - certainly much more so than from numpy/scipy/pandas objects within NetworkX itself. This latter point can be demonstrated relatively simply: see https://github.com/networkx/networkx/pull/7402#issuecomment-2048157399.  I wanted to investigate whether it'd be possible to reorganize the control-flow of `to_networkx_graph` so that the most common (at least according to the internal NX usage heuristic) data structures are handled first. This has the advantage of solving #7401 for basically all cases[^1]. I suspect it may result in a minor performance improvement too, though the use-case where that's actually meaningful may not be that common (i.e. an analysis where many graph instances are being created). I've added a benchmark to demonstrate this last point as well; results on my machine:  <details>   <summary> ~10% improvement for this branch </summary> <pre> | Change   | Before [d7b61bd1] <main>         | After [a2b6d7fe] <reorder-to_networkx_graph>                              | Ratio   | Benchmark (Parameter)                                                                                                                                  | |----------|----------------------------|------------------------------------------------|---------|--------------------------------------------------------------------------------------------------------------------------------------------------------| | -        | 5.54±0.09μs                | 4.95±0.07μs                                    | 0.89    | benchmark_to_networkx_graph.ToNetworkXGraphBenchmark.time_to_networkx_graph_direct(<class 'networkx.classes.graph.Graph'>)                             | | -        | 3.12±0.01ms                | 2.79±0.02ms                                    | 0.89    | benchmark_to_networkx_graph.ToNetworkXGraphBenchmark.time_to_networkx_graph_direct_multi_instance(<class 'networkx.classes.digraph.DiGraph'>)          | | -        | 2.74±0.02ms                | 2.46±0.01ms                                    | 0.90    | benchmark_to_networkx_graph.ToNetworkXGraphBenchmark.time_to_networkx_graph_direct_multi_instance(<class 'networkx.classes.graph.Graph'>)              | | -        | 7.28±0.04μs                | 6.44±0.04μs                                    | 0.88    | benchmark_to_networkx_graph.ToNetworkXGraphBenchmark.time_to_networkx_graph_via_constructor(<class 'networkx.classes.digraph.DiGraph'>)                | | -        | 6.34±0.03μs                | 5.70±0.03μs                                    | 0.90    | benchmark_to_networkx_graph.ToNetworkXGraphBenchmark.time_to_networkx_graph_via_constructor(<class 'networkx.classes.graph.Graph'>)                    | |          | 3.54±0.07ms                | 3.24±0.01ms                                    | 0.91    | benchmark_to_networkx_graph.ToNetworkXGraphBenchmark.time_to_networkx_graph_via_constructor_multi_instance(<class 'networkx.classes.digraph.DiGraph'>) | | -        | 3.18±0.02ms                | 2.85±0.05ms                                    | 0.90    | benchmark_to_networkx_graph.ToNetworkXGraphBenchmark.time_to_networkx_graph_via_constructor_multi_instance(<class 'networkx.classes.graph.Graph'>)     | </details>  The potential downside of this approach is that there may be subtle corner cases due to the change in the order/"breadth" of the isinstance checks. I'm confident that this works for every case that the NetworkX test suite covers (using the procedure described [here](https://github.com/networkx/networkx/pull/7402#issuecomment-2048157399)), but there may be cases that are uncovered. For example, it turns out there is *a single* test in the test suite that uses a tuple of edges instead of a list of edges. That case would have been missed were it not for that single test!  [^1]: The only exceptions being branches where an exception is explicitly raised.
issue
Cleanup remaining usages of deprecated `random_tree` in package#TITLE_END#Replaces the remaining usages of the deprecated `random_tree` with `random_labeled_tree`.
issue
Use nodelist feature of from_numpy_array.#TITLE_END#Minor follow-up to #7412, which added the `nodelist` kwarg to `from_numpy_array`. I did a quick grep through the library to see if there were any opportunities to apply it internally and these are the only instances I found.
issue
Minor updates to simple_cycles docstring.#TITLE_END#Including correcting the parameter description to note that all graph types (simple, directed, multi) are supported, per [this comment](https://github.com/networkx/networkx/issues/7401#issuecomment-2061988760) 
issue
Expires the `forest_str` deprecation#TITLE_END#This was supposed to make it into 3.3 (oops), but better late than never! Removes `forest_str` in favor of `generate_network_text` as the deprecation message specifies.  I went ahead and updated the `forest_str` tests to use `generate_network_text` (rather than delete them) to verify that `generate_network_text` works as expected as a replacement for those use-cases. For tests that probed features/exceptions specific to `forest_str`, I went ahead and removed them. I also snuck in a commit to rm remaining extraneous print statements from the readwrite test suite.
issue
Add a `nodelist` feature to `from_numpy_array`#TITLE_END#Adds a kwarg to allow users to specify a sequence of nodes which map to the dimensions of the adjacency matrix. The mapping implicitly relies on the ordering of the nodes in the adjacency matrix, i.e. `node_labels[0] -> row 0`, `node_labels[1] -> row 1`, etc.  I'm not sure about the name of the kwarg. One motivating factor for this proposal is it fixes #7407 in an efficient way. I've added that change in a1a17ac7, but I'm happy to break that into a separate PR in order to focus on the new feature proposal!
issue
Graph name attribute for `complete_bipartite_graph` verbose for non-integer intputs#TITLE_END#The `complete_bipartite_graph` creation function adds a `.graph["name"]` attribute like so:  https://github.com/networkx/networkx/blob/cca1a715217a102b970800c43a066dd759ece92d/networkx/algorithms/bipartite/generators.py#L65  This works fine when `n1` and `n2` are integers, but can get ugly when they aren't. For example:  ```python # Generator expressions designed to be intentionally verbose >>> n = (i for i in range(4)) >>> m = (i for i in range(4, 7)) >>> G = nx.complete_bipartite_graph(n, m) >>> print(G) Graph named 'complete_bipartite_graph(<generator object <genexpr> at 0x7e94bc447370>, <generator object <genexpr> at 0x7e94bc5352f0>)' with 7 nodes and 12 edges ```
issue
A few more doctest skips for mpl/np dependencies.#TITLE_END#Followup to #7388   Additional test-suite config to skip doctests when dependencies aren't installed. The main change is that the docstrings of the `generators/classic.py` graph creation functions now depend on matplotlib with the addition of the `.. plot` directive. IMO this is totally fine and straightforward to skip (see below).  The other two changes are:  - nx_latex docstring depends on numpy  - alphabetizing the `needs_numpy` list.
issue
Document missing shortest_path functions#TITLE_END#Follow-up to #5959 - adds `all_pairs_all_shortest_paths` and `single_source_all_shortest_paths` to the reference guide.  I also added a minor rst formatting nit to fixup a few sphinx warnings during doc building.
issue
Rm deprecated np.row_stack in favor of vstack.#TITLE_END#Handling a new deprecation from the NumPy 2.1 nightlies.  On an unrelated note, matplotlib has released v3.8.4 with support for numpy 2.0. This release (along with the latest mpl 3.9.0dev nightly wheel) fixes the test failures that had been cropping up in `test_pylab` with numpy 2.0. In other words, NX is now green across the board with numpy 2.0, scipy 1.13, and matplotlib 3.8.4!
issue
Update test suite to handle when scipy is not installed#TITLE_END#Fixes #7383, or at least the actionable part of that issue.  These changes don't really affect user code/experience at all, the main motivation is to make it more visible when functions are added/changed that depend on scipy. Though we don't generally expect to have users with environments where only some of the default dependencies are installed, it's useful during review to automatically catch these cases and can be useful at a high-level to get a sense of just how much of the codebase depends on the default dependencies, with numpy and scipy being "special" in that they both provide data structures (arrays and sparse arrays, respectively) that are used quite a bit throughout NX.
issue
Dispatch test suite incompatible with pygraphviz#TITLE_END#Just something I noticed today while attempting to run the test suite with `NETWORKX_TEST_BACKEND=nx-loopback` with the `pytest-xdist` extension. Doing so gives Segmentation faults, indicating that something about the nx-loopback mechanism isn't threadsafe (perhaps this is known?).  ## To reproduce  Install xdist (`pip install pytest-xdist`) in the development environment and run the test suite with the nx-loopback backend with more than 1 worker, e.g.:  ```bash NETWORKX_TEST_BACKEND=nx-loopback pytest -n auto --doctest-modules --durations=10 --pyargs networkx ```  A selection from the pytest log below (can't include the whole thing due to gh issue character limits):  <details>   <summary>Pytest log</summary> <pre> =========================================================== test session starts =========================================================== platform linux -- Python 3.11.6, pytest-8.1.1, pluggy-1.4.0 Matplotlib: 3.9.0.dev0 Freetype: 2.6.1 rootdir: /home/ross/repos/networkx configfile: pyproject.toml plugins: anyio-4.2.0, mpl-0.16.1, cov-4.1.0, xdist-3.5.0 12 workers [6269 items]    <clipped> ............................................x....x................................................................................. [ 64%] ........................................................................................................Fatal Python error: Segmentation fault  Thread 0x0000747ba7c006c0 (most recent call first):   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 474 in read   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 507 in from_io   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1049 in _thread_receiver   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 296 in run   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 361 in _perform_spawn  Current thread 0x0000747ba9422740 (most recent call first):   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pygraphviz/graphviz.py", line 226 in agnameof   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pygraphviz/agraph.py", line 228 in __repr__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_io/saferepr.py", line 73 in repr_instance   File "/usr/lib/python3.11/reprlib.py", line 63 in repr1   File "/usr/lib/python3.11/reprlib.py", line 53 in repr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_io/saferepr.py", line 61 in repr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_io/saferepr.py", line 111 in saferepr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 843 in repr_args   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 939 in repr_traceback_entry   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 994 in <listcomp>   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 993 in repr_traceback   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 1064 in repr_excinfo   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 699 in getrepr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/nodes.py", line 464 in _repr_failure_py   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/python.py", line 1814 in repr_failure   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/reports.py", line 364 in from_item_and_call   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 367 in pytest_runtest_makereport   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 242 in call_and_report   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 134 in runtestprotocol   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 115 in pytest_runtest_protocol   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/xdist/remote.py", line 174 in run_one_test   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/xdist/remote.py", line 157 in pytest_runtestloop   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/main.py", line 339 in _main   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/main.py", line 285 in wrap_session   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/main.py", line 332 in pytest_cmdline_main   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/xdist/remote.py", line 355 in <module>   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1157 in executetask   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 296 in run   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 361 in _perform_spawn   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 343 in integrate_as_primary_thread   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1142 in serve   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1640 in serve   File "<string>", line 8 in <module>   File "<string>", line 1 in <module>  Extension modules: markupsafe._speedups, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, scipy._lib._ccallback_c, PIL._imaging, matplotlib._path, kiwisolver._cext, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pygraphviz._graphviz, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.optimize._direct, fontTools.misc.bezierTools, lxml._elementpath, lxml.etree, fontTools.varLib.iup (total: 116) ........................... [ 66%] ............................................................................x...............................FFF.................... [ 68%] .............................F...F................................................................................................. [ 71%] ............[gw3] node down: Not properly terminated F replacing crashed worker gw3 collecting: 12/13   <clipped>  Fatal Python error: Segmentation fault  Thread 0x00007eda27a006c0 (most recent call first):   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 474 in read   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 507 in from_io   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1049 in _thread_receiver   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 296 in run   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 361 in _perform_spawn  Current thread 0x00007eda293e3740 (most recent call first):   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pygraphviz/graphviz.py", line 226 in agnameof   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pygraphviz/agraph.py", line 228 in __repr__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_io/saferepr.py", line 73 in repr_instance   File "/usr/lib/python3.11/reprlib.py", line 63 in repr1   File "/usr/lib/python3.11/reprlib.py", line 53 in repr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_io/saferepr.py", line 61 in repr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_io/saferepr.py", line 111 in saferepr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 843 in repr_args   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 939 in repr_traceback_entry   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 994 in <listcomp>   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 993 in repr_traceback   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 1064 in repr_excinfo   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 699 in getrepr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/nodes.py", line 464 in _repr_failure_py   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/python.py", line 1814 in repr_failure   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/reports.py", line 364 in from_item_and_call   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 367 in pytest_runtest_makereport   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 242 in call_and_report   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 134 in runtestprotocol   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 115 in pytest_runtest_protocol   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/xdist/remote.py", line 174 in run_one_test   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/xdist/remote.py", line 157 in pytest_runtestloop   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/main.py", line 339 in _main   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/main.py", line 285 in wrap_session   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/main.py", line 332 in pytest_cmdline_main   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/xdist/remote.py", line 355 in <module>   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1157 in executetask   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 296 in run   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 361 in _perform_spawn   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 343 in integrate_as_primary_thread   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1142 in serve   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1640 in serve   File "<string>", line 8 in <module>   File "<string>", line 1 in <module>  Extension modules: markupsafe._speedups, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, scipy._lib._ccallback_c, PIL._imaging, matplotlib._path, kiwisolver._cext, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pygraphviz._graphviz, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2 (total: 86) .............................[gw12] node down: Not properly terminated F replacing crashed worker gw12 collecting: 13/14 workers.........................[gw4] node down: Not properly terminated F replacing crashed worker gw4 collecting: 13/15 workers...Fatal Python error: Segmentation fault  Thread 0x00007a522ba006c0 (most recent call first):   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 474 in read   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 507 in from_io   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1049 in _thread_receiver   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 296 in run   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 361 in _perform_spawn  Current thread 0x00007a522d3ff740 (most recent call first):   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pygraphviz/graphviz.py", line 226 in agnameof   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pygraphviz/agraph.py", line 228 in __repr__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_io/saferepr.py", line 73 in repr_instance   File "/usr/lib/python3.11/reprlib.py", line 63 in repr1   File "/usr/lib/python3.11/reprlib.py", line 53 in repr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_io/saferepr.py", line 61 in repr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_io/saferepr.py", line 111 in saferepr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 843 in repr_args   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 939 in repr_traceback_entry   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 994 in <listcomp>   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 993 in repr_traceback   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 1064 in repr_excinfo   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/_code/code.py", line 699 in getrepr   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/nodes.py", line 464 in _repr_failure_py   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/python.py", line 1814 in repr_failure   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/reports.py", line 364 in from_item_and_call   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 367 in pytest_runtest_makereport   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 242 in call_and_report   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 134 in runtestprotocol   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/runner.py", line 115 in pytest_runtest_protocol   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/xdist/remote.py", line 174 in run_one_test   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/xdist/remote.py", line 157 in pytest_runtestloop   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/main.py", line 339 in _main   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/main.py", line 285 in wrap_session   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/_pytest/main.py", line 332 in pytest_cmdline_main   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_callers.py", line 102 in _multicall   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_manager.py", line 119 in _hookexec   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/pluggy/_hooks.py", line 501 in __call__   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/xdist/remote.py", line 355 in <module>   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1157 in executetask   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 296 in run   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 361 in _perform_spawn   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 343 in integrate_as_primary_thread   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1142 in serve   File "/home/ross/.virtualenvs/nx-dev/lib/python3.11/site-packages/execnet/gateway_base.py", line 1640 in serve   File "<string>", line 8 in <module>   File "<string>", line 1 in <module>  Extension modules: markupsafe._speedups, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, scipy._lib._ccallback_c, PIL._imaging, matplotlib._path, kiwisolver._cext, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pygraphviz._graphviz, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, fontTools.misc.bezierTools, lxml._elementpath, lxml.etree, fontTools.varLib.iup, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.optimize._direct, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._ansari_swilk_statistics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper (total: 161) 15 workers [6269 items]  ....s...........[gw6] node down: Not properly terminated F replacing crashed worker gw6 16 workers [6269 items]  ..................s..............................................................................................................................................x...............x......x..............................................................................................................................................................................................................s...................................................................................................................................................s.................................................................................................... ================================================================ FAILURES ================================================================= _____________________________________________________ TestPydot.test_pydot[neato-G0] ______________________________________________________ [gw2] linux -- Python 3.11.6 /home/ross/.virtualenvs/nx-dev/bin/python  self = <networkx.drawing.tests.test_pydot.TestPydot object at 0x75a13f3dd050>, G = <networkx.classes.graph.Graph object at 0x75a13f3cd550> prog = 'neato', tmp_path = PosixPath('/tmp/pytest-of-ross/pytest-4/popen-gw2/test_pydot_neato_G0_0')      @pytest.mark.parametrize("G", (nx.Graph(), nx.DiGraph()))     @pytest.mark.parametrize("prog", ("neato", "dot"))     def test_pydot(self, G, prog, tmp_path):         """         Validate :mod:`pydot`-based usage of the passed NetworkX graph with the         passed basename of an external GraphViz command (e.g., `dot`, `neato`).         """              # Set the name of this graph to... "G". Failing to do so will         # subsequently trip an assertion expecting this name.         G.graph["name"] = "G"              # Add arbitrary nodes and edges to the passed empty graph.         G.add_edges_from([("A", "B"), ("A", "C"), ("B", "C"), ("A", "D")])         G.add_node("E")              # Validate layout of this graph with the passed GraphViz command.         graph_layout = nx.nx_pydot.pydot_layout(G, prog=prog)         assert isinstance(graph_layout, dict)              # Convert this graph into a "pydot.Dot" instance.         P = nx.nx_pydot.to_pydot(G)              # Convert this "pydot.Dot" instance back into a graph of the same type.         G2 = G.__class__(nx.nx_pydot.from_pydot(P))              # Validate the original and resulting graphs to be the same.         assert graphs_equal(G, G2)              fname = tmp_path / "out.dot"              # Serialize this "pydot.Dot" instance to a temporary file in dot format         P.write_raw(fname)              # Deserialize a list of new "pydot.Dot" instances back from this file.         Pin_list = pydot.graph_from_dot_file(path=fname, encoding="utf-8")              # Validate this file to contain only one graph.         assert len(Pin_list) == 1              # The single "pydot.Dot" instance deserialized from this file.         Pin = Pin_list[0]              # Sorted list of all nodes in the original "pydot.Dot" instance.         n1 = sorted(p.get_name() for p in P.get_node_list())              # Sorted list of all nodes in the deserialized "pydot.Dot" instance.         n2 = sorted(p.get_name() for p in Pin.get_node_list())              # Validate these instances to contain the same nodes.         assert n1 == n2              # Sorted list of all edges in the original "pydot.Dot" instance.         e1 = sorted((e.get_source(), e.get_destination()) for e in P.get_edge_list())              # Sorted list of all edges in the original "pydot.Dot" instance.         e2 = sorted((e.get_source(), e.get_destination()) for e in Pin.get_edge_list())              # Validate these instances to contain the same edges.         assert e1 == e2              # Deserialize a new graph of the same type back from this file. >       Hin = nx.nx_pydot.read_dot(fname)  networkx/drawing/tests/test_pydot.py:75:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ networkx/utils/decorators.py:789: in func     return argmap._lazy_compile(__wrapper)(*args, **kwargs) <class 'networkx.utils.decorators.argmap'> compilation 1108:5: in argmap_read_dot_1103     ??? networkx/utils/backends.py:538: in __call__     return self._convert_and_call_for_tests( networkx/utils/backends.py:949: in _convert_and_call_for_tests     result = getattr(backend, self.name)(*converted_args, **converted_kwargs) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  path = <itertools._tee object at 0x75a130792800>      @open_file(0, mode="r")     @nx._dispatchable(name="pydot_read_dot", graphs=None, returns_graph=True)     def read_dot(path):         """Returns a NetworkX :class:`MultiGraph` or :class:`MultiDiGraph` from the         dot file with the passed path.              If this file contains multiple graphs, only the first such graph is         returned. All graphs _except_ the first are silently ignored.              Parameters         ----------         path : str or file             Filename or file handle.              Returns         -------         G : MultiGraph or MultiDiGraph             A :class:`MultiGraph` or :class:`MultiDiGraph`.              Notes         -----         Use `G = nx.Graph(nx.nx_pydot.read_dot(path))` to return a :class:`Graph` instead of a         :class:`MultiGraph`.         """         import pydot      >       data = path.read() E       AttributeError: 'itertools._tee' object has no attribute 'read'  networkx/drawing/nx_pydot.py:74: AttributeError _____________________________________________________ TestPydot.test_pydot[neato-G1] ______________________________________________________ [gw2] linux -- Python 3.11.6 /home/ross/.virtualenvs/nx-dev/bin/python  self = <networkx.drawing.tests.test_pydot.TestPydot object at 0x75a13f3dd790> G = <networkx.classes.digraph.DiGraph object at 0x75a13f3cd690>, prog = 'neato' tmp_path = PosixPath('/tmp/pytest-of-ross/pytest-4/popen-gw2/test_pydot_neato_G1_0')      @pytest.mark.parametrize("G", (nx.Graph(), nx.DiGraph()))     @pytest.mark.parametrize("prog", ("neato", "dot"))     def test_pydot(self, G, prog, tmp_path):         """         Validate :mod:`pydot`-based usage of the passed NetworkX graph with the         passed basename of an external GraphViz command (e.g., `dot`, `neato`).         """              # Set the name of this graph to... "G". Failing to do so will         # subsequently trip an assertion expecting this name.         G.graph["name"] = "G"              # Add arbitrary nodes and edges to the passed empty graph.         G.add_edges_from([("A", "B"), ("A", "C"), ("B", "C"), ("A", "D")])         G.add_node("E")              # Validate layout of this graph with the passed GraphViz command.         graph_layout = nx.nx_pydot.pydot_layout(G, prog=prog)         assert isinstance(graph_layout, dict)              # Convert this graph into a "pydot.Dot" instance.         P = nx.nx_pydot.to_pydot(G)              # Convert this "pydot.Dot" instance back into a graph of the same type.         G2 = G.__class__(nx.nx_pydot.from_pydot(P))              # Validate the original and resulting graphs to be the same.         assert graphs_equal(G, G2)              fname = tmp_path / "out.dot"              # Serialize this "pydot.Dot" instance to a temporary file in dot format         P.write_raw(fname)              # Deserialize a list of new "pydot.Dot" instances back from this file.         Pin_list = pydot.graph_from_dot_file(path=fname, encoding="utf-8")              # Validate this file to contain only one graph.         assert len(Pin_list) == 1              # The single "pydot.Dot" instance deserialized from this file.         Pin = Pin_list[0]              # Sorted list of all nodes in the original "pydot.Dot" instance.         n1 = sorted(p.get_name() for p in P.get_node_list())              # Sorted list of all nodes in the deserialized "pydot.Dot" instance.         n2 = sorted(p.get_name() for p in Pin.get_node_list())              # Validate these instances to contain the same nodes.         assert n1 == n2              # Sorted list of all edges in the original "pydot.Dot" instance.         e1 = sorted((e.get_source(), e.get_destination()) for e in P.get_edge_list())              # Sorted list of all edges in the original "pydot.Dot" instance.         e2 = sorted((e.get_source(), e.get_destination()) for e in Pin.get_edge_list())              # Validate these instances to contain the same edges.         assert e1 == e2              # Deserialize a new graph of the same type back from this file. >       Hin = nx.nx_pydot.read_dot(fname)  networkx/drawing/tests/test_pydot.py:75:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <class 'networkx.utils.decorators.argmap'> compilation 1108:5: in argmap_read_dot_1103     ??? networkx/utils/backends.py:538: in __call__     return self._convert_and_call_for_tests( networkx/utils/backends.py:949: in _convert_and_call_for_tests     result = getattr(backend, self.name)(*converted_args, **converted_kwargs) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  path = <itertools._tee object at 0x75a11922a800>      @open_file(0, mode="r")     @nx._dispatchable(name="pydot_read_dot", graphs=None, returns_graph=True)     def read_dot(path):         """Returns a NetworkX :class:`MultiGraph` or :class:`MultiDiGraph` from the         dot file with the passed path.              If this file contains multiple graphs, only the first such graph is         returned. All graphs _except_ the first are silently ignored.              Parameters         ----------         path : str or file             Filename or file handle.              Returns         -------         G : MultiGraph or MultiDiGraph             A :class:`MultiGraph` or :class:`MultiDiGraph`.              Notes         -----         Use `G = nx.Graph(nx.nx_pydot.read_dot(path))` to return a :class:`Graph` instead of a         :class:`MultiGraph`.         """         import pydot      >       data = path.read() E       AttributeError: 'itertools._tee' object has no attribute 'read'  networkx/drawing/nx_pydot.py:74: AttributeError ______________________________________________________ TestPydot.test_pydot[dot-G0] _______________________________________________________ [gw2] linux -- Python 3.11.6 /home/ross/.virtualenvs/nx-dev/bin/python  self = <networkx.drawing.tests.test_pydot.TestPydot object at 0x75a13f3dda50>, G = <networkx.classes.graph.Graph object at 0x75a13f3cd550> prog = 'dot', tmp_path = PosixPath('/tmp/pytest-of-ross/pytest-4/popen-gw2/test_pydot_dot_G0_0')      @pytest.mark.parametrize("G", (nx.Graph(), nx.DiGraph()))     @pytest.mark.parametrize("prog", ("neato", "dot"))     def test_pydot(self, G, prog, tmp_path):         """         Validate :mod:`pydot`-based usage of the passed NetworkX graph with the         passed basename of an external GraphViz command (e.g., `dot`, `neato`).         """              # Set the name of this graph to... "G". Failing to do so will         # subsequently trip an assertion expecting this name.         G.graph["name"] = "G"              # Add arbitrary nodes and edges to the passed empty graph.         G.add_edges_from([("A", "B"), ("A", "C"), ("B", "C"), ("A", "D")])         G.add_node("E")              # Validate layout of this graph with the passed GraphViz command.         graph_layout = nx.nx_pydot.pydot_layout(G, prog=prog)         assert isinstance(graph_layout, dict)              # Convert this graph into a "pydot.Dot" instance.         P = nx.nx_pydot.to_pydot(G)              # Convert this "pydot.Dot" instance back into a graph of the same type.         G2 = G.__class__(nx.nx_pydot.from_pydot(P))              # Validate the original and resulting graphs to be the same.         assert graphs_equal(G, G2)              fname = tmp_path / "out.dot"              # Serialize this "pydot.Dot" instance to a temporary file in dot format         P.write_raw(fname)              # Deserialize a list of new "pydot.Dot" instances back from this file.         Pin_list = pydot.graph_from_dot_file(path=fname, encoding="utf-8")              # Validate this file to contain only one graph.         assert len(Pin_list) == 1              # The single "pydot.Dot" instance deserialized from this file.         Pin = Pin_list[0]              # Sorted list of all nodes in the original "pydot.Dot" instance.         n1 = sorted(p.get_name() for p in P.get_node_list())              # Sorted list of all nodes in the deserialized "pydot.Dot" instance.         n2 = sorted(p.get_name() for p in Pin.get_node_list())              # Validate these instances to contain the same nodes.         assert n1 == n2              # Sorted list of all edges in the original "pydot.Dot" instance.         e1 = sorted((e.get_source(), e.get_destination()) for e in P.get_edge_list())              # Sorted list of all edges in the original "pydot.Dot" instance.         e2 = sorted((e.get_source(), e.get_destination()) for e in Pin.get_edge_list())              # Validate these instances to contain the same edges.         assert e1 == e2              # Deserialize a new graph of the same type back from this file. >       Hin = nx.nx_pydot.read_dot(fname)  networkx/drawing/tests/test_pydot.py:75:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <class 'networkx.utils.decorators.argmap'> compilation 1108:5: in argmap_read_dot_1103     ??? networkx/utils/backends.py:538: in __call__     return self._convert_and_call_for_tests( networkx/utils/backends.py:949: in _convert_and_call_for_tests     result = getattr(backend, self.name)(*converted_args, **converted_kwargs) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  path = <itertools._tee object at 0x75a114fe2880>      @open_file(0, mode="r")     @nx._dispatchable(name="pydot_read_dot", graphs=None, returns_graph=True)     def read_dot(path):         """Returns a NetworkX :class:`MultiGraph` or :class:`MultiDiGraph` from the         dot file with the passed path.              If this file contains multiple graphs, only the first such graph is         returned. All graphs _except_ the first are silently ignored.              Parameters         ----------         path : str or file             Filename or file handle.              Returns         -------         G : MultiGraph or MultiDiGraph             A :class:`MultiGraph` or :class:`MultiDiGraph`.              Notes         -----         Use `G = nx.Graph(nx.nx_pydot.read_dot(path))` to return a :class:`Graph` instead of a         :class:`MultiGraph`.         """         import pydot      >       data = path.read() E       AttributeError: 'itertools._tee' object has no attribute 'read'  networkx/drawing/nx_pydot.py:74: AttributeError ______________________________________________________ TestPydot.test_pydot[dot-G1] _______________________________________________________ [gw2] linux -- Python 3.11.6 /home/ross/.virtualenvs/nx-dev/bin/python  self = <networkx.drawing.tests.test_pydot.TestPydot object at 0x75a13f3cf1d0> G = <networkx.classes.digraph.DiGraph object at 0x75a13f3cd690>, prog = 'dot' tmp_path = PosixPath('/tmp/pytest-of-ross/pytest-4/popen-gw2/test_pydot_dot_G1_0')      @pytest.mark.parametrize("G", (nx.Graph(), nx.DiGraph()))     @pytest.mark.parametrize("prog", ("neato", "dot"))     def test_pydot(self, G, prog, tmp_path):         """         Validate :mod:`pydot`-based usage of the passed NetworkX graph with the         passed basename of an external GraphViz command (e.g., `dot`, `neato`).         """              # Set the name of this graph to... "G". Failing to do so will         # subsequently trip an assertion expecting this name.         G.graph["name"] = "G"              # Add arbitrary nodes and edges to the passed empty graph.         G.add_edges_from([("A", "B"), ("A", "C"), ("B", "C"), ("A", "D")])         G.add_node("E")              # Validate layout of this graph with the passed GraphViz command.         graph_layout = nx.nx_pydot.pydot_layout(G, prog=prog)         assert isinstance(graph_layout, dict)              # Convert this graph into a "pydot.Dot" instance.         P = nx.nx_pydot.to_pydot(G)              # Convert this "pydot.Dot" instance back into a graph of the same type.         G2 = G.__class__(nx.nx_pydot.from_pydot(P))              # Validate the original and resulting graphs to be the same.         assert graphs_equal(G, G2)              fname = tmp_path / "out.dot"              # Serialize this "pydot.Dot" instance to a temporary file in dot format         P.write_raw(fname)              # Deserialize a list of new "pydot.Dot" instances back from this file.         Pin_list = pydot.graph_from_dot_file(path=fname, encoding="utf-8")              # Validate this file to contain only one graph.         assert len(Pin_list) == 1              # The single "pydot.Dot" instance deserialized from this file.         Pin = Pin_list[0]              # Sorted list of all nodes in the original "pydot.Dot" instance.         n1 = sorted(p.get_name() for p in P.get_node_list())              # Sorted list of all nodes in the deserialized "pydot.Dot" instance.         n2 = sorted(p.get_name() for p in Pin.get_node_list())              # Validate these instances to contain the same nodes.         assert n1 == n2              # Sorted list of all edges in the original "pydot.Dot" instance.         e1 = sorted((e.get_source(), e.get_destination()) for e in P.get_edge_list())              # Sorted list of all edges in the original "pydot.Dot" instance.         e2 = sorted((e.get_source(), e.get_destination()) for e in Pin.get_edge_list())              # Validate these instances to contain the same edges.         assert e1 == e2              # Deserialize a new graph of the same type back from this file. >       Hin = nx.nx_pydot.read_dot(fname)  networkx/drawing/tests/test_pydot.py:75:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <class 'networkx.utils.decorators.argmap'> compilation 1108:5: in argmap_read_dot_1103     ??? networkx/utils/backends.py:538: in __call__     return self._convert_and_call_for_tests( networkx/utils/backends.py:949: in _convert_and_call_for_tests     result = getattr(backend, self.name)(*converted_args, **converted_kwargs) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  path = <itertools._tee object at 0x75a114e1d680>      @open_file(0, mode="r")     @nx._dispatchable(name="pydot_read_dot", graphs=None, returns_graph=True)     def read_dot(path):         """Returns a NetworkX :class:`MultiGraph` or :class:`MultiDiGraph` from the         dot file with the passed path.              If this file contains multiple graphs, only the first such graph is         returned. All graphs _except_ the first are silently ignored.              Parameters         ----------         path : str or file             Filename or file handle.              Returns         -------         G : MultiGraph or MultiDiGraph             A :class:`MultiGraph` or :class:`MultiDiGraph`.              Notes         -----         Use `G = nx.Graph(nx.nx_pydot.read_dot(path))` to return a :class:`Graph` instead of a         :class:`MultiGraph`.         """         import pydot      >       data = path.read() E       AttributeError: 'itertools._tee' object has no attribute 'read'  networkx/drawing/nx_pydot.py:74: AttributeError ________________________________________________________ TestPydot.test_read_write ________________________________________________________ [gw2] linux -- Python 3.11.6 /home/ross/.virtualenvs/nx-dev/bin/python  self = <networkx.drawing.tests.test_pydot.TestPydot object at 0x75a13f3c62d0>      def test_read_write(self):         G = nx.MultiGraph()         G.graph["name"] = "G"         G.add_edge("1", "2", key="0")  # read assumes strings         fh = StringIO()         nx.nx_pydot.write_dot(G, fh)         fh.seek(0) >       H = nx.nx_pydot.read_dot(fh)  networkx/drawing/tests/test_pydot.py:88:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <class 'networkx.utils.decorators.argmap'> compilation 1108:5: in argmap_read_dot_1103     ??? networkx/utils/backends.py:538: in __call__     return self._convert_and_call_for_tests( networkx/utils/backends.py:949: in _convert_and_call_for_tests     result = getattr(backend, self.name)(*converted_args, **converted_kwargs) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  path = <itertools._tee object at 0x75a12e9a1f40>      @open_file(0, mode="r")     @nx._dispatchable(name="pydot_read_dot", graphs=None, returns_graph=True)     def read_dot(path):         """Returns a NetworkX :class:`MultiGraph` or :class:`MultiDiGraph` from the         dot file with the passed path.              If this file contains multiple graphs, only the first such graph is         returned. All graphs _except_ the first are silently ignored.              Parameters         ----------         path : str or file             Filename or file handle.              Returns         -------         G : MultiGraph or MultiDiGraph             A :class:`MultiGraph` or :class:`MultiDiGraph`.              Notes         -----         Use `G = nx.Graph(nx.nx_pydot.read_dot(path))` to return a :class:`Graph` instead of a         :class:`MultiGraph`.         """         import pydot      >       data = path.read() E       AttributeError: 'itertools._tee' object has no attribute 'read'  networkx/drawing/nx_pydot.py:74: AttributeError __________________________________________________ networkx/drawing/tests/test_agraph.py __________________________________________________ [gw3] linux -- Python 3.11.6 /home/ross/.virtualenvs/nx-dev/bin/python worker 'gw3' crashed while running 'networkx/drawing/tests/test_agraph.py::TestAGraph::test_agraph_roundtripping[G0]' __________________________________________________ networkx/drawing/tests/test_agraph.py __________________________________________________ [gw12] linux -- Python 3.11.6 /home/ross/.virtualenvs/nx-dev/bin/python worker 'gw12' crashed while running 'networkx/drawing/tests/test_agraph.py::TestAGraph::test_agraph_roundtripping[G2]' __________________________________________________ networkx/drawing/tests/test_agraph.py __________________________________________________ [gw4] linux -- Python 3.11.6 /home/ross/.virtualenvs/nx-dev/bin/python worker 'gw4' crashed while running 'networkx/drawing/tests/test_agraph.py::TestAGraph::test_agraph_roundtripping[G1]' __________________________________________________ networkx/drawing/tests/test_agraph.py __________________________________________________ [gw6] linux -- Python 3.11.6 /home/ross/.virtualenvs/nx-dev/bin/python worker 'gw6' crashed while running 'networkx/drawing/tests/test_agraph.py::TestAGraph::test_agraph_roundtripping[G3]'  ========================================================= short test summary info ========================================================= FAILED networkx/drawing/tests/test_pydot.py::TestPydot::test_pydot[neato-G0] - AttributeError: 'itertools._tee' object has no attribute 'read' FAILED networkx/drawing/tests/test_pydot.py::TestPydot::test_pydot[neato-G1] - AttributeError: 'itertools._tee' object has no attribute 'read' FAILED networkx/drawing/tests/test_pydot.py::TestPydot::test_pydot[dot-G0] - AttributeError: 'itertools._tee' object has no attribute 'read' FAILED networkx/drawing/tests/test_pydot.py::TestPydot::test_pydot[dot-G1] - AttributeError: 'itertools._tee' object has no attribute 'read' FAILED networkx/drawing/tests/test_pydot.py::TestPydot::test_read_write - AttributeError: 'itertools._tee' object has no attribute 'read' FAILED networkx/drawing/tests/test_agraph.py::TestAGraph::test_agraph_roundtripping[G0] FAILED networkx/drawing/tests/test_agraph.py::TestAGraph::test_agraph_roundtripping[G2] FAILED networkx/drawing/tests/test_agraph.py::TestAGraph::test_agraph_roundtripping[G1] FAILED networkx/drawing/tests/test_agraph.py::TestAGraph::test_agraph_roundtripping[G3] ============================= 9 failed, 6237 passed, 12 skipped, 11 xfailed, 36 warnings in 104.94s (0:01:44) ============================= </pre> </details>
issue
More numpy scalars cleanup for numpy 2.0#TITLE_END#Follow up to #7282   The latest [nightly wheels tests](https://github.com/rossbar/networkx/actions/runs/8405408971/job/23018090094) turned up a few more failures related to numpy 2.0 scalar repr changes.  This PR fixes the linked failures that are related to NetworkX itself. Unfortunately, it seems there are some real behavior changes to the drawing related to numpy 2.0. I haven't had the time to chase down exactly what's happening, but there are 4 nx_pylab test failures that are indeed real and seem to stem from behavior changes in matplotlib w/ numpy 2.0. I plan to dig further and report upstream.
issue
Close figures on test cleanup.#TITLE_END#Prevents too-many-figures-open warning from the test suite with the matplotlib nightlies. Ultimately the figure handling (i.e. setup/teardown) should be handled in a fixture, but that's an improvement for another PR!
issue
Future proof xml parsing in graphml.#TITLE_END#As of Python 3.12, the lxml tree parser raises a warning about evaluating truthiness of elements directly, instead recommending an is not None or len() test. For an example of the warnings, see the warning logs on any of the Python 3.12 tests in actions, e.g. https://github.com/networkx/networkx/actions/runs/8349137374/job/22852640690 
issue
Update `LCF_graph` docstring#TITLE_END#Use numpydoc docstring standard and fixup formatting/examples.
issue
Expire steinertree mehlhorn futurewarning#TITLE_END#It looks like a FutureWarning related to changing the default `steiner_tree` method from "kou" -> "mehlhorn" had slipped through the cracks. This PR expires that `FutureWarning` and switches the default (the warning said this would happen in 3.2, but oh well). I also couldn't resist a cosmetic change to remove a local variable.
issue
Remove animation from spectral clustering example to improve performance#TITLE_END#Closes #7235   Removes 3D animation from the image segmentation via spectral clustering gallery example, reducing the runtime by more than 100x so that it is no longer a bottleneck in the doc build (either locally or in CI).  Adds xrefs to the other 3D drawing examples so that users have a recipe for creating animations.
issue
Gallery: spectral graph partition very slow#TITLE_END#Just opening this issue to track a performance issue I noticed with the docs.  The image segmentation and spectral graph example added in #7040 is extremely slow - it takes about 100s to run on a fast machine (ryzen 5950X) and is even worse in CI, taking nearly 4 minutes per doc build to run the example.  This should be investigated ASAP to reduce CI burden. There are multiple 3D animations in the example - I suspect switching them to static images would resolve the issue, though there will have to be a bit of profiling to verify that the animation(s) are the bottleneck and not the analysis itself.
issue
Update docstring example with future-proof pandas assignment.#TITLE_END#Fixes the test failure from the pandas-3.0 nightly wheel by replacing an assignment to `df.values` (which is becoming read-only) with index assignment via `df.iloc`. This should work both on pandas 2 and 3 - I've tested locally against the pandas-3.0dev nightly and it works as expected. I'm certainly no pandas expert so if there's a better/more idiomatic way to do this please push/suggest!  I also made minor adjustments to the example, including adding a second node with self-loop edges to make the adjacency matrix more interpretable in the repr.  Finally, I went ahead and opened pandas-dev/pandas#57687 to raise awareness about the fact that the behavior change associated with assigning to `.values` does not have an accompanying warning.
issue
Undo change in return type of `single_target_shortest_path_length`#TITLE_END#Fixes #7315 by reverting the change in return type until the FutureWarning has expired (scheduled for 3.5).  The other possible solution would be to remove the FutureWarning and implement the change now. I don't recall from previous discussions which was the desired course of action, so if I got this wrong please LMK! Either way - the behavior needs to be consistent with the warning (or lack thereof).
issue
Un-dispatch coloring strategies.#TITLE_END#Closes #7153   Removes dispatch decorator from the `strategy_` generators in the greedy-coloring module.
issue
`generate_random_paths` does not have a `seed` argument#TITLE_END#The `nx.generate_random_paths` iterator creates paths of specified length from `G`. This function is not seeded, so there is currently no way to reproduce results between runs.
issue
Try/except intermittently failing basemaps in geospatial examples#TITLE_END#~~I unfortunately missed one of the instances of `contextily.add_basemap` in #7299 which is also not intermittently failing on CI. This PR disables `plot_points` in the gallery. I'll update #7301 so that it's clear that #7299 and this PR need to be reverted when the underlying contextily issue is solved.~~  ---  **Update**  I just went ahead and wrapped the instances of `contextily.add_basemap` in try/excepts in all examples. That way we don't have to keep tracking CI issues.  Reverts #7299 and closes #7301 
issue
contextily.add_basemap failing in some geospatial gallery examples#TITLE_END#Followup issue to #7299 - the `contextily.add_basemap` had been intermittently failing in some of the geospatial gallery examples, e.g. `plot_lines.py` and `plot_delaunay.py`. These have been temporarily renamed to stop them from running during the doc build to avoid CI blockers (#7299). I haven't looked into it in detail, but my initial guess would be it has to do with a network timeout or some other issue accessing resources over the web. A full traceback can be found at the [end of this log](https://github.com/networkx/networkx/actions/runs/7916641025/job/21610909161#step:7:3889).  @ljwolf I know this isn't specifically a geopandas issue, but is this something you've encountered before?  #7299 should be reverted once a fix is found!
issue
Ensure warnings related to changes in shortest_path returns are visible to users#TITLE_END#As discussed in the community meeting, this PR ensures[^1] that visible warnings will be raised notifying users of proposed changes to the return type of `shortest_path` and `single_target_shortest_path_length`.  I also went ahead and made a couple additional changes to the warning:  - I changed the type from `DeprecationWarning` -> `FutureWarning` since the only thing changing is the return type rather than something being removed from the API  - I reset the deprecation cycle to the current release; i.e. the warning will be raised for 2 cycles (3.3 and 3.4) then the behavior change implemented in 3.5  - Finally, I beefed up the text for the `shortest_path` warning. I wanted to illustrate that it's possible to future proof your code now by wrapping `shortest_path` in `dict` (since `dict(<dictionary_instance>)` is a no-op). I'm happy to hack the text back down to a single sentence if that's what others prefer - just LMK!  As noted [here](https://github.com/networkx/networkx/pull/7156#issuecomment-1854560919), if the visible warnings are added then at least some of the changes in #6584 should be reverted until the reset-deprecation expires.  [^1]: or at least makes more likely
issue
Inconsistency in return type of `all_node_cuts`#TITLE_END#I noticed this while reviewing docstrings in #7258.  From the documentation, it seems `all_node_cuts` is intended to return node sets. This is indeed the behavior for some graphs, e.g.  ```python >>> G = nx.path_graph(5) >>> list(nx.all_node_cuts(G)) [{1}, {2}, {3}] ```  However, for other input graphs, the returned values are sets of 2-tuples (which to me look suspiciously like edges). One example of this behavior is actually available in the docstring itself:  ```python >>> G = nx.grid_2d_graph(5, 5) >>> list(nx.all_node_cuts(G)) [{(3, 0), (4, 1)}, {(0, 3), (1, 4)}, {(3, 4), (4, 3)}, {(0, 1), (1, 0)}] ```  The sets in this last output don't appear to me to be interpretable as node cut-sets. Perhaps there's a missing edge-unpacking step somewhere in the algorithm?
issue
Deprecate the `create` argument of `nonisomorphic_trees`#TITLE_END#Closes #7256
issue
`nonisomorphic_trees`  create argument#TITLE_END#the `nonisomorphic_trees` graph generator has [an argument](https://github.com/networkx/networkx/blob/2da36864c5899cbc55fc98c96635390dd96d5f88/networkx/generators/nonisomorphic_trees.py#L16) named `create` which toggles whether graphs are returned as either nx.Graph instances, or adjacency matrices in list-of-list format.  From a quick review of this function, I'm wondering if it might not be worth deprecating this `create` argument. For one thing, it is very similar to `create_using=` which is ubiquitous among the graph generation functions and has a very different meaning! Also I'm not sure that returning adjacency matrices as `list` should even be an option - if anything I'd expect users to want adjacency matrices in a form more suitable for efficient computation, i.e. numpy arrays. The latter would be possible with something like `np.to_numpy_array(G) for G in nx.nonisomorphic_trees(order)`, so there is a straightforward way to get the (arguably more desirable) behavior after a deprecation. 
issue
Deprecate `reverse_cuthill_mckee_ordering`?#TITLE_END#There are currently two functions in the `nx.utils.rcm` module: `cuthill_mckee_ordering` and `reverse_cuthill_mckee_ordering`. The former is a generator which yields nodes in rcm ordering; but the reversed version is a one-liner: `return reversed(list(cuthill_mckee_ordering(G, heuristic=heuristic)`.  The fact that the forward rcm is a generator whereas the reverse is not (instead returning an instantiated list) means that there is a (potentially) large difference in memory footprint[^1] between these two objects. This difference strikes me as worth resolving. One potential solution would be to deprecated the `reverse` version and recommend using `reversed` explicitly. Another possible solution would be to explicitly implement `reversed_cuthill_mckee_ordering` as a generator. This seems like a challenge, since I'm not sure that it would be possible to invert the ordering from a heuristic generically. However; if anyone has and ideas how this could be accomplished, it'd be interesting to see IMO!  [^1]: In practice I can't imagine this is a constraint that is hit too often, especially since the memory footprint of any graph of >10^6 nodes is likely to be far larger than it's node list.
issue
Temporarily rm geospatial examples to fix CI.#TITLE_END#Renames a couple geospatial examples from the gallery for which `contextily.add_basemap` is failing, causing the doc CI to fail.  This is an attempt at a temporary workaround to fix the doc CI.
issue
 Move arrowstyle input munging after intput validation. #TITLE_END#Fixes #7284   A UserWarning is raised on the value of arrowstyle, but the value is modified internally before this validation is performed.  Move the modification *after* the validation step to suppress the spurious warnings.
issue
Visualization: arrowstyle warning is very noisy#TITLE_END#Follow-up to #7010  The logic determining when the `arrowstyle` UserWarning changed in #7010, with the unintended side-effect of it being raised far more often. For example, it is now raised during the doc build for all of the simple drawings of the classic generators.  The warning logic should be reviewed and modified so that the warning is only raised when appropriate.
issue
Doc infrastructure: replace `nb2plot` with `myst-nb`#TITLE_END#This is a re-submission of #5221 - the changes are the same (more or less) but the order of git operations used to do the conversion was different in order to preserve git blame across the two files that were converted from `.rst` -> `.md`.  To demonstrate that the blame is preserved, here are links to the blame from the two converted files (on my fork - you may have to click the `blame` button):  - [introduction](https://github.com/rossbar/networkx/blame/mystnb-preserve-blame/doc/reference/introduction.md) - [tutorial](https://github.com/rossbar/networkx/blame/mystnb-preserve-blame/doc/tutorial.md)  Unfortunately the summary diffs for the PR still aren't very useful for reviewing, since per-line changes are lost in the file extension change. **However** - I've intentionally structured the commits so that the individual changes from both the auto-converter (`rst2myst`) and the subsequent manual tweaks I made are still visible. The commits whose messages start with `WIP` comprise the outputs from the autoconversion (c04d227 and 3ff8997 for the tutorial and introduction, respectively). Similarly, the additional changes I made to each file after the conversion are captured in 9448144 and a5cdbc4, respectively. If you click on those commits individually, you should see a nice line-by-line diff of the changes from each step in the conversion process!  Now, there's still a question about whether we want to ignore some of the changes in the blame, e.g. changes from the linters. One option might be to temporarily disable the auto-squashing just for this PR, as it might actually be worth preserving each commit here so that it's possible to discern exactly how the conversion took place. The linting commit (323616e) could also then be added to `.git-blame-ignore-revs`. This isn't necessarily a show-stopper, but it's a rare instance where preserving the history may be worthwhile! 
issue
Fix all sphinx warnings during doc build.#TITLE_END#Fix all sphinx warnings related specifically to formatting issues in docstrings. There are still a lot of extraneous warnings from plotting, but those are covered in issue #7284 and should be handled separately.
issue
Add docstring formatting change to blame-ignore-revs.#TITLE_END#Minor followup to #7276 while it's fresh in my mind: adds the commit hash with the docstring reformats to `.git-blame-ignore-revs` to prevent those changes from polluting blame.
issue
Add minimum_cycle_basis to cycle_basis See Also.#TITLE_END#Minor doc improvement related to #7272 - adds `minimum_cycle_basis` to the `See Also` section of `cycle_basis` (thanks @boothby !)
issue
Update docstring of nonisomorphic_trees.#TITLE_END#Closes #7252 .
issue
Update general_k_edge_subgraphs docstring.#TITLE_END#Minor updates to docstring:  * Adds a parameters section  * `Returns` -> `Yields` and update wording to clarify that graphs are yielded, not node sets  * Fix format of links to references  * Update example to use comprehension instead of `map`  Closes #7253  
issue
Intermittent test failures for test_directed_edge_swap#TITLE_END#It looks like CI is occasionally tripping up on the tests added in #6426 due to going over the maximum number of attempted swaps. It should be investigated whether these tests can incorporate a seed to remove this variability (and double check that `double_edge_swap` is performing well!)
issue
Bump copyright year for 2024.#TITLE_END#2024 version of the basic copyright bumping chore (#6322). If anyone knows of an action or linting rule that handles this automatically LMK!
issue
Docs infrastructure: swap out the `nb2plot` sphinx extension with `myst-nb`#TITLE_END#Both the tutorial and the introduction rely on executable code embedded in the corresponding document being run when the docs are being built. This is currently done with the `.. nbplot` directive, provided by the nb2plots sphinx extension. There is some overlap of functionality with this extension and the newer `myst-nb` sphinx extension from the executable books project, which also supports markup documents with executable cells.  I thought I'd give `myst-nb` a try as an alternative to `.. nbplot` to get a sense of whether this is a viable alternative for the documents. After a bit of configuring I think it's quite possible and actually has a really nice workflow with sphinx. The one major change is that using `myst-nb` would *require* the tutorial and introduction to be written in myst-markdown instead of rst. This is because, AFAICT, the `{code-cell}` used by `myst-nb` to denote executable cells is not a generic sphinx directive, and thus is not supported in the rst files.  IMO it's not pressing and there aren't any *major* improvements that motivate the change: it gets rid of a few baked-in sphinx warnings from `nb2plots` and also provides execution caching, but that doesn't have a huge effect in this case since it's only the two files that are executed and neither of them has a very long run time. I do think it's a viable alternative however and would be interested in discussing it if anyone's interested!
issue
Improve testing for directed_edge_swap#TITLE_END#There is an opportunity to improve the testing of the `directed_swap_function`.  There is a concrete suggestions [here](https://github.com/networkx/networkx/pull/5663#discussion_r906225024) for improving `test_directed_edge_swap` by validating that the input and output edges are indeed different. Upon reflection, I think this check could even be improved further by verifying that the number of swaps is correct... I think the difference in the edge sets should be 2*`nswap` - that might be one way to check.  It'd also be good to add more test cases of non-path graphs where there are only a limited number of edge swaps possible, and verify that the algorithm can recover all of them. See [the discussion](https://github.com/networkx/networkx/pull/5663#pullrequestreview-1018715617) for further details.
issue
Replace tempfile with tmp_path fixture in test suite.#TITLE_END#An alternative to #6554 which removes `tempfile` entirely in favor of the [`tmp_path` fixture](https://docs.pytest.org/en/7.1.x/how-to/tmp_path.html#the-tmp-path-fixture) in the test suite.
issue
Refactor geometric_soft_configuration_model tests for performance#TITLE_END##6858 added the `geometric_soft_configuration_model` generator, which included several tests that generated graphs with >1k nodes. These tests were quite slow:  ```bash $ pytest --durations 4 networkx/generators/tests/test_geometric.py  ...  4.07s call     networkx/generators/tests/test_geometric.py::test_mean_degree_S1 4.06s call     networkx/generators/tests/test_geometric.py::test_mean_kappas_S1 1.34s call     networkx/generators/tests/test_geometric.py::test_compare_mean_kappas_different_gammas_S1 0.18s call     networkx/generators/tests/test_geometric.py::test_dict_kappas_S1 ```  These could be decorated with `pytest.mark.slow`, but then they'd still be running in the coverage job in CI. Since the tests are pseudo-stochastic anyways (i.e. they test average properties of graphs created with a seed) I figured there was no harm in refactoring them instead to improve performance. The main thing was to drop the total number of nodes from 1000's to 10's (reducing the `mean_degree` as well, where necessary). I also consolidated two of the tests into one so that the input graph is only created once. These changes result in:  ```bash $ pytest --durations 4 networkx/generators/tests/test_geometric.py  ...  0.18s call     networkx/generators/tests/test_geometric.py::test_dict_kappas_S1 0.03s call     networkx/generators/tests/test_geometric.py::TestRandomGeometricGraph::test_number_of_nodes 0.03s call     networkx/generators/tests/test_geometric.py::TestNavigableSmallWorldGraph::test_navigable_small_world 0.01s call     networkx/generators/tests/test_geometric.py::test_beta_clustering_S1 ```  ... removing the bottlenecks and dropping the total test runtime for the module from ~10s -> 0.4s (on my machine).
issue
Future-proofing and improve tests#TITLE_END#This PR fixes the warnings seen in the test suite from the nightly wheels of the default dependencies. Other improvements include:  - A fix to one of the warning filters in the test config  - Seeding a doctest to avoid intermittent failures
issue
Undeprecate ``nx_pydot`` now that pydot is actively maintained again#TITLE_END#As noted in #5723, pydot has a new lease on life. I test `nx_pydot` against the new pydot 2.0 release and the issues that NX had been running into are now all resolved.  Given that `pydot` is again under active maintenance, I'm +1 on undeprecating and continuing to support NetworkX's interface to `pydot`.  Closes #5723
issue
Update test suite for Pytest v8#TITLE_END#There is a release candidate for Pytest v8 for which two of NX's tests were failing. This PR makes the necessary updates to be pytest-8 compatible:  - f537f0b removes a `pytest.deprecated_call` check from a test which raises an exception. It turns out this never raised the warning as it hits the exception first, it's just that pytest v7 never caught this case.   - 7fe9c50 replaces the deprecated `pytest.warns(None)` pattern with the recommended `warnings.catch_warnings()` context manager.
issue
Should `is_distance_regular` and `intersection_array` have @not_implemented_for?#TITLE_END#Something I noticed while reviewing #5849 :  The docstrings for [`is_distance_regular`](https://github.com/networkx/networkx/blob/d68caf64b057f3c5f0adf4f09ce3843e4f2395c4/networkx/algorithms/distance_regular.py#L21) and [`intersection_array`](https://github.com/networkx/networkx/blob/d68caf64b057f3c5f0adf4f09ce3843e4f2395c4/networkx/algorithms/distance_regular.py#L114) state that they are only defined for undirected graphs; however, the implementations do not check the type of the graph. The functions and definitions should be reviewed to determine whether these should get the `not_implemented_for` decorator.
issue
Fix not_implemented_for decorator for is_regular and related functions#TITLE_END#Closes #7172 and #6904
issue
Modify GML test to fix invalid octal character warning.#TITLE_END#Closes #7149
issue
Future Deprecation: replace utils.pairwise with itertools equivalent#TITLE_END#Good news! The `pairwise` function has been included in itertools, so the home-made version (which, according to the comments, originated from an itertools recipe) in `utils.misc` can be replaced with the itertools version.  This change can't be made quite yet however as `itertools.pairwise` [was only added in Python 3.10](https://docs.python.org/3/library/itertools.html#itertools.pairwise), so NX needs to wait until 3.10 is the minimum supported version.  I thought I'd open an issue to track this since I don't think automatic linters like `pyupgrade` will catch this particular case.
issue
Add dot io to readwrite#TITLE_END#Addresses #4311, though probably doesn't close it.  [One of the suggestions](https://github.com/networkx/networkx/issues/4311#issuecomment-720019303) is to add NetworkX's functionality related to I/O from DOT files to the `readwrite` section of the refguide. This will make the DOT I/O functionality more discoverable for users who are interested in reading/writing graphs in DOT format. 
issue
Proposal to add centrality overview to mentored projects.#TITLE_END#Per discussions at this week's [community meeting](https://github.com/networkx/archive/blob/main/meetings/2023-11-08.md).
issue
Add note about importance of testing to contributor guide#TITLE_END#Adds a blurb to the "new algorithm policy" question of the new contributor FAQ.
issue
DEP: Deprecate random_triad#TITLE_END#Another proposal for deprecation in the `triads` module. This is another one-liner "helper function". It doesn't appear to have any special meaning w.r.t triadic analysis and isn't used at all internally. The one caveat here though is that unlike "triplets" (see #7060), triads are only defined for directed graphs. In other words, if a user did `nx.subgraph(G, random.sample(list(G), 3))` and `G` *wasn't* a directed graph, then this would technically not be a triad (at least given the current definitions - see #4386 for related discussion).
issue
Run test suite without any dependencies in CI with CPython versions#TITLE_END#I noticed this the other day - we're currently not testing against the CPython versions without any dependencies installed. It may be that this was intentional, but I figured I'd raise it on the off chance it's not - @jarrodmillman   The only thing this is likely to catch is when developers forget `pytest.importorskip` for functions that depend on numpy/scipy etc. Perhaps that's not worth the tradeoff of adding back 9 more testing jobs!
issue
Add docstrings to Filter classes in coreviews#TITLE_END#The `Filter` classes defined in `networkx/class/coreviews.py` don't yet have docstrings, so their rendered reference documentation isn't great, e.g. [the module autosummary](https://networkx.org/documentation/latest/reference/classes/index.html?highlight=coreviews#module-networkx.classes.coreviews) or the [class docs](https://networkx.org/documentation/latest/reference/classes/generated/networkx.classes.coreviews.FilterAtlas.html#networkx.classes.coreviews.FilterAtlas).
issue
Add a step to CI to check for warnings at import time.#TITLE_END#Related to #7064 and #7032.  This PR adds a minimal step to the `default` testing job to check for warnings raised at import time. I'm still not able to reproduce #7064 / #7032 locally, so I suspect that they are specific to Python config/environment management systems. Nevertheless, this is a relatively lightweight check that will hopefully help catch any user-facing import warnings prior to releases.  If anyone has ideas how this can be made more robust, please chime in!
issue
Hierarchical clustering layout gallery example#TITLE_END#An alternative to #6923 - adds an example to the gallery with a recipe for visualizing node clusters. This is just a slightly more polished version of [the example I left in #6923](https://github.com/networkx/networkx/pull/6923#issuecomment-1716292391).  The advantage of this approach is that it generalizes to multiple layers of clustering. The example only shows one layer, but if it's preferred I can make sub-clusters within the three communities instead (at the expense of a bit more complexity in the example).  Also, if anyone has suggestions for a better example graph for finding communities, let me know!
issue
Minor touchups to the beamsearch module#TITLE_END#A few minor changes to the beamsearch module and test suite. The biggest change is the addition of the note to the docstring which attempts to explain the difference between `bfs_beam_edges` and `bfs_edges`.
issue
DEP: Deprecate the all_triplets one-liner.#TITLE_END#Proposal to remove the `triads.all_triplets` function, which is simply an alias for `itertools.combinations(G, 3)`.  `all_triplets` is not even used internally in the `triads` module, so AFAICT there is no special meaning attached to the term "graph triplets" - please correct me if I'm wrong!
issue
MAINT: Minor touchups to tadpole and lollipop graph#TITLE_END#A very minor followup to #6999. All of the test suite changes are in 2aa6953 - I'm happy to break that out into a separate PR if they need a closer look!
issue
MAINT: Fixup union exception message.#TITLE_END#It should be a string, but the commas make it a tuple.  Also minor touchups to the message itself, but happy to revert to the original text!
issue
DOC: Add example to generic_bfs_edges to demonstrate the `neighbors` param#TITLE_END#Updates the `generic_bfs_edges` docstring examples to better demonstrate the parameters.
issue
Minor doc cleanups to remove doc build warnings#TITLE_END#Similar to #7042   * remove the expired clique functions from the refguide to fix the doc build.  * Fix reference formatting in maximal_extendability docstring  * Rm `release_dev` from release notes autosummary
issue
Bump pyupgrade minimum supported version to 3.10#TITLE_END#Inspired by #7036 - I wanted to see what pyupgrade would auto-update when we move up to 3.10. It looks like automatically handles bullet-point 2 from #7036
issue
Fix syntax warning from bad escape sequence.#TITLE_END#Closes gh-7032
issue
Rm deprecated clique helper functions#TITLE_END#Removes the 4 deprecated functions from the clique module: `graph_clique_number`, `graph_number_of_cliques`, `number_of_cliques` and `cliques_containing_node`.  @MridulS I know you have a preference for *not* removing helper functions, so just one final check with you :).  One thing I was thinking we could do: in each of these cases, the helper function is replaced by a one-liner[^1]. In order to further reduce the disruption of removing these functions, we could add the four names to the package `__getattr__` and have it raise an exception which includes the recommended one-liner in the message. This seems like the best of both worlds IMO: the confusing function names are removed (and we don't add them to `__dir__` so they are not discoverable), but any code that was relying on them and missed the deprecation cycle would still get an explicit message on how to update their code. WDYT?  [^1]: ... which also happens to be more efficient in some cases, since the one-liners make use of the generator returned by `find_cliques` whereas the helper implementations often create lists!
issue
Minor touchup to the sort_neighbors deprecation.#TITLE_END#A minor follow-up to #5925 - just something I noticed while reviewing the deprecations list.  * Add back the docstring with the .. deprecated:: directive * Make clear in the message that removal is slated for v3.4
issue
Remove `topo_order` kwarg from `is_semiconnected` without deprecation.#TITLE_END#This PR proposes to remove the `topo_order` kwarg from `is_semiconnected` *without* a deprecation period. This represents the most aggressive potential solution to the problem identified in #6645 .  The rationale for removing without a deprecation period is that using the `topo_order` kwarg results in silently incorrect results more often than not. A quick example:  ```python >>> G = nx.DiGraph([(0, 1), (1, 2)])  # directed path graph >>> nx.is_semiconnected(G)  # correct True >>> nx.is_semiconnected(G, topo_order=[0, 1, 2])  # Wrong result, even though topo_order is valid! False ```  Therefore, the `topo_order` kwarg itself can be considered a bug rather than a feature, and should be fixed without a deprecation. The main downside of this approach is that any users who are currently using the `topo_order` kwarg will get an uniformative error message:  ```python >>> nx.is_semiconnected(G, topo_order=[0, 1, 2]) Traceback (most recent call last)    ... TypeError: is_semiconnected() got an unexpected keyword argument 'topo_order' ```  An alternative would be to raise a more informative exception whenever `topo_order` is passed in with a non-default value. The downside of *that* approach is that `topo_order` will remain in the signature. This could potentially be resolved by adding `**kwargs` sentinel instead.
issue
laplacian_centrality not working for graphs with single nodes#TITLE_END#### Discussed in https://github.com/networkx/networkx/discussions/6565  <div type='discussions-op-text'>  <sup>Originally posted by **PurviChaurasia** March 28, 2023</sup> If I add a single node the graph, the `laplacian_centrality` function returns an error state which I'm unable to understand. Can someone help me out with this? Code -  ``` G = nx.Graph() G.add_node(0) nx.laplacian_centrality(G) ```  ``` --------------------------------------------------------------------------- error                                     Traceback (most recent call last) Cell In[15], line 3       1 G = nx.Graph()       2 G.add_node(0) ----> 3 nx.laplacian_centrality(G)  File [d:\outreachy\networkx\networkx\networkx\algorithms\centrality\laplacian.py:129](file:///D:/outreachy/networkx/networkx/networkx/algorithms/centrality/laplacian.py:129), in laplacian_centrality(G, normalized, nodelist, weight, walk_type, alpha)     126 new_diag = lap_matrix.diagonal() - abs(lap_matrix[:, i])     127 np.fill_diagonal(A_2, new_diag[all_but_i]) --> 129 new_energy = np.power(sp.linalg.eigh(A_2, eigvals_only=True), 2).sum()     130 lapl_cent = full_energy - new_energy     131 if normalized:  File [d:\Outreachy\NetworkX\networkx-dev\lib\site-packages\scipy\linalg\_decomp.py:561](file:///D:/Outreachy/NetworkX/networkx-dev/lib/site-packages/scipy/linalg/_decomp.py:561), in eigh(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite, subset_by_index, subset_by_value, driver)     558         lwork_args = {'lwork': lw}     560     drv_args.update({'lower': lower, 'compute_v': 0 if _job == "N" else 1}) --> 561     w, v, *other_args, info = drv(a=a1, **drv_args, **lwork_args)     563 else:  # Generalized problem     564     # 'gvd' doesn't have lwork query     565     if driver == "gvd":  error: (il>=1&&il<=n) failed for 6th keyword il: dsyevr:il=1 ```  Since this will create a laplacian matrix with a single node = 0, shouldn't the centrality be 0? p.s. I might be going completely wrong somewhere :)</div>
issue
Handle edge cases in Laplacian centrality#TITLE_END#Re-submission of #6664 from an editable branch. Original PR description below. All attribution  ---  This code handles a few edge cases of the Laplacian centrality with empty, single-node, and unconnected (edgeless) graphs. Following changes have been made to the laplacian_centrality calculation      NetworkXPointlessConcept is raised for empty and single node graphs     NetworkXPointlessConcept is raised for graphs without edges and normalized=True (we can't normalize to 0.0)  Test cases are provided. The code has been reviewed, and review comments have been processed.  fixes https://github.com/networkx/networkx/issues/6571, fixes https://github.com/networkx/networkx/issues/6626
issue
Deprecate strongly_connected_components_recursive#TITLE_END#Alternative to #6898 , see https://github.com/networkx/networkx/pull/6898#issuecomment-1734207080.  Deprecates the `strongly_connected_components_recursive` function in favor of the non-recursive implementation.  Closes #6897 
issue
API: Rm default value from time_delta for cd_index.#TITLE_END#I'd like to propose a change to the `cd_index` API: removing the default value for `time_delta`. The current default of 5 years as a `timedelta` object feels arbitrary, and I can't think of a strong reason why a default value makes sense here. Furthermore, providing a `datetime.timedelta` object suggests that the `time` node attribute must be `datetime.datetime` objects by default, which feels like too strong a recommendation IMO.
issue
Make label_attribute param of join_trees kwarg-only.#TITLE_END#Followup to #6908 . A proposal to make the `label_attribute` parameter of `join_trees` keyword-only. 
issue
Make weight and seed for `fast_label_propagation_communities` kwarg only#TITLE_END#Followup to #6843 . Proposal to explicitly make the `weight` and `seed` parameters of `fast_label_propagation_communities` keyword-only.
issue
Make new dtype param for incidence_matrix kwarg-only.#TITLE_END#Followup to #6725 . Proposal to make the newly added (in v3.2) `dtype` parameter to `incidence_matrix` keyword-only.
issue
Add support for tuple-nodes to default gml parser#TITLE_END#Minor followup to #6943 - while `literal_(de)stringizer` is still necessary for the nested-sequence corner case, this PR adds support to the default stringizer for nodes which are tuples.
issue
Undeprecate literal_(de)stringizer#TITLE_END#I propose to rescind the deprecation of `gml.literal_stringizer` and `gml.literal_destringizer`.  These functions have been deprecated since 2.5 but were never removed because it is not possible to do so without breaking tests. I spent a frustrating couple hours diving into this and gained a few insights. Currently, `literal_stringizer` is necessary to handle two specific cases that are tested:  1. Graphs with nodes that are tuples, and  2. Attributes that include nested sequences  The first one is relatively easy to "fix" without literal stringizer. For completeness sake, it would involved adding the following snippet:  ```python elif isinstance(value, tuple) and key == "label":     yield indent + key + f" \"({','.join(repr(v) for v in value)})\"" ```  before [L761](https://github.com/networkx/networkx/blob/34f9965c57ea8d5ba14562cfe5d18599872c9c39/networkx/readwrite/gml.py#L761C19-L761C19).  The 2nd one is really tough to crack. The default `stringizer` function has an `in_list` function to decide how to handle elements inside lists, but this only works for "1D" lists - nested sequences break this pattern and will end up raising an exception. `literal_stringizer` gets around this by only unpacking the outermost list, then directly repr-ing everything inside it. I'm not even sure that the GML standard supports nested lists as attributes! However, this currently "works" according to [`test_data_types`](https://github.com/networkx/networkx/blob/34f9965c57ea8d5ba14562cfe5d18599872c9c39/networkx/readwrite/tests/test_gml.py#L442), even if the result isn't actually valid, standards-compliant gml.  IMO, the simplest thing to do would be to revert the deprecation. On the off-chance that there are users depending on this behavior, `literal_stringizer` is currently the only simple way to achieve it. It looks like the original motivation for deprecating the `literal_*stringizer` functions was Python 2->3 cleanup, but these corner cases are still relevant and unrelated to Python 2. Furthermore, I think reverting the deprecation will be relatively painless, as the deprecation warnings aren't currently visible due to the stack level.
issue
Rm deprecated `create_using` kwarg from scale_free_graph#TITLE_END#Expires the deprecation of create_using (replaced by initial_graph) in `scale_free_graph` in preparation for v3.2.
issue
DOC, MAINT: Deduplicate docs instructions.#TITLE_END#Condense documentation building instructions to a single location in doc/README.rst and include that directly in the contributor guide.  This resolves some fragmentation of the contributor documentation, ensuring new contributors see the same information both in the html docs and in the source READMEs.  Closes #6770  
issue
Unify documentation README with contributor guide#TITLE_END#It was pointed out in #6766 that we have a README in the `doc/` folder that describes how to build the documentation. I think the go-to reference for this information is the contributor guide, and these two documents are not necessarily in sync. However, contributors have also indicated that having a README in the doc folder is nice. Therefore I would propose the following:  1. Reformat `doc/README.md` to .rst, then 2. `.. include` `doc/README` in the `Documentation` section of `CONTRIBUTING.rst`  This should check all boxes: it preserves the README in the `doc/` folder, de-duplicates the source of info (preventing divergence of the README from the contributor guide), and guarantees that the info from the README shows up in the html version of the contributor guide.
issue
Expire deprecation for `attrs` kwarg in node_link module#TITLE_END#Removes the deprecated `attrs` kwarg from the `node_link_graph` and `node_link_data` functions in preparation for the v3.2 release.
issue
Desired semantics for nx.join_trees#TITLE_END#This is a follow-up to #6503 and related to #6908 (I meant to add to the #6503 discussion, but I was too slow - apologies!)  #6503 modified the behavior of `nx.join` to match behavior as specified in the documentation for `label_attribute=None`:   > If provided, the old node labels will be stored in the new tree under this node attribute. If not provided, the node attribute '_old' will store the original label of the node in the rooted trees given in the input.  However; I wonder if the default behavior for `label_attribute=None` shouldn't be that new node attributes *aren't* created. In other words, rather than adding `"_old"` attributes to every node in the joined tree by default, would it make more sense to only create these attributes when `label_attribute` is explicitly provided? This would have the advantage of not creating a bunch of extra node attributes by default.  Also, because of the bug which was fixed in #6503, this wouldn't even be a behavior change - so no need for a deprecation/future warning.
issue
Examples section of `to_numpy_array` could be improved#TITLE_END#Currently there is only a single example using an `nx.MultiDiGraph` as input. IMO it would be good to improve a basic example to illustrate the absolute basics of adjacency matrices, then add subsequent examples highlighting the features available via the many keyword arguments.
issue
Export `is_tournament` to main namespace?#TITLE_END#Currently all of the functions defined in the `tournament` module must be accessed through the `tournament` namespace, i.e.  ```python from networkx.algorithms import tournament ```  This makes sense for many of the functions because they require the input graph to be a tournament graph and the `tournament` namespace is an effective way of enforcing this policy without having to add an `is_tournament` validation to every function. There are other cases (e.g. `tournament.is_strongly_connected`) where again the namespace is certainly the correct organization.  However, I think there is an argument for exporting the `is_tournament` function to the main namespace so that users can call `nx.is_tournament(G)` rather than `tournament.is_tournament(G)`. `is_tournament` is a property-checking and is applicable to any graph, and I think there's a usability argument for being able to access it with the other `nx.is_` functions. Thoughts?  I think there may also be an argument for modifying the semantics of `is_tournament`... to be discussed in a separate issue!
issue
Modify `s_metric` `normalized` default so function doesn't raise#TITLE_END#Currently, the `nx.s_metric` function raises a `NetworkXException` when called with the default keyword argument values because normalization is not implmented:  ```python >>> G = nx.cycle_graph(4) >>> nx.s_metric(G) Traceback (most recent call last)    ... NetworkXError: Normalization not implemented ```  I think it's safe to say this function is not frequently used :upside_down_face: Nevertheless, I propose to change the default to `normalization=False`. I don't think the deprecation/futurewarning policy really applies in this case since the default is broken; however, if there are other suggestions for how to handle this (e.g. remove the kwarg outright) LMK!  fixes https://github.com/networkx/networkx/issues/5906
issue
Cleanup LCA tests#TITLE_END#Taking a look at the [LCA test suite](https://github.com/networkx/networkx/blob/main/networkx/algorithms/tests/test_lowest_common_ancestors.py), it looks to me like there are some opportunities to improve the tests.  One obvious one is the test naming scheme: the test methods are enumerated with a comment that explains what the test is for. With the pytest framework, it'd be better if the test method name itself described the test - that way, it'd be easier to get a sense of what's going wrong from the pytest summary when a test fails. For example:  https://github.com/networkx/networkx/blob/8522eea3955f5cf3da43cacc27643d93768aeb03/networkx/algorithms/tests/test_lowest_common_ancestors.py#L131  There are other opportunities to make things easier to read, see e.g. [this comment](https://github.com/networkx/networkx/pull/5736#discussion_r920970920).  As always with tests, the most important thing is that the test behaves exactly as expected.
issue
Deprecate the `dag_longest_path_length` helper function?#TITLE_END#The `nx.dag_longest_path` function has a helper `dag_longest_path_length` which essentially calls `dag_longest_path` then does a little post-processing to derive the length from the result. This generally matches a pattern established by the `shortest_path` functions (i.e. having a `_length` helper), but I thought I'd make the case for breaking that pattern here.  For non-multigraphs, `nx.dag_longest_path_length` essentially boils down to:  ```python sum(G[u][v].get(weight, default_weight) for u, v in pairwise(path)) ```  where `path` is the result of `nx.dag_longest_path`. This one-liner is idiomatic and easy to understand (and would be simplified even further in the case of e.g. unweighted edges). The case of multigraphs is trickier, as there is the matter of how to reduce weighted multiedges into a single value. `dag_longest_path_length` makes the (sensible) decision to use `max` to select the maximum edge weight. There's no reason to be this rigid however - users should be able to use whatever reduction function they want (`sum` e.g.) and IMO it's easier to bake in this flexibility by using Python comprehensions directly rather than expanding the API to support passing in a `callable`.  Like most of my deprecation suggestions, this is very subjective. I prefer a teach-a-person-to-fish approach when dedicated API can generally be replaced with a one-line list comprehension, but that has to be weighed against the potential disruption from deprecating, as well as the breaking of the `_length` helper-function API pattern established by the `shortest_path` package. Please LMK what you think!  See also #6311
issue
Add linting to contributor guide.#TITLE_END#Closes #6691   The contributor guide was a little sparse on info wrt addressing issues in CI with linting. The recommendation is still "use pre-commit", but situations can arise where things are not properly installed/configured in local dev envs, leading to annoying red x's in CI.  The best way (that I know of at least) to deal with this situation is to use `pre-commit` to run all the lint hooks over the entire codebase.
issue
Temporary work-around for NEP 51 numpy scalar reprs + NX doctests#TITLE_END#This is my proposal for how to handle the upcoming changes to NumPy scalar reprs (per [NEP 51](https://numpy.org/neps/nep-0051-scalar-representation.html)); specifically the effects on the doctests - see #6846.  This will silence doctest errors related to changes in the NumPy scalar reprs per NEP 51. The goal is to aid in transitioning to new numpy scalars with the minimum possible churn in code/CI.  In general, the transition plan would be something like:  - Leave this PR in place to suppress doctest failures from changes in numpy scalar reprs.    This way, all doctests can still be run: no need to re-configure CI based on numpy version.  - Update / modify code and/or tests as needed to limit/replace the use of numpy scalars,    where appropriate.  - When NumPy 2.0 becomes the minimum supported version for NetworkX, These 5 lines can    be removed and any remaining issues related to scalar representations fixed    (hopefully there aren't any left... see previous bullet). 
issue
MAINT: replace numpy aliases in scipy namespace.#TITLE_END#Future versions of scipy are removing the symbols from scipy namespace that are simply aliases to numpy objects  This PR removes these aliases for future compatibility.  Note that the `sp.pi` was used in the testing of the lazy importing system. I went ahead and changed it to `sp.special.erf`... I *believe* this covers the same semantics, but please double-check that I'm not changing the test semantics too much! 
issue
Minor fixups to clear up numpy deprecation warnings#TITLE_END#Followup to #6768, fixing up a few more numpy deprecations with some very minor code tightening along the way.
issue
MAINT: minor coverage cleanup.#TITLE_END#Closes #6574
issue
Rm unreachable code for validating input#TITLE_END#Fixes #6572   There are input validation checks in the internal `_bidirectional_shortest_path` function within the `approximation/connectivity` module that are not possible to hit without calling the private functions directly.  The internal function is called in `local_node_connectivity` which raises exceptions for `source=None` or `target=None` and `source == target`. Therefore I vote for removing these validation checks from the private function. 
issue
Pin sphinx<7 as temporary fix for doc CI failures#TITLE_END#It turns out the failures in the circleci job are because we're picking up sphinx 7 which was recently released.  This is a temporary fix to get the circleci job green again.
issue
Example of improving test granularity related to #5092#TITLE_END#An example of improving granularity in the `test_random_graphs` module. This PR extracts the tests related to `watts_strogatz` from the top-level test function, and breaks them down into their own individual tests.
issue
Minor fixups to equitable_coloring docstring.#TITLE_END#Closes #6603 
issue
MAINT: Bump scipy version and take advantage of lazy loading.#TITLE_END#This enables cleaning up a bunch of explicit imports of scipy subpackages.
issue
Document shallow copy behavior for union.#TITLE_END#Closes #6661 by documenting the shallow copy behavior.
issue
Improve test suite for random graphs#TITLE_END#The tests for the random graph generators in `test_random_graphs.py` could benefit from a refactor/review. Some things that immediately stand out:  - Improve granularity: some tests have many `assert` statements and are not particularly specific to testing any one thing. Breaking these up into smaller tests should improve readability, specificity, and reduce instances where code is being run but not explicitly tested.  - There are a lot of `sum(1 for _ in G.edges())` that *I think* can be replaced with `G.number_of_edges()`.  - The outer test class can be gotten rid of without needing any larger reorganization AFAICT  - There are a couple obvious opportunities to parametrize tests, which should help with the granularity as well.  - Review seeding for test reproducibility
issue
Give informative error message when topo_order is used.#TITLE_END#Alternative to #6651.  The main advantage of this approach is that a more informative exception message s raised when `topo_order` is used:  ```python Traceback (most recent call last):    ... TypeError: `topo_order` gives incorrect results and is no longer supported. Remove `topo_order` from the function call. ```  The disadvantage is that `topo_order` remains in the function signature.
issue
Remove Python 3.8 from CI#TITLE_END#Sorry for the multiple CI PRs. The main thrust of this one is dropping Python 3.8 from CI, along with a few other minor changes:  - Drop Python 3.8 from the testing workflows  - Bump the pypy version from 3.8 -> 3.9  - Generally bump the Python version used in all other workflows up to 3.10  Note that all of our CI will continue to fail until #6635 is in, so marking this as draft for now.
issue
Replacing codecov Python CLI with gh action.#TITLE_END#The `codecov` CLI has been yanked from PyPI, so we need to update how we upload the coverage results to codecov.io.  See: https://github.com/networkx/networkx/pull/6634#issuecomment-1507376906
issue
Bump pyupgrade minimum Python version to 3.9.#TITLE_END#I was curious to see if there were any language features that pyupgrade would automagically update for us now that Python 3.9 is the minimum supported version. How I tested: both `ruff .` and `pre-commit run --all-files` with this updated config.
issue
Resolve NXEP4 with justification for not implementing it.#TITLE_END#Adds some text to the discussion section of NXEP4 to capture the conversations at the last several community meetings.  To summarize: I propose *not* to implement NXEP4 as described, as NetworkX would lose strict stream-compatibility using the new `np.random.Generator` as the default in the `nx.random_state` decorators.
issue
Fix module docstring format for ismags reference article.#TITLE_END#Closes #6547   Improperly formatted Notes/References sections was causing a sphinx parsing error which led to a very poorly formatted html page.  Fixing the numpydoc formatting for the module docstring + minor munging of the heading levels should fix the issue.
issue
Minor docs/test maintenance#TITLE_END#A few maintenance-y touchups that I noticed while building docs/running tests locally. This should get rid of extra sphinx warnings/errors (minus those associated with nb2plots... more to come) and catch deprecation warnings when running the tests.
issue
Better default alpha value for viz attributes in gexf writer#TITLE_END#Closes #6443 with @dschult 's fix. The added test should make clear the motivation for this change - this test fails on `main` due to `'None'` being given as the default.
issue
Review tempfile usage in TestAGraph class#TITLE_END#`TestAGraph` has an `agraph_checks` method that creates temporary files via `tempfile.mktemp` and `tempfile.mkstemp`. The docstring of the former says: `THIS FUNCTION IS UNSAFE AND SHOULD NOT BE USED.`, and the latter creates independent file name and file handle objects that don't work well with context managers, though the problem is only visible on Windows.  One solution would be to switch to one of the tempfile classes, e.g. `tempfile.NamedTemporaryFile`.  It would be best to address this after #4561 (or similar Windows CI testing) has been merged.
issue
Deployment workflow for development docs is broken#TITLE_END#The deploy action passes after merges in this repo, but we get an error from `gh-pages` in the `networkx/documentation` repo stating that the gh-pages build fails due to not being able to find `reference.pdf`. Here's the full error text:  ``` The page build failed for the `gh-pages` branch with the following error:  Site contained a symlink that should be dereferenced: /networkx-1.0.1/networkx_reference.pdf. For more information, see https://docs.github.com/github/working-with-github-pages/troubleshooting-jekyll-build-errors-for-github-pages-sites#config-file-error.  For information on troubleshooting Jekyll see:    https://docs.github.com/articles/troubleshooting-jekyll-builds ```  This is definitely related to #5572 . I wasn't immediately able to chase down the right fix so I figured I'd open an issue for visibility.
issue
Bump gh-pages deploy bot version.#TITLE_END#Very minor CI maintenance. Should get rid of a [deprecation warning in the actions log](https://github.com/networkx/networkx/actions/runs/4211756548) due to GH bumping software versions in the actions infrastructure.  See also: https://github.blog/changelog/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12/  Unfortunately this is one of those changes that we won't know whether it worked until after merging as the `deploy-docs` job only runs post-merge!
issue
Fix docstring heading title.#TITLE_END#Very minor tweak to conform to the numpydoc standard and get rid of UserWarnings during the doc build.
issue
Comment out unused unlayered dict construction.#TITLE_END#This is a re-submission of #6207 limited to the changes agreed upon in the review there. 
issue
Fix pre-commit on Python 3.10#TITLE_END#The latest `isort` has a dependency bug that borks development environments with the latest 5.12 isort precommit hooks installed. I just ran into this on Python 3.10 - see pycqa/isort#2083.  Pinning isort to 5.11.5 in the pre-commit config worked for me locally.
issue
Use generator to limit memory footprint of read_graph6.#TITLE_END#Switching from a list to a generator prevents the allocation of a `num_nodes**2 / 2` list of 2-tuples, which is a blocker for graphs with many nodes (see #6510).  Closes #6510
issue
Fix reference formatting in generator docstring.#TITLE_END#Fixes link and removes sphinx warning.
issue
Deprecate dag longest path length#TITLE_END#Closes #6383 
issue
Reset deploy-action param names for latest version.#TITLE_END#Follow-up for #6446 .  According to the warnings in the [deploy action log](https://github.com/networkx/networkx/actions/runs/4254090726), the [parameter names for the gh-pages deploy action](https://github.com/JamesIves/github-pages-deploy-action#optional-choices) changed with the version bump. This PR should fix those and get the deploy job running again.
issue
Add docstring for dorogovtsev_goltsev_mendes generator#TITLE_END#Adds a more detailed docstring for this generator, including Parameters, Returns and Examples section. The info in the docstring is derived from a quick scan of referenced paper on arXiv.
issue
Purpose of `create_using` for generators that don't support other graph types#TITLE_END#This is just something I noticed while looking at the `dorogovtsev_goltsev_mendes` graph generator. The function has a `create_using` arg, but doesn't actually support any of the other non-Graph builtin graph classes:  https://github.com/networkx/networkx/blob/0b0a5fc5c512c2cbacd374860e47742e48a8dcfe/networkx/generators/classic.py#L410-L414  At first glance, this seems like a strange pattern: why provide the option to provide a `create_using` arg if it just results in an exception for the built-in, non-"simple" graph types? After thinking about this a bit though, the most likely motivating use-case is for user-defined subclasses of `nx.Graph`, e.g. `ThinGraph` (see the `nx.Graph` docstring section on subclassing).  I just wanted to raise this as a discussion point as the pattern may apply to other generators. Is this something that graph generators should support generally? This also ties into the wider discussion in NXEP3.
issue
Add documentation building to contributor guide#TITLE_END#There's currently no info in the contributor guide about how to build/view the documentation locally. This PR adds the minimal instructions to do so.  This also necessitated a minor reorganization of the headings for the contributor guide, which makes the diff look scarier than it actually is. I've regrouped the `Adding tests` and `Image comparison` sections as subsections under `Testing`; and grouped `Adding References` and `Adding Examples` as subsections of the newly added `Documentation` section.
issue
Discussion: introducing some keyword-only args for NX 3.0#TITLE_END#Related to #4100  As an experiment, I converted the `convert_matrix` module to use keyword-only args to see what it would take and how disruptive the changes would be within NX (as a proxy for how disruptive the change would be for general users).  I naively chose to be very aggressive in specifying what must be kwarg-only: basically, I made it so that every keyword argument (i.e. that has a default value) *must* be named when calling the function. Some observations from this experiment:  #### Switching to keyword-only arguments is likely to break people  As expected, making this switch did break some tests and some internal usages of e.g. `nx.to_numpy_array` in other NetworkX functions. For example, about half of the tests in the `test_pandas.py` suite fail due to the keyword-only requirement on things like:  ```python nx.from_pandas_edgelist(df, 0, "b", ["weight", "cost"]) ```  Of course, failing on things like this is kind of the point, because the above code is not easy to understand without looking up the `from_pandas_edgelist` signature, whereas the function call that obeys the kwarg-only requirements is much more readable:  ```python nx.from_pandas_edgelist(df, source=0, target="b", edge_attr=["weight", "cost"]) ```  Nevertheless, I think this indicates that switching to keyword-only arguments is likely to catch people out and raise a lot of new `TypeErrors` on previously working code.  #### Deciding which arguments exactly should be keyword only is going to be tricky  In this experiment, I made all of the arguments after the first kwarg-only. This is because all of the conversion functions follow the same general pattern where the first argument is the main data structure to be acted upon (a graph, a numpy array, scipy sparse matrix, etc.) and all of the other arguments are optional. For cases like the pandas conversion functions above I think this made sense, as it's not at all clear what the 2nd and 3rd positional arguments represent without additional context.  However, for e.g. the `to_numpy_array` case, making the `nodelist` argument keyword only caused quite a few failures on internal uses of `to_numpy_array` in other nx functions. There, the common pattern seemed to be:  ```python nodelist = [1, 4] ary = nx.to_numpy_array(G, nodelist) ```  Forcing `nodelist` to be kwarg only causes the above to fail, requiring `nx.to_numpy_array(G, nodelist=nodelist)` instead. Based on the number of places this usage pattern shows up, it seems like there is a pretty strong convention that the 2nd positional argument of the `to_*` conversion functions is `nodelist`. This highlights one of the challenges in deciding where the kwarg-only cutoff should be. My general sense is that we should not be overly aggressive in making things kwarg-only if it will be disruptive without any obvious benefit.  #### Takeaways  My thoughts after conducting this experiment:  - I think there should be an NXEP to clearly layout the motivation and general approach to introducing the keyword only requirement. I suspect that even the most seemingly-obvious/innocuous changes are going to cause new failures for some users, and it would be very beneficial to have a design doc to help frame any discussions.  - Also, because of the potential for breakage, any kwarg-only modifications to signatures should coincide with a major release  - My feeling is that we should be cognizant of how disruptive any change could be, and when in doubt to error on the side of caution. For instance, having conducted this experiment I would advocate for `to_numpy_array(G, nodelist=None, *, ...)` instead of `to_numpy_array(G, *, nodelist=None, ...)`.
issue
Add concurrency hook to cancel jobs on new push.#TITLE_END#This addition to the workflow is intended to cancel running jobs within a workflow when new changes are pushed up. This should help in situations where there are a lot of rapid pushes to a PR, which can trigger a long queue of CI jobs to build up.  See: https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#example-using-a-fallback-value  See also: scipy/scipy#15634 and numpy/numpy#21110. And thanks to @HarshCasper for calling attention to this improvement.
issue
Failure in vf2pp test suite from `pytest-randomly` workflow#TITLE_END#The weekly execution of the test suite in randomized order caught a problem in the vf2pp test suite, specifically `test_candidates.TestCandidateSelection.test_no_covered_neighbors_no_labels`:  https://github.com/rossbar/networkx/runs/8261228458?check_suite_focus=true  I suspect that this is due to the input graphs being stored as class variables (e.g. `self.G1`) which are then modified by subsequent tests leading to unintended crosstalk between the test methods, though I haven't looked at this deeply enough to verify.  The first thing I would try is re-working the tests so that they do not depend on inputs defined as class variables.
issue
Undocumented parameters in `dispersion`#TITLE_END#The signature for `nx.dispersion` is:  ```python def dispersion(G, u=None, v=None, normalized=True, alpha=1.0, b=0.0, c=0.0) ```  but the three parameters `alpha`, `b` and `c` are not described in the docstring.
issue
DOC: Minor formatting fixups to get rid of doc build warnings.#TITLE_END#Just some minor doc formatting fixups to get rid of the 25+ sphinx warnings that had accumulated.
issue
Add clique examples and deprecate helper funtions#TITLE_END#This is a proposal in two parts:  1. Update the `find_cliques` docstring with examples of how to compute various quantities from the returned maximal cliques, and  2. deprecate the four helper functions that wrap these examples.  I've lumped this all into one PR just to keep the discussion all in one place, but I'm happy to split out the docstring example (commit 8602796) from the other commits which deprecate the helper functions.  There was only one internal usage of one of the helpers in `could_be_isomorphic`.  Closes #6180
issue
Deprecate `nx.number_of_cliques`?#TITLE_END#Just a thought I had while [reviewing 6171](https://github.com/networkx/networkx/pull/6171#pullrequestreview-1169036037).  The `number_of_cliques` helper function creates (by default) a dictionary keyed by node where the values are the number of cliques containing the node. The default behavior essentially boils down to a one-liner:  ```python {n: sum(1 for c in nx.find_cliques(G) if n in c) for n in G} ```  The majority of the implementation of `nx.number_of_cliques` is dedicated to munging the various input arguments. IMO it'd be an improvement to instead recommend that users do this computation the Pythonic way rather than provide networkx-specific API for this task. IMO the potential upside for future users is worth the noise of a deprecation, but this is definitely subjective! Thoughts?
issue
`barbell_graph` docstring doesn't follow numpydoc standard#TITLE_END#It would be an improvement to conform to the [numpydoc standard](https://numpydoc.readthedocs.io/en/latest/format.html) to improve readability both in the terminal and in the rendered html.
issue
Rm incorrect test case for connected edge swap#TITLE_END#There is a valid edge swap between (0, 1) and (2, 3) for this case.  Closes #6220 
issue
Revert 6219 and delete comment.#TITLE_END#Reverts the code changes from #6219 - see [this discussion](https://github.com/networkx/networkx/pull/6219#issuecomment-1318650349).
issue
Adds LCA test case for self-ancestors from gh-4458.#TITLE_END#Adds the test case reported in gh-4458 to verify that it is indeed fixed by #5736.  Closes #4458
issue
Potential bug in connected_double_edge_swap#TITLE_END#The following test case failed during a `pytest-randomly` workflow:  https://github.com/networkx/networkx/blob/895963729231fe02153afe92ecc946a400247f1d/networkx/algorithms/tests/test_swap.py#L37-L39  I was able to reproduce locally on `main` by running interactively. When running without a `seed`, I occasionally get `1` instead of `0`. This is also reproducible when seeded:  ```python >>> G = nx.path_graph(4) >>> nx.connected_double_edge_swap(G, seed=140) 1 ```  I tested with both 2.8.3 and 2.7 and was able to reproduce with those versions as well, so I don't think this is something recently introduced (though I haven't looked at blame or bisected).
issue
Minor Python 2 cleanup#TITLE_END#Use dict.keys() for set operations rather than explicitly creating sets, as recommended in the comment.
issue
Remove deprecated project function from bipartite package.#TITLE_END#Another deprecation to be expired for 3.0
issue
Remove deprecated `find_cores`#TITLE_END#It looks like we missed the `find_cores` function when removing deprecated code for NetworkX 3.0. The likely reason it was missed is that we (I) *also* forgot to add the deprecation to the NX 2.7 release notes.  Fortunately, the deprecation warning was being raised, so hopefully any users who were relying on it at least got some notification when running their code. There are a couple things we could do here:  1) Just go ahead an remove it (as proposed here) even though we missed it in the release notes  2) Add the deprecation to the release notes for 3.0 and push back the actual deprecation to 3.2  3) Remove it now, but update the module `__getattr__` for the top-level `__init__.py` to advise users to use `core_number` instead.  1 is the least work, but I'm happy to update the module `__getattr__` if anyone thinks it's necessary. Let me know what you think!
issue
Remove deprecated maxcardinality parameter from min_weight_matching#TITLE_END#Another deprecation removal for 3.0 - the `maxcardinality` parameter of `min_weight_matching` in this case.
issue
Warn on unused visualization kwargs that only apply to FancyArrowPatch edges#TITLE_END#Closes #5857 and #5694  `draw_networkx_edges` has a few keyword arguments that only apply when FancyArrowPatch (FAP) objects are used to represent the edges (as opposed to a LineCollection (LC)). The tricky part is that the logic for determining which object is used is not immediately clear to the user. By default, LCs are used when the input graph `G` is undirected, and FAPs are used when `G` is directed. The reason for this is that LC are *much* faster to draw than FAPs, so using LCs where possible greatly enhances the ability to visualize graphs with many edges. The default behavior can be overridden with the `arrows` kwarg: `arrows=True` will force `draw_networkx_edges` to use FAPs even if `G` is undirected. This may make the visualization much slower, especially if there are a lot of edges, but the tradeoff is that the user can then take advantage of FAP-specific features, like the ability to draw curved edges.  This PR introduces UserWarnings whenever a user calls `draw_networkx_edges` with a FAP-specific kwarg when LC is being used to draw the edges. The current behavior is to silently ignore the FAP-specific kwarg which leads to confusion - see e.g. #5857 and #5694. The warning message specifies how to fix the situation, either by using `arrows=True` to force FAPs (which may be slow) or by deleting the unused kwarg option if it is not needed.  The hope is that the warning is useful without being noisy - note that adding it in caught one instance in the NX test suite where kwargs were silently ignored. The other concern is that the warnings are *not* raised when they shouldn't be - I double checked the output of the gallery examples to ensure that there were no new, unexpected warnings from this change.
issue
Replace .A call with .toarray for sparse array in example.#TITLE_END#Fixes up the `plot_eigenvalues` example to use best-practices when creating a dense array from a sparse array.
issue
Update GML parsing/writing to allow empty lists/tuples as node attributes#TITLE_END#Updates the "stringization" functionality in `gml.py` to allow storing empty lists or tuples as node attributes.  This was formerly disallowed, but AFAICT there's nothing in the GML standard (pdf only) that prohibits empty lists/tuples from being used. This would fix the issue raised in #4953 directly, without having to use the deprecated `literal_stringizer` function.
issue
Minor updates to expanders generator tests#TITLE_END#The main update is splitting the margulis_galil_gabber test into two: one that doesn't depend on numpy/scipy and one that does. In principle this is a minor improvement as previously the entire test was skipped if the dependencies were not installed.  There are some other minor tweaks, like using functions from the `nx` namespace (so this now doubles as a test of the public API); and parametrizing the tests.  IMO this (along with some other testing, see [here](https://github.com/networkx/networkx/issues/6026#issuecomment-1273627062)) is sufficient to close #6026 
issue
Fix warnings from running tests in randomized order#TITLE_END#A few minor fixups to the test suite. I noticed these from a few warnings that were popping up when running the test suite in random order.  - 68144a9 cleans up a `.A` call on a sparse array that was missed during the general sparse cleanup (though the fact that this warning is only raised when the test suite is run in random order means we probably need to take a closer look and ensure the tests in this file are all being run!) - 7978bad fixes crosstalk between pylab tests from the fact that a axis object created when using the `pyplot` interface for matplotlib persists across tests, since the default axis object for most of the nx drawing commands is `plt.gca`. Many of these had been fixed previously.
issue
Rename vf2pp arg from `node_labels`->`node_label`#TITLE_END#Just a quick follow-up to #5788. The argument name for the node label in the vf2pp functions is `node_labels` - I believe the plural here was left over from an older design where we considered supporting multiple labels. The direction we ended up going instead was to support only a single node attribute to simplify the API (users can construct a compound attribute themselves if so desired). Therefore I think the argument should not be pluralized - I propose `node_label` instead but if there are other naming suggestions (`node_attr`?) please chime in!
issue
Unused `root` argument in `has_bridges`#TITLE_END#This is the last remaining issue from #5486. I've split it out into it's own issue as all of the other problems identified in #5486 have been resolved.  As originally discovered by @dtekinoglu `has_bridges` defines a `root` parameter which is not used. It should be passed on to the underlying `bridges` call. There should also be some additional tests for `has_bridges` with the `root` argument - both that it works as expected when `root` is provided, and that it fails as expected when `root` is specified incorrectly (e.g. `NodeNotFound`)
issue
Update documentation surrounding `union`, `disjoint_union`, `compose` and `Graph.update`#TITLE_END#This is an action item extracted from the discussion in #4208.  It's not always immediately clear to users how the various operators `union`, `disjoint_union`, and `compose` differ and for which use-cases each is appropriate. There is also the `update` method of the graph classes, which simply updates the nodes/edges of a graph in-place with those of another, which is related to the above operations. It would be an improvement to ensure that the docstrings for these functions all include a link to the `update` method.  It may also be worthwhile to update the `operators` module docstring with examples highlighting the use-cases and differences between all of these operations.
issue
Minor docstring touchups and test refactor for `is_path`#TITLE_END#Just a few minor changes I'd like to propose to follow-up on #5943:  - A few docstring/formatting touchups  - Condense conditionals using `or`  - parametrize test and remove redundant case  Nothing critically important, feel free to ignore!
issue
Switch to relative import for vf2pp_helpers.#TITLE_END#Switch imports of vf2pp_helpers to relative imports. This should fix the current doc build/deploy error (tested locally).  I think there's multiple things going on here: 1) `vf2pp_helpers` doesn't have an `__init__.py` and therefore isn't a package. However, that's not the *only* thing... `vf2pp_helpers` isn't accessible from the `__init__.py` of any of the parent packages (e.g. algorithms, isomorphism), so doing absolute imports from the root of the directory shouldn't be expected to work. In principle, this was by design as we don't want to expose the `vf2pp_helpers` directly to users.  As for why this wasn't caught in CI... the imports are generally hard to test reliably as it's affected by import order. It's *possible* that the `pytest-randomly` workflow would've caught this down the line, but I can't think of any specific way that the tests could be improved to catch this... ideas welcome!
issue
Add vf2pp_helpers subpackage to wheel#TITLE_END#Follow up to #5973, which didn't actually fix the problem with the doc build.  After more debugging, I determined that the problem was the `vf2pp_helpers` subpackage wasn't actually included in the wheel. I missed this in my first round of debugging that led to my mis-diagnosis in #5973 likely from accidentally using `pip -e` and still building the docs from the source directory. We should probably add/update at least one CI job to not use editable installs to catch this problem in the future, though it's worth noting that the `vf2pp_helpers` subpackage appears to be unique - i.e. there are no other "helper" subpackages that are *not* included in an __init__.py somewhere.  In the slightly longer term, it might be better to refactor the `vf2pp_helpers` subpackage into a submodule instead (e.g. `_vf2pp_helpers.py`) which should remove the need to explicitly list the extra directory in the `setup.py`. That should be straightforward, but will touch a lot of import lines, so I will leave it to a different PR.
issue
Fix failing example due to mpl 3.6 colorbar.#TITLE_END#I thought we had gotten all of these in previous PRs, but it looks like we missed one.
issue
Only run scheduled pytest-randomly job in main repo.#TITLE_END#Prevents this job from running on forks, which should prevent email notifications being sent to fork owners when the job fails. Thanks @mjschwenne for pointing this out.  I grabbed the syntax for this from: https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idif
issue
Replace existing LCA functions with "naive" implementations?#TITLE_END##5736 recently added "naive" implementations of the LCA functions (thanks @dtekinoglu ! :rocket: ) which avoid some [long-standing bugs](https://github.com/networkx/networkx/issues?q=is%3Aopen+is%3Aissue+label%3ALCA) that have been identified by users with the existing implementation. #5736 added the fixes as new functions with the name `naive_` prepended to the existing function names.  My question is: instead of adding new function names, can we replace the existing LCA implementations with the new naive implementations? At face value, I think we can do so *without any deprecation warnings* because the naive implementations are really bug fixes - AFAICT there is no change to the API.  Is there appetite for this? I think it would be a very nice improvement and would allow us to close a lot of the LCA issues. I'm going to milestone this because we should decide whether to do this prior to the next release (so that we don't have to deprecate the new `naive_` function names).
issue
Update mentored projects list#TITLE_END#This moves VF2++ from the list of available mentored projects to the "completed projects" list.
issue
Update documentation header links for latest pydata-sphinx-theme#TITLE_END#By default, the latest `pydata-sphinx-theme` only shows 5 header link items in the navbar, placing the extra in a dropdown menu under "More". In our case, this ended up hiding the `Gallery` link, which I wanted to bring back out to make sure it as always visible. Therefore I made two changes:  1. Keep header links always visible by increasing the number allowed from 5->7  2. Re-ordered the links so that `Gallery` is before the developer guide link and the release notes.
issue
Revert "Add workaround for pytest failures on 3.11b2"#TITLE_END#Reverts networkx/networkx#5680  This should no longer be necessary with Python 3.11b3 out.
issue
Replace LCA with naive implementations#TITLE_END#I did a bit of investigation to see what it would take to solve #5869. The first step is to see what fails in the test suite when the existing LCA functions are replaced with the corresponding naive implementations. I'll break down the findings below.  Upon replacing the existing implementations with the new ones (see 50dc11f) we get 7 test failures from the existing LCA test suite:  <details>   <summary>Initial test failures</summary>   <pre> networkx/algorithms/tests/test_lowest_common_ancestors.py:290: Failed            =========================== short test summary info ============================ FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor1 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor2 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor3 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor4 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor5 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor7 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor8 ========================= 7 failed, 26 passed in 0.15s ========================= </pre> </details>  These failures basically break down into two categories:  1. Failing to raise exceptions  2. "Wrong" LCA results (at least according to the test suite).  Let's address group 1. first.  The main problem here is that the `naive_all_pairs_lowest_common_ancestor` is a pure generator, whereas `all_pairs_lowest_common_ancestor` is actually a function that internally creates and returns a generator. Thus the input validation exceptions (i.e. checking it's a DAG, etc.) raise immediately in the case of `all_pairs_lowest_common_ancestor`, but aren't raised until the generator is *called* in the `naive_` case.  The simplest way to recover the original behavior is to copy the pattern used in `all_pairs_lowest_common_ancestor`; i.e. move the input validation checks *out* of the generator, create the generator *internally*, then return the generator instance - see 2c56aa7. Personally, I don't love this pattern (notice the repr has `<locals>` in it because of the internal definition, which is an implementation detail that users might find confusing if they look too closely), but it seems to be the easiest way to recover the prompt-exception behavior. Maybe there's a better way (`generator.throw?`)?  Anyways 2c56aa7 gets rid of *almost all* of the exception-raising test failures:  <details>   <summary>After internal generator fix</summary>   <pre> ============================= test session starts ============================== platform linux -- Python 3.10.5, pytest-7.1.1, pluggy-1.0.0 Matplotlib: 3.5.1 Freetype: 2.6.1 rootdir: /home/ross/repos/networkx plugins: mpl-0.14.0, cov-3.0.0 collected 33 items  networkx/algorithms/tests/test_lowest_common_ancestors.py .............F [ 42%] FFFF...............                                                      [100%]  =================================== FAILURES =================================== ______________ TestDAGLCA.test_all_pairs_lowest_common_ancestor1 _______________  self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f3e5c650490>      def test_all_pairs_lowest_common_ancestor1(self):         """Produces the correct results.""" >       self.assert_lca_dicts_same(dict(all_pairs_lca(self.DG)), self.gold)  networkx/algorithms/tests/test_lowest_common_ancestors.py:239:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f3e5c650490> d1 = {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, ...} d2 = {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, ...} G = <networkx.classes.digraph.DiGraph object at 0x7f3e5c6536d0>      def assert_lca_dicts_same(self, d1, d2, G=None):         """Checks if d1 and d2 contain the same pairs and         have a node at the same distance from root for each.         If G is None use self.DG."""         if G is None:             G = self.DG             root_distance = self.root_distance         else:             roots = [n for n, deg in G.in_degree if deg == 0]             assert len(roots) == 1             root_distance = nx.shortest_path_length(G, source=roots[0])              for a, b in ((min(pair), max(pair)) for pair in chain(d1, d2)): >           assert (                 root_distance[get_pair(d1, a, b)] == root_distance[get_pair(d2, a, b)]             ) E           assert 2 == 3  networkx/algorithms/tests/test_lowest_common_ancestors.py:233: AssertionError ______________ TestDAGLCA.test_all_pairs_lowest_common_ancestor2 _______________  self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f3e5c650670>      def test_all_pairs_lowest_common_ancestor2(self):         """Produces the correct results when all pairs given."""         all_pairs = list(product(self.DG.nodes(), self.DG.nodes()))         ans = all_pairs_lca(self.DG, pairs=all_pairs) >       self.assert_lca_dicts_same(dict(ans), self.gold)  networkx/algorithms/tests/test_lowest_common_ancestors.py:245:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f3e5c650670> d1 = {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, ...} d2 = {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, ...} G = <networkx.classes.digraph.DiGraph object at 0x7f3e5c6536d0>      def assert_lca_dicts_same(self, d1, d2, G=None):         """Checks if d1 and d2 contain the same pairs and         have a node at the same distance from root for each.         If G is None use self.DG."""         if G is None:             G = self.DG             root_distance = self.root_distance         else:             roots = [n for n, deg in G.in_degree if deg == 0]             assert len(roots) == 1             root_distance = nx.shortest_path_length(G, source=roots[0])              for a, b in ((min(pair), max(pair)) for pair in chain(d1, d2)): >           assert (                 root_distance[get_pair(d1, a, b)] == root_distance[get_pair(d2, a, b)]             ) E           assert 2 == 3  networkx/algorithms/tests/test_lowest_common_ancestors.py:233: AssertionError ______________ TestDAGLCA.test_all_pairs_lowest_common_ancestor3 _______________  self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f3e5c650850>      def test_all_pairs_lowest_common_ancestor3(self):         """Produces the correct results when all pairs given as a generator."""         all_pairs = product(self.DG.nodes(), self.DG.nodes())         ans = all_pairs_lca(self.DG, pairs=all_pairs) >       self.assert_lca_dicts_same(dict(ans), self.gold)  networkx/algorithms/tests/test_lowest_common_ancestors.py:251:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f3e5c650850> d1 = {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, ...} d2 = {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, ...} G = <networkx.classes.digraph.DiGraph object at 0x7f3e5c6536d0>      def assert_lca_dicts_same(self, d1, d2, G=None):         """Checks if d1 and d2 contain the same pairs and         have a node at the same distance from root for each.         If G is None use self.DG."""         if G is None:             G = self.DG             root_distance = self.root_distance         else:             roots = [n for n, deg in G.in_degree if deg == 0]             assert len(roots) == 1             root_distance = nx.shortest_path_length(G, source=roots[0])              for a, b in ((min(pair), max(pair)) for pair in chain(d1, d2)): >           assert (                 root_distance[get_pair(d1, a, b)] == root_distance[get_pair(d2, a, b)]             ) E           assert 2 == 3  networkx/algorithms/tests/test_lowest_common_ancestors.py:233: AssertionError ______________ TestDAGLCA.test_all_pairs_lowest_common_ancestor4 _______________  self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f3e5c650a30>      def test_all_pairs_lowest_common_ancestor4(self):         """Graph with two roots."""         G = self.DG.copy()         G.add_edge(9, 10)         G.add_edge(9, 4)         gold = self.gold.copy()         gold[9, 9] = 9         gold[9, 10] = 9         gold[9, 4] = 9         gold[9, 3] = 9         gold[10, 4] = 9         gold[10, 3] = 9         gold[10, 10] = 10              testing = dict(all_pairs_lca(G))              G.add_edge(-1, 9)         G.add_edge(-1, 0) >       self.assert_lca_dicts_same(testing, gold, G)  networkx/algorithms/tests/test_lowest_common_ancestors.py:271:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f3e5c650a30> d1 = {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, ...} d2 = {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, ...} G = <networkx.classes.digraph.DiGraph object at 0x7f3e5c55b7f0>      def assert_lca_dicts_same(self, d1, d2, G=None):         """Checks if d1 and d2 contain the same pairs and         have a node at the same distance from root for each.         If G is None use self.DG."""         if G is None:             G = self.DG             root_distance = self.root_distance         else:             roots = [n for n, deg in G.in_degree if deg == 0]             assert len(roots) == 1             root_distance = nx.shortest_path_length(G, source=roots[0])              for a, b in ((min(pair), max(pair)) for pair in chain(d1, d2)): >           assert (                 root_distance[get_pair(d1, a, b)] == root_distance[get_pair(d2, a, b)]             ) E           assert 3 == 4  networkx/algorithms/tests/test_lowest_common_ancestors.py:233: AssertionError ______________ TestDAGLCA.test_all_pairs_lowest_common_ancestor5 _______________  self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f3e5c650c10>      def test_all_pairs_lowest_common_ancestor5(self):         """Test that pairs not in the graph raises error.""" >       pytest.raises(nx.NodeNotFound, all_pairs_lca, self.DG, [(-1, -1)]) E       Failed: DID NOT RAISE <class 'networkx.exception.NodeNotFound'>  networkx/algorithms/tests/test_lowest_common_ancestors.py:275: Failed =========================== short test summary info ============================ FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor1 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor2 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor3 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor4 FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor5 ========================= 5 failed, 28 passed in 0.13s =========================  </pre> </details>  The next set of failures are due to the `root_distance` not being as expected. Fortunately, @dtekinoglu, @MridulS , and @dschult noticed that there was an error in one of the ground-truth values for one of the tests. If we fix that, then these errors disappear - see 3bf51ac.  <details>   <summary>Test failures after fixing wrong self-ancestor in test reference</summary>   <pre> ============================= test session starts ============================== platform linux -- Python 3.10.5, pytest-7.1.1, pluggy-1.0.0 Matplotlib: 3.5.1 Freetype: 2.6.1 rootdir: /home/ross/repos/networkx plugins: mpl-0.14.0, cov-3.0.0 collected 33 items  networkx/algorithms/tests/test_lowest_common_ancestors.py .............. [ 42%] ...F...............                                                      [100%]  =================================== FAILURES =================================== ______________ TestDAGLCA.test_all_pairs_lowest_common_ancestor5 _______________  self = <networkx.algorithms.tests.test_lowest_common_ancestors.TestDAGLCA object at 0x7f8d806f4b80>      def test_all_pairs_lowest_common_ancestor5(self):         """Test that pairs not in the graph raises error.""" >       pytest.raises(nx.NodeNotFound, all_pairs_lca, self.DG, [(-1, -1)]) E       Failed: DID NOT RAISE <class 'networkx.exception.NodeNotFound'>  networkx/algorithms/tests/test_lowest_common_ancestors.py:275: Failed =========================== short test summary info ============================ FAILED networkx/algorithms/tests/test_lowest_common_ancestors.py::TestDAGLCA::test_all_pairs_lowest_common_ancestor5 ========================= 1 failed, 32 passed in 0.10s ========================= </pre> </details>  So that's good news... it looks like the original LCA implementation was only passing the test case because the test case itself was obviously incorrect! The "naive" implementation fails the incorrect test and passes the correct one - just one more indication that the naive implementations are more correct and reliable than the existing ones!  At this point there's one failure remaining, and it's another failure to raise a prompt exception. In this case, `all_pairs_lowest_common_ancestor` actually did pre-evaluation of the `pairs` argument to verify that all of the nodes provided in `pairs` are actually in `G`. In order to match this behavior, we'd have to add the same pre-processing to the naive implementations. Unfortunately, there's another quirk that makes this a bit clunkier: the docstring for `all_pairs_lowest_common_ancestor` says that `pairs` must be an *iterable* of 2-tuples of nodes, but the test suite also explicitly tests that *iterators* (e.g. generators) are accepted (see the `_ancestor3` test). This means that, in order to match the original behavior, we have to add an extra conversion step to accept both inputs (search for `pairset` in the original implementation). This is all done in b535ca1. Again, not the prettiest solution, but the easiest way (that I could think of at least) to recover the original behavior of `all_pairs_lowest_common_ancestor`.  In summary, a few changes would be required to replace the LCA function implementations. Having gone through this exercise, I think it's definitely worthwhile because one of those changes actually shows how the current implementation gives flat-out wrong results.  I'm going to open this as a draft PR and hope to get feedback on the individual changes made in each of the three commits 2c56aa7, 3bf51ac, and b535ca1. If those look acceptable, then I remove the aliasing in 50dc11f and replace the implementations of `all_pairs_lowest_common_ancestor` and `lowest_common_ancestor` with the updated implementations.
issue
Minor doc fixups#TITLE_END#Just a few fixups for sphinx warnings that I noticed during a local doc build.
issue
Add warning to nx_agraph about layout nondeterminism.#TITLE_END#Adds warnings to the write/draw/layout functions in `nx_agraph` to address #299 .  It seems some graphviz layouts are not guaranteed to be deterministic, i.e. multiple calls with the same input may result in different layouts. See https://gitlab.com/graphviz/graphviz/-/issues/1767 for details. This has caught out some NX users via the `nx_agraph` package, so this PR adds a note about the potential non-determinism of some graphviz layouts.
issue
Modify DAG example to show topological layout.#TITLE_END#I recently revisited the new example gallery on [visualizing dags with different layouts](https://networkx.org/documentation/latest/auto_examples/graph/plot_DAG_layouts.html#sphx-glr-auto-examples-graph-plot-dag-layouts-py) and realized that none of the layouts really work that well for DAGs. I think #5124 was *really* looking for was a good way to visualize a DAG... thinking about this a bit, I thought the best way would be to compute a layout based on a topological ordering. Fortunately, there's a really simple way to do this combining `topological_generations` and `multipartite_layout`!  This example keeps the DAG from the original example, but rather than compute 8 different layouts, it computes a single layout based on the topological order. Based on the resulting visualization, I think this layout is preferable to any of the others (just my opinion!)
issue
Improve LCA input validation#TITLE_END#Some minor fixups I noticed while reviewing #5876:  1. The `if None in G` checks are not necessary as it is not possible to add `None` as a node to the base graph classes (as of #4892)  2. Some of the raising branches used `elif` instead of `if`.
issue
Propose to make new node_link arguments keyword only.#TITLE_END#Just an idea I had while following up on #5787   Since we're adding these new arguments to replace the strange `attrs` munging (an obvious improvement, thanks @brocla ) I thought it might make sense to set the new args to keyword only.  Note - if we decide to do this, it should go in before the next release!
issue
Update mapping logic in `relabel_nodes`#TITLE_END#Related to #5903 and #5896.  #5903 was a very nice solution to the problem identified in #5896 - i.e. the input-munging logic for the `mapping` parameter used a `hasattr("__getitem__")` check which was fragile in the case of the built-in `str`. The trick here was that `str` is a Python *type*, which both has a `__getitem__` attr, *and* is a Callable.  The logic has been updated to fix this specific case in #5903, but I'd like to propose using `isinstance` checks against ABC's if for no other reason that I personally find them slightly easier to reason about.  One major difference with this change is how the code fails. Note that previously if a user passed in an object that was neither a `Mapping` nor `Callable`, then they would eventually get an `AttributeError` when the `.get` method was called. For example  #### On main (after #5903)  ```python >>> G = nx.path_graph(4) >>> s = "0123" >>> l = ["0", "1", "2", "3"] >>> nx.relabel_nodes(G, s) Traceback (most recent call last)    ... AttributeError: 'str' object has no attribute 'get' >>> nx.relabel_nodes(G, l) Traceback (most recent call last)    ... AttributeError: 'list' object has no attribute 'get' ```  #### This PR  ```python >>> G = nx.path_graph(4) >>> s = "0123" >>> l = ["0", "1", "2", "3"] >>> nx.relabel_nodes(G, s) Traceback (most recent call last)    ... TypeError: 'str' object is not callable >>> nx.relabel_nodes(G, l) Traceback (most recent call last)    ... TypeError: 'list' object is not callable ```  Note the exception changes from an `AttributeError` to a `TypeError` with this proposed change. In principle, the check could be made more specific to improve the exception message, e.g. something like:  ```python if not isinstance(mapping, Mapping):     if not isinstance(mapping, Callable):         raise TypeError("mapping must either be a mapping or callable")  # or NetworkXError, if preferred     m = {n: mapping(n) for n in G} ```  Of course, the change in exception type is a breaking change, so it could well be that this churn isn't worth it. Nevertheless I wanted to submit my two cents on the topic (since I missed the review in #5903!)
issue
Bump nodelink args deprecation expiration to v3.2#TITLE_END#I propose to bump the deprecation expiration for the arguments of the `node_link_` functions from v3.1 to v3.2. See https://github.com/networkx/networkx/pull/5928#issuecomment-1213017715 and https://github.com/networkx/networkx/pull/5899#issuecomment-1212910911 for further discussion.  Also includes minor fixup to the warnings filter.
issue
`node_link` functions `attrs` kwarg#TITLE_END#`node_link_graph` and `node_link_data` have a similar API to the `cytoscape` functions, where there is a `attrs` kwarg which expects a dictionary containing specific keys that is then unpacked internally.  This API is a bit un-Pythonic and was removed in favor of keyword arguments - see the related discussion in #4199. It's worth checking if the `node_link` functions wouldn't benefit from a similar treatment.
issue
Update `all_pairs_lca` docstrings#TITLE_END#Just a few potential improvements I noticed while reviewing #5736:   - [ ] The `Returns` section should actually be a `Yields` as this is a generator. This should also simplify the return description  - [ ] Add a `Raises` section to indicate when exceptions are raised (graph is undirected, graph isn't DAG, graph is empty, etc.)  - [ ] Update example to show usage both with and without the `pairs` argument.  Note that these changes should be made to the `naive` fn docstring as well.
issue
Gallery example: Morse code alphabet as a prefix tree#TITLE_END#I was recently wasting time on the internet when I came across a Morse code signal decoders chart, which immediately struck me as a good example of a prefix tree. I thought it might make a nice gallery example!
issue
Update distance parameter description.#TITLE_END#Alternative to #5695.  Update the description of the  parameter in the closeness_centrality function to note that by default, the edge distances (weights) are assumed to be 1, and that no input validation is performed.  Closes #3880
issue
Duplicate directed edges in `min_edge_cover` set#TITLE_END#I was surprised by the following result when reviewing the `min_edge_cover` in #5478:  ```python >>> G = nx.complete_graph(5) >>> nx.min_edge_cover(G) {(0, 2), (2, 0), (3, 1), (4, 0)} ```  The duplication of the `(0, 2)` edge (disregarding order) seems incorrect for a *minimal* edge cover, at least based on [my limited understanding of the min edge cover problem](https://en.wikipedia.org/wiki/Edge_cover).
issue
Held-Karp ascent failures due to optimize.linprog infeasibility#TITLE_END#The recent scipy 1.9rc release caught some failures related to our use of `optimize.linprog` in the Held-Karp implementations. For example, the following tests are failing:  ``` FAILED networkx/algorithms/approximation/tests/test_traveling_salesman.py::test_held_karp_ascent FAILED networkx/algorithms/approximation/tests/test_traveling_salesman.py::test_ascent_method_asymmetric FAILED networkx/algorithms/approximation/tests/test_traveling_salesman.py::test_ascent_method_asymmetric_2 FAILED networkx/algorithms/approximation/tests/test_traveling_salesman.py::test_held_karp_ascent_asymmetric_3 FAILED networkx/algorithms/approximation/tests/test_traveling_salesman.py::test_asadpour_real_world FAILED networkx/algorithms/approximation/tests/test_traveling_salesman.py::test_asadpour_real_world_path ```  The specifics related to the release are addressed elsewhere (see scipy/scipy#16466 and #5816), but taking a closer look at these failures it turns out that an underlying problem is that the results from `optimize.linprog` are actually not successful (this was true for scipy 1.8 as well, but we only got warnings instead of exceptions). In each of these cases, the returned `OptimiztionResult.success == False`. I didn't look closely at each case, but the `.message` attribute for at least one case indicates redundancies in `A_eq` and `b_eq` make the problem infeasible. I'm not sure exactly what's going on and whether this has something to do with the problem formulations or maybe some problem in how we're using `optimize.linprog`.
issue
Add validation for distance kwarg in closeness_centrality.#TITLE_END#Closes #3880  Validates that *at least one* edge in the graph has the `distance` parameter as an attribute in the case when `distance` is provided by the user. This should make the function more robust against e.g. misspellings or otherwise incorrect keys, at the cost of looping through all edges in the graph. Whether or not this is a tradeoff worth making I leave to others to decide. FWIW I would be okay with closing this and #3880 as a "won't fix" and perhaps just adding an admonishment to the docstring to be extra careful about the `distance` kwarg.
issue
Temporary fix for failing tests w/ scipy1.9.#TITLE_END#SciPy 1.9 (currently in rc1) switches the default `method` for `sp.optimize.linprog` from `internal-point` to `highs`. The new method results in a slight behavior change in that `None` is returned when `linprog` fails instead of an array of zeros.  This temporary fix switches back to the scipy1.8 default method, which warns but doesn't result in an exception raise. In the longer run this needs to be revisited as it turns out our held-karp test cases all result in failed optimizations from `optimize.linprog`. I'll create a separate issue for that.  See also scipy/scipy#16466
issue
Check that nodes have "pos" attribute in geometric_edges#TITLE_END#Closes #5046 .  Adds input validation that raises a more clear exception in the case where any node in the input graph does not have a `"pos"` attribute, which is required by the `geometric_edges` function.  I structured this in such a way that the input validation is only done in the user-facing function so that it doesn't add overhead to all of the other geometric graph generators.
issue
Should `geometric_edges` raise a specific exception if nodes don't have "pos" attribute?#TITLE_END#The `networkx.generators.geometric.geometric_edges` function assumes that the input graph contains nodes that have a `"pos"` attribute representing the 2D position of the node. If this is not the case, exceptions are raised (depending on whether or not scipy is installed) that don't really indicate what the problem is:  ```python >>> G = nx.path_graph(3) ```  ### with scipy  ```python >>> nx.generators.geometric.geometric_edges(G, radius=1, p=2) Traceback (most recent call last)    ... ckdtree.pyx in scipy.spatial.ckdtree.cKDTree.__init__()  ValueError: data must be 2 dimensions ``` ### without scipy ```python >>> nx.generators.geometric.geometric_edges(G, radius=1, p=2) Traceback (most recent call last):    ... TypeError: 'NoneType' object is not iterable ```  The thing that makes the most sense to me would be to raise an exception with a specific message to indicate to users that the nodes need a "pos" attribute. Another potential solution would be to have a default value for position when the attribute isn't found, but I don't know what a sensible default would be (`(0, 0)`)?
issue
Rm `to_numpy_recarray`#TITLE_END#Remove deprecated function `to_numpy_recarray` for NX 3.0.
issue
Add example of topo_order kwarg to dag_longest_path#TITLE_END#Implements the suggestion from #5712 to add an example of the `topo_order` kwarg to the `dag_longest_path` docstring.
issue
CI: add pytest-randomly workflow.#TITLE_END#Adds a single workflow that runs the test suite with the tests executed in random order. This can help identify problems with poorly-designed tests.  On the one hand, adding this to CI can help improve the robustness of the test suite. OTOH, it can also introduce failures to PRs that are not related to the submitted changes.  One alternative could be to de-couple this from submitted PRs and just have a time or status-based workflow (e.g. run the workflow once a day). LMK if you think this is worthwhile to pursue!
issue
Touchups to MG and MDG edges docstrings.#TITLE_END#Follow-up to #5389 to further refine the docstrings for the `edges` property of the MultiGraph and MultiDiGraph classes.  Closes #4962, which was already mostly addressed in #5389.
issue
Recover order of layers in multipartite_layout when layers are sortable#TITLE_END#Closes #5691.  Sorts the positions by layer in the case where the `subset_key` is sortable, but still supports the case where it isn't. Note that this is an alternative to #5515 which preserves support for non-sortable subset_keys.
issue
Default to lightmode for documentation#TITLE_END#Closes #5714.  Sets the default documentation theme back to light mode, but adds a toggle to the navbar to allow users to try the dark mode theme if they wish. I advocate for defaulting to light mode until we've had time to investigate how well dark mode works for the docs and whether there's anything that needs to be tweaked (darkmode logo, banners, pygments (syntax highlighting) themes, etc.)
issue
Add durations flag to coverage run on CI.#TITLE_END#This is the longest-running job so it'd be good to have a bit more info on the test runtimes in github actions.
issue
Update simple_cycles docstring w/ yields and examples#TITLE_END#`Returns` -> `Yields` as `simple_cycles` is a generator. I also took the liberty to make minor adjustments to the examples to make them a bit more descriptive.
issue
Add default value p=2 for minkowski distance metric.#TITLE_END#Closes #5047   Gives the `p` parameter in the `geometric_edges` generator a default value of 2, corresponding to Euclidean distance. This converts `p` to an optional parameter with a default value that matches the defaults for the other geometric generators.
issue
Update multigraph docstrings to reflect `remove_edges_from` behavior.#TITLE_END#Addresses the first bullet of issue #5158 , i.e. updating the documentation to correct the stated behavior of `remove_edges_from` when 2-tuples are passed. The remaining parts of the issue are larger discussions that should probably be moved to their own topic.
issue
Add docstring example for attr transfer to linegraph.#TITLE_END#Closes #4483 by adopting @dschult 's suggested docstring example.  Co-authored-by: Dan Schult <dschult@colgate.edu>
issue
Add initial_graph parameter to scale_free_graph and deprecate create_using#TITLE_END#Fixes #4729 by adding a new keyword argument named `initial_graph` to replace `create_using`. See the reference issue for details. Note that creating a scale-free graph starting from an existing MultiDiGraph is itself not actually tested, but that's a separate issue.
issue
Update ISMAGS.analyze_symmetry docstring.#TITLE_END#Closes #4774 by adopting @pckroon 's suggested docstring update from the discussion in that issue.
issue
Graph instance modified in place by `scale_free_graph`#TITLE_END#`nx.scale_free_graph` has a `create_using`argument which accepts different types of inputs. When a graph instance is provided, it is used as a starting point for the graph to which more nodes will be added as is clearly described in the docstring.  However, the instance passed in via `create_using` is also modified in place:  ```python >>> G = nx.scale_free_graph(10) >>> G.nodes() NodeView((0, 1, 2, 3, 4, 5, 6, 7, 8, 9)) >>> H = nx.scale_free_graph(20, create_using=G) >>> G.nodes() NodeView((0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19)) ```  This behavior should either be documented, or the `create_using` argument should be copied (when a graph instance) to prevent the in-place modification. I think the second option is more consistent with the other functions in NetworkX, which generally try to avoid in-place modification of inputs.
issue
Heads up: Latest documentation now uses dark mode by default depending on browser defaults#TITLE_END#With `pydata-sphinx-theme==0.9`, the documentation now defaults to light/dark mode depending on your browser preferences. Mine happen to be set to dark, so I see the following at https://networkx.org/documenation/latest  ![Screenshot_20220609_083711](https://user-images.githubusercontent.com/1268991/172772573-a00da108-528f-415e-94de-8a9aef4a7f57.png)  IMO the dark mode is still a little rough around the edges - the logo and banner don't look great and the dark background is a deeper black than the default darkmode for other sites.  I think we should do a little experimentation with dark mode for the site before releasing it to users. I'd propose to configure the theme to force light mode (i.e. the behavior up until v0.9) until we've had a chance to play around with dark mode a bit more.
issue
Add default distance metric to `geometric_edges`#TITLE_END#The `nx.generators.geometric.geometric_edges` function currently takes 3 varpos parameters. The last of these parameters `p` is the "Minkovsky distance metric", which allows users to select the power used to compute the distances.   Having looked at the function a bit, I was wondering if it wasn't worth perhaps turning `p` into an optional kwarg and giving it a default value. IIUC, `p=2` corresponds to Euclidean distance, which is the default distance measure used for many of the other functions in the module. Thoughts about making this the default value for this function?
issue
Review ISMAGS methods#TITLE_END#Related to discussion in #4761   Some of the methods of the [ISMAGS class](https://github.com/networkx/networkx/blob/d70b314b37168f0ea7c5b0d7f9ff61d73232747b/networkx/algorithms/isomorphism/ismags.py#L227) are poorly documented; for example, methods like [`analyze_symmetry`](https://github.com/networkx/networkx/blob/d70b314b37168f0ea7c5b0d7f9ff61d73232747b/networkx/algorithms/isomorphism/ismags.py#L579-L593) are missing a detailed parameter description in the docstring. There is also some question (see #4761) as to whether this particular method was ever intended to be public in the first place.  At the very least, the documentation for `analyze_symmetry` in particular (and some of the other methods) should be improved to explain the parameters in the signature more clearly. 
issue
Add workaround for pytest failures on 3.11b2#TITLE_END#Temporary workaround for pytest failures on 3.11b2. See #5679, pytest-dev/pytest#10008, and the [Python 3.11b2 release notes](https://www.python.org/downloads/release/python-3110b2/) for more info.
issue
`is_tournament` behavior for undirected and multigraphs#TITLE_END#Currently, `is_tournament` is decorated with `not_implemented_for` for the undirected and multigraph cases, so users see the following:  ```python >>> from networkx import tournament >>> G = nx.Graph() >>> tournament.is_tournament(G) Traceback (most recent call last)    ... NetworkXNotImplemented: not implemented for undirected type >>> MG = nx.MultiGraph() >>> tournament.is_tournament(MG) Traceback (most recent call last)    ... NetworkXNotImplemented: not implemented for multigraph type ```  Based on the docstring and module description, I would've expected `is_tournament` to return `False` for these cases rather than throw a `NotImplemented` exception. I would propose to modify the behavior as such, so the above examples become:  ```python >>> from networkx import tournament >>> G = nx.Graph() >>> tournament.is_tournament(G) False >>> MG = nx.MultiGraph() >>> tournament.is_tournament(MG) False ```  Thoughts on the proposed behavior change?  This would be a breaking change, but I would argue that this is closer to a defect in semantics and thus would just need a release note and *not* require a `FutureWarning`.
issue
Review return types of matrix objects in linalg#TITLE_END#The return types for various functions in linalg submodules are not consistent. For example, in the `laplacianmatrix` submodule, `laplacian_matrix` and `normalized_laplacian_matrix` return `scipy.sparse` matrices, whereas `directed_laplacian_matrix` and `directed_combinatorial_laplacian_matrix` return NumPy (dense) `matrix` objects.  I'm not sure if this is intentional or an implementation detail. If the latter, it's worth considering whether to make the return types of the various functions consistent with one another.
issue
Deprecate `adjacency_matrix` alias of `to_scipy_sparse_matrix`?#TITLE_END#The [adjacency_matrix](https://github.com/networkx/networkx/blob/2e61dacc1ffcdcf44edb5fd68dca5f51e09db219/networkx/linalg/graphmatrix.py#L99-L157) function in the `linalg` package is an alias for `to_scipy_sparse_matrix`. Could it be a candidate for deprecation? Or maybe it should be removed from the main namespace and only available in the `linalg` namespace, where it makes more sense that adjacency is represented by a sparse matrix (instead of the nested-dict structure that adjacency often refers to).
issue
Rm unnecessary input validation from moral_graph.#TITLE_END#Just something I noticed when looking at #5633 ...  I don't think this validation is necessary - in fact, I don't think it's possible to hit this branch since the `not_implemented_for` will catch any non-graph inputs first and raise an `AttributeError`.
issue
Question about `maximal_matching` vs. `min_edge_dominating_set`#TITLE_END#I recently noticed that the `min_edge_dominating_set` function simply calls `maximal_matching` (after checking that the graph is not empty). I was curious about the distinction between the two names/concepts and whether there should be two separately named functions, or if one should be preferred over the other.
issue
Replace np.flip with indexing in layouts.#TITLE_END#Replaces the usage of `np.flip` with the equivalent indexing operations in the various layout functions.  In principle indexing is much faster than `flip` as there is no overhead for indexing:  ```python In [1]: a = np.random.random((10, 2))  In [2]: %timeit np.flip(a, 1) 1.62 µs ± 8.84 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)  In [3]: %timeit a[:, ::-1] 169 ns ± 3.37 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each) ```  In practice this won't have any much positive impact on the performance of the layout functions as the coordinate swapping is unlikely to be a performance bottleneck. The main motivation for the change is more pedagogical - users are encourage to use indexing rather than the `flip` helpers when writing numpy code, so I propose to follow that practice in NetworkX! 
issue
Remove `_mat_spect_approx` in favor of simpler procedure#TITLE_END#In the `spectral_graph_forge` module there is an internal helper function called `_mat_spect_approx` which is used in the `spectral_graph_forge` algorithm. AFAICT, the purpose of the helper is to compute a low-rank approximation of a matrix from the eigenvectors that correspond to the `n` largest eigenvalues of the matrix.  The procedure seems straightforward enough, but I found the implementation in `_mat_spect_approx` to be quite confusing; especially since there are a lot of unused parameters in the function. There are several keyword arguments in the helper function that are never changed from their default value when the function is used, which means that the function itself has a bunch of extra logic and branches that are never hit. In addition, the interpretation of some of the parameters is counterintuitive. For example, there is a line similar to:  ```python if not reversed:     k = np.flip(k) ```  ... which is reverses the order of the array! Only after carefully reading the rest of the function does it become clear that `reverse` is meant in the context of whether the eigenvalues should be sorted in ascending or descending order.  Since none of this logic is used, I propose to eliminate the internal helper function entirely and replace it with the simplest implementation as it's currently run. I think this will make the code slightly easier to understand. 
issue
Fix for MyPy 0.95#TITLE_END#MyPy just released a new version a few hours ago which was causing CI failures.  The problem is related to using `sum` to concatenate functions in one of the test files. I fixed this by switching to sets instead of concatenating lists, which I think is a better fit anyways.
issue
Fix doc build error in planarity#TITLE_END#A quick followup to #5523 to fix the broken doc build.
issue
PlanarEmbedding in autosummary instead of autoclass.#TITLE_END#A minor organizational proposal for the `planarity` docs:  The `PlanarEmbedding` class is currently included in the ref guide with the `.. autoclass` directive, which dumps the entire docstring into the planarity summary page. If we instead just include it in the `.. autosummary`, it will be listed/linked to in the same manner as `check_planarity`. The full `PlanarEmbedding` class docstring will still be autogenerated & accessible, but it is linked to rather than embedded in the main planarity page. The methods/attributes of `PlanarEmbedding` are *also* summarized instead of embedded which should further improve the digestibility of the page.  This may also help with #5090 .
issue
Regression: Delayed exceptions for Generators decorated with `not_implemented_for`#TITLE_END#This was originally identified in #5486 - splitting out into a separate issue so we can focus on this specifically.  There has been a behavior change in `not_implemented_for` after NX 2.5. Previously, the `not_implemented_for` would raise exceptions "eagerly" - i.e. behave as if the input checking occurred before any of the body of the decorated function was run. After 2.5, `not_implemented_for` switched to raising "lazily", at least for Generators. In other words, the exception is not raised when the generator is *created*, only when it is iterated over.  ## Minimal reproducing example  ```python >>> import networkx as nx >>> from networkx.utils import not_implemented_for >>> @not_implemented_for("multigraph") ... def my_gen(G): ...     yield 1 >>> G = nx.MultiGraph() ```  #### In NX 2.5:  ```python >>> mg = my_gen(G)  # Error at generator creation time Traceback (most recent call last)    ... NetworkXNotImplemented: not implemented for multigraph type ```  #### NX > 2.5  ```python >>> mg = my_gen(G)  # Generator created without exception >>> mg <generator object my_gen at 0x7f3425964580> >>> list(mg)  # Exception only raised when Generator values accessed Traceback (most recent call last)    ... NetworkXNotImplemented: not implemented for multigraph type ```  I think the change is related to switching from the `decorator` to `argmap` in 2.6. It could be that this behavior is actually intentional, in which case this issue can be ignored! However I think there are some cases (see e.g. the use-case in #5486) where it is useful to raise the `NetworkXNotImplemented` exception at generator creation-time rather than access-time.
issue
Use double-sided arrows by default for `arrows=True` w/ undirected graphs#TITLE_END#Follow-up to #5514 to modify the default arrowstyle when `arrows=True` and `G` is undirected. The current behavior is arguably incorrect because `arrows=True` results in edges without arrows. The following example illustrates the proposed change:  ```python >>> G = nx.Graph([(0, 1)]) >>> pos = {n: (n, n) for n in G.nodes} >>> nx.draw_networkx_edges(G, pos, arrows=True) ```  #### Current result  ![current](https://user-images.githubusercontent.com/1268991/163593184-b8374aeb-3a01-49c0-9cc5-2a540cabd98e.png)  #### Proposed change  ![double_sided](https://user-images.githubusercontent.com/1268991/163593103-aa533cf9-0c2a-4b87-86c4-9e7692cbafae.png)  #### Further details  This problem comes about because the `arrows` kwarg serves double-duty: it is used to specify whether or not to draw arrows, but is *also* used as a switch to determine whether edges are drawn with `LineCollection` or `FancyArrowPatch` objects. This is largely an implementation detail, but an important one because there is a big difference in performance between the two. It is possible to recover the current behavior with `nx.draw_networkx_edges(G, pos, arrows=True, arrowstyle="-")`, though there is almost no reason to do this[^1] as the result will look exactly the same as the `arrows=False` (default for undirected) case, but the drawing will be much slower.  [^1]: One reason would be if the user wanted to modify individual edges *after* they've been drawn.
issue
Add notes about NumPy/SciPy integration to NX 2->3 migration guide#TITLE_END#I've added a list along with some paragraph-length explanations of the numpy/scipy integration work we've been doing. The goal is to give a quick summary of the changes related to numpy/scipy and explain what users can expect for NX 3.0 on this front.
issue
Rm incorrect reference from spiral_layout docstring.#TITLE_END#Just a little cleanup to ensure the incorrect reference doesn't make it into the release.  Figuring out an appropriate reference can be done in follow up. 
issue
Improve bethe_hessian_matrix#TITLE_END#There are a couple of opportunities to improve `nx.bethe_hessian_matrix`:   - [ ] The return type in the docstring is incorrect  - [ ] The docstring needs (at least) one relevant example  - [ ] At least one of the docstring ref links is improperly formatted  - [ ] The initialization of `r` could be improved (i.e. only construct a container from the degree generator once).
issue
`spiral_layout` needs more documentation/context#TITLE_END#It is difficult to determine how the `spiral_layout` is intended to look. The docstring is relatively sparse and doesn't include a functional example (the example simply calls `nx.spiral_layout` without visualizing or validating the output). Other ways this could be improved might be:  - Include a gallery example  - Improve the test suite so that there are some validated checks against expected output  - Add references to the docstring to the origin of this layout (paper, similar layout in other graph visualization libraries, etc.)
issue
Deprecate or improve `utils.misc.to_tuple`#TITLE_END#`to_tuple` is a simple function that recursively converts nested lists to tuples. It is only used in one place within networkx in the `json_graph` module:  https://github.com/networkx/networkx/blob/2e490a307b97518c98066c058136981dcae95606/networkx/readwrite/json_graph/node_link.py#L167  The usage here makes sense as there's a potential use-case for a node that is stored in json as a nested list (though this case isn't tested). However, it should be noted that this specific usage depends on the fact that `to_tuple` is a no-op for every input *except* Python lists.  Given the fact that it is only used once internally, has a relatively specific use-case, and doesn't seem to have any other obvious connection to common operations in NetworkX, I would propose to do the following:  - Move the definition over to the `json_graph.node_link` module as a private function, and  - Deprecate the public function `nx.utils.misc.to_tuple`  If deprecating/removing the function seems too strong, then another option would be to improve the documentation and testing. Documentation should have usage examples to illustrate where it might be useful and also highlight the fact that the function is a no-op for every input except lists.  At face value I'd vote for deprecation because I don't see a strong connection to the rest of the library, but please share your opinion and any other ideas for how to handle this!
issue
CI: `black` is currently broken#TITLE_END#Just an FYI - discussed in detail at psf/black#2964.  The recommended short-term solution is to pin `click <=8.1.0`. My vote is to wait a bit and see if `black` doesn't release a patch in the very near future.
issue
Deprecate `to_tuple`#TITLE_END#Closes #5426  Moves `to_tuple` to a private, internal function in `json_graph.nodelink` and deprecates the public-facing function for removal in 3.0. See #5426 for discussion.
issue
Parametrize tutte polynomial tests#TITLE_END#Just a quick followup to #5265   This PR parametrizes the tests suite, which saves a few lines of code and in principle makes it easier to add new test cases if desired (just add a new `graph: polynomial` to the `_test_graphs` mapping).
issue
Deprecate dict to numpy helpers#TITLE_END#I think this one is relatively uncontroversial, so I didn't bother opening an issue first...  There are three functions in `utils.misc` for converting nested dicts to numpy arrays. Note that these are *not* for Graph objects themselves, but for other nested dicts that are produced by networkx functions like eccentricity or shortest_path. There is one main function `dict_to_numpy_array` which handles both dicts and dict-of-dicts by calling `dict_to_numpy_array1` and `dict_to_numpy_array2` respectively. It's pretty clear form the code that `dict_to_numpy_array` is intended to be the public function while the other two are dimension-specific implementations.  This PR adds a deprecation warning to the public helpers and moves them to private functions. 
issue
Minor improvements from general code readthrough#TITLE_END#I was recently stuck on a plane and passed the time by reading through the networkx source. Here are a collection of (hopefully uncontroversial) improvements that I collected while reading. 
issue
Rm unnecessary `_inherit_doc` decorator#TITLE_END#There was a helper-decorator in `networkx.utils.heaps` that would ensure that the methods of a subclass would inherit the docstrings of the parent method by default. This explicit docstring-forwarding is no longer necessary as this is the default behavior for interactive documentation (e.g. `help()`, IPython's `?`, etc.) as of Python 3.5 due to changes in [`inspect.getdoc`](https://docs.python.org/3/library/inspect.html#inspect.getdoc).  See the [second-to-last bullet in the release notes for Python3.5](https://docs.python.org/3/whatsnew/3.5.html#changes-in-the-python-api), which states:  > The [inspect.getdoc()](https://docs.python.org/3/library/inspect.html#inspect.getdoc) function now returns documentation strings inherited from base classes. Documentation strings no longer need to be duplicated if the inherited documentation is appropriate. To suppress an inherited string, an empty string must be specified (or the documentation may be filled in). This change affects the output of the [pydoc](https://docs.python.org/3/library/pydoc.html#module-pydoc) module and the [help()](https://docs.python.org/3/library/functions.html#help) function. (Contributed by Serhiy Storchaka in [bpo-15582](https://bugs.python.org/issue15582).)
issue
Add NXEP4 to developer toctree and fix broken links#TITLE_END#I forgot to add NXEP4 to the nxeps toctree so it wasn't showing up in the list. Also fixed up a couple intersphinx/external links that were warning during the doc build.
issue
Update sparse6 urls to use https#TITLE_END#[ci skip]
issue
Deprecate extrema bounding#TITLE_END#Closes #5415  Marks `extrema_bounding` for deprecation in favor of the `usebounds` kwarg in the related distance functions. In other words, users should use `nx.diameter(G, usebounds=True)` instead of `nx.extrema_bounding(G, compute="diameter")`.
issue
Is `extrema_bounding` intended to be a public function?#TITLE_END#I mentioned this in #5402 but I think the point deserves it's own issue:  > Looking at extrema_bounding more carefully, I wonder if it was ever intended to be called directly? It seems like it's mostly used internally for the supported measures mentioned in the docstring and extrema_bounding itself is untested!  There are currently two ways of computing the bounds for the various distance measures. Take the `diameter` metric for example:  1. Use `extrema_bounding` directly: `nx.extrema_bounding(G, compute="diameter")`  2. Call `diameter` with the `usebounds` kwarg: `nx.diameter(G, usebounds=True)`  The latter simply calls `extrema_bounding` under-the-hood:  https://github.com/networkx/networkx/blob/6a0b4faf09ec9d3d40ad93e2ec9b431d6bab5dc4/networkx/algorithms/distance_measures.py#L296-L297  To me, the API for option `2.` is preferable to `1.`. This, along with the fact that `extrema_bounding` is neither tested nor has an `Examples` section in the docstring made me question whether it was ever intended to be publicly exposed (as `nx.extrema_bounding`) or only intended for internal use in `distance_measures`.  I was just curious whether there are explicit reasons for this and/or any other opinions. At first glance I would be tempted to make `extrema_bounding` private (deprecate from the main nx namespace) and stick with the `usebounds` approach. Either way, `extrema_bounding` should be tested (and intended usage examples would be great if it stays public) and the `usebounds` kwarg needs to be documented (unless it's decided to remove that instead...)
issue
Finish up NXEP 4 first draft#TITLE_END#Rounds out the last few TODO's in NXEP 4:  - Adds a section linking to related discussions in `scikit-learn` regarding rng policy  - Adds a very basic implementation to help frame the discussion  - Adds discussion about alternative implementation (a package-level switch to toggle the rng backend for all random functions) that was brought up in a  previous discussion
issue
Updates to greedy_modularity_communities docs#TITLE_END#Followup to #5227  Adds note to developer docs about the parameter deprecation. This should also include a release note, but that will depend on which release this is included in. If #5227 is to be included in the 2.7.2 release, then a deprecation note should be added to the 2.7.2 release note. To be discussed...
issue
NXEP 4: Updating the default numpy random number generator in NetworkX#TITLE_END#A proposal to switch to the `numpy.random.Generator` interface for the cases where users pass in `None`, `numpy.random` or integer values to the `seed` parameter of functions decorated with `np_random_state`.
issue
Add Generator support to create_py_random_state.#TITLE_END#Follow up to #5336.  In adding support for `numpy.random.Generator`, I somehow forgot to also modify the `create_py_random_state` function which is called by the `@py_random_state` decorator.  This was a major oversight on my part since the majority of functions that use randomness are decorated with `py_random_state`, so without this most functions in NX won't work with `numpy.random.Generator`.  For this reason, I think it'd be good to get it into a 2.7 release if possible.
issue
Problem with spiral layout with `equidistant=True`#TITLE_END#`nx.spiral_layout` has an `equidistant` kwarg that is False by default. According to the docstring parameter description, this is supposed to enforce that the laid out nodes are all equidistant from one another when True. However, the iterative implementation is such that the node in the first iteration is handled differently than all the rest, resulting in the following behavior:  ### Current Behavior  ```python >>> G = nx.path_graph(5) >>> nx.draw(G, pos=nx.spiral_layout(G, equidistant=True)) ```  ![fi](https://user-images.githubusercontent.com/1268991/144682663-1655099a-b524-4cfc-a101-68f2ef97865a.png)  ### Expected Behavior  The "first" node should (presumably) also be equidistant from it's neighbor  ### Environment Python version: Python 3.9.7 NetworkX version: 2.7rc1.dev0 (766becc1)  ### Additional context  `spiral_layout` was added in #3534
issue
Update `convert_matrix` module docstring#TITLE_END#Just something I noticed while working on the `to/from` numpy functions in this module.  The module docstring contains an example of creating a random DiGraph with `np.random.randint` + `from_numpy_array`. I updated the example to use `rng.integers` instead. I also made a few other minor changes:  - Making the reference to `to_networkx_graph` a link  - Demoing both `from_numpy_array` and the `DiGraph` constructor instead of `to_networkx_graph`  - Referencing scipy.sparse arrays instead of matrices.
issue
First pass at 2.7 release notes.#TITLE_END#Just a listing of notes and thoughts from having reviewed all the merged PRs since the 2.6 release.
issue
Deprecate `to_numpy_recarray`#TITLE_END#Depends on #5324 so marking as draft for now.  Alternatively - `to_numpy_recarray` could still be deprecated *even if* the structured dtype support is not added to `to_numpy_array`, though then there would be a loss of functionality (and the warning messages here would have to change). 
issue
Add structured dtypes to `to_numpy_array`#TITLE_END##5250 added generic dtype support, which means arrays can now be created with structured dtypes, opening the path for creating multi-attribute adjacency matrices, i.e. what `to_numpy_recarray` does. This PR shows how this feature could be implemented within `to_numpy_array`. It takes the same basic approach as `to_numpy_recarray`, where the attribute names are taken from the names of the dtype fields. This fully replicates the behavior of `to_numpy_recarray` and would allow it to be removed, which I think is worthwhile since recarrays are not really a first-class citizen in NumPy, just a convenience layer for accessing structured arrays.  The thing that I don't like about this approach is that it makes `to_numpy_array` less readable. I don't think there's a good way to loop through multiple attributes that wouldn't cause a performance hit for the single-attribute (e.g. `weight="weight"`) case, so it becomes necessary to have a special branch for the multiattribute processing.  Marking this as draft as it's intended as a discussion point, but still needs work re: API and way more testing.
issue
Add FutureWarning to `attr_matrix` to notify users of return type change#TITLE_END#The `attr_matrix` function currently returns a `numpy.matrix`. One of the goals for NetworkX 3.0 is to remove uses of `numpy.matrix` in favor of array semantics, so this PR adds a `FutureWarning` that the return type will change from `np.matrix` to `np.ndarray` in NetworkX 3.0.  Another option would be to return a `scipy.sparse` array object (and then deprecate the `attr_sparse_matrix` function - see #5195 ). Either way, I think we definitely want to remove the usage of `np.matrix`.
issue
More numpy.matrix cleanups for NX2.7#TITLE_END#I did a final check to make sure we had covered all instances of `numpy.matrix` in the codebase. From extensive grepping, there were a few left - mostly in docstrings, comments, and exception messages.  This PR corrects a few docstrings and removes as many explicit references to `numpy.matrix` objects as possible.
issue
sampling from dict_keys objects is deprecated.#TITLE_END#This warning is raised during execution of the full test suite (including slow tests) - that's why it was missed during the initial round of cleanups.
issue
Add support for `numpy.random.Generator`#TITLE_END#NumPy introduced a [new interface for generating random numbers](https://numpy.org/devdocs/reference/random/index.html) back in [version 1.17](https://numpy.org/devdocs/reference/random/index.html). The random number generator machinery in NetworkX does not yet support the newer `numpy.random.Generator` interface.  This PR adds support for `numpy.random.Generator` to the random number machinery, primarily in `networkx.utils.misc` and `networkx.utils.decorators`. Now the following will work:  ```python >>> import numpy as np >>> from networkx.utils import np_random_state >>> my_rng = np.random.Generator(np.random.Philox(12345)) >>> @np_random_state(1) ... def foo(num, seed=None): ...     return [seed.random() for _ in range(num)] >>> foo(10, seed=my_rng) [0.42075435954078155,  0.6531709678504624,  0.4331635821770152,  0.538923263838466,  0.7038664083032369,  0.2048422212324087,  0.002752438882253183,  0.29876369406414405,  0.3651050776915541,  0.8820466660293489] ``` Whereas before this would give: `ValueError: Generator(Philox) cannot be used to generate a numpy.random.RandomState instance`.  This PR is limited to adding support for `numpy.random.Generator` - it does not change any defaults and is intended to be completely backward compatible.  For additional context, see #4891. This PR addresses step 1 from [this comment](https://github.com/networkx/networkx/issues/4891#issuecomment-860722726). 
issue
Potential resolution to full paths to functions in docs#TITLE_END#Related to #4855   The pydata-sphinx-theme appears to construct the sidebars elements in the docs directly from the html toctree elements. So, one way to address the "full paths in the sidebar" problem would be to modify the templates that the toctrees are constructed from for the files generated by autosummary.  This PR illustrates this approach by overriding the default autosummary template to use the specific `objname` var instead of the `fullname`. This "fixes" the sidebar problem, but also has other effects. For example, the titles of the autogenerated pages no longer include the full path, just the function/class name. Whether or not this is an improvement is in the eye of the beholder.  This is a pretty invasive solution to the problem that may have more subtle effects, since it involves changing the default template for all autogenerated docs. It would be nice to have a more specific solution that only effected the sidebar elements, but as of now I don't think the theme supports this - I'll open an issue there to see if there is a better solution. In the meantime, any feedback on the results here would be welcome!
issue
Add informative exception for drawing multiedge labels.#TITLE_END#Closes #5294   Improve the exception message when a user attempts to draw edge labels with multiedges that include keys (i.e. edges as 3-tuples).
issue
MAINT: TODO's for 3.10 support#TITLE_END#Follow-up for #4807  - [x] unpin the `python_version` on scipy, matplotlib, and pandas once we have wheels for these projects on all three platforms. - [x] Update Python version for testing matrix in `nx-guides` workflows.
issue
Update `draw_<layout>` docstrings with usage examples#TITLE_END#This PR adds to the existing `draw_<layout>` convenience functions to emphasize their intended use-case and highlight using the layout functions explicitly for more involved usages. I've done this primarily by adding a short line to the extended summary explaining what the convenience function wraps, as well as a `Notes` and `See Also` section that highlight other use-cases.  This also closes #5215 by updating the parameters and simplifying the argument parsing. Technically this is a change to the signature, but I don't think it needs any sort of warning as the behavior is equivalent.  This addresses most of https://github.com/networkx/networkx/discussions/5220#discussioncomment-1961800, though there is still the matter of adding convenience functions for the missing layouts, which I propose to do in a separate PR.
issue
Deprecate `make_small_graph` and `make_small_undirected_graph`#TITLE_END#Closes #5266.  This PR depends on #5267, so marking as draft until that's in.
issue
Deprecate `make_small_graph` and `make_small_undirected_graph` from `small` submodule?#TITLE_END#These two functions are defined in the `generators.small` submodule, and are used internally within the module to generate many of the graphs therein. `make_small_graph` is publicly exposed in the namespace, while `make_small_undirected_graph` is not.  Looking at the implementation however, it seems that these functions insert an unnecessary layer of indirection (i.e. a `graph_description` argument that is actually a custom structured list that must be unpacked/parsed)  and complexity (see the node remapping) on top of functionality that is already provided by e.g. `nx.from_edgelist` and `nx.from_dict_of_lists`. AFAICT, `make_small_graph` doesn't add anything that can't be achieved in a more straightforward manner with `from_edgelist` and `from_dict_of_lists` - would there be appetite for it (and `make_small_undirected_graph`) and refactoring the `small` module to use more "canonical" NetworkX graph creation functions?
issue
Remove unused import#TITLE_END#This is leftover from the MyPy cleanup - there is a stray import of `AbstractSet` from the typing module that is no longer used.  Note that there is also another instance where objects from the typing module are used in `networkx/readwrite/gml` to define a `NamedTuple`. I think this syntax is more clear than the `namedtuple` constructor, so I'm inclined to leave it.
issue
Document default dtype in to_numpy_recarray docstring.#TITLE_END#Closes #5309
issue
Refactor `to_numpy_array` with advanced indexing#TITLE_END#A different approach to constructing the adjacency matrix with numpy. Currently, `to_numpy_array` creates an internal array with float type with `np.nan` as a hard-coded sentinel value, that is then modified/replaced in a for-loop over the edges in the graph. As noted in #5245, this causes problems for numerical weights that are not safely castable to floats, such as complex numbers.  The approach in this PR is based on [@dschult's idea](https://github.com/networkx/networkx/pull/5245#issuecomment-1008215532) to use the dtype and user-specified sentinel directly in the array construction. This would avoid the hard-coded sentinel issue and would allow users to have e.g. complex weights by specifying so in the `dtype` argument. In fact, this approach should give the users access to the full range of numpy dtypes, including allowing arbitrary weights via `dtype=object`.  Another advantage of this approach is that it would allow arbitrary reduction functions to be used for reducing multiedge weights down to a single value. The current implementation is limited both by the fact that a) functions must ignore nans and b) the iterative approach, where intermediate values are stored in the array for each key, making reductions that depend on the entire set of weights for a single edge (e.g. mean or median) not feasible. This approach here removes this implementation, albeit at the cost of an extra intermediary data structure that remaps multiedge weights into a single container per edge.  There are still some things I'd like to do (add a fast path, fuzz the parameter types and add more tests), but I figured I'd put this out here now for feedback before I go too much further down this path. LMK what you think @dschult @adeak
issue
Deprecate scipy sparse matrix conversion functions#TITLE_END#The scipy.sparse array alternatives that supersede `to_scipy_sparse_matrix` and `from_scipy_sparse_matrix` are added in #5139. This is a follow-on PR to deprecate `to/from_scipy_sparse_matrix`.  Note this depends on #5139, so the history & diffs are a mess - marking as draft for now; I will clean up the history after #5139 is in.
issue
Write_graphml fails on contracted graphs#TITLE_END#### Discussed in https://github.com/networkx/networkx/discussions/5291  <div type='discussions-op-text'>  <sup>Originally posted by **taylorn3** January 31, 2022</sup> I'm unsure if this is a known bug, so I'll write it here first.  I'm trying to export a graph with write_graphml that is the output of a contraction, and it throws an error.  A minimal working example: ``` import networkx as nx G=nx.Graph() G.add_edge("A","B") G=nx.contracted_edge(G,["A","B"]) nx.write_graphml(G,"test.graphml") ``` The error thrown is  ``` ~/.local/lib/python3.9/site-packages/networkx/readwrite/graphml.py in write_graphml_lxml(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)     169         )     170  --> 171     writer = GraphMLWriterLxml(     172         path,     173         graph=G,  ~/.local/lib/python3.9/site-packages/networkx/readwrite/graphml.py in __init__(self, path, graph, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)     727      728         if graph is not None: --> 729             self.add_graph_element(graph)     730      731     def add_graph_element(self, G):  ~/.local/lib/python3.9/site-packages/networkx/readwrite/graphml.py in add_graph_element(self, G)     768         for node, d in G.nodes(data=True):     769             for k, v in d.items(): --> 770                 T = self.xml_type[self.attr_type(k, "node", v)]     771                 self.get_key(str(k), T, "node", node_default.get(k))     772         # Edges and data  KeyError: <class 'dict'> ```  It appears that once I contract an edge, it adds 'contraction: {B: {}}' as an attribute of A, and the writer doesn't know how to handle dictionary attributes.  Is this a known bug?  If not, is it actually a feature (tongue in cheek), or should I start a new issue?  I'm new to GitHub, so I'm not completely clear on the proper procedure.</div>
issue
Use from_dict_of_lists instead of make_small_graph in generators.small#TITLE_END#Related to the question in #5266 - there are many internal uses of `make_small_graph` to generate some of the named graphs in the `generators.small` module. In these use-cases, `make_small_graph` is used to generate graphs from a list-of-list adjacency representation. This PR refactors the functions to use `nx.from_dict_of_lists`, which is a more generic function for constructing graphs from adjacency info.  The `create_using` behavior is not currently tested for the `small` graphs (except `tutte_graph`). I added a regression test to verify that the graph generators that don't currently directed outputs continue to raise an exception when `create_using` is some form of directed graph. To cut down on the amount of boilerplate, I implemented this check in a decorator; if this adds too much complexity I'm happy to do something simpler, especially since these graph generators are relatively cheap to begin with.
issue
Make small graph generator node test more specific.#TITLE_END#Increases the specificity of the tests for the `small` graph generators (`bull_graph`, `chvatal_graph`, etc.). Instead of testing that the number_of_nodes is correct, check that the sorted sequence of nodes is as expected (i.e. integer nodes starting from 0).
issue
Use scipy.sparse array datastructure#TITLE_END#This is a currently in-progress experiment to see what it would take to adopt the sparse **array** interface proposed in scipy/scipy#14822. Supporting array semantics for sparse data structures will in the long run be a huge improvement for NetworkX IMO. Both dense arrays (i.e. numpy arrays) and sparse arrays are used extensively throughout, and a decent amount of code is dedicated to implementing something for one or the other data structure. The ability to have these data structures behave the same and use them interchangeable would be excellent! Of course, scipy/scipy#14822 doesn't get all the way there yet, but it is a step in the right direction. This PR is to help flesh out the work there and to help start to frame the type of improvements we'd see in NetworkX if such a feature were to be added upstream.  Note also that this is very much a work in progress - the scipy.sparse array API itself is going to be changing a lot, so a lot of the changes here (for example, wrapping objects created with `spdiags` in `sparse.csr_array` calls) is subject to change.
issue
Graph generators in the `small` module need docstrings#TITLE_END#There are a number of named graphs defined in `networkx.generators.small` that are not well-documented:  https://github.com/networkx/networkx/blob/766becc132eda01c9d0d458de2d75ab763f83493/networkx/generators/small.py#L183-L575  Each of these functions should have a more extensive docstring with at least a `Parameters`, `Returns`, and `Examples` section, and preferably also a reference to the corresponding paper or wiki page that describes the graph.
issue
Add an FAQ about assigning issues.#TITLE_END#Adds a Q&A to the new contributor FAQ clarifying that issues are not typically assigned to users.
issue
Touchups and suggestions for subgraph gallery example#TITLE_END#Some follow-up suggestions for #5165 . My main goal was to try to shorten the example and reduce any redundancy so that the underlying example of working with subgraphs shines through. Also a couple minor touchups to use more idiomatic patterns e.g. with node attribute access.  - d02ed5a : use set operations to simplify the construction of a graph containing a subset of edges from two existing graphs. Saves several lines of code and IMO improves clarity. - a7748e6 : removes copying over of the `.graph` attribute, since it is unused in this example. - 29a725e : It turns out these containers are not indices, but the nodes themselves, so I've renamed the variables. In addition, since they are used in membership checks, I've switched from using lists to sets. Finally, I thought it'd be better to use set comprehension to construct the containers, as I find this more clear (even if it's less efficient - see inline comment). - 329c044 : Minor modifications to creating containers from node attributes to improve readability. Also fixes a bug in the node coloring for the removed edges. - f54249 : Simplify isomorphism check - the RST header makes the text in the print statement redundant IMO - b87d537 : Rm the unit test, which is for all intents and purposes a duplication of the example itself. It's a nice reformulation of the example as a test, but is redundant and a bit out of place in the gallery IMO - b962f05 : Rm loop that plots all of the subgraphs when the `graph_partitioning` function is run. This is already done explicitly in the `Plot the results: every subgraph in the list` section in the example.  If these changes go in, I think there's also an opportunity to re-organize the example to make the flow more clear (i.e. move introduction of the example graph above the definition of the analysis function, to better match the order of the visualization in the output). However I'd like to leave any re-organization ideas to a separate PR.
issue
Fix Python 3.10 deprecation warning#TITLE_END#Running the full test suite (with all dependencies) w/ Python 3.10 resulted in a new DeprecationWarning from `randrange`, which will require integer input in Python 3.12:  ``` networkx/generators/tests/test_random_graphs.py: 4830 warnings   repos/networkx/networkx/generators/random_graphs.py:223: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version     if seed.randrange(mmax - t) < m - k: ```  This PR fixes the warning by switching to integer division for computing `mmax`.
issue
Fix a few broken links in the html docs#TITLE_END#Chipping away at the link warnings when running sphinx in nit-picky mode.  For reference: you can have sphinx report broken links when building like so: `make SPHINXOPTS=-n html`. You should also comment out the `default_role = obj` from `conf.py` (there is a strange interplay between the rtd theme, numpydoc, and sphinx here... not quite sure what's the best way to resolve it so skipping over the details for now). Doing so, I get about 540 broken links (on de9c66ff). This PR should reduce that number by 8.
issue
Remove instances of random.sample from sets (deprecated in Python 3.9)#TITLE_END#Closes #4555  I took a conservative approach and used `sorted` to convert sets to lists. This may cause a performance hit, but I figured this was safer from the perspective of reproducibility, though I'm not sure it's necessary. In some circumstances it may be sufficient to simply convert to `list` directly.
issue
Deprecate Ordered graph classes#TITLE_END#Closes #4614   Deprecate the `Ordered` variants of the four graph classes. Adds deprecation notices in the docs to the `networkx.class.ordered` module, as well as the class docstrings. Adds an `__init__` method to each `Ordered` class which is responsible for raising a `DeprecationWarning`, then delegates to the parent constructor.
issue
CI: persist pip cache between circleci runs#TITLE_END#Currently, the pip install step of the docs build job on circleci takes ~2min each run, largely due to some wheels being built locally. This time-consuming step could be significantly improved by persisting the pip cache between CI runs. The approach taken here is to persist the cache between circleCI runs, and add an `--upgrade` flag to all of the dependency installs. This ensures that the newest versions of libraries are always grabbed and (hopefully) eliminates the need to manually manage the cache.
issue
CI: Fix cartopy build failure in docs workflow#TITLE_END#Attempt to temporarily fix the cartopy wheel build failure in circleCI.
issue
Remove assertion test method in favor of assert statements#TITLE_END#Minor cleanup: removes two unnecessary lines and converts test suite assertions to the preferred style (i.e. assert statements rather than using `assert_` functions).  This would also make it easier to refactor these tests using a non-OO design in the future.
issue
Replace internal `close` fn with `math.isclose`.#TITLE_END#There is an internal function called `close` defined in `isomorphism/matchhelpers` that appears to duplicate the functionality of the built-in `math.isclose`. I think it would be an improvement to use the built-in instead.  The safest thing to do would probably be to deprecate it (or add a `__getattr__` exception) since it doesn't follow the "private" naming-convention (i.e. starting with an underscore); however, I've simply removed it in this PR since it wasn't in the `__all__` for the module. I'm leaning towards deprecating instead, but wanted to see what others thought for this particular case.  Note also that there is an internal implementation of `allclose` which mirrors the element-wise behavior of `numpy.allclose`. Switching to the `numpy` function would inject a numpy dependency into the isomorphism package, so I decided to leave it in (though I'm tempted to add a `FutureWarning` and make it a private function `allclose` -> `_allclose`).
issue
Remove dictionary from signature of tree_graph and tree_data#TITLE_END#Closes #4783.  Similar to #4284, though the approach here is actually better in that it allows the user to use the new kwargs now to get rid of warnings and be forward compatible. If others agree I will also apply this pattern to the cytoscape functions.
issue
Get rid of invalid escape sequences causing DeprecationWarnings#TITLE_END#When running the test suite, there are new deprecation warnings about invalid escape sequences. This is from LaTeX math in some docstrings, which often feature backslashes, e.g. `\gamma`. The simplest way to get rid of these warnings is to convert the docstrings into raw strings.
issue
Remove pyyaml dependency via module getattr#TITLE_END#An alternative to removing `pyyaml` with a full deprecation cycle. Adds a `__getattr__` to each module/package from which `read_yaml`, `write_yaml`, and `nx_yaml` were accessible. Since the functions were one-liners, raise an informative error message describing how to use `pyyaml` directly to replicate the functionality of the `nx_yaml` module.
issue
Pin upper bound on decorator for 2.6 release.#TITLE_END#Adds upper bound to decorator dependency in preparation for 2.6 release.  Addresses #4732 in the short term for the 2.6 release.
issue
Fix edge drawing performance regression#TITLE_END##4360 switched all edge drawing from LineCollection (in some cases) to always using FancyArrowPatches. This enabled nice new features such as self-loop drawing #4370 and a consistent, iterable return type, which makes it easier for users to modify individual edges (as FancyArrowPatches) after they have been created by `draw_networkx_edges`.  One downside of removing LineCollection is a large performance regression for drawing edges in undirected graphs, see #4806.  This PR tries to get the best of both worlds by re-introducing LineCollection for faster edge drawing, while keeping all the nice features of #4370 including drawing self-loops and the ability to draw curved edges.  The basic approach is:  1. Add back the LineCollection drawing functionality that was removed in #4360.  2. Encapsulate the two different drawing methods in internal functions  3. Investigate how to trigger the different drawing methods based on existing kwargs, e.g. `arrows`, `arrowstyle`, `connectionstyle` etc.
issue
Draft 2.6 release notes#TITLE_END#I went through all of the PRs since the 2.5 release and added links and a brief description to the new features/improvements/api changes that have been merged in preparation for the upcoming release.
issue
Limit number of threads used by OMP in circleci.#TITLE_END#May speed up any long-running examples that rely on numpy/scipy linalg.  See how the sphinx-gallery build goes in circleci, apply to other CI services if it has an effect.
issue
Fix bad import pattern#TITLE_END#**EDIT**: Partially addresses #4426; fixes `from networkx import nx` by adding some missing `__all__`'s to modules.  Original discussion (which attempted to tackle multiple issues at once) below.  ---  Related to #4426 - attempting to fix bad import patterns like `from networkx import nx` or `from networkx import networkx`.  Adds tests for the two patterns mentioned above. Adding the `__all__` dunders to a couple modules where they were missing (found with `grep -L`) fixes the `from networkx import nx` problem, but I haven't been able to crack the second case in this first pass. Suggestions welcome!  If worse comes to worst, one hack-y solution would be to add `del networkx` to `networkx/__init__.py`; though IMO it would be preferable to find where/how `networkx` is being imported into itself and stop that from happening.  I'm marking with the 2.6 milestone for visibility. If we don't manage to figure out how to resolve the second test case, then the three courses of action I could see would be:  1) Comment out the second test and merge as is. This at least fixes the `from networkx import nx` problem for the 2.6 release  2) Add the hacky `del` to also resolve the `from networkx import networkx` issue for the 2.6 release.  3) Bump everything to 3.0  My personal preference (assuming the 2nd case isn't solved before the release candidate) is option 1 - fix part of the problem, leave #4426 open and leave the other problem for a follow-on PR.
issue
Remove "networkx" from top-level networkx namespace#TITLE_END#Followup to #4839. This was originally attempted there, but ultimately ended up seemingly like a more complicated issue.  The goal is to prevent `networkx` from being imported in the top-level `networkx` namespace. In other words, this should not be possible: `from networkx import networkx`. See the discussion in #4839 for original discussion and some ideas for how to probe/fix this problem.
issue
Update arrows default value in draw_networkx.#TITLE_END#Followup to #4825   In #4825 the `arrows` kwarg was switched from being a boolean to an optional boolean with `None` as the default value. This was correctly updated in `draw_networkx_edges`, but I forgot to update the default in `draw_networkx`! This results in all graphs drawn with `nx.draw` or `nx.draw_networkx` being drawn with arrows by default, which was a major oversight.  This PR should go in before the 2.6 release to avoid a behavior change from 2.5.
issue
Update docstrings for dfs and bfs edges and fix cross links#TITLE_END#Some docstring improvements that may further help to address issues like #4899   The main improvements include:  * Fixing cross-references between the various algorithms both in the text and the `See Also` sections.  * Converting `Returns` -> `Yields` for the edge generators  * Adding a `References` for more nicely-formatted links to external references.  I only touched the docstrings directly linked-to from `dfs_edges`. I expect that similar improvements could be made to some of the other dfs/bfs functions. I'll either review them myself when I have more time or open an issue about it if you think these changes are worthwhile!
issue
Deprecate numeric_mixing_matrix.#TITLE_END#Re-submission of #4920 - I had a hard time resolving all the conflicts during rebasing.
issue
Add matrix market to readwrite reference#TITLE_END#Adds an .rst stub to the readwrite docs in the reference guide to provide background and examples for I/O with the Matrix Market format using SciPy's built-in parsers.  Related to the discussion in https://github.com/networkx/networkx/pull/4922#issuecomment-866651631.
issue
Minor updates to tutorial.rst and add docstring for data method of nodes/edges#TITLE_END#A few documentation improvements from another readthrough of the tutorial. There are two major changes here:  1. Improvements (at least IMO) to the tutorial, and  2. Adding docstrings to the `.data` method of NodeView and the various EdgeView classes.  The changes to the tutorial are mostly along the lines of making certain terms linkable and adding/modifying examples.  The new data method docstrings were inspired by the following:  ``` In [1]: import networkx as nx  In [2]: G = nx.Graph()  In [3]: G.nodes.data? Signature: G.nodes.data(data=True, default=None) Docstring: <no docstring> File:      ~/repos/networkx/networkx/classes/reportviews.py Type:      method  In [4]: G.edges.data? Signature: G.edges.data(data=True, default=None, nbunch=None) Docstring: <no docstring> File:      ~/repos/networkx/networkx/classes/reportviews.py Type:      method ```  I toyed with the idea of somehow trying to use the NodeDataView class docstring as the `NodeView.data` method docstring, but in the end I decided it was better to have separate, specific docstrings for each. I'm certainly open to other opinions though if anyone has any creative ideas for how we could synchronize them!  Also since these two changes are unrelated, I'd be happy to split them into separate PRs to make review easier - just LMK!
issue
Mark two atsp tests as slow#TITLE_END##4740 recently included a lot of really nice tests for the aTSP problem. Two of those tests in particular have [quite long runtimes](https://github.com/networkx/networkx/pull/4740#discussion_r693340037). This PR proposes to mark these tests as slow so that they are not run in the test suite by default.
issue
Document `geometric_edges` and add it to main namespace#TITLE_END#Adds `geometric_edges` to top-level `nx` namespace and improves documentation for this function.  Closes #5025
issue
Support `comments=None` in read/parse edgelist#TITLE_END#Closes #4989  Adds support for `comments=None` to allow users to indicate that *no* character should be interpreted as indicating the start of a comment. This is a pretty niche case, but one could imagine a scenario where node names in an edgelist can be composed of a wide range of characters. If a user reading an edgelist wants to guarantee that part of a node string won't erroneously be taken as the start of a comment, they need to select a character to pass into the `comments` kwarg that is *not* used in any of the node names in the data file. Adding support for `comments=None` is a more straightforward solution for this case.
issue
Add references to literal de/stringizer in gml function docstrings#TITLE_END#Closes #4953
issue
Deprecate `random_state` decorator#TITLE_END#This was originally identified by @boothby in #4739:  There are three decorators for setting functions with a reproducible random seed: `py_random_state`, `np_random_state`, and `random_state`. Of the three, [only `random_state` is documented in the ref guide](https://networkx.org/documentation/latest/reference/utils.html#module-networkx.utils.decorators). However, `random_state` is only an alias for `np_random_state`. This PR proposes to deprecate the `random_state` alias in favor of the more explicit `np_random_state` name, and document the two functions.  
issue
Update mentored projects section in docs#TITLE_END#Update the [mentored projects](https://networkx.org/documentation/latest/developer/projects.html) section in the developer docs to reflect the successful completion of several ideas from the latest round of GSoC.  I decided to remove the community detection section entirely since it was all about Louvain, but if we'd prefer to simply update it (e.g. w/ Leiden instead), please revert 4952ee8 and update the wording as you see fit!
issue
More informative GraphML exceptions#TITLE_END#Closes #5024  Modifies how the dict that defines supported GraphML types is accessed so that an informative exception is raised if a user tries to save an attribute of an unsupported type.  This solution is technically backward incompatible, as it changes the (currently public) `xml_types` attr of the `GraphML` class from a dict to a method. The reason for doing it this way is that it provides an easy way to ensure that the exception is raised for all of node, edge, and graph attrs. If this is a problem though, we can always go with the clunkier (but backward compatible) approach which would add try-excepts each time the dict is accessed. LMK what you think!
issue
Add note about checking for path existence to all_simple_paths.#TITLE_END#Closes #4518.  Based on the feedback in the original issue, it seems that an update to the docs is the preferred resolution for now. This can always be revisted/expanded upon later if this proves insufficient.
issue
Parametrize shortest path node-checking tests.#TITLE_END#Just something I noticed while reviewing #5033:  Slightly refactors the tests of the shortest-path functions to parametrize over the function being tested. The main motivation is to make it more clear which test is failing if/when they fail.  Other changes include:  - using `pytest.raises` as a context manager in these tests  - Adding a src==tgt branch for the multisrc tests.
issue
Refactor linestyle test for FancyArrowPatches.#TITLE_END#Follow up to #5131  Reorganizes the testing of the `style` kwarg for `draw_networkx_edges`. There is a lot of discussion already in #5131, but to summarize:  - Removes testing of edges drawn with `LineCollection` objects, since the parsing/cycling in that case is all handled by matplotlib internally  - The above allows the removal of the `test_styles` function for simplification  - Breaks one test up into 3: one for the default value, one for "singleton" style inputs (i.e. a single string or tuple), and one for sequences of inputs. All three are parametrized over the style input.
issue
Update nx_pylab drawing edge color and width tests#TITLE_END#As noted in the discussion of #5131, the current implementation of `test_edge_colors_and_widths` in the `nx_pylab` test suite only checks that calling `nx.draw_networkx_edges` with various argument combinations doesn't cause any exceptions to be raised. There currently is no checking of the resulting `Artists` to ensure that the options being passed in are producing the expected results in the visualizations.  This PR refactors this test into several smaller tests of both the `edge_color` and `width` parameters. In most cases, the resulting matplotlib object that represents the edges (either a `LineCollection` or list of `FancyArrowPatch`) are checked to ensure that the object has the expected properties as set by `draw_networkx_edges`. The major exception is when there is a per-edge color or width specification for undirected edges, since there is no easy way to introspect the properties of individual lines within a `LineCollection`.
issue
Refactor node_classification to improve conciseness and readability#TITLE_END#This was something I noticed while working on #5139: the `node_classification` package was implemented in such a way that there were a lot of simple (one-or-few-liner) expressions that were wrapped up in "helper" functions. In principle this was done because the functionality was reused in both the `harmonic_function` and `local_and_global_consistency` functions, but IMO the code is improved by reducing the indirection introduced by all the utility functions. I also made a few other minor tweaks (while -> for, removing some local vars, use f-strings, etc.) and fixed the docstrings up a bit, but I did not fundamentally change how the functions are implemented, just reorganized them.  IMO it would make sense to refactor the `node_classification` package and convert it to a module, but this would require deprecation warnings so I will leave it to a follow-up PR. I also think there's an opportunity to improve the implementation by using sparse data structures throughout, but would also like to leave that to a follow-up PR since changing the implementation would require more careful review.
issue
Compatibility updates from testing with numpy/scipy/pytest rc's#TITLE_END#A few minor cleanups and removal of usage-patterns that will result in DeprecationWarnings in the upcoming scipy/pytest releases. These were identified by running the test suite with the numpy-1.22rc2, scipy-1.8rc1, and pytest-7.0.0rc1.  - Use the pytest `recwarn` fixture instead of `pytest.warns(None)`, which is deprecated in 7.0  - Access various scipy.sparse.linalg tools from the `scipy.sparse.linalg` namespace, since many of the sparse.linalg subpackages have been made private.  
issue
Deprecate `hmn` and `lgc` modules from the `node_classification` package#TITLE_END#Deprecates the `hmn` and `lgc` modules in the `node_classification` package. These names are not particularly descriptive, and are more of an implementation detail than useful objects or namespaces.  Refactors the `node_classification/__init__.py` with `__getattr__` and `__dir__` so that the same functions are visible/accessible from the package, but a deprecation warning is raised if a user tries to access the `hmn` or `lgc` attributes specifically. Access to the functions they define (`harmonic_function` and `local_and_global_consistency`, respectively) should be exactly the same as before.   This deprecation is in preparation for refactoring `node_classification` from a package to a module for NetworkX 3.0. The functions defined in `lgc.py` and `hmn.py`, along with an internal function in the currently unexposed `utils.py`, will be moved to a `node_classification.py` module, which will replace the current package. I've taken the liberty of combining the `hmn` and `lgc` tests into one top-level `test_node_classification.py` file that lives one level up. I left the tests themselves entirely unchanged.  I've also added tests to ensure that deprecation warnings are raised at the appropriate times.
issue
Rm passing ax.transOffset to LineCollection when drawing undirected edges#TITLE_END#From the discussion in #matplotlib/matplotlib#21517, ax.transOffset would have had no effect prior to matplotlib-3.5.0rc1. Looking at `blame` doesn't indicate any specific reason why the `transOffset` arg was passed to the LineCollection constructor - [this has been the case since `nx_pylab` was introduced](https://github.com/networkx/networkx/blame/3f478bba187e34a3bed6609be1bd9c21486fe875/networkx/drawing/nx_pylab.py#L316).  In principle, removing this should have no effect. I've built the docs locally and compared the NX gallery examples both with and without this change and didn't notice anything visually.  Closes gh-5167
issue
Revert "Add temporary pyparsing pin to fix CI."#TITLE_END#Reverts networkx/networkx#5156  There is now a conflict in `packaging` with the new pyparsing, and they have pinned to pyparsing < 3, which causes problems for our fix. Removing the pin here should mean that we default to the `packaging` pin, which should keep the `pyparsing`/`pydot` problems at bay.  Closes #5157 
issue
Add regression test for ancestors & descedants with undirected graphs#TITLE_END#`nx.ancestors` and `nx.descendants` currently work with undirected inputs, but there are currently no tests for undirected inputs. This PR adds a regression test so that a behavior change w/ undirected inputs would be caught in the test suite - see e.g. #5176 
issue
Remove check/comment for scipy 1.1 behavior.#TITLE_END#Minor cleanup of old code designed to catch a behavior change around scipy 1.1. The minimum required `scipy` version for NetworkX is now well above 1.1, so I think it's safe to remove the extra check/comment.
issue
Fix missing import and tests for directed Laplacian functions#TITLE_END#Calling `nx.directed_laplacian_matrix` currently results in:  ```python >>> import networkx as nx >>> G = nx.gn_graph(10) >>> nx.directed_laplacian_matrix(G) Traceback (most recent call last)    ... AttributeError: module 'scipy.sparse' has no attribute 'linalg' ```  This is because `directed_laplacian_matrix` and `directed_combinatorial_laplacian_matrix` are missing internal imports of the scipy.sparse.linalg subpackage.  Alarmingly, this was not caught by the test suite - likely due to imports persisting within the test class. Breaking the two test methods for the `directed` Laplacians out of the class into test functions is sufficient to catch the missing imports (this is the only change to the tests that I made for this PR).
issue
Remove internal laplacian function in favor of laplacian_matrix.#TITLE_END#The `flow_matrix` module defines a `laplacian_sparse_matrix` helper-function that is used to calculate the Laplacian for the flow_betweenness centrality measures. The `laplacian_sparse_matrix` function duplicates the functionality of `nx.laplacian_matrix`, so I propose removing the former in favor of the latter.  Though not preceded by an underscore, `laplacian_sparse_matrix` is not exposed in any namespaces so I'm operating under the assumption that it is safe to remove it outright. I'm happy to deprecate it instead if others feel extra caution is warranted - LMK what you think!
issue
Improve `resistance_distance` implementation with advanced indexing#TITLE_END#Removes an internal function that was used to remove rows/columns from a Laplacian matrix in favor of a simpler approach based on advanced indexing. Also removes the tests for the internal function.  I think there are other opportunities to improve the function (e.g. see the repeated code below for computing `LdiagA` and `LdiagAB`) but I'll keep the scope of the PR limited to make reviewing easier.
issue
Investigate pre-release test failures#TITLE_END#Closes #5202   So far no one has been able to reproduce the CI failures locally, so opening a draft pull request to investigate if refactoring the failing test has any effect on the problem.  At this point I have no concrete idea as to why this is failing on CI (ubuntu) but not any locally tested platforms (arch, debian). The test does rely on `linalg` so maybe a linalg backend issue? Seems unlikely as this probably would have resulted in much more noisy failures elsewhere.
issue
A few `np.matrix` cleanups#TITLE_END#Removes a couple other instances of `numpy.matrix` being used internally.
issue
Add FutureWarning about matrix->array output to `google_matrix`#TITLE_END#[The `google_matrix` function](https://github.com/networkx/networkx/blob/4be27b6eb32b77ccc0242368e7a5cadaaa77e7af/networkx/algorithms/link_analysis/pagerank_alg.py#L175-L268) currently returns a `np.matrix` object and uses matrix semantics internally. This PR:  1. Refactors the function to use array semantics internally, and wraps the outputs in `np.asmatrix` for compatibility.  2. Adds a `FutureWarning` to warn users that the output type will change from `np.matrix` to `np.ndarray` in NetworkX 3.0  When the `FutureWarning` expires in version 3.0, the `np.asmatrix` wrappers can simply be removed.  Alternatively, one could do away with the FutureWarning and simply change the behavior now, but IMO it's safer to provide a `FutureWarning` given the behavior differences between matrix and array objects.
issue
Add Mypy type checking infrastructure#TITLE_END#The purpose of this PR is to evaluate the impact of adding the static type checker Mypy to the development workflow of NetworkX. This PR **does not add any type annotations**! The goal is to see what impact the static type checker has on the existing code base. Adding this infrastructure is the first step in enabling support for type annotations moving forward. This PR also aims to focus the discussion in #5073 and provide an objective accounting of what sort of things `mypy` catches in the NX codebase and how these issues can be fixed.  ## Mypy errors in the existing code  The first step in the analysis is to ascertain what mypy reports on the existing codebase, which doesn't contain any type annotations (with the exception of one function). This can be done on `main` in an existing development environment as follows:  ```bash pip install mypy pip install -e .  # A development install of networkx mypy --ignore-missing-imports -p networkx ```  Note: the `--ignore-missing-imports` flag is necessary to suppress mypy errors from missing type annotations in dependencies (e.g. scipy). Note also that the `-p networkx` flag is preferable as it only runs the type checking on the installed `p`ackage, ignoring things like `setup.py`. The above incantation will limit the reported errors to those found specifically within the networkx codebase.  On a50c8e6d, this results in 53 mypy errors:  <details>   <summary>Mypy errors (added lineno for convenience)</summary> <pre> <ol> <li>networkx/readwrite/nx_shp.py:330: error: Assignment to variable "e" outside except: block</li> <li>networkx/classes/reportviews.py:1327: error: Incompatible types in assignment (expression has type "Type[OutMultiEdgeDataView]", base class "OutEdgeView" defined the type as "Type[OutEdgeDataView]")</li> <li>networkx/classes/reportviews.py:1379: error: Incompatible types in assignment (expression has type "Type[MultiEdgeDataView]", base class "OutEdgeView" defined the type as "Type[OutEdgeDataView]")</li> <li>networkx/classes/reportviews.py:1405: error: Incompatible types in assignment (expression has type "Type[InMultiEdgeDataView]", base class "OutEdgeView" defined the type as "Type[OutEdgeDataView]")</li> <li>networkx/utils/decorators.py:95: error: Need type annotation for "_dispatch_dict"</li> <li>networkx/utils/decorators.py:96: error: Incompatible types in assignment (expression has type overloaded function, target has type overloaded function)</li> <li>networkx/utils/decorators.py:97: error: Incompatible types in assignment (expression has type "Type[BZ2File]", target has type overloaded function)</li> <li>networkx/utils/decorators.py:98: error: Incompatible types in assignment (expression has type overloaded function, target has type overloaded function)</li> <li>networkx/readwrite/graphml.py:1002: error: Assignment to variable "e" outside except: block</li> <li>networkx/generators/line.py:122: error: All conditional function variants must have identical signatures</li> <li>networkx/classes/graphviews.py:162: error: All conditional function variants must have identical signatures</li> <li>networkx/algorithms/d_separation.py:109: error: "AbstractSet[Any]" has no attribute "union"</li> <li>networkx/algorithms/community/lukes.py:186: error: Need type annotation for "bp_buffer" (hint: "bp_buffer: Dict[<type>, <type>] = ...")</li> <li>networkx/algorithms/bipartite/edgelist.py:136: error: Assignment to variable "e" outside except: block</li> <li>networkx/algorithms/bipartite/edgelist.py:141: error: Assignment to variable "e" outside except: block</li> <li>networkx/algorithms/approximation/kcomponents.py:216: error: Incompatible types in assignment (expression has type "Callable[[_AntiGraph], Any]", base class "Graph" defined the type as "Type[Dict[Any, Any]]")</li> <li>networkx/algorithms/connectivity/__init__.py:13: error: Need type annotation for "__all__"</li> <li>networkx/algorithms/connectivity/__init__.py:15: error: Name "connectivity" is not defined</li> <li>networkx/algorithms/connectivity/__init__.py:16: error: Name "cuts" is not defined</li> <li>networkx/algorithms/connectivity/__init__.py:17: error: Name "edge_augmentation" is not defined</li> <li>networkx/algorithms/connectivity/__init__.py:18: error: Name "edge_kcomponents" is not defined</li> <li>networkx/algorithms/connectivity/__init__.py:19: error: Name "disjoint_paths" is not defined</li> <li>networkx/algorithms/connectivity/__init__.py:20: error: Name "kcomponents" is not defined</li> <li>networkx/algorithms/connectivity/__init__.py:21: error: Name "kcutsets" is not defined</li> <li>networkx/algorithms/connectivity/__init__.py:22: error: Name "stoerwagner" is not defined</li> <li>networkx/algorithms/connectivity/__init__.py:23: error: Name "utils" is not defined</li> <li>networkx/readwrite/tests/test_gml.py:291: error: Name "test_float_label" already defined on line 274</li> <li>networkx/classes/tests/test_subgraphviews.py:196: error: List item 0 has incompatible type "Tuple[int, int, int]"; expected "Tuple[int, int]"</li> <li>networkx/classes/tests/test_subgraphviews.py:196: error: List item 1 has incompatible type "Tuple[int, int, int]"; expected "Tuple[int, int]"</li> <li>networkx/classes/tests/test_subgraphviews.py:196: error: List item 2 has incompatible type "Tuple[int, int, int]"; expected "Tuple[int, int]"</li> <li>networkx/classes/tests/test_subgraphviews.py:197: error: Argument 1 to <set> has incompatible type "Tuple[int, int, int]"; expected "Tuple[int, int]"</li> <li>networkx/classes/tests/test_subgraphviews.py:197: error: Argument 2 to <set> has incompatible type "Tuple[int, int, int]"; expected "Tuple[int, int]"</li> <li>networkx/classes/tests/test_subgraphviews.py:197: error: Argument 3 to <set> has incompatible type "Tuple[int, int, int]"; expected "Tuple[int, int]"</li> <li>networkx/classes/tests/test_subgraphviews.py:197: error: Argument 4 to <set> has incompatible type "Tuple[int, int, int]"; expected "Tuple[int, int]"</li> <li>networkx/classes/tests/test_reportviews.py:1100: error: Incompatible types in assignment (expression has type "Type[DiDegreeView]", base class "TestDegreeView" defined the type as "Type[DegreeView]")</li> <li>networkx/classes/tests/test_reportviews.py:1110: error: Incompatible types in assignment (expression has type "Type[OutDegreeView]", base class "TestDegreeView" defined the type as "Type[DegreeView]")</li> <li>networkx/classes/tests/test_reportviews.py:1160: error: Incompatible types in assignment (expression has type "Type[InDegreeView]", base class "TestDegreeView" defined the type as "Type[DegreeView]")</li> <li>networkx/classes/tests/test_reportviews.py:1210: error: Incompatible types in assignment (expression has type "Type[MultiDegreeView]", base class "TestDegreeView" defined the type as "Type[DegreeView]")</li> <li>networkx/classes/tests/test_reportviews.py:1260: error: Incompatible types in assignment (expression has type "Type[DiMultiDegreeView]", base class "TestDegreeView" defined the type as "Type[DegreeView]")</li> <li>networkx/classes/tests/test_reportviews.py:1270: error: Incompatible types in assignment (expression has type "Type[OutMultiDegreeView]", base class "TestDegreeView" defined the type as "Type[DegreeView]")</li> <li>networkx/classes/tests/test_reportviews.py:1320: error: Incompatible types in assignment (expression has type "Type[InMultiDegreeView]", base class "TestDegreeView" defined the type as "Type[DegreeView]")</li> <li>networkx/algorithms/traversal/tests/test_edgedfs.py:7: error: Module has no attribute "edgedfs"; maybe "edge_dfs" or "edge_bfs"?</li> <li>networkx/algorithms/traversal/tests/test_edgedfs.py:8: error: Module has no attribute "edgedfs"; maybe "edge_dfs" or "edge_bfs"?</li> <li>networkx/algorithms/traversal/tests/test_edgebfs.py:7: error: Module has no attribute "edgedfs"; maybe "edge_dfs" or "edge_bfs"?</li> <li>networkx/algorithms/traversal/tests/test_edgebfs.py:8: error: Module has no attribute "edgedfs"; maybe "edge_dfs" or "edge_bfs"?</li> <li>networkx/algorithms/tests/test_smallworld.py:11: error: Incompatible types in assignment (expression has type "int", variable has type "Random")</li> <li>networkx/algorithms/tests/test_graph_hashing.py:458: error: Name "test_isomorphic_edge_attr_and_node_attr" already defined on line 184</li> <li>networkx/algorithms/tests/test_cycles.py:8: error: Module has no attribute "edgedfs"; maybe "edge_dfs" or "edge_bfs"?</li> <li>networkx/algorithms/tests/test_cycles.py:9: error: Module has no attribute "edgedfs"; maybe "edge_dfs" or "edge_bfs"?</li> <li>networkx/algorithms/flow/tests/test_maxflow.py:22: error: Need type annotation for "interface_funcs"</li> <li>networkx/algorithms/flow/tests/test_maxflow.py:23: error: Need type annotation for "all_funcs"</li> <li>networkx/algorithms/approximation/tests/test_traveling_salesman.py:42: error: "classmethod" used with a non-method</li> <li>networkx/classes/tests/test_multigraph.py:220: error: Need type annotation for "single_edge"</li> </ol> Found 53 errors in 22 files (checked 560 source files) </pre> </details>  Some of these errors are obvious bugs with easy fixes, some are the result of mypy having some style checking baked in, and others seem to be deeper issues related to some design patterns in NetworkX not necessarily agreeing with rules enforced by mypy, or issues surrounding inheritance. The remainder of the commits in this PR address the errors individually.  ## Resolving MyPy errors  It should be noted that the universal way to suppress mypy errors is to ignore type checks on individual objects using `# type: ignore`. I tried my best to minimize this usage here and address errors directly rather than ignoring them.  Below is a table that maps the commit hash to which mypy error(s) the patches fix. Below the table is a bit more discussion on the various mypy errors.  ### Summary  | commit | mypy err no | note | | ------ | ----------- | ---- | | c2b8061 | 42-45 |  Properly import sentinels from traversal.edgedfs. | | 4cda3fd | 1, 9, 14, 15 |  mypy doesn't like variables named \"e\".  | | 832cd96 | 12 |  Rm annotations from single function. | | 9f282da | 27, 47 |  Fix name collisions in test suite. | | 2b8bfc8 | 46 |  Rm unused random seed in test setup. | | f0c1c87 | 17-26 |  Rm redundant __all__ specification. | | 1e4a262 | 50, 51 |  Silence mypy error from sum(). Mypy bug? | | 4276842 | 52 | Fix tsp test instantiation nit. | | 8600277 | 10, 11 |  \"type: ignore\" to suppress conditional fn sigature errors. | | 81bbe5b | 5-8, 13, 16, 53 |  Remaining \"type: ignore\" to appease mypy. | | 7607f8c | 2-4, 28-41 | Configure mypy to ignore inheritance issues. |  I've broken down the errors in the `details` section into two categories: those that require minor changes, and those that might be more involved. I've also set the history so that each commit deals with a particular type of mypy error.  ### Minor fixups  -  c2b8061 fixes some import patterns, switching to importing objects from the correct module directly rather than making local variables that point to objects in a different namespace. A minor improvement IMO.  - 4cda3fd: `mypy` doesn't allow `e` as a variable name unless it is bound to an exception in a try/except statement. IMO this is more of a style consideration than type-checking, and I'm a little surprised `mypy` is so opinionated about it. Nevertheless, it's easy enough to change to a more descriptive variable name.  - 832cd96: It turns out there was at least [one function in NetworkX](https://github.com/networkx/networkx/blob/ead0e65bda59862e329f2e6f1da47919c6b07ca9/networkx/algorithms/d_separation.py#L70) that already had inline type annotations. The mypy errors could be fixed by `AbstractSet` -> `Set`, but I just removed them in this PR so that we're starting from a clean slate.  - 9f282da: `mypy` identified a couple instances in the test suite where different tests had the same name. Fixed these here, which is an obvious minor improvement.  - 2b8bfc8: The type checking also identified an unused local variable (in this case, an unused random number seed) in the test suite.  - f0c1c87: There's [some discussion in 5073](https://github.com/networkx/networkx/pull/5073#discussion_r725312949) about a limitation in mypy in handling imported objects. However; AFAICT the `__all__` definition here is redundant so I just removed it.  - 1e4a262: Not sure what's going on here - `mypy` complains about some lists that are defined by combining other lists using the `sum([l1, l2], [])` pattern. For whatever reason `mypy` stops complaining if I modify one of these definitions (but not the other). Maybe this is a mypy bug?  - 4276842: Mypy identified a usage of the `@classmethod` decorator outside of a class. Fixed this by slightly reorganizing the TSP tests.  - 81bbe5b: This represents the final set of mypy errors that I couldn't figure out how to "fix" without typing, so I ended up adding the `# type: ignore` to suppress them.  ### Larger issues  There was a second class of mypy errors related to larger issues where potential paths forward were less obvious than the above. These were largely related to two things:  1. Class inheritance.  2. Mypy disallows conditionals that define functions with the same name but different signatures.  I'll start with the second point. Mypy disallows conditionally defining a function that has a different signature, e.g.  ```python if True:     def myfunc(a):         pass else:     def myfunc(a, b):         pass ``` NetworkX uses this pattern in at least a couple places to deal with e.g. difference in edge lookup for graphs and multigraphs:   https://github.com/networkx/networkx/blob/44680a2466c3c429d9c01f55d71952efbb09b25c/networkx/generators/line.py#L115-L123  There are ways to "hack" around this by restructuring the conditionals and/or renaming to get rid of the mypy errors, but they make the code less readable. There is already some discussion on how best to fix one of these instances in #4459 . Fixing these generically in a way that doesn't reduce the readability of the code wasn't immediately obvious to me, so I kicked the can by marking these with `# type: ignore` in 8600277  The last set of mypy errors were related to (implied) type collisions when overriding members of child classes. Some of these occur in the test suite and would be remedied by switching from a class-based structure to a more pytest-y/fixture-based design (which IMO is a good idea regardless of mypy). Others are related to core structures like those in `reportviews.py`. #5073 proposes one solution for solving these issues by tweaking the initialization of some of the view objects, pulling it out of `__init__` methods and into a `_create_attributes` method, but I'm not sure it improves the readability of the code. Since dealing with these inheritance-based typing issues struck me as more complicated than some of the others, I proposed to ignore them by `excluding` the files where they are defined from type checking in 4918c7e.  ## Adding type checking to workflow  Finally, this PR adds `mypy` to the GH workflow: 737ebe3 and 4918c7e.  
issue
Add a Q&A to the contributor FAQ about algorithm acceptance policy.#TITLE_END#Mostly states that there is no "official" policy, but that algorithms should be published (and preferrably cited) to warrant inclusion in NetworkX. Also adds a link to the Mission & Values docs to give more high-level guidance.
issue
CI: xfail pydot tests.#TITLE_END#A second attempt at a temporary solution to the failing pydot tests due to its pyparsing dependency (see #5155, pydot/pydot#277 pyparsing/pyparsing#338).  Rather than alter dependency pinning which has proven to be fragile, the pydot tests can be xfailed which should both fix CI now *and* notify us when the pydot/pyparsing issue is fixed, because the xfailed tests will start passing again.
issue
Remove unused internal solver from algebraicconnectivity#TITLE_END#There is an internal `_CholeskySolver` class that does nothing other than raise an exception upon instantiation. The exception message indicates that it is intended to be replaced by the LU solver, though there is no deprecation warnings or anything like that.  Since this is a private class, and since it was otherwise accessible only via one undocumented keyword argument, I think it is safe to remove. Users would already get an exception message if they tried to use it - this will still be the case, though the exception message is slightly less specific.
issue
Discussion: Add mypy to pre-commit hooks?#TITLE_END#A follow-up to #5127 which added a CI job that runs mypy for new PRs. We could also add `mypy` as a pre-commit hook so that contributors are notified of problems at commit-time rather than having to wait for CI to run.  This is similar to how `black` is incorporated into the workflow; however, there is one major difference between the mypy pre-commit hook and the `black`/`pyupgrade` hooks: `mypy` doesn't automatically fix errors! In other words, contributors might get caught in a situation where they're *forced* to deal with mypy problems locally before they can make/update a PR. IMO this is overly prohibitive, but I'm curious what others think.
issue
MAINT, CI: Revert #5156 when pydot/pyparsing issue is resolved.#TITLE_END#Remove the explicit `pyparsing` version pin that was added in #5156 as a temporary fix due to issues with some pyparsing versions in pydot.
issue
CI/MAINT: drop gdal tests#TITLE_END#It looks like `gdal==3.0.4` is failing with the latest version of `setuptools`. Try removing this upper bound to see if CI gets unstuck.
issue
Add temporary pyparsing pin to fix CI.#TITLE_END#Closes #5155.  Note this is just a temporary fix for CI. NetworkX doesn't depend on pyparsing directly so we should remove this pin as soon as there's a fix upstream. I will create a tracking issue for this and milestone it so it doesn't slip through the cracks.
issue
MAINT: Test suite failing on pydot tests#TITLE_END#The tests related to pydot in the test suite are now causing problems. It's not clear exactly what the problem is - the tests seem to fail on Python versions 3.8, 3.9, and 3.10 (though the failures may be slightly different). It also doesn't appear that pydot [has had a new release](https://pypi.org/project/pydot/) (last release was Feb. 2021).  See https://github.com/networkx/networkx/pull/5154#issuecomment-953111796 for more details
issue
Replace internal `_n_choose_k` with `math.comb`#TITLE_END#Python 3.8 added [`math.comb`](https://docs.python.org/3/whatsnew/3.8.html#math) which can be used to replace an internal helper function:  https://github.com/networkx/networkx/blob/486c89bb710b13f4c6dcd6751c260b678d37b322/networkx/algorithms/similarity.py#L1518-L1557  This would involve:  - [ ] removing the internal _n_choose_k function  - [ ] replace all uses of `_n_choose_k` with appropriate calls to the new built-in math fn  - [ ] grep through the source to make sure all other instances of `_n_choose_k` are properly update/removed.
issue
Fix bug in selfloop drawing in draw_networkx_edges#TITLE_END#Fixes #4994.  There is a bug in the current code where if a user tried to exclude self-loop drawing via the `edgelist` parameter, it would not work and the selfloops would be drawn anyways. This fixes that by only drawing selfloops when they are part of the original edgelist.
issue
`edgelist` is ignored when drawing selfloops#TITLE_END#An issue with self-loop drawing was identified in the discussion in #4991. The `edgelist` parameter for `nx.draw_networkx_edges` is not properly handled for self-loop drawing.  ### Steps to Reproduce  ```python >>> import matplotlib.pyplot as plt >>> G = nx.path_graph(3) >>> G.add_edge(0, 0) >>> G.edges() EdgeView([(0, 1), (0, 0), (1, 2)]) >>> nx.draw(G)  # Draws self-loop, as expected >>> plt.figure(); >>> nx.draw(G, edgelist=[(0, 1), (1, 2)])  # Still draws self-loop (bug) ```  ### Environment <!--- Please provide details about your local environment --> Python version: 3.9.6 NetworkX version: 2.6.2   ### Additional context  At first glance, it seems the problem lies here:  https://github.com/networkx/networkx/blob/2eb274e39f712047cebf5666ee9caf2ba2e51ee4/networkx/drawing/nx_pylab.py#L853-L856  This code was added to ensure that self-loop edges are drawn even when edges are represented by a LineCollection object, but there should be some additional logic to check whether `edgelist` is `None` before setting it. 
issue
descendants_at_distance appears to be untested#TITLE_END#AFAICT `descendants_at_distance` is not explicitly tested anywhere. I was confused by this because the coverage report for the `breadth_first_search` module where the function is defined is at 98% (notably - the untested line is in `descendants_at_distance`), but I don't see any explicit calls to `descendants_at_distance` when grepping the test suite.  Adding some tests should help codify behavior and avoid issues like #4952 .
issue
Fix automatic documenting of class methods via autosummary#TITLE_END#In the documentation currently, when a class is added to an `.. autosummary` directive, the docstring for the class itself is automatically generated for the reference docs, but any methods/attributes are not. For example, see the [docs for the argmap class](https://networkx.org/documentation/latest/reference/generated/networkx.utils.decorators.argmap.html#networkx.utils.decorators.argmap.__init__). The `__init__` method is automatically documented and embedded in the page (though in this case it doesn't have a docstring) while all of the other methods (`assemble`, `compile`, and `signature`) don't link to their respective docstrings.  The reason for this is that the default class template for `autosummary` is missing the `:toctree:` option, which is what signals to sphinx that `sphinx-autogen` should be used to autogenerate stubs for the items listed in the autosummary. This (and related) seems to be a pretty common feature request (see e.g. sphinx-doc/sphinx#7912) but is not (yet) built in to sphinx.  However, the linked issue also illustrates how we could achieve the desired behavior by overriding the default autosummary class template. This PR implements this approach by adding `:toctree:` to the default `.. autosummary` directive that enumerates class methods and attributes. This will autogenerate stubs for all class methods/attributes automatically, instead of having to indicate them manually (as was previously done in the docs for ISMAGS). I've chosen to preserve the default behavior of handling the `__init__` method specially (i.e. the `__init__` docstring is generated+rendered under the class docstring instead of in the `Methods` section) but that's easy to change if we want.
issue
Update documentation for geographical_threshold_graph#TITLE_END#The documentation state that the return value of `p_dist` must be in the range `[0, 1]`, but the implementation differs from this behavior. The documentation should be updated to remove this discrepancy and better reflect the current implementation.  This was originally raised on the mailing list (thanks jimbo): see the [original post](https://groups.google.com/g/networkx-discuss/c/Z_UXaYZcsxw/m/2c377udtAgAJ) for further details.
issue
Review decorator dependency#TITLE_END#Review the use of the `decorator` in NetworkX. It seems that nx currently only uses a [small subset of `decorator`](https://github.com/micheles/decorator/blob/949e8e0dbc251f3aa30546761f5dc220822d6e7e/src/decorator.py#L199-L272) and doesn't (currently) ever hit some of the code branches (e.g. for coroutines). Some things to keep in mind while investigating:  - How does nx's use of decorator differ from what could be achieved with builtins like `functools.wraps`  - Consider how any potential changes would impact import time
issue
Parallel betweenness example bad runtimes in docs#TITLE_END#There is a nice example in the gallery for [computing betweenness centrality with multiprocessing](https://networkx.org/documentation/stable/auto_examples/advanced/plot_parallel_betweenness.html). Unfortunately, the example in the docs show poorer performance for the parallel version than the non-parallelized version, likely due to the CI service only using single-core (or otherwise not very performant) nodes for computation. At the very least it might be worth adding some text somewhere to the example to explain why the example shows performance results that differ from what is expected.  It's also probably worth taking a look at the example in its entirety and consider alternative approaches to multiprocessing, e.g. `concurrent.futures`.
issue
Add destringizer example to read_gml docstring#TITLE_END#IMO it would be a nice addition to add a simple example of the `destringizer` parameter to the `read_gml` docstring. See #4854 for inspiration.
issue
Deprecate numeric mixing matrix#TITLE_END#Follow-on to #4851 - deprecates `numeric_mixing_matrix`.
issue
Doc/fix 403 error drawing custom icons#TITLE_END#Implements option 1 from #4905 to close #4905.  Adding local copies of the icons we were using only adds ~2kB to the repo.
issue
Getting 403 Forbidden errors when running plot_custom_node_icons example#TITLE_END#The way `plot_custom_node_icons` is currently set up, we grab resources from `materialsui.com` every time anyone runs this example which of course includes our own CI runs. We are now getting 403 forbidden errors (rightly so) since we are accessing these resources programatically. I can think of 2 ways around this:  1) Store the icon png's locally so we're not constantly having to download them from an external source  2) Add header spoofing to the url request.  Though it makes the example a bit larger, I prefer option 1). The second option would add boilerplate to the example, and isn't exactly setting an example of good web citizenship. I'm certainly open to other ideas though!
issue
CI: update circleci doc deployment.#TITLE_END# * Try removing manual known_hosts addendum  * Update fingerprint
issue
Revert "CI: Configure circleCI to deploy docs. (#4134)"#TITLE_END#This reverts commit 9775f834f527b68c367c9bb2ad44804acf8a283d and subsequent related commits 359aa427a, b3364edc6, and 5360f5237; all of which were related to the doc deployment config.
issue
Update documentation and testing of arbitrary_element#TITLE_END#Related to #4224.  Given how extensively `nx.utils.arbitrary_element` is used in the library (and test suite), my impression was that is should not be deprecated even though it is essentially a one-liner.  Updates the documentation and adds tests for `nx.utils.arbitrary_element`:  * Reorganize docstring to include headings (`Parameters`, `Returns`, `Examples`, etc.)  * Adds `arbitrary_element` to the reference guide  * Adds tests.
issue
Less strict on mayavi constraint for doc building.#TITLE_END#Re-organize image scraper configuration for sphinx gallery so that the docs build will not completely fail if mayavi not installed.  With this change, individual sphinx gallery examples will fail, but the sphinx build process will now complete.  <!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> 
issue
Unpin pygeos#TITLE_END#Fixes #4562   Un-pin pygeos and set lower bound for geopandas to a version that supports pygeos 0.9.
issue
Pin upper bound of decorator dep.#TITLE_END#Not sure if we want to introduce this upper bound or just wait for micheles/decorator#103 to be resolved. It'd be nice to get the CI test suite back up and functional, but if the fix is imminent we can just be patient and not have to deal with adding/removing pins like this.  On a side note, it would've been nice to catch this before the new major dependency release. We run the test suite against pre-releases of the main dependencies but `decorator` must not have had a release candidate. Maybe we could consider running against the development branch of dependency projects instead? That could also help catch things like #4626.
issue
rm nx import from docstring example.#TITLE_END#Just to keep things consistent with #4163 .  I had meant to comment this on #4735 but I wasn't fast enough - sorry about that!
issue
TST: be more explicit about instance comparison.#TITLE_END#Some tests relied on `!=` to check whether or not two Graph instances where the same object. This PR updates these lines to `is not` instead, which I think is more idiomatic. Inspired by the discussion in #4746.
issue
Fix sphinx warnings during doc build.#TITLE_END#Just some cleanup to get rid of sphinx-build warnings.
issue
network_simplex is untested#TITLE_END#The `networksimplex` module defines a single monolithic function `network_simplex` that currently is not explicitly tested. The function itself is very large, and includes internal definitions of quite a few other functions/generators. This will likely make it very difficult to write tests for the function. Since the `networksimplex` module defines only the one function, this also might be a candidate for a refactor, breaking out the internally-defined functions to (probably) private functions in the module, with their own unit tests. Doing so would likely greatly improve the robustness of the function.  See #4641 for additional details.
issue
API: Incomplete docstring for authority_matrix and hub_matrix#TITLE_END#Both `authority_matrix` and `hub_matrix` defined in `algorithms/link_analysis/hits_alg.py` have incomplete docstrings. They are also missing direct unit tests. Perhaps they were not intended to be public API? These functions should either be:  1. Removed (potentially with deprecation cycle) from the public API or  2. Documented & tested.
issue
Fix signatures in json_graph.tree module: use explicit kwargs instead of dicts#TITLE_END#This was identified in #4678 but I'm pulling it out into it's own issue so we can put a milestone on it. See [this comment](https://github.com/networkx/networkx/pull/4678#issuecomment-826370554) for details and #4284 for a blueprint on how the FutureWarning could be put in place.
issue
MAINT: Add missing __all__'s to utils modules + test.#TITLE_END#Adds `__all__` attrs to modules in `utils` and adds a test to ensure objects and aliases defined in `utils` modules are not unintentionally exposed in the `utils` namespace.  Closes #4752  
issue
`networkx.utils.misc` missing `__all__`#TITLE_END#The `misc` module in the `utils` package doesn't define an `__all__`. As a consequence, some non-networkx functionality including e.g. objects from `collections` and builtin modules are incorrectly exposed in the `networkx.utils` namespace, including networkx itself:  ```python from networkx.utils import nx  # yikes ```
issue
Random sampling from set has been deprecated in Python 3.9#TITLE_END#Python 3.9 has deprecated calling `random.sample` on sets, so users will now see a warning if they do something like:  ```python >>> import networkx as nx >>> G = nx.path_graph(25) >>> import random >>> random.sample(G.nodes(), k=3) DeprecationWarning: Sampling from a set deprecated since Python 3.9 and will be removed in a subsequent version. [15, 1, 2] ```  It doesn't look like this will have an affect on `PythonRandomInterface`, but there are a few instances of the `sample(G.nodes(), **kwargs)` in the test suite which should be fixed.
issue
Test suite failing due to change in default dtype when creating sparse matrices#TITLE_END#As seen in the CI runs of #4624   Not sure what the root cause is yet, but it appears to be tied to the scipy 1.6.1 release:  ```python >>> import scipy >>> scipy.__version__ '1.6.0' >>> import networkx as nx >>> G = nx.Graph([(1, 1)]) >>> A = nx.adjacency_matrix(G) >>> A <1x1 sparse matrix of type '<class 'numpy.int64'>'         with 1 stored elements in Compressed Sparse Row format> ```  ```python >>> import scipy >>> scipy.__version__ '1.6.1' >>> import networkx as nx >>> G = nx.Graph([(1, 1)]) >>> A = nx.adjacency_matrix(G) >>> A <1x1 sparse matrix of type '<class 'numpy.float64'>'         with 1 stored elements in Compressed Sparse Row format> ```  Note the change in dtype from int64 to float64.
issue
Deprecate `Ordered` graph classes#TITLE_END#Since the next NetworkX release will drop support for Python 3.6, dictionaries should be ordered by default in all supported Python implementations moving forward (though it's worth double-checking that this is true for PyPy). Thus the `Ordered` graph classes will be redundant moving forward and can be deprecated for 2.6 in preparation for removal in 3.0.  This [SO post](https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6) compiles a lot of useful links re: the status of guaranteed insertion order for various Python versions. The conclusion is that insertion-order preservation is guaranteed for dictionaries from Python 3.7 onward.
issue
MAINT: Remove dependency version info from INSTALL#TITLE_END#The info on required versions of dependencies in INSTALL.rst had gotten out-of-sync with the actual requirements files. Rather than update them, I removed the version info from INSTALL and replaced it instead with a link to the requirements folder so that version info doesn't need to be maintained in multiple documents.  Currently, the link is external as the `requirements/` folder is not included in the docs build, though I could include it and use an internal link if that is preferred.  Also removed a note about needing to compile C/C++ for SciPy as this is generally not the case anymore.
issue
TST: remove int64 specification from test.#TITLE_END#Potential fix for #4054
issue
MAINT,TST: Parametrize algebraic connectivity tests#TITLE_END#Refactors `test_algebraic_connectivity.py` using pytest's parametrization facilities to break up tests into more fine-grained units (while maintaining the original class-based organization of the tests). Replacing the loops over certain kwargs (e.g. `method`) in the test suite gives more specific failures to help pinpoint potential problems (stumbled across this in conjunction with #4045).  A quick summary of the number/runtime of tests:  **Before**: ``19 passed in 0.48s`` **After**: ``83 passed in 0.63s``  So about 4x increase in test granularity at the cost of ~30% runtime increase.
issue
DOC: Minor reformatting of contract_nodes docstring.#TITLE_END# * Remove duplicate Notes section  * Remove unnecessary indentation.
issue
TST: add dtype to pandas test#TITLE_END#Closes #4054 - hopefully for good this time. Applies the same explicit dtype to a df created in the test suite via the `DataFrame` constructor to prevent incorrect integer sizes being used on 32-bit platforms.
issue
DOC: Update docstrings in cytoscape module#TITLE_END#Add missing parameter and additional sections to `cytoscape_data` docstring, and add a docstring to the `cytoscape_graph` function.  Related to discussion in #4173 
issue
DOC,BLD: Fix doc build warning from markup error.#TITLE_END#I just noticed there are two new warnings when building the docs on 99fc1bb6. This fixes one of them. The other is in `doc/release/release_dev.rst` (line 276). I didn't change the latter as it seems the list is auto-generated from PR titles.
issue
DOC: Update docstrings for public functions in threshold module#TITLE_END#Adds properly-formatted docstrings for `is_threshold_graph` and `find_threshold_graph` which were recently added to the reference documentation in #4161 .
issue
MAINT,TST: Improve coverage of nx_agraph module#TITLE_END#Improves the test coverage of `networkx/drawing/nx_agraph` module from ~85% -> 94%. Most of the improved coverage is related to keyword arguments whose non-default values were uncovered. This PR also includes a few ancillary fixes including things like fixing docstring formatting issues, fixing typos, etc. I'm happy to pull these out into a separate PR if it will make this easier to review.   A couple notable points:  - Currently, there is no way to suppress the opening of the rendered graphs from `view_pygraphviz` (assuming an appropriate graph viewer is installed on the system). This means that running the test suite will open 2 new windows. With the additional tests in this PR, many more (>10 new windows) are opened. I've proposed a new kwarg in #4155 that allows users to toggle the opening of these windows, which would allow for suppressing the opening of windows in the test suite.  - Roundtripping with `to_agraph/from_agraph` fails due to implicit conversion of all nodes types to strings. This is documented e.g. in the Notes section of `view_pygraphviz`. I've added an xfailed test to indicate that roundtripping is *not* expected to be successful.  - There is a function called `graphviz_layout` which simply calls `pygraphviz_layout` (with no remapping of argument names or any modification to inputs at all). Perhaps the former could be considered for deprecation as it is purely duplicate?  - There was a failure in previously un-tested behavior in `view_pygraphviz`. I've added an xfailed test for this and the bug will be addressed in a follow-up PR.  - The remaining lack of coverage is mostly related to `except` branches when `import pygraphviz` fails.
issue
API: Add `show` kwarg to view_pygraphviz.#TITLE_END#Adds a new keyword argument to view_pygraphviz to toggle whether or not the graph produced by `view_pygraphviz` will actually be displayed.  Currently, there are tests in `test_agraph` that result in graphs being displayed via external viewers (platform dependent) but that don't actually test whether the rendered graphs are correct.
issue
MAINT: Switch to abc-based isinstance checks in to_networkx_graph#TITLE_END#Refactors `to_networkx_graph` to use ABCs from `collections.abc` for the `isinstance` checks which map the input data to the corresponding sub-function that does the conversion. The main improvement is that this replaces a hand-curated list of types (list, set, tuple, etc.) with the more general ABCs. A couple points that need special attention:  - I removed the `hasattr` checks for `_adjdict` and `next` though I'm not sure which kinds of objects they were designed to check. The test suite didn't complain about this, but if there are objects/types that depended on these (that perhaps aren't tested) please LMK.  - The execution order has to be modified for this check to work. Since the ABC-based check is the most general, I moved it to the last `if` (right before the final `raise` when the input type isn't recognized).   Closes #3945   
issue
CI: Configure circleCI to deploy docs.#TITLE_END#Closes #4118 - configure CircleCI to deploy the development documentation. Largely drawn from the doc deployment configuration for [scipy](https://github.com/scipy/scipy/blob/master/.circleci/config.yml).  Other tasks (beyond reviewing the proposed configuration) to be done before removing draft status:  - [x] remove doc deployment from travis CI
issue
CI: hypothetical fix for CI latex installation issues#TITLE_END#The CircleCI build intermittently fails to install the necessary dependencies for building the latex doc via the container's package manager (`apt-get`). This PR switches to a more modern version of Debian with the hope that the base-OS package management will be more reliable. 
issue
MAINT: fixups to parse_edgelist.#TITLE_END#Some minor maintenance to `parse_edgelist` originating from review of #4125. This will cause merge conflicts with #4125, which should have higher priority. Changes include:   * Catch Exception rather than BaseException  * Add default values to docstring  * Fix poorly-formatted f-strings.
issue
DOC: Suggestions and improvments from tutorial readthrough#TITLE_END#Some suggestions and improvements to the tutorial after a quick readthrough. Note that there are quite a few different types of changes ranging from simple linking improvements which are likely uncontroversial, to more subjective "improvements" that may require a bit of discussion. I am happy to split this up into multiple PRs to make reviewing easier if need be - please LMK if that would be helpful!
issue
Test and document missing nodes/edges in set_{node/edge}_attributes#TITLE_END#Closes #4341.  Add examples to the documentation of `set_node_attributes` and `set_edge_attributes` for the case where `values=` is a dictionary (or dict-of-dicts) containing nodes or edges that are not in the graph. These entries are silently ignored, e.g.:  ```python >>> G = nx.Graph() >>> G.add_node(0) >>> attr_dict = {0: {'name': 'foo'}, 1: {'name': 'bar'}} >>> nx.set_node_attributes(G, attr_dict)  # There is no node 1 in G, so this entry is silently ignored >>> G.nodes(data=True) NodeDataView({0: {'name': 'foo'}}) ```  Also adds tests for this behavior, improving test coverage of `networkx/classes/functions.py`.
issue
TST: improve multigraph test coverage to 100%#TITLE_END#Adds a test for 4-tuple inputs to `MultiGraph.add_edges_from` that have the `key` and `data_dict` flipped. 
issue
Add rainbow coloring example to gallery.#TITLE_END#Add an example of colorizing the edges of a complete graph by distance between the corresponding nodes along a circle. Could be a nice candidate to expand upon in a `networkx-guides` example.  Inspired by: https://www.quantamagazine.org/mathematicians-prove-ringels-graph-theory-conjecture-20200219/
issue
Update to_dict_of_dict edge_data#TITLE_END#Closes #4305 .  Updates the documentation of `to_dict_of_dicts` to better describe and provide examples for the `edge_data` kwarg. Also adds tests for the (previously uncovered) case when the `edge_data` kwarg is not `None`.
issue
TST: Refactor exception handling to improve coverage#TITLE_END#Refactor exception handling in `nbunch_iter` and improve test specificity.  Should get `networkx/classes/graph.py` to 100% test coverage.
issue
Improve test coverage of convert module#TITLE_END#Try to improve test coverage of the `networkx/convert.py`.   Todo:  - Remove warnings on ImportError for numpy/scipy/pandas etc.    * Tentatively in favor of leaving this as-is.  - [x] Resolve expected behavior of `to_dict_of_dicts` for multigraphs (see #4305)
issue
Updates to slicing error message for reportviews#TITLE_END#Follow-up to #4300 - I had intended to make the suggestions there, but didn't get to it in time.  This PR attempts to accomplish several things:  - Fix a typo in OutMultiEdgeView: `list(G.nodes)` -> `list(G.edges)`  - Modify the exception message so that the type of reportview is always correct, regardless of subclassing  - Adds error messages to the InMultiEdgeView and InEdgeView  - Modify the error message so that the full slice object is reported, instead of `{s.start}:{s.stop}`, which fails to include the `step` (if used).  The last bullet would be improved even further if there were a built-in for mapping slice objects back to their nice string representation (e.g. `slice(None, 10, 2)` -> `:10:2`). I'm not sure how to do this and didn't want to add a custom utility function for it.
issue
TST: Increase test coverage of convert_matrix#TITLE_END#This PR increases the test coverage of the `convert_matrix` module to 100%.  It would be nice to also clean up these test modules (e.g. get rid of importorskip for numpy, scipy, etc.), but I only focused on the coverage in this PR.
issue
TST: An approach to parametrizing read_edgelist tests.#TITLE_END#Parametrizes the tests related to edgelist reading and adds additional tests of `parse_edgelist`, originally authored by @AndrewEckart in #4192. The `parse_edgelist` tests complemented some additional tests added in #4310 - I moved all of the `parse_edgelist` tests so that they are all together while rebasing.  Closes #4127 
issue
Add FutureWarning in preparation for simplifying cytoscape function signatures.#TITLE_END#Closes #4199 .  The biggest change here is the introduction of a FutureWarning to notify users of the function signature change in the cytoscape functions. Added a detailed warning about the change in API that would take affect in 3.0, but the current behavior & signature remains unmodified for now. I'm not sure if this is the best way to do this, but it is at least consistent with the deprecation policy. Might be worth adding a tracking issue with a 3.0 milestone if the FutureWarning approach is acceptable.
issue
Deprecate utils.is_list_of_ints#TITLE_END#Deprecates the (apparently) unused and untested `nx.utils.is_list_of_ints` function. Uses of this function within networkx appears to have been replaced with `nx.utils.make_list_of_ints` in #3617.
issue
Deprecate `utils.is_iterator`#TITLE_END#The function `is_iterator` from the utils module is only used in one other place in NetworkX and is not currently tested. I think it can be safely replaced with an `isinstance` check against the `Iterator` abc.
issue
Improve axes layout in plot_decomposition example#TITLE_END#Modifies the figure aspect, axes layout, and axes margin padding to reduce the overlap of nodes in the visualization of the junction tree.   Note that the figure itself is not entirely reproducible because `junction_tree` is not deterministic.
issue
Add seeds to gallery examples for reproducibility#TITLE_END#Adds seeds (or other deterministic layouts) to the gallery examples so that the generated figures are reproducible across runs. In some cases this includes seeding both the layout as well as the graph generation (if any graphs in the example are created with a random graph generator).  Other minor improvements to some examples include:   * using canonical naming for imports (`import numpy as np` insteady of `import numpy`)   * Add axis padding/change axes layout for examples where nodes and/or labels are being clipped.
issue
Add a 3D plotting example with matplotlib to the gallery#TITLE_END#Adds a plotting example based on the `mayavi2_spring` example, but using matplotlib for visualization instead.  The example illustrates the basics of converting from the dict-based layout returned by the layout functions to an array of coordinates that can visualized via `Axes3D`.
issue
Update giant component example#TITLE_END#As a first attempt, I tried reducing the alpha value for nodes that are not part of connected components to de-emphasize their contribution to the visualization overall. Making the non-connected nodes have consistent position between the axes is trickier than I expected as the layout is re-computed for each graph to take into account the new edges. It should be possible with a little set intersection code, but I went with the simplest solution first.  LMK what you think - whether the alpha reduction is an improvement or a different approach is needed.
issue
Replace degree_histogram and degree_rank with a single example#TITLE_END#The gallery currently has both `plot_degree_histogram` and `plot_degree_rank` examples. These examples are quite similar, so I thought I would propose combining them into a single example (I chose the name "degree analysis", but am certainly open to a better name).  This condenses down the two examples into one, showing two different ways of visualizing the distribution of node degree for a graph, with all three visualizations in a single figure.  LMK what you think! I'm happy to close if having separate examples is preferred - just thought I'd make a pitch.
issue
Update "house with colors" gallery example#TITLE_END#Improve the axis scaling so that all nodes are contained within the figure.
issue
Refactor and improve test coverage for restricted_view and selfloop_edges#TITLE_END#This PR increases the test coverage of the `functions` module to 100% and refactors one large selfloop test into several smaller tests.   * 5f20f29 and a0f9d8b add tests to improve coverage of `restricted_view` and `selfloop_edges`, respectively  * facee99 refactors `test_selfloops` into several smaller tests.   facee99 is not critical to improving test coverage, so I'm happy to pull it out into a separate PR to make reviewing easier if desired.
issue
Add additional libraries to intersphinx mapping#TITLE_END#Something I noticed when reviewing `draw_networkx_edges` - there were references to matplotlib classes for which the links were not being built because matplotlib wasn't in intersphinx mapping. I went ahead and added pandas and scipy too since they're default requirements and there are likely (or at least could be) references to their documentation.  Also removed some unnecessary `:py:class:` instances from `nx_pylab` docstrings.
issue
MAINT: remove deprecated numpy type aliases.#TITLE_END#NumPy is [deprecating some type aliases in 1.20](https://numpy.org/devdocs/release/1.20.0-notes.html#using-the-aliases-of-builtin-types-like-np-int-is-deprecated). We had used some of these aliases in the test suite - this PR replaces them with the corresponding Python or numpy scalar type. I grepped through the rest of NX as well and didn't notice any other instances.
issue
TST: Boost test coverage of nx_pylab module#TITLE_END#Boosts the test coverage of `nx.drawing.nx_pylab` to 100%, minus the lines mentioned in #4374, which will be addressed separately after discussion of that issue.  Two points that I'd like to call attention to for review:  1) I've removed an `except` branch from nx_pylab in 59cc4c6 because I couldn't come up with a test case that would raise a ValueError here. I checked the `blame` and couldn't find any motivating case that way either. Please LMK if you have an idea of what case this branch was originally designed to detect.  2) The test I added in 9fe21a3 is a bit of a departure from most of the tests of `nx_pylab` in that I've tried to test the behavior of the visualization. On the one hand, this is nice, but it's also more detailed and arguably harder to grok. In addition, the test performs checks on data in pixel space, which feels like it might be fragile --- I'm not sure about best-practices for testing visualizations. In any case I'd love to hear what you think - if it's too complicated or fragile, I'm happy to replace it with a simpler smoke test.
issue
Modify and document behavior of nodelist param in draw_networkx_edges.#TITLE_END#Closes #4374  Prevents edges whose endpoints are not in `nodelist` from being drawn by `draw_networkx_edges` (see #4374 for more details).  Also documents missing `node*` parameters in `draw_networkx_edges` docstring.
issue
CI: travis font naming bug#TITLE_END#Proposed fix based on: https://bugs.launchpad.net/ubuntu/+source/texlive-base/+bug/556071
issue
Handle self-loops for single self-loop (drawing)#TITLE_END#Fixes a corner-case of the new self-loop drawing in `nx_pylab`.  The self-loop drawing code uses the width/height of the box formed by the edge positions to determine the size of the self-loop curve. The current code runs into problems if the user passes in an `edgelist` that contains only a single self-loop, as the computed width and height are then 0, which results in a FancyArrowPatch with 0 extent. Example:  ```python >>> G = nx.path_graph(2) >>> G.add_edge(0, 0) >>> pos = {n: (n, n) for n in G.nodes} >>> nx.draw_networkx_nodes(G, pos) >>> # Intend to draw only the self-loop >>> nx.draw_networkx_edges(G, pos, edgelist=[(0, 0)]) ``` **Expected**: ![expected](https://user-images.githubusercontent.com/1268991/101292864-f5b0f780-37c6-11eb-88f9-a5bb8f568c41.png) **Observed**: ![obs](https://user-images.githubusercontent.com/1268991/101292835-c4d0c280-37c6-11eb-95bc-fb2dda859f29.png)    The fix proposed in this PR is to compute the width and height from *all* edge positions in the local scope of `_connectionstyle`.
issue
Add gallery example for drawing self-loops.#TITLE_END#Inspired by and hopefully closes #4406  Adds an example related to drawing self-loops (with pylab) to the gallery.  I wasn't sure if it should be "selfloop" or "self-loop"... I went with the latter, but please correct me if that's the wrong terminology.
issue
Add coreview objects to documentation#TITLE_END#Adds the `coreviews` module and all classes defined therein to the reference guide.  A few things I was unsure about:  - Where to add this to the documentation. I went with `reference/classes/index.rst` because it felt like it fit in with `Graph Views` and `Filters` sections, though it is unfortunate that it gets nested under "Graph Types" top-level heading.  - I tried to briefly summarize the `coreviews` module in the module docstring (which provides the summary for this section in the reference guide), but I'm sure that could be massively improved.  - The `Filter*View` objects are all missing class docstrings, so the generated documentation for these classes is bad. Maybe they should be removed from the `autosummary` (at least for now)?  Closes #4397
issue
DOC: Switch from napoleon to numpydoc sphinx extension#TITLE_END#**Motivation**  `numpydoc` and `sphinx.ext.napoleon` are two sphinx extensions that implement the [numpydoc docstring standard](https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard). I propose switching to the `numpydoc` extension as it is a bit more strict about enforcing the standard, and catches things like when sections are mis-named, sections are repeated multiple times, etc. The more stringent enforcement of the standard also helps make the docstrings themselves more consistent, should help with working with other sphinx themes, and may improve compatibility with external tooling e.g. code highlighters/lexers. There are also some nice docstring validation features that will (hopefully) make it into the next release of numpydoc.  8d1c908 encapsulate the changes related to switching to numpydoc, and 758c65e fixes the warnings/errors raised by `numpydoc` during the sphinx-build process.
issue
generate_unique_node: Switch to uuid4 and improve docstring.#TITLE_END#Related to #4224   `nx.utils.generate_unique_node` uses the `uuid` module to generate a unique string that can be used as a node. The idea (I think) is to have a function that generates a node that is guaranteed not to already be included in a graph. I've updated the docstring to try to capture this sentiment and provide examples.  I also changed the default uuid function from `uuid.uuid1()` to `uuid.uuid4()` based on the opening paragraph of the [uuid docs](https://docs.python.org/3/library/uuid.html) which notes "uuid1 may compromise privacy".  It is another untested one-liner that is only used in a few places in the library:  <details> <pre> ./networkx/generators/line.py:from networkx.utils import arbitrary_element, generate_unique_node ./networkx/generators/line.py:        a = generate_unique_node() ./networkx/generators/trees.py:from networkx.utils import generate_unique_node ./networkx/generators/trees.py:            new_head = generate_unique_node() ./networkx/generators/trees.py:    root = generate_unique_node() ./networkx/algorithms/flow/capacityscaling.py:from ...utils import generate_unique_node ./networkx/algorithms/flow/capacityscaling.py:    s = generate_unique_node() ./networkx/algorithms/lowest_common_ancestors.py:    generate_unique_node, ./networkx/algorithms/lowest_common_ancestors.py:        super_root = root = generate_unique_node() ./networkx/algorithms/shortest_paths/weighted.py:from networkx.utils import generate_unique_node ./networkx/algorithms/shortest_paths/weighted.py:    newnode = generate_unique_node() ./networkx/utils/misc.py:def generate_unique_node(): ./networkx/utils/misc.py:    >>> G.add_nodes_from([generate_unique_node() for _ in range(3)]) </pre> </details>  but I do think the name ~~`generate_random_node`~~ `generate_unique_node` is much more clear than having calls to uuid functions scattered throughout the library. In the long run maybe it's worth auditing the uses to see if it can't be remove/reimplemented.
issue
Gallery support for pygraphviz examples#TITLE_END#This PR is intended mostly for demonstration purposes to illustrate/motivate pygraphviz/pygraphviz#258.  I've re-broken the pygraphviz examples up and put them back in their own sub-category to illustrate how pygraphviz examples *could* with the proposed image scraper for sphinx-gallery.  Demo here: https://1068-890377-gh.circle-artifacts.com/0/doc/build/html/auto_examples/index.html#pygraphviz
issue
Setup cross-repo doc deploy via actions.#TITLE_END#Move devdoc deployment from Travis to GH actions.  Still to do:  - Setup deploy keys/secrets for cross-repo deploy
issue
Partially revert #4378 - Modify behavior of nodelist param in draw_networkx_edges.#TITLE_END#Reverts the behavior changes of the `nodelist` kwarg in `draw_networkx_edges` - see #4505 for discussion and motivation.  Note that the docstring changes to `draw_networkx_edges` are retained. If preferred, I can do a full reversion of #4378 and re-introduce the docstring changes in a separate PR.
issue
TST: Fix error in katz centrality test setup.#TITLE_END#This is an example of the type of error that can be caught by `pytest-randomly`, see #4553.  Assuming you have NX set up for development (including the test requirements), the error that this PR fixes can be reproduced by:  ``` pip install pytest-randomly pytest --randomly-seed=3100993940 --pyargs networkx ```
issue
Add an FAQ to the developer guide for new contributors#TITLE_END#This is an attempt to address some common questions to help new contributors get started contributing to networkx. 
issue
Deprecate networkx.utils.empty_generator.#TITLE_END#Related to #4224   `networkx.utils` provides `empty_generator` to create an empty Generator object. The only use of this utility was in `all_simple_paths`. The motivation for deprecating this function is that it seems like it is not very specific to NetworkX and is generally achievable by various one-liners, e.g. `(x for x in ())`. IMO it makes more sense to deprecate than to test/document it - LMK what you think!  One tricky thing about this - the warning won't be emitted *on instantiation* of the generator object, but rather only if it is ever iterated over:  ```python >>> empty = nx.utils.empty_generator()  # Doesn't raise deprecation warning >>> next(empty) DeprecationWarning Traceback   ... StopIteration ```  I think this is probably okay, but if we want to work out a way that the warning is raised on generator instantiation, I wasn't quite sure how to do that. Suggestions welcome!
issue
Use Pillow for viewing AGraph output and deprecate default_opener#TITLE_END#`networkx.utilities.misc` has the `default_opener` function which is used in `nx_agraph` to display the results of drawing with pygraphviz. The pygraphviz drawing functionality writes an image to file, and `default_opener` is used to open that file using the default image viewing utility on various platforms.  Now that matplotlib is a default dependency, `Pillow` should be installed by default, so we can instead rely on the purpose-built Pillow utilities to display images from file. If this is an acceptable alternative, then `default_opener` can be deprecated.
issue
Deprecate hub_matrix and authority_matrix#TITLE_END#Related to #4091 - adds deprecation to `hub_matrix` and `authority_matrix` from the `hits_alg` module. As noted in the linked issue, the functions are primarily pedagogical, so I updated the `hits_numpy` docstring example with a section showing how the two matrices are computed. I also tried to provide a detailed deprecation warning message to illustrate how the hub and authority matrices can be computed from a given Graph.  This addresses part of #4091, but it would still be nice to have a gallery example illustrating the hits algorithm. I don't have a compelling example, so I will leave it for a followup PR. However, if anyone does have a nice example, feel free to push to this branch!
issue
Add scipy-1.6.1 to blocklist.#TITLE_END#Temporary fix for #4626, at least for failing CI. Prevents scipy version 1.6.1 from being picked up by `pip`.  Avoid test failures due to change in behavior for default dtype when creating coo_matrices.
issue
Improve doc example for find_cycle.#TITLE_END#Just a minor doctest improvement I noticed while reading about cycles. 
issue
MAINT: Replace internal usage of to_numpy_matrix and from_numpy_matrix#TITLE_END#Related to the cleanup in #3107   Replaces all internal instances (i.e. when used in the implementation of the function, but not affecting the output - see #3107 for details) of `to_numpy_matrix` and `from_numpy_matrix` with `to_numpy_array` and `from_numpy_array`. In preparation for the deprecation of the `*_numpy_matrix` functions.  If the changes are approved, I would propose that I squash this down to 2 commits, one for doc-only changes and one for code changes.
issue
Review untested functions in utils.misc module#TITLE_END#There are several functions in `nx.utils.misc` that are not tested. The following gives a rough idea:  ```python >>> import networkx.utils.tests.test_misc as nxtum >>> set(dir(nx.utils.misc)) - set(dir(nxtum))    ... see below ```  Some of these functions are used infrequently and/or one-liners that may be candidates for deprecation. In any case, the following list of functions should be reviewed and probably have unit tests added if they are to be kept:  - [x] `arbitrary_element` #4451     * Used extensively, esp. in tests. Initial feeling is that it'd be better to document & test this one rather than deprecate  - [x] `consume` #4449   - [x] `default_opener` #4600  - [x] `empty_generator` #4599   - [x] `flatten` #4359  - [x] `generate_unique_node` #4454  - [x] `is_iterator`  #4279   - [x] `is_list_of_ints`  #4280
issue
Improve `test_edgelist.py`#TITLE_END#The tests in `networkx/readwrite/tests/test_edgelist.py` contain a lot of repetition and poorly-formatted strings/bytes objects used as inputs. In addition, the public function `parse_edgelist` is not itself directly tested - it is only tested indirectly via unit tests for `read_edgelist`.  See also: related discussion in #4125
issue
CI: Add recommended fonts to travis.yml.#TITLE_END#Attempt to fix missing fonts for travis CI doc deploy
issue
CI: Setup circle CI for documentation builds#TITLE_END#First part of addressing #4118 - this PR adds a circleCI config defining a workflow for building the documentation. Note that this workflow is *only* for documentation building - testing remains on travis.  Note: I set this up within the NetworkX organization so that all members of the organization *should* have full access on CircleCI to modify settings etc. This will likely require some additional tweaking but if there are access problems please let me know.
issue
Question: behavior of `node_list` kwarg in draw_networkx_edges#TITLE_END#Like the other pylab drawing functions, `nx_pylab.draw_networkx_edges` currently has a `nodelist` keyword argument. It is not included in the `Parameters` listing of the docstring and so it's behavior is not well-defined.   Naively, I would expect that any edges incident on a node *not* in the node list would not be drawn. For example, I would expect the following:  ```python >>> G = nx.path_graph(3) >>> pos = {n: (n, n) for n in range(len(G))} >>> nx.draw_networkx_nodes(G, pos, nodelist=[0, 1]) >>> nx.draw_networkx_edges(G, pos, nodelist=[0, 1]) ``` to produce the following, without the edge (1, 2) since 2 was not included in the nodelist:  ![expected](https://user-images.githubusercontent.com/1268991/99858963-8f2fa680-2b43-11eb-8292-bad080df0456.png)  Instead, the above code results in the following image:  ![reality](https://user-images.githubusercontent.com/1268991/99859092-e3d32180-2b43-11eb-9d6c-c701c4766dd7.png)  Is this expected? Right now, the `nodelist` is only used internally in `draw_networkx_edges` to determine the size of the nodes. Either way, the parameter needs to be added to the docstring and the behavior tested - I just wanted to raise the question about what the desired behavior was in order to do so!
issue
TST: Add decorator for testing import errors.#TITLE_END#Adds a utility decorator called `missing_modules` that allows for the testing the expected raising of `ImportWarning`s and `ImportErrors`.  This was an idea I had when trying to design tests to improve test coverage. A common pattern is:  ```python try:     import numpy as np     <do something> except ImportError:     <do something else> ```  This PR adds a utility to try to make it easier to write tests for the `except` branches.  It would be nice to be able to remove the `try/except ImportError` pattern now that  `numpy`, `scipy`, etc. are dependencies. In the meantime this might be one approach to improving the ability to test these branches.  Use of the decorator is demonstrated to improve coverage of the coverage module.
issue
CI: Add mayavi conf to travis and GH for doc deploy#TITLE_END#Followup to #4297 - adds configuration for headless mayavi to both gh-actions and travis configuration.
issue
test deploy workflow#TITLE_END#<!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. --> 
issue
Enable mayavi in sphinx gallery.#TITLE_END#Should generate images/thumbnails for the 3d drawing example that uses mayavi.  See: https://sphinx-gallery.github.io/stable/configuration.html?highlight=mayavi#image-scrapers
issue
Question: set_node_attributes ignoring missing nodes#TITLE_END#I wanted to double-check on the desired behavior of `nx.set_node_attributes` when `values` is a dictionary that contains nodes that are not in `G`.  Currently, `set_node_attributes` ignores any nodes that are in the `values` dict but not in `G`, e.g.  ```python >>> G = nx.Graph() >>> G.add_node(0, data='foo') >>> nx.set_node_attributes(G, values={0: 'bar', 1:'baz'}, name='data') >>> G.nodes.data() NodeDataView({0: {'data': 'bar'}}) ```  Whereas if you tried to do the equivalent operation "by hand", you would get an exception:  ```python >>> G = nx.Graph() >>> G.add_node(0, data='foo') >>> G[0]['data'] = 'bar' >>> G[1]['data'] = 'baz' Traceback (most recent call last)    ... KeyError: 1 ```  Ignoring the `KeyError` in `set_node_attributes` is currently not covered by the test suite, so I just wanted to double check that it is desired behavior before adding tests for it. 
issue
Question: MultiGraph with to_dict_of_dicts with edge data#TITLE_END#I was reviewing the tests for `nx.convert.to_dict_of_dicts` which led to a question about what the expected behavior of `nx.to_dict_of_dicts(G)` is when G is a MultiGraph and `edge_data` is not None.  Things seem to make sense when `edge_data` is None:  ```python >>> G = nx.MultiGraph() >>> G.add_edge(0, 1, key='a') 'a' >>> G.add_edge(0, 1, key='b') 'b' >>> nx.to_dict_of_dicts(G, edge_data=None) {0: {1: {'a': {}, 'b': {}}}, 1: {0: {'a': {}, 'b': {}}}} ```  In the above example, if I call `nx.to_dict_of_dicts(G, edge_data={'weight': 1})`, my naive expectation was that i would get  ```python {0: {1: {'a': {'weight': 1}, 'b': {'weight': 1}}}, 1: {0: {'a': {'weight': 1}, 'b': {'weight': 1}}}} ```  Instead, you get:  ```python >>> nx.to_dict_of_dicts(G, edge_data={'weight': 1}) {0: {1: {'weight': 1}}, 1: {0: {'weight': 1}}} ``` which seems to clobber the multiple edges. [The code](https://github.com/networkx/networkx/blob/9aee8ef9739913ebd1e34b1be97cd91a7d6cc520/networkx/convert.py#L240-L277) doesn't have any special cases for MultiGraphs, and I couldn't tell from there and/or docstring whether this behavior is expected.  I expect I'm missing something, so I thought I'd ask! Adding examples to the docstring would also be a nice improvement to show the expected behavior.
issue
Simplify cytoscape functions#TITLE_END#The functions for reading/writing graphs in cytoscape format have a complicated signature/implementation based on passing in an attribute with required keys while the rest are ignored. It would be much more straightforward to simply use keyword arguments. Since this involves changing a function signature, this change should follow the [deprecation plan](https://networkx.github.io/documentation/stable/developer/deprecations.html). See #4180 for detailed discussion.   - [ ] Update the function signature with `name` and `ident` kwargs, which is more clear than the current `attrs` dict. This change in API would require a Deprecation/FutureWarning  - [ ] Update parameter checks (e.g. uniqueness of keys in `attrs` -> "name" != "ident" ; if not attrs -> check for None). The approach here depends on the bullet point directly above.
issue
Add nx.info to str dunder for graph classes#TITLE_END#Follow up to #4193 to close #4139.  Replaces the original `__str__` method of Graph and friends with the output from the newly-modified `nx.info`, which summarizes the name, number of nodes, and number of edges in a nice narrative form.  Modifies the test suite to check the new `__str__` output.
issue
MAINT,TST: Replace hash function for test of weighted astar #4236 #TITLE_END#Closes #4203   The `heuristic` function for the weighted astar search test formerly computed the hash of the input. The hypothesis is that the hashing function can vary between platforms, which means that this test was not deterministic and could fail on some untested platforms.  Replaced `hash` with a sum-of-squares function which should be platform independent and give the minimum path result on all platforms.
issue
MAINT: Deprecate numpy matrix conversion functions#TITLE_END#Adds deprecation warnings + associated tests to `to_numpy_matrix` and `from_numpy_matrix`.   Originally, `from_numpy_array` as simply a pass-through function calling `from_numpy_matrix`. Re-organized so that `from_numpy_matrix` is now the pass-through, and the implementation is in `from_numpy_array` to make the removal easier down the road.  Added tests to ensure the deprecation warnings are raised, and warnings filters to suppress warnings related to `to/from_numpy_matrix` in the remainder of the test suite.
issue
Update Knuth miles example.#TITLE_END#Uses `cartopy` to add a map of the US as a backdrop to the `plot_knuth_miles.py` example.  As of now, the example is organized so that it will run whether or not cartopy is available.  CI configuration for the `cartopy` dependency cherry-picked from #4250 
issue
NXEP: Add discussion to NXEP 2.#TITLE_END#Adds a discussion to NXEP2 on the various alternatives and proposes a plan of action.  I tend towards "quite verbose", so please feel free to slash/reword liberally!
issue
Improve legibility of labels in plot_labels_and_colors example#TITLE_END#Attempt to improve legibility of labels by:  * Increasing background alpha and background node size  * Increasing font size  * Changing to a light font color to contrast with the red/blue bgnds
issue
Update "four_grids" gallery example#TITLE_END# * Prevent node clipping.  * Add slightly more descriptive intro to the example.  * Switch from `plt.subplot` mechanism to using the `ax=` kwarg of nx.draw to show how to populate existing axes instances
issue
Improve readibility of chess_example in gallery#TITLE_END#The current visualization produced by `plot_chess_masters.py` results labels that are not very readible.  This PR contains some minor tweaks to improve the visualization without changing the overall aesthetic (colors etc., though the layout has changed), as well as some minor improvements to the code in the example.
issue
MAINT: Replace hash function for test of weighted astar#TITLE_END#Closes #4203   The `heuristic` function for the weighted astar search test formerly computed the hash of the input. The hypothesis is that the hashing function can vary between platforms, which means that this test was not deterministic and could fail on some untested platforms.  Replaced `hash` with a sum-of-squares function which should be platform independent and give the minimum path result on all platforms.
issue
Review public functions in thresholds module.#TITLE_END#Related to #4157 - There are ~27 functions in the `algorithms.thresholds` module, but only two of them (`is_threshold_graph` and `find_threshold_graph`) are included in `__all__`. Of the non-included functions, some seem like they may be useful while others seem like they may be for internal use only. It would be nice to review this module to include the former in `__all__` and prepend the latter with `_` to further reinforce that they are only for internal use.  Additionally, it seems there may be some functions (e.g. `eigenvectors`) which provide functionality which is implemented elsewhere in networkx. It would be good to consolidate and get rid of any duplicate implementations.
issue
DOC: Fix return types in laplacianmatrix.#TITLE_END#The docstrings for most of the functions in the `laplacianmatrix` module listed incorrect types for the objects returned by the functions.  This PR fixes the return types so they match the current implementations. The implementations themselves will likely be modified as part of the #3107 cleanup.
issue
MAINT: replace private function in favor of numpy builtin.#TITLE_END#Replaces `_truncate` which was used in spectral_graph_forge to limit values to the interval `[0.0, 1.0]` with `np.clip`.
issue
MAINT: Remove instances of np.matrix used in internal code.#TITLE_END#Partially addresses #3107.  Removes the use of numpy `matrix` objects from the implementation of two functions, one private and one public.   * 26e1e60 re-implements `_tracemin_fiedler` using `@` and `np.atleast_2d` in place of the corresponding functionality provided by the `matrx` class.   * ac7678d does the same for the public `attribute_ac` function  This PR does introduce subtle differences in un-tested behavior. For example, the `asmatrix` function is not strictly equivalent to `atleast_2d` w.r.t. shape checking: the former will raise an exception if the input array has ndim > 2. In the case of `_tracemin_fiedler`, it is only intended for internal use and I didn't notice any codepaths that could generate input arrays with `ndim > 2`. For `attribute_ac`, there is a reduction in the amount of input checking, though the necessary input is clearly documented. I'd be happy to add additional input checking (and corresponding tests) to more closely mimic the exact behavior of `asmatrix` for the inputs to these functions, but since the behavior (i.e. behavior with non-2D input arrays) was untested, I wasn't sure that it was necessarily desired. LMK what you think!
issue
MAINT: Update from_graph6_bytes arg/docs.#TITLE_END#Closes #4032  The argument for `from_graph6_bytes` was originally named "string" but the function expected a bytes object. Fixes the mismatch between implementation and naming convention by updating argument name from "string" to "bytes_in". Updates associated docstring accordingly.  More extensive changes are possible (e.g. adding an `isinstance` check), but IMO updating the variable name/docstring signature is sufficient to resolve the ambiguity in this case.
issue
DOC: Add testing bullet to CONTRIBUTING.rst#TITLE_END#Adds a step to the procedure for first-time contributors in the contribution guidelines that encourages users to run the test suite locally. Links to other sections on setting up the build environment and testing later in the same document.
comment
Okay, this one looks ready - thanks @MridulS @amcandio and @dschult !
comment
`nx.info` has been deprecated, but the `describe` method has not yet been added. Suggestions for what should be in the `describe` summary are welcome!
comment
I think there is general agreement here on adding a `describe` method. Now the question is 1) what should go in it and 2) how should the output be formatted. Proposals welcome!
comment
Thanks for reporting - this is reproducible and I think a subtle bug related to attribute names. For example, if you rename the attribute "x" instead of "pid", you get the expected behavior. I suspect there's a fuzzy-match in the code looking for "id" (which is a "special" keyword to gexf) that is grabbing more than it should.
comment
Thanks @dschult - indeed after digging through the [gexf 1.3 schema](https://gexf.net/schema.html) `pid` is a reserved keyword for specifying parent node id. So - the code is indeed working as intended according to the GEXF standard - the issue in this case is an unfortunate naming collision with a user-defined attribute name. Switching to e.g. `unix_pid` should resolve the specific issue.  In terms of what can be done about this in general, I'm -0.5 on doing input validation on all user-defined names as it would add complexity, slow things down, and add more maintenance headaches. I think a more prominent warning in the docstring about reserved names would be worthwhile though.
comment
This one has been stale for a while, any plans to move forward @aphedges ? FWIW I'm not sure I'd consider the lack of determinism a defect - there are a lot of places in NetworkX where set and other non-order-guaranteeing semantics are used.  Do the other referenced algorithms guarantee order? If so, maybe they should be implemented and can be used via a `method=` kwarg?
comment
This one's been stale for a while so I'll go ahead and close it - please feel free to reopen/open a new issue/PR if interested in rekindling the discussion!
comment
I think getting rid of the erroneous exposures of the top-level namespace (e.g. `from networkx import nx`) *should* be straightforward: auditing the `__all__` attributes for the modules/pkgs should do it, though this might be pretty time consuming. It'd be nice to have for 2.6 but I don't think it's critical.  Changing the import style to use relative imports would probably also fix this issue, but would likely be more churn. IMO that feels like a 3.0 task, but perhaps only to me because I don't have a real sense of what should be done and how long it would take!
comment
I think this issue has been largely addressed, at least in practice, with the advent of lazy imports and standardization of the import conventions (i.e. numpy as np, scipy as sp, etc.) in #7821.  The relative vs. absolute import discussion can be picked up again, but I think it fits best in a new, dedicated issue/discussion!
comment
We should probably revisit this discussion in light of the proposed pydot deprecation. If `pydot` is removed from the equation, that leaves the `bipartite` loading functionality. Assuming the mirrored functionality is sufficiently different from its non-bipartite version, I think leaving maintaining it in the separate `bipartite` namespace makes sense. Of course, we should review to be sure this is actually the case!
comment
Yeah it gets pretty tricky... my initial attempt at categorization would be that "visualization" deals with viewing graphs live/interactively, whereas "readwrite" deals with loading/saving/transporting graphs.  Even this distinction breaks down in some cases though: for example, some visualization modalities use an intermediary text representation for the graph like LaTeX or PyDot. Then, in order to actually *visualize* the graph, you need some external backend (a latex engine or graphviz, respectively) to convert the intermediate format into an actual image.  All this is to say - I don't think there's a clear-cut way to organize things, nor would it be worthwhile to get *too* hung up on where things live. I'd just as soon approve just about any proposal in order to avoid a potential bikeshedding trap :)
comment
This one's been stale for a while so I will go ahead and close it. I think we can improve the situation on the documentation side by ensuring e.g. pydot is visible from multiple places - see #8204 for instance.
comment
This one's been stale for a while without any discussion so I will go ahead and close it. Please feel free to reopen or open a new issue/PR if interested in attempting to rekindle discussion!
comment
In addition to making docs more descriptive, it seems like adding explicit wording about `compose` in a more descriptive error message (see #3422) would improve the situation.
comment
There's a lot of info in the above discussion so I went ahead and extracted a few of the suggestions that are actionable into separate issues to try to break things up and make it easier to get eyes on them. See the issue links above.
comment
This one's been stale for a while, and at least a subset of some of the discussion points were addressed in #5827 and #5828. I will go ahead and close this as resolved, but please feel free to open a new issue for any remaining bits that can be addressed separately!
comment
There have been some significant developments since the conversation left off here; namely that some bigger projects like numpy and scipy have laid out a plan and started supporting type annotations. I would be in favor of using the approach taken in other projects as a blueprint here. Note that these projects specify the type annotations in separate .pyi files which are part of the main repository. I had already preferred this over annotating signatures directly as I think it offers a much clearer path forward for incremental adoption - the fact that some of the larger projects in the ecosystem have taken this approach only reinforces this opinion.
comment
> Can you go into some detail as to how it helps?  Having the annotations separate from the code decouples the development of the types from the code itself. As mentioned above, I think this is better for incremental adoption - we can have a ton of churn as we figure out what works without having to worry about touching the code itself, breaking `blame`, etc. Once we've converged on a working set of types, then it would be very straightforward to move them.  > Normally, I agree that numpy is a project that should be emulated. However, when it comes to type annotations, numpy is a special case because numpy array annotations are extremely complicated. I don't think networkx's type annotations are anywhere near the same complexity.  I have very little experience with types, so I have no sense of what specific components make typing more complicated. IYO what specifically makes numpy more complicated and why you would expect to NetworkX to be simpler?
comment
> That's fair enough, but I wonder if on balance it's going to save you time or cost you time to do the stubs. Most projects are not going with stubs anymore--even for the initial implementation of type annotations.  To me the tradeoff of a less efficient workflow is definitely worth having the ability to work on types with more independence, especially during the adoption phase. Honestly I think the biggest limiter here is experienced reviewer bandwidth. Having the typing initially implemented separately in `.pyi` files significantly reduces this concern because changes that are only related to typing will be obvious. IMO the primary motivation for supporting typing is so that downstream users can type-check their code that uses NetworkX: @fmagin 's [example above](https://github.com/networkx/networkx/pull/4014#issuecomment-658281530) does a great job of illustrating. AIUI, the ability to support this is entirely independent of whether the type annotations are in-line or defined in `.pyi` files.
comment
As noted in #3988 , NetworkX is relying on typeshed for developing and maintaining the type annotations for networkx. The NX developers are incredibly grateful both for the typeshed project and all of the folks who have contributed/continue to contribute to the annotations there!
comment
Thanks for sharing this @fuglede . I am interested in this and trying to get a better handle on the landscape surrounding linear algebra-based approaches to network analysis - the examples and discussions you've referenced are very helpful. I agree with both bullets 1 and 2 - networkx already takes the "adapter code" approach for some linear-algebra based algorithms; e.g.   https://github.com/networkx/networkx/blob/99fc1bb6690ac1a45124db2a01c12fd64dcb109b/networkx/linalg/laplacianmatrix.py#L49-L57  In cases like this, the networkx function could serve simply as the "adapter", relying on csgraph under the hood for the computation.  I'm not sure how well this approach generalizes or whether there might be a better way to organize things, but looking at the overlaps between functions from networkx and csgraph seems the obvious place to start.  I also agree with your point about the value of pure Python implementations - it would indeed be a shame to lose them. I think an ideal solution would give users straightforward access to different implementations while minimizing the number of shims required to link different functions/data structures together. I plan to look into this more closely and hope to compile some sort of document that describes the current situation. Any thoughts/references/other inputs you may have are extremely welcome!
comment
In the intervening time since this discussion took place, NetworkX has added a [backend system](https://networkx.org/documentation/latest/backends.html) to enable exploration of different implementations of algorithms that plug into the NetworkX itself. The design of this system encompasses many of the ideas mentioned here.  A `networkx-sparse-csgraph` backend would be a natural place to start for folks interested in exploring integration of `sp.sparse.csgraph` in NetworkX. I will go ahead and close this issue, but please don't hesitate to open issues/PRs pointing to backend implementations!
comment
This one's been stale for a while, so I will go ahead and close it.  I think @dschult 's take in the previous comment summarizes the discussion well. Note also in the meantime that the proliferation/advancement of graph learning libraries and related technology has exploded. Any contributors who are interested in such things are of course encouraged to start discussions/propose ideas in the NX repo!
comment
This discussion is more relevant than ever what with the advent of the NetworkX [dispatching system](https://networkx.org/documentation/latest/backends.html), especially the [`nx-parallel`](https://github.com/networkx/nx-parallel) backend, and freethreaded Python!  Here's a snapshot of the current state of parallelism in NetworkX:  1. The `nx-parallel` backend is the primary place for developing/testing parallel implementations of graph algorithms for NetworkX. Those interested in developing and/or trying parallel implementations of algorithms are directed to [the nx-parallel backend](https://github.com/networkx/nx-parallel).  2. Re: freethreading - there have been discussions about adding native support for "naively" or "embarassingly" parallel algorithms in NetworkX itself specifically for freethreaded Python (e.g. Python-3.14t). This is still in the proposal/discussion phase and will likely be hashed out in a formal [NXEP](https://networkx.org/documentation/latest/developer/nxeps/index.html) document.  nx-parallel is the current landing spot for parallel discussions, so I will close this issue and direct those interested to open issues/PRs/discussion on the [nx-parallel](https://github.com/networkx/nx-parallel) repo!
comment
Thank you @fmagin for your nice summary, I think your take on the situation largely aligns with my own.  I'll go ahead and restate my stance here: I'm personally +1 for adding stub files (`.pyi`) directly to this repository. This enables static type checking, which is currently not available in NetworkX.  > There is a genuine trade-off here, because if we put the stub files into the networkx repository as separate files, this still puts the burden of reviewing PRs on the networkx team itself.  This is the main concern from my perspective. Like all OSS libraries, NetworkX is review-bandwidth-limited. There isn't nearly enough review for the PRs that actually deal with the functionality of the library itself. The review consideration is also a big part of the motivation (for me personally at least) to use stub files instead of inline annotations. That way, all of the typing information is self-contained, and the review pattern can essentially be: "this PR only touches `.pyi` files, so we can be more aggressive about merging". This is the approach that was taken by NumPy (another community I participate in) and IMO it worked very well.  > I'm doing this because I have the gut feeling that this is the better distribution of effort across the ecosystem, as long as the networkx project would only accept stubs.  I'm not fully aware of how typeshed works, but this sounds like a good approach - anything that helps provide review specifically for annotations sounds good to me. Again, I'm +1 on adding stubs directly to networkx itself as well.  > I think some of us were pushing for types in networkx in a bit too strong way, and I think it comes down to this conceptual mixup between a graph library and a software project built for research on graphs.   For my part, I wouldn't way that anything is necessarily "too strong", but these conversations in the past have always seemed to stall on the specifics. Type annotations have been discussed many times in community meetings and it's generally something networkx would like to support. However, as has been stated many times before, the developers *do* have an opinion on an adoption plan that the community is most comfortable with - i.e. starting with stub files instead of inline annotations. In the past, this has been met with a lot of push back from those who seem most interested in type annotations (i.e. the participants of these conversations) so we've been at an impasse and generally haven't been able to push forward.    
comment
Official types for NetworkX are available on [`typeshed`](https://github.com/python/typeshed/tree/main/stubs/networkx) and can be installed (explicitly) with `pip install types-networkx` and are automatically grabbed by type-checking tooling. The NetworkX developers are incredibly grateful for `typeshed` and all the folks who contribute to maintain the networkx type annotations!
comment
This is an interesting discussion, thanks @dschult and @OrionStar25 ! Having gone through the above I'm wondering if there's any action-items here that we could do to improve NetworkX - perhaps a documentation example showing how `max_flow_min_cost` can be used to solve a problem like the one above? Are there any multiedge flow algorithms that would be nice to consider for addition to NX?
comment
This has been stale for a while without there being any obvious action items, so I'll go ahead and close it. If folks run into this (or similar) issues and are interested in e.g. improving the docs or rekindling discussion, please open a new issue!
comment
Thanks for bringing this up! This one's been stale for a while with no activity, so I'll go ahead and close it. Feel free to reopen / open a new issue/PR if folks would like to attempt re-kindling discussion!
comment
I reverted the `iplotx` example on a hunch as the doc failures started around that time and indeed it seems to be green again: #8299 .  I have no idea why this would be... I have no problems locally and there's no real information in the CI build logs. Without any real info to go on, my initial hypothesis would be threading/parallel processing - circleci (all ci services really) often behave in unexpected ways when it comes to parallelism. We have always run with [`OMP_NUM_THREADS=1` in CI](https://github.com/networkx/networkx/blob/0f0c1e8534847dc70919a410d15249469c013550/.circleci/config.yml#L63) to deal with implicit threading in the doc build, e.g. with BLAS called via numpy/scipy. @iosonofabio does anything come to mind why this would interact badly with `iplotx`?
comment
> You are right at the edge of your CI allowance and any additional example would kill it  I think we're actually well over our allowance but for now things are still working 🙃   > iplotx depends explicitly on pandas, numpy, and matplotlib. Maybe one of the first two trips CI? E.g. maybe it's trying to compile either from source?  These are the same dependencies in networkX's `default` list, and everything should be coming from pypi (with the exception of the backends) so there should be no building from source.  
comment
FWIW I don't think it's a timing/performance thing - the iplotx example is very fast relative to the others; 3 orders of magnitude faster than the slowest example. It would be very likely that this would be the one to push it over the edge - especially since it looks like the CI terminiation is ocurring in the middle of the gallery generation.
comment
> I'm pinning versions of numpy/pandas to >=2.0, but that could change. Maybe numpy 2.0 is the culprit?  It shouldn't be... NX has the same `pandas>=2.0` pin already and while we technically still support numpy~1.25 (side note: we should probably bump this per [spec0](https://scientific-python.org/specs/spec-0000/)), the codebase has been fully numpy 2.0 compliant for several releases.
comment
According to the [CI build logs](https://app.circleci.com/pipelines/github/networkx/networkx/12243/workflows/d6a904ee-61f0-4b51-b0b6-e14c4fdfd0b9/jobs/21334) (expand the "Install" step just before the failing `build docs` step), numpy 2.3.3 is the version being picked up. We don't have any upper-bound pins anywhere so this matches the expectation that we get the latest released version for all dependencies.
comment
> I'm not sure if it's great fun or no fun at all...  Just to spice things up, it seems to be intermittent - this [latest set of completely unrelated changes](https://app.circleci.com/pipelines/github/networkx/networkx/12266/workflows/3aa41f87-823f-483d-888d-0cab7f6bb732/jobs/21406) passes just fine!
comment
I've done some more debugging[^1] and have a new hypothesis. I think the underlying issue that is causing the docs CI to bork during the sphinx gallery build is running out of memory. The main reason I think so is the following bit from the failed logs:      make: *** [Makefile:48: html] Killed/algorithms... [ 76%] plot_parallel_betweenness.py  The key thing to me is that `Killed`, which I most often have come across when running out of memory. Python of course has a `MemoryError` exception, but that's only reliably raised when the memory issue arises within the Python process. There are many other ways (e.g. binary extensions, other concurrent processes) to hit RAM limits without this exception.  The default resource class we use on circleci only has 4GB of ram. None of the examples we run are particularly memory intensive, but then again 4GB really isn't a lot in the context of a typical PC these days.  My current hypothesis, then, is that we are hitting RAM limits likely due to running the `coverage` and `build-docs` workflows simultaneously *in the same container*. This would explain the stochastic nature of the failure. It also jives with the expectation of multiprocess-based parallelism, which requires spinning up multiple Python processes, each of which has a non-trivial memory footprint (likely O(100MBs)).  Now I haven't *proven* this to myself, but I think this is a likely explanation given the evidence we do have. If we assume this is true, there are a few things we could do about it:  1. Raise our resource_class on circleci to "large" to get 8GB of RAM. This is a nice option, but would also 2x our credit burn rate  2. Try to limit the memory footprint of examples.  My approach would be to try 2) first. I'm also holding off on more serious debugging as it looks like this 2) is a viable path forward. However, if the issue persists the next experiment I'd run is a dummy job on CI with tmux and htop installed to get a sense of real-time resource utilization in the running job.  I believe #8291 and (especially) #8305 are both meaningful steps towards 2)!  [^1]: Including using circleci's ssh-into-a-running-container feature, which is nice.
comment
This is the part I'm unsure about - we have defined them as separate "workflows" in the same "pipeline" so in principle they should already be separate. However, how resource management policies map onto these terms is (IMO) always opaque in CI-land. The assertion that we're hitting memory limits due to shared resources across the coverage+docs jobs is really just a guess on my part based on the fact that this error only cropped up recently - right around the time we moved the coverage job over. In principle these jobs are run in separate containers, but I don't know whether the resource limitations imposed apply across "pipelines" or "workflows" etc.
comment
Things have been stable for a few weeks now without much in the way of changes on our part, though the linked PRs that were merged can only have helped.  I'll go ahead and close this as "resolved" - if circleci gets wonky again we can open a new issue. Thanks @dschult and @iosonofabio for the helpful discussion, and apologies for the high-volume and unfortunate timing!
comment
> You're absolutely right that subgraph isomorphism can be used to retrieve occurrences of treelet-like motifs in graphs, and it offers a very general and flexible approach. However, this treelet implementation follows a more constrained, ad hoc strategy which, indeed, isn't as general, but it's much more scalable.  Thanks for providing the context! My personal opinion is that the formulation here based on the specific subset of motifs is less of a fit for NetworkX than a more general formulation, irrespective of boundedness (which is not the same as performance!). That's not to say an efficient approach for computing such motifs isn't valuable; just that it might be better suited to a library that focuses on chemoinformatics where these motifs are (apparently) quite prominent. That's just my two cents though!  > The general subgraph isomorphism problem is NP-hard, while the treelet extraction method we implemented has a worst-case complexity of O(n·d^5) (with n the number of nodes and d the maximum degree), which is linear in n for graphs with bounded degree — making it practical even for large graphs.  This would be really interesting to probe... is there a compelling "real-world" example that we could highlight? IMO it'd be a great tutorial to take this application (i.e. a graph representing a chemical compound) and "decompose" it into the sub-structures that might be of interest for that chemical. It's outside the scope of this PR, but if this is something you might be interested in I think it would make a great addition to [`nx-guides`](https://github.com/networkx/nx-guides)!  Re: simple bounded paths - I tend to agree with @dschult . At face value I don't see any particular inefficiency in using `all_simple_paths` (leveraging the fact it's a generator) with `cutoff` and multiple `targets`. 
comment
> If you would prefer, I am happy to reword the proposed parameter descriptions. If you don't like mentioning the word edges, maybe we could just say something like (number of nodes) - 1  Yes - it's the term "edges" that's still the sticking point for me. Your point about the typical interpretation is well-taken, but "edges" means something specific in practice in NetworkX so for the sake of precision and consistency I would like to avoid that term specifically. Re-wording in terms of `number of nodes - 1` seems fine to me!
comment
I'm not sure this is correct - each of the degree measures normalizes by `len(G) - 1`, but an `nbunch` would seem to throw the factor off. Thoughts?
comment
>The degree centrality is the degree divided by len(G)-1. len(G)-1 is a constant in a certain network, so it may just change the scale.   Right - to follow up, my question is: is this correct? Does degree centrality have a meaningful interpretation when the calculation is being limited to a subset of nodes like this (but the scaling is not). In principle, another way to do this would be to use an explicit subgraph, e.g.  ```python >>> nx.degree_centrality(G.subgraph(nbunch)) ```  Note that this will (depending on nbunch) give you a different answer than `nx.degree_centrality(G, nbunch=nbunch)` as propsed here. So the question is: does this matter? Are there use-cases for each? In general I don't like the thought of having to correct for an arbitrary scaling outside of the function.
comment
> But I am still not able to complete 1 test (ci/circleci: image artifact Pending — Waiting for CircleCI ...). Can you please tell me what I am doing wrong?  @Astroakanksha24 you're not doing anything wrong, that looks like an unrelated CI blip!
comment
Thanks @acarbonetto - looking at the rendered docs my initial impression is that the "additional parameters" bit is too much extra information for the docstring. My preference would be to limit the additional docs to a link to the related backend docs as is done with the other backends. In an ideal world, the link target would take you straight to the neptune docs page for the linked function, but I'm not sure whether the infrastructure is in place to make that easy. Just my 2 cents.  Since this PR now also includes the doc cross-linking, I'm going to go ahead and remove my approval until we've had time to discuss that!
comment
> When is 3.6 planned to be released?  There is no exact date, but NX typically does a release in October following the Python version bump.
comment
Thanks @Peiffap for the round of review! I will merge this to get the largest bit of the overhaul in so folks can start focusing in on individual todo's/idea exploration in follow-up PRs. Many thanks @dschult for the excellent PR and accompanying discussion, and to @Peiffap for the nice review!
comment
Continuing the work in #8318 - thanks for getting the conversation started @sourabh-sudesh-paradeshi !
comment
Thanks for raising @Aka2210 - just for reference, the deprecation strategy can be found in  [step 7 of the development workflow](https://networkx.org/documentation/latest/developer/contribute.html#development-workflow). It's basically what you've already outlined, with a few bits of extra info re: warnings filters and where to record deprecations!
comment
On a related note - it's not recorded anywhere but one lesson I've learned is that it's best to keep deprecations in their own, dedicated PRs. The reason being - if users come back with a lot of feedback that they don't want a function deprecated, then this makes it easy to revert.  I mention this here in case you were thinking of adding a gallery example all in one step. My (slight) preference would be for that to come in two separate PRs: 1 for the deprecation, and a second for the gallery example!
comment
> I just had a small question: I noticed you updated test_steinertree.py to wrap the metric_closure call in pytest.deprecated_call(). I didn’t see this step explicitly mentioned in the deprecations guideline — did I miss something, or is this more of an established testing convention for deprecated APIs?  You didn't miss anything - it's not required to do and doesn't apply to every case so it's not explicitly mentioned in the contributor guide. It's more of a double-check that the tests which touch metric closure have been reviewed at the deprecation stage!
comment
> What's the harm in using np.testing/what are the advantages of doing assert np.allclose(...) like you suggest instead?  There's no harm, it's just the latter pattern (i.e. assert statements as opposed to `assert_` fns) is more idiomatic. The numpy `assert_` functions are very old and follow patterns/address shortcomings of testing frameworks which no longer exist.
comment
> but replacing that in a similar way looks like assert (a == b).all() so I held off on it for now.  I think a better replacement would be `assert np.array_equal`
comment
> What does NumPy suggest about using assert np.allclose vs np.testing.assert_allclose?  Here are NumPy's guidelines for test writing: https://numpy.org/doc/stable/reference/testing.html#writing-your-own-tests  There is no specific recommendation for `assert all_close` vs. `np.testing.assert_allclose` - the only specific recommendation is that the builtin `assert` statement is preferred over the legacy `numpy.testing.assert_` function.  Personally, I don't feel strongly about this - IMO the `numpy.testing.assert_*` functions make sense for libraries where numerical precision has the potential to vary e.g. across platforms. NetworkX has no binary extensions and instead relies entirely on Python/numpy/scipy's test suites to have that handled. Therefore I feel comfortable using `assert np.allclose` as opposed to `npt.assert_all_close` - we generally don't need the extra bells-and-whistles, and very rarely (if ever) use tight precision bounds to catch floating point precision issues. Similarly, I don't personally care about the test failure reporting - if one of these tests starts failing it is (IMO) much more likely to be due to breakage in the code rather than precision issues. In fact I don't think the motivation from scipy/scipy#23592 really applies to NX. The specific text of the recommendation:  > From docstring of np.testing.assert_almost_equal:            It is recommended to use one of `assert_allclose`,           `assert_array_almost_equal_nulp` or `assert_array_max_ulp`           instead of this function for more consistent floating point           comparisons.  I don't think this is relevant for NX's use-cases where we're not trying to probe precision limits but instead trying to determine whether objects are "almost equal" with tolerances much wider than numerical precision.  Given all this, I think my actual preference would be to just leave these tests as they are! But, if others feel strongly about changing things I have no real opinion in what pattern is used instead.
comment
> I'd be happy either way: perhaps keeping it small and merge and then worry about the rest in a future PR?  This would be my vote!  > Apart from the docs CI which looks likely to be unrelated  Agreed this is unrelated, it's just unfortunate timing - see #8292 . Apologies for the inconvenience!
comment
It looks to me like excluding doctests makes essentially no difference in the CI runtime - I'm seeing ~9.5 min both with or without doctests. This is not necessarily surprising since we run things in parallel and the doctests *should* be both quick in terms of runtime and simple in terms of computing coverage[^1].  Given these results, I'm inclined to leave the doctests in the coverage job. I personally think it's fine to treat the doctests as part of the "official" test suite - pytest makes this very easy after all.  [^1]: We expect the doctests to focus primarily on the function they test, so there are fewer instances of tests that touch hundreds of lines of underlying code than some of the more complicated tests from the test suite.
comment
Thanks @dschult - I concur with this diagnosis and think it fully addresses the question, so I will go ahead and close this as resolved.
comment
This has been partially addressed and the remainder is out-of-sync, plus there's a newer PR advocating for the same thing so let's focus the discussion there. Thanks for the proposal @Tortar !
comment
> All the changes you're describing are in https://github.com/networkx/networkx/commit/1651b6025ea7fda45bade221d3fdbab900d1c9fb; I'm happy to revert it if that makes the update more likely to get merged!  That would be my vote!
comment
> What is the motivation for using the itertools version instead of the recipe?  To me anyway the main improvement is the more direct connection to a Python builtin. This is admittedly a very tenuous improvement, but `itertools.pairwise` is new as of Python 3.10 so code-readers may be less aware of it than other itertools functionality (at least I wasn't until @eriknw pointed it out!)
comment
> Could be related to matplotlib backend not clearing canvas after previous use in another test.  Yes this would be my suspicion as well.  > Not sure where the randomness is from though.  If `pytest-randomly` is installed (which I expect is common for nx devs as it's in the `test-extras` requirements) then it will re-order the tests by default and is likely to surface issues like failed figure/axis object cleanup.
comment
> The problem is that the induced subgraph isomorphism returns False. To get the result "True" we would need to be looking for a monomorphism.  Thanks @dschult - I am very prone to mixing up the details, *especially* when it comes to isomorphism. This makes sense and I'd be happier without the test case I had originally proposed due to my faulty understanding!
comment
> supposed to be the case? I'm not familiar enough with these centrality measures to tell, but it does strike me as slightly weird... Note that this PR does not constitute a change in that behavior. :)  My hypothesis would be that it was copied over from a pre-existing `_rescale` fn for the node betweenness centrality. The results of [blame](https://github.com/networkx/networkx/commit/d92f21067857b56ca7a7369ecd3cf1a0cec73752) sees to suggest this was the case, though it's not definitive.
comment
> I'm happy to make the change if that's what you believe is bes  :+1:   > but I'm curious to better understand your reasoning for wanting to avoid the (imo minor) increase in complexity here, especially since the more complex parts shouldn't need to be modified again in the near future.  Here are a few reasons  1. The simpler the recipe, the simpler it is to debug/fix if/when something unexpected happens  2. The CI workflows serve also serve as developer documentation. If you don't remember how to run a certain workflow, referring to the CI configs is often a good reference (this is true for many projects, not only NetworkX).  > especially since the more complex parts shouldn't need to be modified again in the near future.  This is not necessarily true - if/when a CI service decides it's time to stop/limit their free offerings for open source, then the simpler the recipe the easier the migration.
comment
I went ahead and dropped the parallelism back down to 4 runners - my reasoning for doing so was looking at the [test durations with 16 workers](https://app.circleci.com/pipelines/github/networkx/networkx/11894/workflows/bad62e0d-a90d-4a31-a65f-631781dee1b2/jobs/20556) which show runtimes ~2.5x longer than [the single threaded runtimes](https://app.circleci.com/pipelines/github/networkx/networkx/11883/workflows/74cdc3af-d273-41b9-912f-f9623b564337/jobs/20529). Fewer runners seems to be more balanced, the relative timings for the listed runs are: 26m42s for single worker, 22m41s for 4 workers and 21m14s for 16 workers. My vote is: try 4 workers and see how close we come to our free allotment of compute from circleci. If there's headroom at the end of the month we can bump it up to more workers.
comment
> If runtime becomes a real issue  I think the highest-impact approach to "fixing" the coverage job is to get the `runslow` tests out of there. I suspect that at least some of the tests marked `slow` have no impact on coverage at all. So I'd propose something like:  1. Produce a coverage report now, as-is-, with the slow tests included  2. Produce one without the slow tests  3. diff the two coverage reports to identify the subset of functions where coverage is reduced when the slow tests are removed  4. For this subset of tests, see if we can't develop *new* unit tests that cover the missing lines with a more manageable resource load  5. Add these tests to get coverage back to where it was before removing the `slow` tests from the coverage job  6. Move the `slow` tests to a different, dedicated job in CI.  I haven't gone through this exercise myself so I don't know what steps 3 and 4 look like and how much churn such a project would actually be!
comment
@MridulS WDYT about this one, should we give it a try?
comment
@Yasserelhaddar thank you for your contributions. I would just like to leave a personal note here: while there's no official policy re: the use of AI-generated pull requests, I will say as a reviewer these types of pull requests are not inspiring and often take more effort to review than they are worth. If you're going to use code gen tooling, I'd at least encourage you to see if you can't identify the core of the idea from the generated PR and simplify both the PR message and the code itself into a minimal, clear and concise proposal for a change. IMO this is a very valuable skill worth cultivating.  Anyways - I don't intend to discourage you from contributing to open source, I'd just like to share my perspective as a reviewer of contributions. Note that everything here is my personal perspective and not that of the project or any other organization/entity!
comment
I think one result for the discussion above is that it may be worthwhile to just close the original issue! The whole idea of adding a benchmark is to be able to evaluate performance regressions, but if there are no (or at least no realistic) cases where the implementation details are probed by reasonable inputs then I think we can call this done!
comment
Thanks @Peiffap for diving in here as well. Given what we've learned, I will go ahead and close this as "not planned". Thanks for looking into this @Yasserelhaddar and @Peiffap !
comment
No worries, @Rhyd-Lewis - and thanks for the proposal!  As mentioned above, the first step will be to fix the style issues: at this point, it's not clear to reviewers how extensive the changes to the test suite actually are, as the modifications are intertwined with style/reformatting changes. Once those are rectified (see procedure in the previous comment, or [the contributor guide](https://networkx.org/documentation/latest/developer/contribute.html) it will be more straightforward to tell whether there are any backwards-incompatible changes and to what degree (ha!).   At a higher level - the reason why this constraint exists is that these functions are already in use by users for all sorts of applications. It's important to ensure that any changes that go into the library don't silently yank the rug out from under them and change the behavior of *existing* code, at least not without some form of forewarning.
comment
@mrecachinas thanks for this PR and #3886!  There will be a NetworkX release in the near future (O[weeks]) which will have `panther_similarity` in it. One thing we would like to try to confirm is that the API for Panther and Panther++ is consistent, and that future development in this PR won't necessitate changes in the `panther_similarity` API. For example, in one of the bullets above, you mention changing the name from `panther_similarity` to `panther_path_similarity`. Changing the function name/signature for `panther_similarity` is painless now, but will require a deprecation cycle after release. I just wanted to check to see how happy you were with the current function names and signatures. Ideally we'd be able to get the function names and signatures settled prior to the NX 2.6 release. LMK what you think!
comment
Re: naming, I agree with @dschult and @storopoli : **Panther** -> `panther_similarity` and **Panther++** -> `panther_similarity_pp`.  > I think we should leave generate_random_paths where it is until NXEP3 is figured out.   I agree. If it is likely that `generate_random_paths` will be moved or wrapped up in some other object, it might be worth making it "private" for the 2.6 release (i.e. prepend `_` to the name). That way, it will still be available but the `_` will indicate to users that there may be a name change/API reshuffle on the horizon. OTOH this may be overly cautious, and the arguments for moving it somewhere else may never come to anything. I'm ambivalent, but if others think that there is a very high likelihood that we'd want to move `generate_random_paths` to a new location in the near future, it's worth thinking about.
comment
TL;DR - I agree with @dschult , I'm +1 on having `generate_random_paths` remain fully public (no `_`-prepended) for 2.6 and beyond.  I think the points in @dschult 's points are the most compelling. One thing that I hadn't considered was that `generate_random_paths` is available in the top-level namespace (i.e. `nx.generate_random_paths`), so which pkg/module it is defined in is basically an implementation detail that isn't interesting for most *users*. It can be moved to pretty much any module, including a new one - as long as it's still imported into the top-level namespace, the usage-impact will be minimal. Of course, moving it would cause minor changes, e.g. where it can be found in the reference docs etc, but the search bar alleviates that concern.
comment
> I suspect the nondeterminism may be coming from "ties" when querying the KD-tree. I'm sorting after the ties, but I don't know how the KD-tree internally would handle ties/stability.  I see - in that case it's probably worth a note in the docstring.  On a related note - while reading through the [kdtree docstring](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) I noticed this:  > For large dimensions (20 is already large) do not expect this to run significantly faster than brute force. High-dimensional nearest-neighbor queries are a substantial open problem in computer science.  This seems directly relevant to our default choice of `D`, which is 50. IIUC, we're essentially constructing the tree out of an NxD array, which would be very inefficient. I see that the default `D=50` appears to have come from the paper, but perhaps we should consider a more reasonable (i.e. smaller) default given computational limitations. Note that KDTree switches over to brute force via the `leafsize` parameter, with a default value of 10. Again, IIUC, this means we're essentially getting no benefit from similarity vectors in this way.
comment
> I'm thinking we should probably merge this and figure out how to handle the KDtree issues in follow-up PRs. Thoughts?  Works for me, thanks for taking a look @dschult !
comment
Thanks for diving into the history @dschult !  While it may look strange in the code, unless it negatively affects the visualization I'm inclined to leave it as-is. @rudyarthur did you come across this because you had a visualization that looks appreciably different when this value is changed?
comment
> I can give some example code if you like.  This would be great, especially if there's a relatively minimal example that illustrates the behavior!  >  if all you care about is the final positions, it doesn't matter. Though from experimenting with different networks, removing it could make things converge quicker.  No convergence rate is definitely of concern! If it can be improved without degrading the layout results (likely difficult to judge as it's subjective) then it's definitely worth considering!
comment
Thanks for the exmample @rudyarthur ! This is a very nice illustration of the issue and certainly demonstrates the issues w.r.t. convergence. When I run locally with thresh == 0.01 I see vibrations all the way til ~iteration 200 wherease with thresh=0.1 things seem to stabilize before iteration 100.  As you've noted, this is very likely dependent on other factors like the network itself/initial conditions, but this is at the very least a compelling illustration of a subtle phenomenon.  IMO the effect of this threshold warrants further investigation - if the demonstrated convergence behavior generalizes to other graph examples I'd be inclined to modify the value to improve performance.  @HirokiHamaguchi I know you've recently been working on force-directed layouts, any chance you have an opinion/insight here?
comment
Thank you very much @HirokiHamaguchi for yet another excellent writeup - your investigation/insights (and excellent summary) are very much appreciated!  Given your findings and the previous discussion I'd be happy with either resolution as well. Even if things are left as-is, I think it's probably worth adding a comment to the line in question (preferably with a reference to this issue so folks can benefit from the excellent work in this thread).
comment
I've added an example to #7856 to address this use-case!
comment
Thanks @MridulS , an environment issue would be my first guess too. If it turns out not to be then I'd consider opening an issue to the linter!
comment
I don't think there's anything actionable here; I suspect an environment issue but without additional information we won't know for sure. I will close this for now, but if the issue persists feel free to open with updated info about the environment/installation method.
comment
> No. The center described there is maximizing the eccentricity. A tree centroid is the point or points that have minimum "weight" (not to be confused with edge weights -- this weight is just the number of nodes accessible down each branch).  In that case then I think we need to expand the test suite - the current tests give the same results for every case if you replace `nx.tree_centroid` with `nx.center`
comment
Adding the `close?` label for now - see https://github.com/networkx/networkx/pull/7965#issuecomment-3053379335 for discussion!
comment
Adding the `close?` label for now - see https://github.com/networkx/networkx/pull/7965#issuecomment-3053379335 for discussion!
comment
Adding the `close?` label for now - see https://github.com/networkx/networkx/pull/7965#issuecomment-3053379335 for discussion!
comment
Adding the `close?` label for now - see https://github.com/networkx/networkx/pull/7965#issuecomment-3053379335 for discussion!
comment
See also #6896 , which proposed to add another extension of A*. Given the interest in this particular sub-domain, it'd be great if we could develop a more comprehensive understanding of the lay-of-the-land and figure out if/how/whether such extensions can be offered in a coherent way!
comment
Just wanted to ping on the A* projects - are folks still working on them or is the semester over?  My personal opinion is that absent a strong, dedicated "A* champion" (or champions) who is/are willing to review these and engage with the NetworkX community, these are unlikely to move forward. Bear in mind that any algorithms that are added to the library become the responsibility of the NetworkX community to maintain in the future. IMO the biggest blocker here is a lack of expertise in the existing pool of active reviewers/community members to feel comfortable making this promise.  Absent this level of engagement, I am inclined to close them and will label them as such until others have a chance to weigh in. Note that my doing so is in no way an indictment of code quality or a judgement that these are not valuable algorithms, but simply my personal judgement that at present time such additions would be unmaintainable. It would be wonderful if that were to change! If there's any interest at all in getting more involved in NetworkX, we would strongly encourage it! Looking at existing functionality and opening issues/PRs is a great way to familiarize oneself with the codebase and build ties with the community for future proposals. Checking out the [community meetings (scroll to the bottom for rendered calendar)](https://scientific-python.org/calendars/) is another great way to get involved if you can make it!  
comment
>  I am wondering if we can use pre-commit hooks like: pydocstyle and darglint to ensure this doctoring style alignment:  This has been proposed before and the general consensus was that the docstring linting tools are extremely noisy, see e.g. #7554
comment
> What's the current tradition about Yields vs Returns in the docs?  Good point - I'm personally still not sure what to do with the pattern of "fn that returns an iterator" as is the case here. It's *technically* a function in the sense that it promptly executes and returns, but from a user's perspective the result is an iterator regardless of whether it was *defined* as a generator or *returned* from a procedure that creates iterators. At various times I've convinced myself that both sides of the argument are correct (to `Yields` or not to `Yields`) :shrug:   My vote is to punt on it in this PR!
comment
I'm -1 on adding a method to the classes. Documentation improvements are welcome
comment
Are there specific features you need that are not currently supported? If so - feel free to raise issues about them. Implementing/fixing the graph formats happens more on a rolling basis rather than reimplementing new standards from the ground up. Therefore, specific features that are wrong/missing are helpful!
comment
Your use-case may already be supported - I know there was a recent PR to bolster support for dynamic graphs in GEXF (#7914) but that was a corner case... IIRC most of the functionality for timestamps is already in place.
comment
@tschoellhorn is this still an issue in nx v3.5? That should improve support for "dynamic" graphs in gexf but that may not have been the original issue. If you have a minimal reproducing example to illustrate the bug that'd be helpful!
comment
> Could you point me to a good existing gallery example I can use as a template for the format and structure? Thanks  I agree with @dschult , the Davis club example is a good starting point. I also agree that the `Algorithms` section is the correct place for a trust rank example.  There is additional information in the contributor guide related to [adding gallery examples](https://networkx.org/documentation/latest/developer/contribute.html#adding-examples) that may also prove useful!
comment
> Thanks @rossbar for the feedback and changes! Is there anything else needed to move forward with merging?  I left a few suggestions above which, if you agree, you can accept using the `add suggestions to batch` button; or you can modify/comment as you see fit. If you have no objections to the suggestions I've left I'm happy to just push those up too if it's easier, just LMK!
comment
Thanks @Peiffap - this is too complex to have been caught by pyupgrade.
comment
I tested a few configurations locally and this is due to the recent release of [pytest 8.4](https://pypi.org/project/pytest/) which came out on June 2nd. FWIW there was a warning in place which describes the problem:  ``` networkx/drawing/tests/test_pylab.py::test_house_with_colors   /.virtualenvs/nx-dev/lib/python3.13/site-packages/_pytest/python.py:163: PytestReturnNotNoneWarning: Expected None, but networkx/drawing/tests/test_pylab.py::test_house_with_colors returned <Figure size 460.8x345.6 with 1 Axes>, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`? ```  FWIW - I recall there was an explicit reason why we didn't run the `pytest-mpl` jobs in all CI runs where pytest was installed. I don't recall the exact reason though - perhaps @jarrodmillman remembers?  If this is still the case, then I think the right solution is to break these out into their own test file and add a module-level `pytest.importorskip("pytest_mpl")`. Alternatively, if pytest decides to bring the warning back (cf. pytest-dev/pytest#13477) then we can just add a `warnings.simplefilter` for the pytest.PytestReturnNotNone warning (I've tried this locally and it works).  If it's no longer the case, then let's go with #8094 .
comment
> however, that paragraph also states that the node cut is a set S , such that S ⊂ V , so the set of all nodes would not be a valid node cut.  IIUC the objection here is about set semantics, i.e. that `S` must be smaller than `V` (right?) I'm not a mathematician but AIUI equivalent sets are valid subsets of each other - there's no built-in requirement that `S` have lower cardinality than `V` to be a valid subset. Indeed this is borne out in Python's set semantics:  ```python >>> s1 = {1, 2, 3} >>> s2 = {1, 2, 3} >>> s1.issubset(s2) True ```  Relating this back to the original point, I believe this supports @dschult 's interpretation for complete graphs.
comment
I'm -1 on adding a function for this to NetworkX solely based on the fact that I don't think "pure descendants" is a well-defined/recognizable term. The concept seems sufficiently niche that I think it's best to leave users to address their needs with existing functionality.
comment
Thanks for bringing this up @JohnyXia001 . If I'm understanding correctly, the issue is that the current implementation in NetworkX has O(N^2) complexity whereas [references for Kosaraju's algorithm](https://en.wikipedia.org/wiki/Kosaraju's_algorithm#Complexity) note that it can be implemented with linear time if the input is an adjacency list. Is this correct? If so, a refactoring of the algorithm to improve scaling would certainly be welcome.
comment
This one has been inactive for a while - the best way to push this forward IMO would be to e.g. try the above proposed update. Would you be interested in submitting a PR @JohnyXia001 ?
comment
> I am curious if the documentation here allows moving gifs in addition to static png images.  Agreed - according to the sphinx gallery documentation, [animations are supported](https://sphinx-gallery.github.io/stable/auto_examples/plot_8_animations.html#sphx-glr-auto-examples-plot-8-animations-py) so this would be worth looking into!
comment
> maybe try spectral 2D and 3D layouts checking if they are visually OK but faster.  The layout is not the bottleneck, it is the rendering. It's possible that using `set_data_3d` in the updating function instead of doing a full redraw could help, but it's no guarantee.
comment
For other reviewers, here's the updated [gallery](https://output.circle-artifacts.com/output/job/296a75af-8f2f-405b-8c40-cb0540d86793/artifacts/0/doc/build/html/auto_examples/index.html#d-drawing)!
comment
This one has been stale for quite some time so I will go ahead and close it. If there's interest in picking it up again, feel free to reopen the PR after addressing the first round of review!
comment
I'd definitely not want to pollute the API with functions for each option. I'd be much more inclined to add docstring examples.
comment
> Another way to do a multi-target is to add a sentinel node and multiple > cost-0 edges connecting from each of your targets to the sentinel node. > Then you call Dijkstra with one target and drop the last hop.  Thanks @amcandio , this is really slick! I'd advocate for documenting it!
comment
>  I'd be happy to modify the PR to implement a unique_triangles(G) generator instead of a counting function, if that would better align with NetworkX’s design.  It would be more compelling to me at least! Re: naming, I'd propose `all_triangles` which matches a naming pattern used elsewhere, see e.g. `all_triads`. In fact, that's another bit of existing functionality that meets this need: `nx.all_triads(G.to_directed())`.
comment
Thanks for updating the benchmarking results @amcandio ! They give me pause though: the speedups for the long paths are obvious and undisputed, but it also looks like there are real performance regressions for small, non-path-y graphs. IIUC, that would mean a significant performance hit for a user who were doing something like e.g.  ```python for G in my_giant_list_of_relatively_small_graphs_with_say_10_nodes:     nx.single_source_dijkstra(G, source=0, target=10) ```  Is that still the case?
comment
Indeed #7739 fixed this for `bar` so `arc3`, `angle3` and `bar` are currently supported. @dg-pb any thoughts about the remaining 2 connection styles?
comment
Thanks again for this @Maninder-sd . Given the discussion in #8014, I'm inclined to close this one - not because the proposal isn't good (very nicely done!) but because I think the underlying feature request is not necessarily a great fit for the `generate_random_paths` function.  I agree with your comment on a probabilistic approach, though whether that fits in networkx or a more specific causal inference library is another topic for discussion!
comment
>  The parameters arrowstyle should be edge_arrowstyle and arrowsize should be edge_arrowsize. Should I add this fix in the same pull request?  Great catch @Maninder-sd - yes please do make those changes here as well!
comment
I'll go ahead and put this in so that the dev docs are updated at least, thanks again @Maninder-sd !
comment
@auderson can you share the following additional info:  1. The full environment where you're seeing this error, i.e. the output of `mamba list` 2. Whether you are attempting to use a networkx backend such as nx-cugraph 3. Whether you have any environment variables related to backends set, e.g. the output of `printenv | grep NETWORKX`
comment
Gentle ping @auderson - any chance you can provide the additional information? Without it I don't think folks will have much chance of chasing down the cause as no one who has tried has been able to reproduce the issue.
comment
I have access to a machine with Python3.10 and nvidia GPUs - here's a running log of things I've tried to reproduce this issue:  1. Ensure there is no problem with just networkx installed  ```bash $ python --version Python 3.10.12 $ python -m venv nx-env $ source nx-env/bin/activate $ pip list  # ensure the environment is empty Package    Version ---------- ------- pip        22.0.2 setuptools 59.6.0 $ pip install networkx==3.4.2  # install networkx $ pip list  # Check environment is as expected Package    Version ---------- ------- networkx   3.4.2 pip        22.0.2 setuptools 59.6.0 $ python -c "import networkx as nx"  # Test import ```  This works as expected.  1a. Try installing `torch` in the same environment, which will pull down `nvidia-*` libraries as dependencies:  ```bash $ (nx-env) pip list  # ensure the environment has only nx 3.4.2 installed Package    Version ---------- ------- networkx   3.4.2 pip        22.0.2 setuptools 59.6.0 $ pip install torch  # Install pytorch to get all the `nvidia-` dependencies $ pip list ``` <details>   <summary>Output</summary> <pre> Package                  Version ------------------------ --------- filelock                 3.18.0 fsspec                   2025.7.0 Jinja2                   3.1.6 MarkupSafe               3.0.2 mpmath                   1.3.0 networkx                 3.4.2 nvidia-cublas-cu12       12.6.4.1 nvidia-cuda-cupti-cu12   12.6.80 nvidia-cuda-nvrtc-cu12   12.6.77 nvidia-cuda-runtime-cu12 12.6.77 nvidia-cudnn-cu12        9.5.1.17 nvidia-cufft-cu12        11.3.0.4 nvidia-cufile-cu12       1.11.1.6 nvidia-curand-cu12       10.3.7.77 nvidia-cusolver-cu12     11.7.1.2 nvidia-cusparse-cu12     12.5.4.2 nvidia-cusparselt-cu12   0.6.3 nvidia-nccl-cu12         2.26.2 nvidia-nvjitlink-cu12    12.6.85 nvidia-nvtx-cu12         12.6.77 pip                      22.0.2 setuptools               59.6.0 sympy                    1.14.0 torch                    2.7.1 triton                   3.3.1 typing_extensions        4.14.1 </pre></details>  At this stage, note that the installed `nvidia-` dependencies are identical (down to sub-patch versions) with @Eureka10shen 's [second set of working dependencies](https://github.com/networkx/networkx/issues/8115#issuecomment-3084273000), so we'd expect this to be fine based on their previous testing, and indeed:  ```bash $ python -c "import networkx as nx" ``` No problems!  2. Testing with backends  2a. `nx-cugraph`  ```bash $ python -m venv nxc-env $ source nxc-env/bin/activate $ pip list  # ensure empty $ pip install networkx==3.4.2  # Install networkx first from pypi $ pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com  # Install cugraph per [the docs](https://rapids.ai/nx-cugraph/) $ pip list  # check environment ```  <details>   <summary>Output</summary> <pre> Package               Version --------------------- ---------- cuda-bindings         12.9.0 cuda-python           12.9.0 cupy-cuda12x          13.5.1 fastrlock             0.8.3 libcugraph-cu12       25.6.0 libraft-cu12          25.6.0 librmm-cu12           25.6.0 networkx              3.4.2 numpy                 2.2.6 nvidia-cublas-cu12    12.9.1.4 nvidia-curand-cu12    10.3.10.19 nvidia-cusolver-cu12  11.7.5.82 nvidia-cusparse-cu12  12.5.10.65 nvidia-nccl-cu12      2.27.6 nvidia-nvjitlink-cu12 12.9.86 nx-cugraph-cu12       25.6.0 pip                   22.0.2 pylibcugraph-cu12     25.6.0 pylibraft-cu12        25.6.0 rapids-logger         0.1.1 rmm-cu12              25.6.0 setuptools            59.6.0 </pre></details>  ```bash $ python -c "import networkx" ``` No problems (unsurprisingly, since this is an officially-supported package from nvidia)  2b. Add `graphblas-algorithms` to the mix  In the same environment that already has `nx-cugraph`, install `graphblas-algorithms`  ```bash $ pip install graphblas-algorithms $ pip list  # Check environment ```  <details>   <summary>Output</summary> <pre> Package               Version --------------------- ----------- cffi                  1.17.1 cuda-bindings         12.9.0 cuda-python           12.9.0 cupy-cuda12x          13.5.1 donfig                0.8.1.post1 fastrlock             0.8.3 graphblas-algorithms  2023.10.0 libcugraph-cu12       25.6.0 libraft-cu12          25.6.0 librmm-cu12           25.6.0 llvmlite              0.44.0 networkx              3.4.2 numba                 0.61.2 numpy                 2.2.6 nvidia-cublas-cu12    12.9.1.4 nvidia-curand-cu12    10.3.10.19 nvidia-cusolver-cu12  11.7.5.82 nvidia-cusparse-cu12  12.5.10.65 nvidia-nccl-cu12      2.27.6 nvidia-nvjitlink-cu12 12.9.86 nx-cugraph-cu12       25.6.0 pip                   22.0.2 pycparser             2.22 pylibcugraph-cu12     25.6.0 pylibraft-cu12        25.6.0 python-graphblas      2025.2.0 PyYAML                6.0.2 rapids-logger         0.1.1 rmm-cu12              25.6.0 setuptools            59.6.0 suitesparse-graphblas 9.4.5.0 </pre></details>  ```bash $ python -c "import networkx" ```  Again, no problems.  2c. Add `nx-parallel`  For the sake of brevity, `pip install nx-parallel` and repeat the above steps. Everything works fine.  ### What have we learned so far  At this stage, we've tried the officially supported versions of the backend packages via wheels and everything works as expected. Note that this agrees with @Eureka10shen 's observations - all of the wheels that come from pypi work as expected. In these experiments, since we're using `venv`, all non-Python dependencies are system native. For example, we're not installing any bundled version of cuda etc., but using what is native on the system. I'm not saying that I think this is a CUDA problem, but rather that I suspect this is a packaging problem somewhere in conda/mamba packaging land.  This hypothesis is supported by what little concrete info we have in the upper part of the issue. @Eureka10shen sees this issue only when `cuda-` dependencies are installed from the `*tuna*` mirror via conda. Similarly, though we don't have full environment info from @auderson 's initial post, the traceback shows that `mamba` is being used.  I'm not a conda/mamba user so my ability to chase down packaging issues in that universe is limited, but that's where I'd look next. If any conda/mamba users who experience this issue can share a reproducer consisting of 1) an `environment.yml` that results in this error along with 2) the channels from which packages were installed, that would at least give us an idea of which packagers to ping. Unfortunately NX has no control/insight into how others package the software!  In the meantime, I'd recommend that users who are running into this problem try forcing their conda/mamba channels to the open-source forge channels (e.g `-c conda-forge`). That way, if there are problems on that channel (please report here!) we can at least dive into the recipes and see if we can't find the underlying cause.
comment
I'm inclined to close this one and focus on the work in #8024 if for no other reason than we managed to hammer out the API through extensive discussions in that PR. WDYT @akshitasure12 ?
comment
> Also didn't we decide to stop using codecov after https://about.codecov.io/security-update/?  Yup - I completely forgot about that. I'd definitely be in favor of moving away from codecov.  One immediate idea for replacement would be to move the `coverage` job from github over to `circleci`. The `pytest-cov` extension already has native support for generating nice (static) html coverage summaries which we'd be able to view directly with circleci's artifact system.
comment
This PR technically has multiple approvals but there were a lot of changes between the different approvals so I will hold off from pushing the green button until others have had a chance to look at latest!
comment
I believe this one was superseded by #7422 which resolved the underlying issue. I will close this since the underlying issue was addressed separately. If there are still pieces related to non-terminal leaves in the steiner tree problem that folks think are relevant, please go ahead and open a new PR!
comment
I'm going to go ahead and close this - as mentioned this is custom infrastructure that isn't really part of the shipped library and thus isn't really intended as a target for active development.  Thanks for your interest in the library @Akshay-gurav-31 - if you're interested I'd encourage you to take a look at the code that lives inside the top `networkx/` directory for opportunities to work on NetworkX itself!
comment
I'll go ahead and close this one in favor of #8147, thanks @rudyarthur !
comment
> Why did the TODO get removed? Is it not relevant?  I'm personally fine with removing it. It's essentially a feature request rather than anything that has to do with future compatibility. If it's something folks really want/need, it'd be better to track it somewhere visible like github issues. Given that it's been there for 10 years with no activity, that's a pretty strong indication that no one has a practical use-case for it!
comment
I agree with @dschult and suspect that whatever code generated the second image in the OP was not actually using a spring layout (perhaps `circular_layout` or `shell_layout` was called accidentally, or some other arbitrary dict was named `pos`).  As noted above, `draw` just calls `spring_layout` under the hood, which is *unseeded by default*. This means that every time you call `draw` (without explicitly passing in `pos`) you will get a different layout. Whether or not it "looks good" will be arbitrary[^1].  Try the following:  ```python >>> G = nx.Graph([ ...     (1, 4), (1, 13), (1, 8), (1, 12), (1, 27), (1, 34), ...     (4, 10), (4, 31), (13, 14), (13, 37), (13, 15), (8, 5), ...     (8, 26), (12, 21), (12, 3), (12, 16), (27, 28), (27, 29), ...     (34, 35), (34, 36) ... ]) >>> pos = nx.spring_layout(G) >>> pos_seeded = nx.spring_layout(G, seed=0xdeadc0de) >>> fig, ax = plt.subplots(2, 3) >>> nx.draw(G, ax=ax[0, 0], with_labels=True)  # unseeded spring layout computed internally >>> nx.draw(G, pos=pos, ax=ax[0, 1], with_labels=True)  # `draw` with pre-computed layout >>> nx.draw_networkx(G, pos=pos, ax=ax[0, 2], with_labels=True)  # same layout as previous figure >>> nx.draw(G, pos=pos_seeded, ax=ax[1, 0], with_labels=True)  # Seeded layout, same on your computer and mine >>> nx.draw_networkx(G, pos=pos_seeded, ax=ax[1, 1], with_labels=True)  # No difference between `draw` and `draw_networkx` ```  ![Image](https://github.com/user-attachments/assets/04b1d68a-cd2e-42a8-aeb4-96bd84d766d7)  The upper-left layout will be arbitrary and different than all the others (unseeded spring_layout computed internally). The upper-center and upper-right figures will also have arbitrary layouts, but they (the layouts) will be identical because the same pre-computed `pos` dict has been passed in to each -- there is no difference between `draw` and `draw_networkx` (in terms of layout) when this is the case. The two figures on the bottom row will not only have the same layout as each other, they will have the same layout *on your computer* because they have been computed with an explicit seed for reproducibility.  [^1]: Generally if you want to improve `spring_layout`, the first thing to try is to increase the number of iterations, e.g. `iterations=200`. Of course, you can't do this through `draw` - it requires calling `spring_layout` explicitly.
comment
This one's been stale for a bit and I'm quite confident the behavior in the OP is not due to a bug. Hopefully the above explanation is sufficient to address any uncertainty about default layouts & seeding, but if not - please feel free to reopen!
comment
> I would like to entertain a first PR that just changes "and" to "or" and corrects the broken test (and adds more tests -- though not test_is_aperiodic_random). That will hopefully lead to expedited review and inclusion in the v3.5 release (coming in the next week or two). It's always good to fix bugs! :)  I agree - the discussion here has generally had wider scope than the set of changes. IMO the bug-fix is unobjectionable and should go in before 3.5, so breaking *just* that part out into a separate PR might be helpful!
comment
I agree with @dschult - ultimately I think this is an API problem rather than an implementation problem!  One thing this PR *does* point out is that the test suite isn't perfect on this front. A simple test for isomorphism that passes on `main` but *fails* with this proposed change would be a worthwhile addition to the test suite!
comment
> I agree that this could be made more explicit in the semantic check. I would like to continue thinking about this API problem if that's okay (until I find another doable issue).  Personally, I would recommend finding another issue... I don't think there is a code-change solution to this one. Rather, the idea is that users should be encourage to provide a matching function that fits their application rather than relying on the built-in matching functions. The latter case can very easily run into combinatorial explosion if we try to cover all possible cases in the "builtin" matching functions. Indeed, they're already much more complicated than is likely necessary for most cases! So in other words - if you're looking for an opportunity to make code changes, #7758 is likely not a good starting point!  > In this context, does an API mean a method of a python class?  What I mean by API is: how users access this functionality; in this case specifically, this refers to the built-in matching functions. I think most users would be much better off defining their own matching functions rather than relying on the built-in ones. However, it's not possible to simply change everything due to backward compatibility constraints. Therefore this is the type of problem that is best addressed (IMO) with comprehensive, high-level documentation changes to emphasize the "better" patterns and de-emphasize the others - in other words, make suggestions to users about how *best* to use these functions.
comment
@ishrathtahaseen-9 just a heads up - there was already a PR open for this. Generally it's good to double-check that there aren't already existing PRs for issues. No worries in this case, as the other one had gone stale, so let's focus on getting this in!
comment
It looks like @MridulS 's suggestions have been overridden by a force-push. @ishrathtahaseen-9 please make sure you pull before pushing again so we don't lose the contributions from reviewers!
comment
Reading through this thread, I think there are at least 2 (perhaps more) independent discussions happening:  1. The issue from the [original post](https://github.com/networkx/networkx/issues/6052#issue-1407107111) re: a suspected infinite loop  2. A second issue related to [a single cycle being produced infinitely](https://github.com/networkx/networkx/issues/6052#issuecomment-1826799456)  I believe `1.` is not a bug and has been [sufficiently addressed](https://github.com/networkx/networkx/issues/6052#issuecomment-1284137961). The number of cycles in a graph can change drastically with even very small modifications to the edges. What seems like an "infinite loop" is much more likely to be due to the fact that there may be *many* more cycles in the graph than one might expect. For example, take a look at how the number of cycles grows in [undirected complete graphs](https://oeis.org/A002807). As a concrete reference point, running `sum(1 for _ in nx.simple_cycles(nx.complete_graph(11)))` takes about 6.5 seconds to count all 5,488,059 cycles on my laptop. If we assume that the computation time scales with the number of cycles (a best-case scenario) then I would expect the same computation for `complete_graph(15)` to take 23,000x longer, or about 42 hours. Not an *infinite* loop, but definitely longer than I'd be willing to wait!  The memory component of this comes from trying to store all the cycles --- in fact, this is one of the main reasons why `simple_cycles` is implemented as a generator! Sticking with `complete_graph(15)` as an example: there are `127661752406` cycles in this graph. For the sake of argument, let's assume the average cycle length is 3[^1] and each node is represented by a 64-bit integer. The total memory footprint for storing all the cycles[^2] would be `num_cycles * num_nodes_per_cycle * num_bytes_per_node` = `127661752406 * 3 * 8` = ~3 TB. In other words, the explosion in memory is a symptom of the same fundamental cause: there may be *many* cycles in the graph - so many that storing all of them in memory is not feasible.  Point `2.` sounds like it could be a real bug: `simple_cycles` is not expected to yield the same cycle over and over; however, absent a reproducing example there's no path forward to investigate whether the problem is related to `simple_cycles` or something else.  Given the above, I'm going to close this issue. For users who still have issues with `simple_cycles`, please open a new issue (preferrably with a minimal reproducing example) so that others can try to help concretely address it!  [^1]: We **know** it's longer because 3 is the minimum length of a cycle and there are certainly longer cycles in `K15`, but this is fine for an overly conservative estimate that illustrates the point.  [^2]: Ignoring overhead from data structures - just the data itself!
comment
> Would you happen to know other workarounds? 😅  IMO it's difficult to get a real handle on performance differences when comparing across years worth of development. Python itself has changed quite a bit from 3.2 -> 3.10. You may see improvements if you're able to switch to 3.11. I don't think there's a whole lot that can be done to reproduce the same performance characteristics of nx v1.11 with the modern machinery. If graph copying is the bottleneck of your analysis pipeline, perhaps you can consider trying to vendor the subgraphing system (or performance-relevant bits) from nx v1.11.
comment
I think the (likely) root cause has been identified here - the views system. The short answer to "is there any way to replicate the way it worked in 1.11" is: no --- though I'd be interested in how subgraph worked in 1.11 if it wasn't creating copies.  I think @MridulS may be on to something re: the overall slowdown of the specific analysis loop in the OP, as that involves a view (AdjacencyView) as well.
comment
We're falling behind on these dependabot updates because we're blocked by the codecov bump. I'm going to go ahead and tell dependabot to ignore codecov so we can continue getting the other updates in. At some point I'm sure codecov will force our hand and the old API will stop working, at which point we'll have to figure out the new pattern.
comment
I too get the correct result. What versions of nx and Python are you using @javieriserte ?
comment
I'm going to go ahead and put this one in to unblock the CI problems!
comment
There was some confusion related to the terminology surrounding min/max matchings that was recently fixed in #8062. If I'm understanding the original post correctly, the confusion seems to stem from the expectation that the result of `max_weight_matching` and `min_weight_matching` will give the same result when the weights are negated. As is (now) noted in the docs, the `min_weight_matching` fn actually computes the minimum-weight *maximum cardinality* matching of `G`. In other words, the expectation should be that `nx.min_weight_matching(G)` with negated weights gives the same result as `nx.max_weight_matching(G, maxcardinality=True)`.  If I've understood correctly I think this al stems from a terminological issue that was corrected in #8062 - does this make sense @wangyechao123 ?
comment
Thanks @lkk7 for the fix and ping! We'll see if we can't be more diligent about upstreaming issues we get related to `nx_pydot`.  Closing as resolved by the pydot 4.0.1 release.
comment
A couple things immediately come to mind:  1. NX 2.8.8 is quite old and no longer supported. There have been updates to `louvain` since then that may have fixed this problem, such as it is.  2. Do you have floating point weights? Precision issues could in principle cause non-deterministic behavior
comment
> I suspect that by using this decorator, everytime one is trying to call the Louvain partitioning, a new interface is instantiated, resulting in a different partitioning even when the seed is set.  If you use an integer seed, then the machinery is creating a consistent random state (either with NumPy or `random`) at each call from that integer, so there should be no variation. The only exception would be if there were a call to either `random` or `np.random` directly, which I don't see in the source code.  For reproducibility, you should definitely pass in integer seeds (or explicitly seeded random states, like `seed=np.random.default_rng(my_seed)`) to ensure you are avoiding any sort of global random state.  I'm still not convinced there's a problem that hasn't already been fixed in subsequent NX versions - there's nothing we can do to help with bugs in 2.8.8 or 3.1.
comment
It looks like this one has been cleared up, so I'll close it. Thanks @amcandio @dschult and @supreethmv for taking a closer look at the example in the OP to find the issue!
comment
> I believe it would be beneficial to make this ordering behaviour an explicit contract of NetworkX, rather than an implicit consequence of Python’s dict implementation. The dict is just the current mechanism by which NetworkX fulfils this contract -- not the contract itself.  I agree with @dschult on this front - I'd prefer to rely on the underlying Python data structures rather than make any claims that we don't actually keep. It strikes as (almost) a moot point now anyways as Python itself has committed to ordering as a feature. With the exception of some non-CPython implementations which are not fully language-compliant, this is no longer an issue.
comment
@Schefflera-Arboricola it's true that the deprecation policy was followed in this case - this PR proposes a solution to a different problem. What happens when code that was written *before* the deprecation warnings were added is run now? When it hits the function that has been removed, you get an `AttributeError` which is not at all informative. This replaces the uninformative error with something much more clear and directly related to the problem at hand!
comment
> Do we need to update contributors guidelines or something?  My vote is for incremental adoption. I don't think that *every* removed function necessarily needs this treatment, especially "helper" functions from subpackages etc. In the general case there are other questions that would need to be answered, e.g. should we add module `__getattr__` s with specialized exceptions to each namespace from which the removed function is (was) accessible? Again, I tend towards no - but there are enough moving parts in the general case that I don't think it's worthwhile to codify a policy (yet).
comment
Thank you very much for raising @supreethmv , this type of user feedback is invaluable!  Using the module __getattr__ to provide better exception messages for removed (i.e. post-deprecation) functionality is something I've long thought might be a good idea. +1 from me for doing so!
comment
Thanks for reporting @mgorny , and the subsequent diagnosis - indeed you are correct!  To dive a little deeper into the nitty gritty - the issue lies in `grid_graph`:  https://github.com/networkx/networkx/blob/d1c41f886add2ab86c1ce4c1d788d3c286721351/networkx/generators/lattice.py#L132-L135  In Python <3.14, the expression under the `try` raises immediately, e.g.  ```python >>> import sys >>> sys.version '3.13.2 (main, Feb 13 2025, 22:11:36) [GCC 11.4.0]' >>> (1 if p else 2 for p in True) Traceback (most recent call last):   File "<python-input-2>", line 1, in <module>     (1 if p else 2 for p in True)                             ^^^^ TypeError: 'bool' object is not iterable ```  However, in 3.14 it appears the evaluation machinery has changed (arguably fixed) this corner case so that the expression now evaluates lazily as expected:  ```python >>> import sys >>> sys.version '3.14.0b2 (main, Jun  3 2025, 11:35:29) [GCC 11.4.0]' >>> (1 if p else 2 for p in True) <generator object <genexpr> at 0x77cff19fec20> >>> ```  So the try-except, which is based on an eager raising of a TypeError, no longer works as expected.
comment
> The main point is that you cannot call sorted(G.nodes) inside Networkx algorithms. So you cannot prevent the library from giving you inconsistent results. Take Dijkstra's for example, you might return different paths depending on the order you visit the nodes. User cannot easily influence that.  It's true that this isn't the case in general, but the patterns that NetworkX tends to use to deal with ordering (in cases where there may be many permutations of "correct" answers) are:  1) Have an option to generate *all* permutations, often (hopefully) as a generator, e.g. `all_topological_sorts`  2) Have hooks in the function to allow users to modify how nodes are explored during the algorithm, e.g. the `sort_neighbors` argument in the `edge_{d,b}fs` functions.  In the case of `dijkstra`, the hook is the `weight` keyword argument, which accepts a callable that can then be used to customize ordering (the callable accepts both the nodes comprising the edge, *and* the dict of data attributes).  So - while there's no *general* pattern, NetworkX does attempt to make this possible for the subset of functionality where users may want it. That said, there are likely other places where such hooks would be useful but are not yet implemented. My preference would be to expand the existing patterns (as needed) rather than recommend modifications to the data structures!
comment
See also #1618 which is related - at the very least, the API changes in #1618 are more in line with what I'd advocate for in introducing this feature to the user (i.e. kwarg switch rather than adding/renaming functions)
comment
> not a heavy user of networkx (yet :-)) so do not have much of a skin in this game but raising the python version could create a bar for usage. just saying. maybe you guys have a good reason... could file an issue that mentions this if you think it is worth it   The policy for which Python versions to support is governed by [SPEC 0](https://scientific-python.org/specs/spec-0000/) which is formulated via discussion across scientific Python projects. Check out the link if you're interested!
comment
Thanks @eriknw @Schefflera-Arboricola & @dschult for all the iterations! Let's get this in - further updates/suggestions welcome in follow-up PR(s)!
comment
I'm tempted to get this one in - @eriknw any major objections? My current feeling is "let's not let perfect be the enemy of good" - so I'd vote to merge then if there's another wording update proposal, submit another PR - WDYT?
comment
Thanks @eriknw and @dschult !
comment
Awesome - thanks again all!
comment
> Question: tree.center would only be for unweighted graphs, should we clarify that in method name  I'd vote to leave it out of the name - I think it's reasonable that someone would want to generalize to weighted graphs in the future, and that's much easier to implement if we don't have to modify function names. A simple note in the docstring that node/edge attributes are ignored should be sufficient IMO!
comment
> Got it! Any concern about changing the order of isomorphism list?  While it's probably not super important, it's also easy to preserve so we should do so - see my comment above about `deque`!
comment
Thanks @lmeNaN for the example - that makes sense then. Let's go ahead and turn that into a test and I think this is good to go!
comment
Thanks for the wording updates @QianZhang19 , my approval stands!
comment
Thanks for the suggestion @eriknw - I agree faster tests are better!  > This is mostly just an experiment to try to speed up the two slowest CI jobs. Are there any "gotchas" to be concerned about? Why wouldn't we always enable this option for all pytest runs?  From my end, it's bascially down to a lack of confidence between the distinction between "parallel runners" in github and and parallelism in individual workflows. I had always assumed that if you used multiple CPUs per workflow that it counted against the number of parallel workflows that would be run. I've personally never investigated whether this is the case! If it's *not* the case and there are extra processors per workflow that we're currently not using, then I don't see any downside to enabling it!
comment
> then the code could remain the same and the only thing that would need updated is the docstring.  Thanks for pointing this out @eriknw - I really like this idea! AIUI it's basically just taking advantage of a (new-ish?) feature in scipy to provide users with dense arrays with zero code change!
comment
> And another question: how can we make sure this is kept updated as we add more methods?  My preference is to cross that bridge when we come to it... presumably the author of any other new methods would want to draw attention to the new feature, and updating the gallery example would be a great way to do so!
comment
Thanks for reporting - this was fixed in #8027 which will be included in the upcoming nx 3.5 release. I'll close as "resolved", but thanks for bringing it up!
comment
Just to quickly add my two cents: one thing that I think is important to keep in mind when updating references is to make sure information that is related to the specific implementation of the algorithm *in NetworkX* is not lost. Attribution and historical perspectives are both interesting and important, but it's also important to keep in mind that the docstrings are not research papers and the primary role of the references is to provide context for the software.  In this particular case the reference in question was unreadable (at least by me - I was redirected to what appears to be a "link not found" message in Cyrillic) so there doesn't appear to be any risk of technical/implementation-specific information being lost by updating the reference. Therefore +1 for updating this; I just wanted to share my general rule-of-thumb for assessing docstring reference relevance!
comment
Thanks for checking @Peiffap - let's go ahead and close as resolved. If there are still updates to be done, please feel free to open a new issue/PR!
comment
> Can I make this correction?  Sure - IMO the parameter description from `tol` could be updated to note that the convergence condition is `N * tol`, since that is an accurate representation of the current behavior.  Note that you'll likely have to update multiple docstrings, as `pagerank` calls the private `_pagerank_scipy` under-the-hood.
comment
Thank @Peiffap - let's go ahead and close it. Please feel free to open a new issue (linked back to this one) for any follow-up discussion!
comment
I'd like to take another look at test failures before merging!
comment
> From the resolved suggestions of @dschult I take it this PR should be closed?  This is my take as well, so I will go ahead and close.  On a more general level, the pattern of "validating" inputs is often avoided in NetworkX which tends to lean more towards the `dict.get` model for handling "missing" data. That said, it should be possible for users to implement validation for their use-cases and it sounds like it is possible, albeit with a non-trivial amount of typing.  I'm happy to have further discussion on this point, but agree that a new issue/PR with a clean slate might be a good starting point!
comment
I'm -1 on this change, as there's already a way to do this with `nx.set_node_attributes`. If I had a time machine, I'd actually go back and push hard against adding a `nodelist` argument in the first place and instead advocating for users to use `nx.relabel_nodes` instead, but that's not relevant :)  In other words, rather than adding to the nodelist kwarg, I'd advocate for better documenting how to achieve this behavior with existing functionality that is arguably (though subjectively) more clear.
comment
I agree with @dschult, the naming of internal variables is arbitrary and in many cases subjective. As illustrated by the discussion here there's nothing incorrect in the existing naming scheme, and as @dschult points out there are downsides to unnecessary code churn.
comment
CI failures are unrelated - it seems that there has been an update in pydot that breaks nx_pydot, but I haven't looked into it. Either way, a separate issue!
comment
> Wait -- I thought the nodes were tuples -- not integers from 0 to 2**(n-1).  Indeed - this is the main change. I think a smaller, minimal wording update would be best - but we can get there!
comment
This one hasn't seen activity in a bit and there's since been another PR with a bit more activity so I will go ahead and close this. Let's focus the discussion on this topic in #8012 !
comment
Out of curiosity I triggered a rerun for the failing workflow and it appears to have run fine. I suspect it's related to #7987 if anything. Let's keep an eye out to see if it reoccurs.
comment
Thanks for raising @dschult  - at least in the case of #7902, the errors are from failed identity tests. Specifically, there are patterns like:  ```python if not copy:     assert result is G ```  Where the `is` is intentional - when copying is false, the expectation is that the input and output are the same object. IMO this *should* work, but it looks like the loopback disptaching system violates the identity assumption. 
comment
Yes, most actions triggered on push/PR run on forks. I'm not sure what the best practice is here - I had always assumed that when jobs were enabled on forks then GH was smart enough to use the results from the fork's actions to populate the CI report in the base repo. I've never actually checked this though - one would think GH would enforce such behavior, otherwise there are probably huge amounts of duplicate CI jobs being run on the platform.  FWIW I sometimes rely on CI on my fork to make sure something will work on all platforms (e.g. windows) before I open a PR; so I'm not sure that disabling CI on forks across the board is necessarily something we'd want to do (scheduled jobs are already disabled on forks). WDYT @bsipocz , are additional pros/cons that come to mind?
comment
IMO this is more of a discussion than something that needs implementation. In particular, I'd be interested in learning:  1. How github actions handles CI jobs across forks. Particularly, when a job is triggered by a push to a fork, is that action reflected in the PR or is it duplicated (i.e. there is an independent job running on both the fork *and* the main repo)  2. What the best-practices are for other open-source libraries.  So if you're interested in researching around a bit, by all means!
comment
Thanks @bsipocz ! That does seem pretty wasteful then... Since NetworkX is pure Python, the use case of "I use my own fork to test my changes on windows before submitting a PR" is not really relevant. I'd be in favor of limiting `push` runs to the main repo - I'll add this to the community meeting agenda!
comment
We looked into this a bit in the community meeting and found that it seems to be under the user's fork settings. For example, on your fork if you go to `Settings` then `Actions` in the left panel, then `General` - you get to a page that gives 4 options relating to action configuration. The first two of these are "Allow all actions and reusable workflows" and "Disable actions".  To summarize - I think we're then in the state we want to be: users can control whether or not actions run on their fork.
comment
>  I lean slightly toward keeping it private -- and hoping that people who want to use it in a non-numpy environment are willing to use a function name that starts with _. (of course, many private functions in NetworkX don't use leading _, so we could probably take off the leading _ making some more comfortable with using it.  My two cents: I tend to agree. I think having a single public interface (that is dispatched) is the right way to go. The vast majority of the time, I doubt users care or even notice these implementation details. The case I'm most concerned about is exact reproducibility (e.g. for publications) in which case there is a pattern to achieve explicit behavior, even if it involves "private" functions. To my knowledge, there's never been any issues raised about reproducibility concerns for patterns like this (`pagerank`, e.g. , is even more stark).  
comment
> Sorry, it's been too long! I'm afraid I don't remember the reason for that choice.  Reading through the [spec linked above](https://users.cecs.anu.edu.au/~bdm/data/formats.txt) the only thing I see is the warning near the header that says `(without trailing newline!)`. If I'm reading that correctly, I think that applies *only* to the header statement and not to the bytes, so I'm inclined towards the interpretation in the OP!
comment
I'm going to go ahead and close this - if you want to take a stab at my suggestion, please go ahead and open a new PR.
comment
I was recently looking at the gexf spec for another PR and given my most recent reading I believe @dschult 's interpretation/explanation is correct.  IMO leaving the conversion of "liststrings" to valid Python collections (assuming that's what a user wants) is best left to the user. I will close this as completed though if folks wanted a docstring example for dealing with liststrings I'd be +1
comment
The documentation component of this issue was closed by #7703 so I will close - however if there is followup discussion related to extrema bounding itself please feel free to reopen!
comment
Closed by #7901 - thanks all!
comment
Are you sure you aren't somehow modifying multi_g_subgraph in other code (perhaps in another cell in a notebook)? I can't reproduce this - here's my interpretation of the example:  ```python >>> G = nx.path_graph(5) >>> G.add_edge(1, 1) >>> G.edges EdgeView([(0, 1), (1, 2), (1, 1), (2, 3), (3, 4)]) >>> MDG = nx.MultiDiGraph(G) >>> MDG.edges OutMultiEdgeDataView([(0, 1), (1, 0), (1, 2), (1, 1), (2, 1), (2, 3), (3, 2), (3, 4), (4, 3)])  >>> H = G.edge_subgraph([(0, 1), (3, 4)]) >>> H.edges EdgeView([(0, 1), (3, 4)]) >>> H.has_edge(1, 1)  # as expected False >>> MDH = MDG.edge_subgraph([(0, 1, 0), (3, 4, 0)]) >>> MDH.edges() OutMultiEdgeDataView([(0, 1), (3, 4)]) >>> MDH.has_edge(1, 1)  # also as expected False ```  The only suspicious thing that jumps out to me from the original example is the line `multi_g: nx.Graph = nx.MultiDiGraph(simple_g)` where the annotation is incorrect, but that shouldn't matter AIUI.
comment
Do you see the same thing outside of poetry? E.g. if you're on ubuntu 22.04 (via WSL) the system Python should be v3.10, so you can do something like:  ``` python3.10 -m venv test-env source test-env/bin/activate pip list  # env should be empty, modulo pip/wheel/setuptools pip install networkx ```  Then rerun the example?
comment
Okay I understand what's going on now - this was already fixed on `main` in #7729 - that's why I was having trouble reproducing. Sorry for all the trouble @slvrfn !  So - good news is this is just another symptom of #7724 and the underlying problem is already fixed!
comment
Great, I'll close as (not-as-obvious-as-it-should've-been) duplicate of #7724, fixed in #7729 !
comment
Just to make sure I have a clear picture of the current state:  This PR will change the results for the undirected case, and this change is necessary - i.e. is a bug and not a stylistic choice - is that right?
comment
> Another approach to all this which would allow this to work without scipy would be to use to_numpy_matrix instead of adjacency_matrix. We eventually turn the matrix into a numpy array anyway. It would also require changes to some of the expressions (like removing .toarray() and similar).  Agreed (though `to_numpy_array`, not `to_numpy_matrix`) --- The `1 - mw1 @ mw2` is a necessarily dense operation (i.e. sparse arrays can't be used). Now that doesn't mean that using sparse arrays for prior calculations isn't worth it - the operations may be more performant and/or memory efficient - but it's not a given. Definitely worth thinking about!
comment
IIUC, it seems that the returning `nan` for isolated nodes is agreed upon as the correct, or at least sensible, behavior. So the remaining question is isolated nodes with self-loops. I'm no expert, but looking at Borgatti's formula in the docstring, it seems that an isolated node with a self-edge would still result in `0 - 0/0`, and therefore should also result in `nan`. Absent any explicit information on how self-loops are treated from the Borgatti/Burt papers, this is what I think makes the most sense.
comment
This is not necessarily surprising - if you look at what `pydot` is doing under the hood, it essentially serializes data, *writes it to disk*, calls the specified graphviz layout on the tempfile, then *reads the results back in from disk*. See the docstring for [pydot's create method](https://github.com/pydot/pydot/blob/2852e71ae14a12440ad0bfa79f4e43e5150c9e41/src/pydot/core.py#L1716).  I/O is almost always going to be the bottleneck (at least for graphs of the size illustrated in the OP). Note this is fundamentally how pydot works, as it is essentially a programmatic Python interface to graphviz's command line interface. In other words, there's nothing that can really be done to speed this up, at least not with this particular approach in the library.  You may get better performance with `nx_agraph.graphviz_layout`, but that requires pygraphviz to be installed.  I will close this as "not planned" as I don't think there's anything that can be done about it - hopefully the explanation above helps clarify why things are the way they are at least!
comment
IIUC this is superceded by #7850 - is that right? Can we close this?
comment
This one's been stale for a bit - @kajal072001 are you planning on iterating here? If not let's go ahead and close it (it can be re-opened at any time if you have increased bandwidth in the future!)
comment
I suspect this is related to switching to the `cached_property` decorator for properties of the graph classes, see #5614 which indeed has the 2.8.1 milestone, so the version matches up as well.  I don't recall object persistence/refcounting being discussed w.r.t. `cached_property`, so this seems most like an implementation detail. However if the object persistence is a major sticking point for some applications it's probably worth discussing. You may be able to work around this by deleting attributes after they are accessed, see the [`cached_property` docs](https://docs.python.org/3/library/functools.html#functools.cached_property) for details.
comment
I think this was completely fixed by #7746 and the intermittent failures are related to PRs that were opened before #7746 was merged. So - merging with/rebasing on `main` should make the failure go away for PRs that branched from commits prior to #7746 !
comment
> Luckily this one is easy to fix! Let me know if you want a PR.  Please do if you have the bandwidth - especially if we can add a test case from your investigations!
comment
The CI failures are unrelated, so I'll go ahead and get this in. Thanks again @mjschwenne !
comment
FWIW there exists the [expanders module](https://github.com/networkx/networkx/blob/main/networkx/generators/expanders.py). Perhaps this (and existing functionality) is relevant to the discussion?
comment
At this stage I'd say there isn't really a concrete plan for path forward, though if you have one that fits within the wider context of the expanders module then please feel free to add to the discussion!
comment
These warnings are known and expected. The easiest thing to do would be to filter them out, though it might be nice to actually try to fix them by increasing the number of iterations or the tolerance in the tests. IIRC folks have tried this in the past and (as often happens) it ended up being a bit more complicated than it seemed at first. It could be a worthwhile exercise for anyone who's interested in taking a closer look at how parameters are passed around to the various matrix solvers in algebraicconnectivity! Alternatively I wouldn't be opposed to just adding warnings filters for these.
comment
AFAICT this arises from an inconsistency in the return types from pandas whether one is accessing objects with `__getitem__` or iterating over items:  ```python >>> pd.__version__ '2.2.3' >>> np.__version__ '2.1.3' >>> nodes = pd.Series([333, 555]) >>> type(nodes[0])  # accessing the scalar with __getitem__ numpy.int64 >>> type(next(iter(nodes)))  # iterating over series int >>> # Or, equivalently: >>> for n in nodes: print(type(n)) <class 'int'> <class 'int'> ```  FWIW the pandas behavior is inconsistent with numpy (again, AFAICT):  ```python >>> a = nodes.to_numpy() >>> a.dtype dtype('int64') >>> for i in a: print(type(i)) <class 'numpy.int64'> <class 'numpy.int64'> ``` As for whether this is intended/desired behavior I can't say. NetworkX is faithfully capturing the types returned by the various operations - the differences in return types appear to come from pandas itself!
comment
> yet one ends up in the graph edge as int and the other ends up in the graph edge as np.int64. What could explain this inconsistency in NetworkX's result when accessing the values the same way as each other?  Because the original nodes are added via `add_nodes_from`, which *iterates* over the nodes to add them to the underlying data structure(s). The code for this is here:  https://github.com/networkx/networkx/blob/32c898dfd76fb384297747a4a053f35c40005618/networkx/classes/graph.py#L634-L649  but it essentially boils down to:  ```python for node in nodes:  # When `node` not already in the graph     G._adj[node] = {} ```  which is where the difference between how pandas treats iterating/accessing scalars gets injected.  If you were to take the `add_nodes_from` out of the original example, you would get the expected result, i.e. both nodes would be scalars since the node addition *and* edge creation were handled by `add_edge`:  ```python >>> G = nx.Graph() >>> nodes = pd.Series([333, 555]) >>> G.add_edge(nodes[0], nodes[1]) >>> G.nodes  # Now both scalars, since originally added via `__getitem__` NodeView((np.int64(333), np.int64(555))) >>> G.edges EdgeView([(np.int64(333), np.int64(555))]) ```
comment
My initial assessment is that there's nothing to be done about this on NetworkX's side - adding special casing for iterating over pandas objects seems fragile to me. I'd raise it upstream and see what pandas says - if it's an intended feature then it's something that may need to be handled downstream.
comment
> What do you think about this approach?  I agree, I think this is the best way forward (and what I was going to suggest!)  So the API (for users) will look something like:  ```python >>> SG = nx.approxmiation.densest_subgraph(G, method="greedy++", iterations=3) ```  In that case, it may also be worth making `iterations` keyword-only, so that it is independent of ordering in the signature. WDYT? Either way, no need to stress about the finer points of API at this stage!
comment
Another improvement would be to add tests to ensure the weighted `harmonic_diameter` is correct!
comment
Since this didn't make the 3.4 release, I took the liberty of pushing up a signature change to maintain backward compatibility. My approval stands, though testing would be welcome!
comment
> I think this is now ready. @rossbar can you take a look at the tests?  Tests LGTM, thanks @dschult ! 
comment
The warnings are due to the `pytest-mpl` extension not being installed. You can fix this by installing `pytest-mpl` in your development environment.  At the very least this should be mentioned in the contributor guide (if it isn't already). I think we could also consider adding `pytest-mpl` to the `test` dependencies explicitly, though that may interfere with the CI setup.
comment
> why adding pytest-mpl to the test dependencies may interfere with the CI setup?  It's complicated: `pytest-mpl` obviously depends on `matplotlib`, which is a soft dependency of networkx. We have CI to test against networkx with none of the soft dependencies installed - adding `pytest-mpl` to `test` will interfere with the environments for those tests.  Long story short: I vote not to touch this!
comment
Thanks for reporting - I confirm I can reproduce this on da5a0103.  Adding `connectionstyle` to the edge label drawing was done in #7010 - @dg-pb any quick thoughts?
comment
Thanks @ShunyangLi - I'll go ahead and close this then. Generally I think the idea is really cool, so I went ahead and opened a PR converting this to a gallery example in #7757 (and listed you as a co-author, since this is your idea!).  If you're interested in adding other bipartite motifs, please feel free to use #7757 as a template/example for doing so!
comment
I will go ahead and close this as superseded by #7751 !
comment
I'm glad you were able to find a work-around, but I just wanted to add that this approach seems like a misuse of `key` in order to work around a limitation in the plotting library. Note that `key` is intended to indicate a specific multiedge between two nodes. In your original example, there is only one (directed) edge between each node, so there is no need to specify unique keys to individual edges.  For a more robust solution, I would look into whether/how yEd supports multigraph visualization; but if this works for your use-case then by all means feel free to use it! 
comment
> graph_clique_number shows up in clique.pyi, but can not be imported since the code seems to be missing from clique.py, node_clique_number seems to be there.  Networkx does not provide type stubs (.pyi files) so this issue has arisen because wherever you are acquiring type stubs from is out-of-sync with NetworkX itself. Consider reporting the typing issue to the provider of the .pyi files.
comment
I'm a little more strongly -1 on this change mostly because I don't think we should start treating numpy scalars as different than Python scalars, at least in the context of nodes. As @dschult noted, the results are consistent with the past (i.e. numpy 1.X). The above code has *always* returned numpy scalars as nodes in the subgraph for the `subraph_numpy` cases, and that hasn't (nor shouldn't be expected to) cause problems in user code. The only real difference is that with the updated numpy scalar repr in NumPy 2.0, it's more obvious to users that the numpy scalars are indeed numpy scalars.
comment
> Although IMHO it would be preferable for returned node types in subgraphs to be consistent with the original object, just to avoid potential confusion, I see now that the current behavior has always been so, and fully stand by its reasoning.  Generally I agree - numeric scalars (especially integer scalars) just happen to be a special case where, from the perspective of how they behave as dictionary keys, they are identical. In some sense this aligns with a slightly tortured interpretation of duck-typing :upside_down_face: 
comment
> RE: naming - I was unsure on this as well, I went back and forward on this a few times in my commits. In the early commits I called it greedy_source_expansion exactly as you suggested. I went for clauset in the end as there seemed to be several different greedy source expansion algorithms that each had pros and cons in different situations. [This](https://doi.org/10.1109/ACCESS.2022.3213980) touches on a number of the different algorithms. My thought was in future PR's I could add some of the other algorithms. Maybe clauset_greedy_source_expansion is a more descriptive name?  In that case, another option might be to call the function `greedy_source_expansion` but add a `"method"` keyword argument to specify *which* method of greedy source expansion is being used. AFAIK this is a (relatively) common pattern within the library: for example, see [`steiner_tree`](https://networkx.org/documentation/latest/reference/algorithms/generated/networkx.algorithms.approximation.steinertree.steiner_tree.html#networkx.algorithms.approximation.steinertree.steiner_tree). WDYT?
comment
> I tried adding a test for isolated nodes. But the test failed. Should I add a condition to handle isolated nodes? Or is there something I am missing here?  I'm certainly no expert on trophic levels, but the docstring does mention that:  > and nodes with $k^{in}_i = 0$ have $s_i = 1$ by convention.  which on it's own means that isolated nodes are currently handled properly:  ```python >>> G = nx.DiGraph([(0, 1), (1, 2), (2, 3)]) >>> G.add_node(4) >>> nx.trophic_levels(G)  # node 4 has trophic level 1, as expected {0: 1, 4: 1, 1: 2.0, 2: 3.0, 3: 4.0} ```  From a quick scan of the [wikipedia article on trophic coherence](https://en.wikipedia.org/wiki/Trophic_coherence) I didn't see any specific mention of isolated nodes. My impression is that isolated nodes are sort of uninteresting in the context of trophic analysis, but like I said I'm no expert!
comment
Any followup here @fblamanna ? We'll need a concrete understanding of the Pajek semantics in order to determine whether anything needs to be changed.
comment
I'm going to go ahead and close this one for now - if there are any updates that clarify the Pajek data spec please share here and reopen!
comment
I'm going to go ahead and close this as it seems to be autogenerated code that doesn't conform to any of the contribution guidelines. Improvements remain welcome, but should be incorporated with the existing codebase.
comment
I will close this as hopefully *improved* by the examples added in #7627 - any other ideas for how the documentation can be improved are of course welcome!
comment
Indeed - thanks all for the careful review!
comment
This is the expected behavior --- in MultiGraphs, the edges are stored according to a `key` attribute; in other words, there is another layer of dict-nesting in order to support the ability to store edge attributes for multiple edges between two nodes. Therefore, *individual* edge attribute access in multigraphs looks something like `G[node_1][node_2][edge_key]`.  This is baked into the structure of the MultiGraph, even when there is only a single edge between two nodes (the default key is `0`) - so to get to the edge-attribute-layer of MultiGraph, you have to provide the edge key as well:  ```python >>> h = nx.MultiGraph(g) >>> h[1][2] AtlasView({0: {}}) >>> h[1][2][0]  # where 0 is the edge key {} ```
comment
FWIW I took the liberty of applying my suggested changes and tagging this with the benchmark label to see if it would run. The good news is, the benchmarking environment appears to be set up correctly [in the action](https://github.com/networkx/networkx/actions/runs/11378181238/job/31653691275?pr=7647).  Now, the benchmarks aren't actually being *run* - it seems the action is having trouble finding the specified commits. This doesn't strike me as related to the environment setup, but I'm not 100% sure!
comment
There is already an open PR that would address this issue (#7347), so I will go ahead and close this - let's focus the discussion over in #7347!
comment
There is already a PR open related to this topic - see #7487 . Since there's already been quite a bit of discussion there, I'll go ahead and close this - please don't hesitate to weigh in on the discussion there!
comment
Generally, I think we want to drive as much interaction as possible to GitHub so I think there's value in keeping the communication-channel-footprint small. Therefore I'm -1 on adding discord, though if others feel very strongly in favor I can be convinced otherwise (or ignored :upside_down_face: )  > Also any thoughts on adding networkx's twitter here, or on the docs website?  Personally, I'm very -1 on driving any traffic to twitter.
comment
I suspect there's something wrong with the environment. For the record, networkx v3.3 does not support Python 3.8 (oldest supported version is Python 3.10) so it's not possible to have a working installation of NetworkX v3.3 with Python 3.8.
comment
I'm confident this is an environment issue, so I will go ahead and close. If the issue persists, please feel free to reopen with updated info about the environment and how networkx was installed.
comment
Thanks @dschult I'm personally +1 on the idea of modifying `check_create_using` in the proposed way. I very much like the pattern of being able to do the property check (i.e. directed, multigraph) before creating any graphs as opposed to the create-then-validate workflow. This prevents some of the more subtle code paths that were caught in the last round of testing.
comment
Fixed in #7644 
comment
There were indeed bugs with LCA in the past, but they've long since been fixed - I think #5736 did most of the heavy lifting. IIRC this patch made it into 2.8.6 - either way, upgrading your networkx version should fix the issue!  I will close this as resolved; however, if the issue persists on the latest version of networkx (3.3) then please reopen!
comment
> the frequency of graphs such as this where vertices cross each other  Do you mean edges crossing each other (instead of vertices)? Generally edge crossings are completely fine! The main issue here is when the nodes 1, 6, and 7 are colinear, which causes the edge masking. The success criterion is that all three edges (1, 6), (6, 7) and (1, 7) are visible, regardless whether they cross one another.  > The simplest solution(to avoid any interesting phenomena) I could think of is to add a weak force between unconnected vertices. I haven't tried it yet but it could possibly solve the issue in a aesthetically pleasing way, but it might diverge from the original algorithm.  This could be interesting to consider but is certainly out-of-scope for this PR. Please feel free to investigate the FR algorithm (I'd recommend starting with the paper), but I'd advocate opening separate issues/PRs for that so we don't block this fix.  For this PR, IMO the best way forward is to pick a combination of `seed` and `iterations` that produces the desired visualization and go with that. For example, `iterations=100, seed=42` seems to work!
comment
I went ahead and split up the benchmark in a naive way in order to unstick this - maybe @MridulS can doublecheck that this makes sense. Anyways, my approval of the changes themselves (regardless of benchmarking changes) stands!
comment
@mohamedrezk122 if you have the bandwidth it'd be good to push this one forward as I think there's a clear path to getting this fix in!
comment
@OrionSehn there is already a PR open for this issue at #7136 with a fair bit of discussion - perhaps you could take a look at that one and weigh in there?
comment
There is a lot of overlap here with the [numpydoc validation checks](https://numpydoc.readthedocs.io/en/latest/validation.html#built-in-validation-checks), which I've never really wanted to enable for the same reason :)
comment
FWIW I was recently looking at pydocstyle and noticed that [it no longer seems to be maintained](https://pypi.org/project/pydocstyle/#history), at least not at the pace of other linting tools.
comment
I'm going to go ahead and close this one - AIUI the purpose of the PR is to demonstrate the difficulties in adopting automated doc linting. That's not to say it's not worthwhile, but I don't think anyone's like to make progress with the "turn everything on at once" approach! Feel free to reopen if you disagree!
comment
I'll go ahead and close this in favor of #7618 just to avoid the duplication.
comment
> Thanks for reviewing! Is there anything I need to do on my side?  Nope! We're just waiting on another pair of eyes from someone with a more comprehensive understanding of the CI setup @MridulS @jarrodmillman 
comment
FWIW, there is no real behavior change - the above `G_subgraph_npy` example has *always* returned subgraphs with numpy integer scalars as nodes - it's just that prior to numpy 2.0, the distinction between numpy and Python scalars was not made explicit to users (due to type information not being included in numpy scalar reprs). You'd have to explicitly probe the types in order to see this. Here's a functionally equivalent example with numpy 1.26:  ```python >>> import numpy as np >>> np.__version__ 1.26.0 >>> G = nx.karate_club_graph() >>> G_subgraph_npy = G.subgraph(np.array([0, 1])) >>> G_subgraph_npy.nodes  # These *look* the same as Python scalars NodeView((0, 1)) >>> [type(n) for n in G_subgraph_npy.nodes]  # but are in fact numpy scalars [numpy.int64, numpy.int64] ```  All that's to say - nothing has fundamentally changed: NetworkX continues to handle numerical scalars (whether from Python, NumPy, Pytorch, etc.) the way that it always has. The only difference is what information is displayed to the user.
comment
>  I was wondering if we could somehow simplify whatever nx.drawing.nx_pydot.graphviz_layout is doing, while at the same time fixing the problem.  IMO this would be a very worthwhile endeavor, especially since y'all are doing so much work on pydot these days (awesome!). I haven't personally looked at `nx_pydot` all that much, but if there are opportunities to simplify and/or better integrate with pydot that'd be a big improvement.
comment
I will go ahead and close this one in favor of #7591. Thanks @buddih09 
comment
I think @dschult and @Peiffap are correct that the confusion results from the data rather than a bug in `adjacency_matrix`. Therefore I will close this; however, if you believe there is still an issue, please feel free to reopen with a minimal reproducing example which includes all of the necessary info to reproduce the buggy behavior.
comment
Just to add... I looked at this a bit this morning but I want able to reproduce locally with the scipy nightly wheels. Note that it's only the macos runs that are failing, though at first glance I wouldn't expect this to be platform dependent!
comment
> I can reproduce the error with the wheel, but didn't go bisect and build scipy to find the exact commit 😅  Are you on macos? I can't reproduce the error with the wheel on linux!:  ```bash $ python --version 3.12.3 $ python -m venv nx-devdeps $ source nx-devdeps/bin/activate $ pip install -U --pre --extra-index-url https://pypi.org/simple -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple numpy scipy matplotlib $ pip install -r requirements/test.txt $ pip install -e . $ pip list <see details below> $ pytest networkx/algorithms/centrality/tests/test_current_flow_betweenness_centrality.py ```  <details> <summary>Result of pip list</summary> <pre> Package         Version                   Editable project location --------------- ------------------------- ---------------------------- contourpy       1.3.0.dev1 coverage        7.5.4 cycler          0.12.1 fonttools       4.53.0 iniconfig       2.0.0 kiwisolver      1.4.5 matplotlib      3.10.0.dev344+g651e9109e9 networkx        3.4rc0.dev0               /home/rossbar/repos/networkx numpy           2.1.0.dev0 packaging       24.1 pillow          10.4.0 pip             24.0 pluggy          1.5.0 pyparsing       3.1.2 pytest          8.2.2 pytest-cov      5.0.0 python-dateutil 2.9.0.post0 scipy           1.15.0.dev0 six             1.16.0 </pre> </details>  This completes without any errors or warnings for me. If this failure really is platform dependent, then that suggests a problem upstream in scipy - there shouldn't be behavior differences across platforms! Perhaps the platform-specific wheels are out-of-sync?  Either way, I think we should chase down whether there is really a change upstream before patching NX!
comment
> And I should add that I don't understand why this code seems to be failing on some platforms and not others. The change in iteration results of scipy sparse csr_array from 2D to 1D should be the same across platforms.  My hypothesis is that the nightly wheels for the different platforms are not in sync. For example, maybe the nightly wheels for macos are several commits ahead (or behind) the linux wheels. I haven't actually checked/tested, just a hunch :)  It's probably worth bisecting to see exactly which commit in scipy introduces the behavior to help answer the "how should this be handled in scipy" question.
comment
Rather than relying on code that needs to be compiled (which would be a blocker, even optionally), I wonder if this implementation could instead be recast in terms of sparse/vectorized operations. FWIW the current force-directed layouts provided by NetworkX (kamada-kawai, fruchterman-reingold) are implemented this way.
comment
I took the liberty of pushing up a minor change just to retrigger the CI.
comment
> Perhaps we can queue this one with #5910   IMO the big blocker for these layouts is still testing. AIUI the forceatlas layout is quite popular, so I think we should really endeavor to test that the implementation in NetworkX gives correct results. Unfortunately, it seems there are no "canonical" test cases for the forceatlas layout that we can implement directly - the only solution seems to be to come up with some ourselves. Of course, tests of the form "input G produces exact positions X" are probably not particularly useful, but I think we can formulate some cases where *general* properties of the layout are deterministic.  For example, a simple test would be that the layout when applied to a wheel graph will result with the central (hub) node being roughly in the center, and all the other nodes roughly equidistant from the hub and each other. This is just one example - I'm not especially familiar with forceatlas2, so maybe there are even better test cases.  Tests along these lines may not be the most rigorous, but they are better than nothing. I think it's important to have something in place so that we can 1) verify that the *general* properties of the layout are correct for certain repeatable cases, and 2) be confident that future changes to the algorithm don't break the expected behavior.  I'm happy to take a stab at adding some tests cases that might be valuable, though I can't make any promises re: timeline. I encourage others to share ideas for potential test cases as well!
comment
> can we go ahead with the merge -- I am a bit confused on the status of the merge after the approval.  Sorry for the confusion - NetworkX requires approvals from two maintainers before merging. For cases like this one, where there has been *a lot* of back-and-forth, it is useful to verify that the suggestions of the reviewers are making it all the way into the code.  Indeed it seems there are still some changes that have been "undone" by the latest pushes, so this is still not ready IMO. I will take a stab at manually resolving things to see if I can't get it over the finish line.
comment
Actually I take that back - looking at it more closely, it looks like most of the suggestions have been reverted... I think perhaps the last attempt to "fix" a missed pull may have actually made things worse. Did you perhaps force-push from a different local branch?
comment
> hmm could be that I rebased wrong, could you push your latest changes, then I can merge them here locally.  I'll hold off on making any changes - just ping me when you think you've got the PR in the state you want it!
comment
It's more than just the docstrings, many of @dschult's original comments still seem to not be addressed (though they were marked as resolved - I un-resolved them).
comment
> I don't have access to this older commit anymore, and this thread is spanning several years now. If there is anything needed on my part that requires changing, please ping me but from my side this PR is done.  It sounds like some work may have been lost due to force pushing - in that case, I propose to resubmit the current commits in a new PR (to prevent unintentional force-pushes) so that we can finish polishing off the remaining issues. Re: attribution, the current commit history on this branch will be preserved. Does this sound okay to you @cvanelteren ?
comment
Closing in favor #7543 .  Sorry for not being clear in [my comment](https://github.com/networkx/networkx/pull/5392#issuecomment-2191332224) here @cvanelteren . The reason why I proposed to open a new PR *from a maintainer account specifically* is that then the default branch protections will prevent any force-pushing to the branch except by maintainers. It's the same exact set of commits, so attribution will still be correct (i.e. you will be shown as the author of this feature in the git log)!
comment
Let's go ahead and get this in now to get CI running!
comment
Thanks for raising @dfacoet - I think the crux of the issue is the interpretation of `n`, which is different than `t` in the paper. Full disclosure: I added these docs back in #6450 based on my interpretation of the arXiv paper in the context of the existing implementation.   The paper does indeed make reference to `t = -1` (this is probably most clearly depicted in figure 1) which I interpreted as the initial condition. Essentially, `n` represents the "generation", i.e. the number of transitions between states, whereas `t` from the paper is used to denote the states themselves. So `n=0` represents "no transitions", meaning you are at the initial condition, denoted by the paper as t = -1. In other words, the current interpretation is: "what is the graph that results after `n` steps of the DGM procedure have been applied". I agree that this could be made more explicit in the docstring.  Note that the docs were added to explain the current implementation ex post facto, which is >19 years old(!) and predates the project's migration to github.  IMV this is a documentation issue - the docs should be expanded to clearly explain the difference between the graph state (which the paper denotes with "time step", `t`) and the "generation number", `n`, used in the function. 
comment
Let's go ahead and get this in to un-break the docs build/deployment!
comment
> The failing labels actions feels a tiny bit weird, is there maybe a nicer user experience solution?  I agree that this is probably not the best UX for contributors - it basically guarantees that contributors who don't have triage rights will *always* be confronted with the red x when they open a PR, regardless of the state of tests/CI.  Thanks for the feedback @bsipocz - I'm going to add this to the agenda for the next community meeting!
comment
@Peiffap a PR here would be welcome!
comment
I can't reproduce with the installed package; here's a recipe I used to evaluate (from the source repo):  ```bash $ python --version Python 3.11.6 $ python -m venv nx-3.3-specdeps $ source nx-3.3-specdeps/bin/activate $ pip list  # Verify venv is empty Package    Version ---------- ------- pip        23.2.1 setuptools 65.5.0 # Install specific versions of default deps from OP $ pip install numpy==1.26.4 scipy==1.12.0 pandas==2.2.1 matplotlib==3.8.4 $ pip list  # verify environment Package         Version --------------- ----------- contourpy       1.2.1 cycler          0.12.1 fonttools       4.51.0 kiwisolver      1.4.5 matplotlib      3.8.4 numpy           1.26.4 packaging       24.0 pandas          2.2.1 pillow          10.3.0 pip             23.2.1 pyparsing       3.1.2 python-dateutil 2.9.0.post0 pytz            2024.1 scipy           1.12.0 setuptools      65.5.0 six             1.16.0 tzdata          2024.1 $ git checkout networkx-3.3 $ pip install . $ pip install -r requirements/test.txt $ pytest --pyargs networkx ```  I don't reproduce the failures with the above workflow. From the attached test logs, it looks at first glance like something has gone wrong with the numpy installation on this system: the attribute errors related to `np.random` and `np.int_` shouldn't be cropping up. Are you able to import numpy itself in the build/test environment?
comment
Thanks for the followup - it does indeed seem like the computed values in `dq_dict` will be floats regardless of whether `weight` is floating point due to `q0`.  Rather than share a download link to a diff, it would be easier to open a PR (feel free to create it as "draft" if you're intent is mostly for illustration/feedback).
comment
I agree with @dschult - I think correcting the behavior is the major priority. This has been affecting a lot of users so getting a fix out asap would be beneficial IMO.
comment
I agree that this is undesirable behavior, thanks for the report!
comment
> But Paley graphs ain't defined for p non prime. You shouldn't expect normal behavior in such a case.  True... but it makes even less sense to introduce artifacts (like self-loops) arbitrarily for some undefined values but not others. IMO it's a matter of degree - if a user call `paley_graph` with invalid `p` you'll get a result that is not technically a Paley graph in the strict sense but the procedure used to generate is consistent. Changing this behavior breaks that consistency for no real upside.  > checking if the value is a prime in the first place, although I think it's also not needed.  I agree - I don't think adding a primality check is a good idea. The [docstring](https://networkx.org/documentation/stable/reference/generated/networkx.generators.expanders.paley_graph.html) has a paragraph at the end of the summary that attempts to distinguish this implementation from other definitions of Paley graph. There's always room for improvement in the docs!
comment
A quick bit of context - this is not related to the recent nx v3.3 release. This behavior (i.e. ImportWarnings on soft dependencies when the warnings filter is turned all the way up) has always been around. That's not to say it shouldn't be changed insofar as possible, just that it's not related to the recent release!
comment
Is this a candidate for [`functools.cached_property`](https://docs.python.org/3/library/functools.html#functools.cached_property)? I'd lean against custom caching machinery, but if this is a case where functools might work it'd be worth exploring.
comment
> by moving the check for the list, tuples, generators, etc. on the top but there was a note in the comments(which you added in PR https://github.com/networkx/networkx/pull/4136) telling specifically to keep it at the end.  Yes, I don't think the entire conditional can be moved as-is, however it might be worth trying adding an additional condition before the attempted imports specifically for edge lists.  > Is there a reason why the import is done on every call to to_network_graph instead of a try import/except at module level? Is that intentional?  Yes, this is intentional - numpy, scipy, and pandas are soft dependencies and everything is lazy-loaded, so only functions that actually use these libraries attempt to import them. 
comment
FWIW this definitely extends beyond just `simple_cycles`. To test:  1. Setup a development environment with only networkx+test dependencies installed, e.g.  ``` python -m venv nx-only source nx-only/bin/activate pip install -r requirements/test.txt -e . ```  2. Run the test suite, elevating all warnings to errors:  ``` pytest -Werror --doctest-modules --pyargs networkx ```  This gives ~500 errors/failures, most of which are related to the `ImportWarnings` from `pandas`.  So - there are a couple options here; replacing the warnings with `pass` will cut down on verbosity and should alleviate any performance concerns given the inherent caching for module lookup (should be validated with the benchmark suite). Alternatively, the logic of `to_networkx_graph` can be re-organized to handle Python built-ins (i.e. lists, dicts, and NX-native objects like graphs and edge views) first.
comment
There are a few different things reported here, I'll try to address them separately:  Re: the original reported test failure, i.e. the fact that `matplotlib` has no `use` attr - I can't reproduce this one. Matplotlib 3.8.3 certainly does have this method:  ```bash python -m venv mpl-test source mpl-test/bin/activate python -m pip install matplotlib==3.8.3 python -c "import matplotlib as mpl; mpl.use('PS')"  # Works fine ```  However - in the test incantation, `pytest.importorskip` [is used](https://github.com/networkx/networkx/blob/76c3e9bc00fb7d159b00e4c71c7274d185fcb254/networkx/drawing/tests/test_pylab.py#L8) to load matplotlib. It's possible there's some bad interaction with the importorskip, then immediately calling a method on the bound `mpl` name - though I would expect that to raise some form of `ModuleNotFound` error rather than an `AttributeError`. Perhaps there's some sort of partial/malformed matplotlib package in the test environment?  ---  The missing scipy imports are real - these snuck in because we only test two environments w.r.t default dependencies: when *none* of them are installed, or when *all* of them are installed. There is no explicit testing (IIRC) of a subset of default dependencies. Patches to fix the missing scipy importorskips welcome!  ---  The remaining test failures that aren't covered above have been fixed already on `main` and will be incorporated in the next release (within the next couple weeks).
comment
> I'm not 100% but my understanding is that mainly all those issues are related only to test suite and no actual tested code.  Correct - these are all warts on the test suite and should not affect user-facing code. It's possible that some of the scipy dependencies make it possible to have an environment where some NX functionality is not avaialbe, but that's expected and the resulting `ImportError`/`ModuleNotFoundError` should make it very clear to users which "soft" dependencies are missing.
comment
I'm going to go ahead and close this one as well with the same general comment as #7365. Let's focus review efforts on #7367 !
comment
I agree with @dschult - based on the feedback here I will go ahead and close this one to avoid bikeshedding. Overall, I think that the match statement can have a positive impact in the code base, but +1 for avoiding the temptation to simply replace every conditional with `match`!
comment
Thanks for this @dzy49 ! I think there's general consensus that this is a good idea and the right way to go. It'd be good to add some tests for the new functions, though we likely don't need a ton of testing since these new functions are thin wrappers on top of existing (hopefully well-tested) functionality.
comment
> however automatic testing is saying "Your tests failed on CircleCI", and the output isn't making much sense, is there something I forgot to do?  No these failures are unrelated. The underlying issue should be fixed on main so you should be able to fix this by simply merging with main and pushing!
comment
> Okay. I merged with main but it seems like the issue is still there  Oops - sorry about that. I thought we had fixed all of these but it turns out we missed one. Everything should now be fixed as of #5994 - one final merge with main should do the job.
comment
> I'm fine with making this PR just be about the perfect_elimination_order stuff -- and having the change to is_chordal be considered in a separate PR. The first functions will almost certainly get merged faster that way.  I second this - I think moving the `is_chordal` changes out of this PR to a followup will help focus the discussion and move this forward!
comment
Linking to #5423 which discusses the `pairwise` bullet explicitly.
comment
Also, the [match statement](https://peps.python.org/pep-0636/) is a really exciting feature!
comment
The "automatic" features have already been handled by `pyupgrade` - the other more involved changes (e.g. using the match statement) should be handled on a case-by-case basis, so I'll go ahead and close this tracking issue!
comment
I can't reproduce either - I too suspect an environment issue, similar to #7047 (with the solution being to make sure to re-install networkx in your dev environment after changing branches).
comment
> It should return an empty graph  Technically the `clear()` method doesn't *return* anything as it modifies the graph instance in-place. When I try this locally I'm getting the documented behavior:  ```python >>> G = nx.complete_graph(5, create_using=nx.DiGraph) >>> print(G) DiGraph with 5 nodes and 20 edges >>> G.clear() >>> print(G) DiGraph with 0 nodes and 0 edges ```  I agree with the above - absent a minimal reproducing example it'll be difficult to chase down whether/where a potential problem lies.
comment
I know this has been merged already, but I think we should resist the temptation to "3d-itize" more examples. For example, the above added code does not actually affect the gallery (sphinx gallery automatically uses the first plot in an example as the thumbnail). It also makes the example itself much longer, adding a lot of code related to plotting/rendering that doesn't have anything to do with greedy coloring.  That's not to say these 3D animations aren't valuable contributions @lobpcg (thanks again)! My vote would just be to try to focus efforts on 3D visualization in the "3D drawing" category of the gallery.
comment
> Also, I don't think pytest-xdist has anything to do with these errors  I suspect it does, at least indirectly - I always run the test suite with `xdist` on all my machines and have never hit these problems with the un-dispatched tests. It's only when I add the `NETWORKX_TEST_BACKEND=nx-loopback` to the test invocation that I see the failures reported in #7354 
comment
Yes, multigraph edge drawing was added in #7010 which is not yet included in a released version of NetworkX. This will be available in the next NetworkX version (3.3). The reason you're seeing the discrepancy with the docs is because the gallery example is included in the *latest* docs (i.e. built from main). To limit documentation to only those features that are in the latest stable release, see https://networkx.org/documentation/stable/
comment
> if it is important that all exceptions that are emitted from NetworkX is of type   IMO this isn't important, but perhaps others have more informed opinions. I'd prefer to stick with builtin exceptions where possible, and *I think* KeyError works nicely for this case (NX graphs are "dicts all the way down" after all). I can be persuaded otherwise though; the only thing I'm -1 on is a backward-incompatible change in the exception type!
comment
Indeed this is an upstream issue with pydot: pydot/pydot#258. IMO the exception message is clear and includes the workaround - I'm not sure what else NetworkX can do to handle this other than hard-code a workaround, but that becomes tricky to maintain and a likely point where more bugs would be introduced. I will close this as dupcliate of #4663 
comment
This one too is now out-of-date with the current config so it'd probably be easiest just to start fresh rather than try to resolve conflicts. From a quick look at the changes themselves, the changes related to reducing scope of `pytest.raises` are an obvious improvement.
comment
Are you still seeing this issue @jamesjer ? Have you tried NX>2.8.5? I can't reproduce it locally either. 
comment
I took another look at the implementation and the first thing that jumped out as a potential point of variability is the use of `scipy.optimize.linprog` in the `held_karp_ascent` function. By default, `linprog` uses the `"highs"` method, which chooses between one of two sub-methods, either `highs-ipm` or `highs-ds` - see the [linprog("highs") method docstring](https://docs.scipy.org/doc/scipy/reference/optimize.linprog-highs.html#optimize-linprog-highs) for details.  The docstring mentions that when the default `"highs"` is specified, the submethod is chosen by scipy. I didn't chase down the selection logic, but perhaps it's possible different methods are being selected on different platforms. Another possibility is that the methods give different results on different platforms - they are binary extensions after all, so their building/packaging may have something to do with it.  If someone were going to dive into this further, that's where I'd start looking. In the end though, there may not be a whole lot we can do about it. If we're lucky, perhaps explicitly specifying one of `highs-ds` or `highs-ipm` removes the variability. The worst case scenario, such as it is, is that we cannot guarantee exact reproducibility for cases where there are multiple valid solutions. IMV this is not the end of the world. I'd be inclined to just document it and update the test to include all valid solutions to the test case!
comment
This one has been stale for awhile and looks like it underwent a bad merge that is folding 300+ files into the diff therefore I'm going to go ahead and close it. If interested in continuing this proposed addition, you might consider creating a new branch off of current `main` (i.e. don't use `main` on your fork) and cherry-picking the relevant changes there to make a start.
comment
This one has been stale for a while, so I will go ahead and close it. @PurviChaurasia if you're interested in picking this up again, the place to start would resolving the merge conflicts then re-opening!
comment
This one has been stale for quite awhile so I will go ahead and close it. @salfaris if you're interested in picking this up again, please feel free to do so - the last comment re: a (proposed) simpler way of producing the components would be a good starting point to re-kindle the discussion!
comment
For context: in the latest released version (3.2.1) it still returns a generator:  ```python In [1]: import networkx as nx  In [2]: nx.__version__ Out[2]: '3.2.1'  In [3]: G = nx.path_graph(10)  In [4]: nx.single_target_shortest_path_length(G, 0, 9) Out[4]: <generator object _single_shortest_path_length at 0x771b69589d80> ```  so this inconsistency is only on the development branch and should be rectified before the next release!
comment
A quick ping on this one - IMO the best course of action is to *not* dispatch the `strategy_` functions. AFAICT, those are callables that are intended only for explicit use with `greedy_coloring`.
comment
Personally, I'd vote to handle these types of refactors (i.e. moving where things are defined) wholistically rather than on a case-by-case basis. As you've noticed (cf. #7065), this is not the only instance where there may be an opportunity to refactor the module/package structure. I'd prefer to do so all at once rather than introduce individual warnings cycles for each instance.
comment
@peijenburg your proposal has a lot of moving parts - if you'd like to pursue it I'd recommend splitting it up into multiple PRs to make review easier. As @dschult points out, the easiest thing to start with is to add docstring examples showing how to compute the number of spanning trees with `total_spanning_tree_weight`. The next two points in your proposed plan are trickier as they involve API changes (thus require a warnings cycle), so those should definitely be in a separate PR!
comment
Looks like these are still failing - or at least every other run. Very strange.
comment
This will hopefully be addressed by #7309 - let's keep an eye on it for a couple days and see if the red x's go away!
comment
I believe the specific issues that were causing this have been addressed by #7311 . If new uploader issues crop up let's open a fresh issue!
comment
IMO this is would be one of the most impactful features we could add to `nx_pylab`! I too like @ShirsenduP 's solution and it is likely to work well for some use-cases, but I'm hesitant about adding it to `draw_networkx_edge_labels` as I don't think it'd be good to have the implicit dependence on the aspect ratio.  > My gut feeling is this might be possible by inheriting from FancyArrowPatch and binding a matplotlib.text.Text instance for the label somehow. Then the label position and updates upon rerendering can be accomplished with minimal duplication of existing matplotlib code.  This is a really interesting idea and in principle would solve some of the trickiest problems re: making sure the labels are kept in sync with their corresponding edges. I'm not sure how feasible it is, but :+1: if someone has the bandwidth to investigate along these lines. If it proves very useful we might even consider trying to upstream the fancy arrow patch + text object back into matplotlib itself.
comment
> I believe the changes in https://github.com/networkx/networkx/pull/7010 fix this issue of label placement along curved edges.  Agreed - from a quick readthrough it looks like all of the major features have been addressed - there are potentially some minor tweaks/improvements in which case I'd vote to open new issues. @Jogala you may be interested to take a look at #7010 or give the new multiedge label drawing a try. It's not yet in a released networkx version but you can try the development version with `pip install --pre --upgrade --extra-index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simple networkx`. Feedback welcome!
comment
> Is it possible that your main branch is from a month or two ago?  I think the problem in this case is that the PR is opened against the `networkx-3.2` tag instead of `main` - I will go ahead and close this one in favor of #7319!
comment
Thanks @eriknw , let's get this in!
comment
Thanks @matthewfeickert and @MridulS for investigating. I'll go ahead and close this, but if there's any new info on what's happening please feel free to re-open or if there's anything NetworkX needs to do to better support entrypoints on other systems, feel free to open new issues/PRs!
comment
Given the discussion above, I will go ahead and close this. I do think it's worthwhile to add an example of using the `bfs_layers` generator to only extract the first N layers, but we can do so in a separate PR!
comment
NetworkX is a pure Python package and does not require rust to be installed. If you have gotten warnings related to a rust requirement from following the contributing guide, please provide additional info about your system OS.
comment
Any follow-up here @DreadTheNaught ? I'm certain this isn't an NX issue so I'm inclined to close, but if you've managed to track down specifically what happened during environment setup it may be worth documenting.
comment
I'll go ahead and close this as "not an nx issue", but if you figure out precisely what happened in your environment @DreadTheNaught please feel free to post here in case any other users run into this issue!
comment
Quick ping here @ahmetTuzen - do you have the bandwidth for another iteration on the review feedback?
comment
This one has been stale for awhile so I'll go ahead and close it. @ahmetTuzen if you are interested in pursuing, please feel free to reopen this with commits/comments addressing the discussion in the review!
comment
> But don't you think it would be a nice extra filter option to have in the function itself  IMO it's much nicer to leverage Python's language features (i.e. comprehensions), which allow this to be expressed in an extremely clear, concise way that will be immediately understandable to anyone who knows Python. Adding a new kwarg loses this advanatage. Of course, everything's a tradeoff - if there were a significant performance gain (say >2x) from doing so, then it'd be more compelling. Absent that though, I'd be -1 on adding the kwarg.  I would however be +1 on adding an example to the docstring demonstrating the filtering!
comment
I'll go ahead and close this one here - @arunmathaisk if you'd like to update the docstring examples please feel free to do so by opening a new PR! That will make the discussion more clear anyways (and it can always link back to the discussion here)
comment
Thanks for the report (and the proposed fix). It'd be worthwhile to bisect this to figure out exactly which changeset introduced the problem - I'll aim to do that later today!
comment
I bisected using the reproducer in the OP (thanks for that!) and it looks like the refactor in #6743 is the culprit. Hopefully that provides useful context!
comment
Indeed the original incantation doesn't make sense - you can either install from a package index like PyPI *or* from source, but not both.  If you want to install version 3.2.1 from a package repository, you'd do something like `pip install networkx==3.2.1`. If you want to install version 3.2.1 from source, then indeed @MridulS 's suggestion is the correct one.
comment
> It seems like some docstring code might not be formatted currently. Am I understanding that correctly?  This is entirely possible - diving into the configuration/running of `blacken-docs` is likely a worthwhile activity, but not a blocker for these updates to the contributor guide!
comment
Let me just preface by saying I'm certainly no expert, so apologies if I've misdiagnosed the issue!  I suspect the confusion may arise from the exact definition of a cycle basis. From the docstring (emphasis mine):  > A basis for cycles of a network is a minimal collection of > cycles such that any **cycle** in the network can be written > as a sum of cycles in the basis.  Here summation of cycles > is defined as "exclusive or" of the edges.  Note that a cycle basis is not the minimal collection of cycles that will cover every *edge* in the graph, but the minimal collection of cycles necessary to represent all other *cycles* in the graph.  In this specific case, this means e.g. the cycle `[11, 12, 13, 19, 17, 14, 23, 9, 10]` cannot be constructed from a combination of any other cycles in the cycle basis. Verifying that this is true would be the next step in determining whether there is a bug.  ---  It may also help to look at a simpler example, e.g. `wheel_graph`:  ```python >>> G = nx.wheel_graph(10) >>> pos = nx.spring_layout(G, iterations=200, seed=0xdeadc0de) >>> nx.draw(G, pos=pos, with_labels=True) ``` ![wheel](https://github.com/networkx/networkx/assets/1268991/bfdb896e-5fe0-45c5-b0ab-f151c6a9d667)  And it's cycle_basis:  ```python >>> cb = nx.cycle_basis(G) >>> cb [[0, 1, 9],  [0, 2, 1],  [0, 3, 2],  [0, 4, 3],  [0, 5, 4],  [0, 6, 5],  [0, 7, 6],  [8, 7, 6, 5, 4, 3, 2, 1, 9],  [0, 8, 9]] ```  This may seem surprising: there is a "long" cycle in this basis too. If you think about it though, this is perfectly valid as there are multiple ways to represent the cycle basis. One way (perhaps the more obvious) is to take the collection of 3-cycles composed of a pair of adjacent nodes on the "rim" and the central "axle" node. In this case, the "rim" cycle is computed by summing each of the 3-cycles. However, the above basis is equally valid: the "rim" cycle is included explicitly, and the cycle `[0, 7, 8]` (the "missing" 3-cycle) can be constructed from summing the rim cycle and `[0, 7, 6]` and `[0, 8, 9]`, bearing in mind "summing" in this context means taking the symmetric difference of edge sets. Both cycle bases are "minimal" in that they contain the same number of cycles.  In any case - I suspect this is what is happening in your original example too. However, if there is a cycle basis that has fewer cycles than the one returned above (i.e. the original cycle basis is not minimal) or it can be shown that not *every* cycle can be constructed from summing those in the original basis (i.e. it's not actually a cycle basis) then there is indeed a real bug!
comment
I hope this doesn't distract the discussion too much... but since this PR largely deals with performance, I thought it'd be a good opportunity to try adding some benchmarks!  I went ahead and pushed up benchmarks for `nx.non_neighbors` (bcd2d2e) and `nx.common_neighbors` (6a36798) just so others can try out benchmarks while reviewing. You can then use these new benchmarks to compare the performance improvements of these changes relative to `main`. For example:  ``` # --bench accepts a regex for limiting which benchmarks are run asv continuous --bench CommonNeighbors main speedup_neighbors ```  Which on my system (m1 mac running asahi linux) gives  <details>   <Summary><code>asv continuous --bench CommonNeighbors main speedup_neighbors</code></summary> <pre> | Change   | Before [d6569adf] &lt;main&gt;   | After [6a367983] &lt;speedup_neighbors&gt;   |   Ratio | Benchmark (Parameter)                                          | |----------|----------------------------|----------------------------------------|---------|----------------------------------------------------------------| | -        | 1.18±0.02μs                | 742±10ns                               |    0.63 | benchmark_neighbors.CommonNeighbors.time_star_rim_rim(100)     | | -        | 1.18±0.03μs                | 735±10ns                               |    0.62 | benchmark_neighbors.CommonNeighbors.time_star_rim_rim(1000)    | | -        | 1.19±0.01μs                | 728±7ns                                |    0.61 | benchmark_neighbors.CommonNeighbors.time_star_rim_rim(10)      | | -        | 3.79±0.1μs                 | 941±10ns                               |    0.25 | benchmark_neighbors.CommonNeighbors.time_complete(10)          | | -        | 5.07±0.08μs                | 665±5ns                                |    0.13 | benchmark_neighbors.CommonNeighbors.time_star_center_rim(10)   | | -        | 30.5±0.3μs                 | 3.80±0.05μs                            |    0.12 | benchmark_neighbors.CommonNeighbors.time_complete(100)         | | -        | 311±8μs                    | 28.6±0.4μs                             |    0.09 | benchmark_neighbors.CommonNeighbors.time_complete(1000)        | | -        | 44.0±0.6μs                 | 683±10ns                               |    0.02 | benchmark_neighbors.CommonNeighbors.time_star_center_rim(100)  | | -        | 439±7μs                    | 732±10ns                               |    0    | benchmark_neighbors.CommonNeighbors.time_star_center_rim(1000)  </pre> </details>  Anyways - IMO it's really nice to be able to get a sense for the magnitude of performance changes in these types of PRs[^1], and I wanted to share my experimentation! I'm also happy to pull these two commits back out and put them in a separate PR!  [^1]: These changes are actually a little trickier to benchmark than you'd expect since the return types are switching from a generator to a set, so the benchmarks have to have something baked-in to consume the generator and ensure the outputs are comparable.
comment
Based on the discussion above, it doesn't seem like there's anything actionable here so I will go ahead and close as "won't fix". If there's a desire to continue on this, I'd vote to start by defining tests with the desired behavior to help re-kickstart the discussion!
comment
> we thought that the user should be responsible for ensuring at least 4 nodes since double_edge_swap() needs at least 4 nodes.  This is fine, but I'd think you'd want the behavior for the special case of graphs without edges to be the same regardless of the `normalized` parameter, since that's essentially a special code path that says: if edgeless, return empty dict. That's my two cents anyways, if @Schefflera-Arboricola and/or @dschult disagree then I'm fine with whatever special-case behavior!
comment
> Action: If <= 3 nodes, I make all the rich club coeffs 1. Will add tests for the same  The reasoning behind this is solid, thanks for explaining - however I would've come to a different conclusion. From the above, it seems that it's not possible to produce a random graph with fewer than three nodes - therefore the concept of normalizing by a "random" graph with the same degree distribution doesn't apply in the case where there are fewer than 4 nodes. In other words - my interpretation that the *normalized* rich club coefficient isn't a valid property for graphs where it's not possible to produce random graphs with identical degree distribution.  Assuming I've understood correctly, I'd actually advocate for the behavior you originally had; i.e. raising an exception when `normalized=True` and `len(G) < 4`. However, I'd update the exception message to indicate the reason why: i.e. the normalization procedure isn't possible if it's not possible to construct a randomized graph with the same degree distribution.  Sorry for all the back-and-forth - it's my fault for not digging more deeply into what "normalization" actually meant in this context!
comment
Thanks @DreadTheNaught , but there is already an open PR for this: #7166. I will go ahead and close this to avoid duplication. Please feel free to add any comments suggestions related to these changes over in the other PR!
comment
> I don't think I understand the motivation for doing this.  Thanks for the question! The main motivation is to enforce a policy that improves readability of code. For example, without keyword only arguments a user could write the following:  ```python my_edges = nx.dfs_edges(G, 0, 2, sorted) ```  which will execute just fine. However, to someone *reading* this code, they might be confused about what all these function parameters represent and would be forced to look them up in the docs to really understand what's going on.  However, with the keyword-only restriction, the last parameter will have to be passed in via keyword, which arguably makes the code more readable:  ```python my_edges = nx.dfs_edges(G, 0, 2, sort_neighbors=sorted) ```  Of course, it'd be better if the other two parameters were passed in with keywords too, but if we were to force them to be keyword-only that would break backwards compatibility. Since the `sort_neighbors` parameter is new for all the DFS functions, we have an opportunity to enforce this policy without breaking any compatibility!  > And will we let users use sort_neighbors as a positional arg after 2-3 version releases... when it's old?  No, the intent would be to keep this policy in perpetuity. Again the only reason we limit ourselves to the `sort_neighbors` parameter is because it's a *new* parameter, so we can make it keyword-only without breaking any existing code!
comment
> Shall we merge this to fix the bug and then solve the directed random_tree problem as a different issue?  I vote yes!
comment
Just a quick ping here @MridulS - if there are no objections to making `weight` keyword-only, then we can quickly add that and merge. Otherwise, we can just merge!
comment
Thanks @Schefflera-Arboricola ! I will close this as resolved as this is indeed the correct solution.
comment
Force-pushed to resolve merge conflicts - once everything's green it should be good to go!
comment
I'm still -1 on adding this - this makes the unit test much harder to follow.
comment
Thanks all for trying this out - I think it's reasonable to expect that the root cause was the old version of networkx, so I will close this. If the behavior persists or something else comes up please feel free to reopen (or better yet open a new issue and link this one)!
comment
IMO adding these actually motivates keeping a separate module for the related indices. From the user perspective it should be a moot point as users are encouraged to call indices from the top level namespace rather than go spelunking through the package hierarchy. In other words, it's largely an implementation detail that shouldn't hold anything up in this PR!
comment
> Should the name remain prefixed with an underscore forever?  Definitely not - we should have a discussion on the dispatching roadmap, which I won't dump so as not to distract from the PR.  > If this change is "one last time", then maybe the name is no longer preliminary or in flux?  I will say however that this is likely not the last change that would motivate removing the underscore!
comment
> Thus I think the new name needs to be paired with an explanation of the decorator, under the name "dispatchable", in the function reference documentation.  Agreed - the renaming here is certainly not sufficient to address #7189 on its own. There should definitely be a very visible and easily discoverable reference with the explanation you describe. In addition to reference docs, it might also be worth adding a term to the glossary and using the `:term:` role liberally throughout the documentation to link to the high-level overview page of the `_dispatchable` decorator.
comment
> As an aside, I think these features were not included because they led to headaches -- and a user could recreate the graph with the removed edges if needed. But that was obviously not documented nor enforced.  Perhaps the `PlanarEmbedding` could be made immutable, at least for adding/removing nodes/edges (attributes should still be okay, right?). If a user wants to modify the embedding, they can create a graph object from the embedding, e.g. `H = nx.Graph(my_embedding)`. It puts more burden on the user in terms of LOC, but at least that way things are explicit - planarity is no longer implied by the type of the object, and the need to re-compute planarity after a change is clear.
comment
> If you feel my text is rude or not polite, it is because of my lack of english. sorry about that.  Nothing at all to be sorry about @frhyme , thanks for the question.  I'm not familiar at all with `optimal_edit_paths`, but I just wanted to call your attention to #4102 - maybe the discussion will prove useful.
comment
It sounds like the documentation should be improved along the lines discussed above in order to close this one.
comment
Closed by #7199 
comment
Excellent, thanks again @smokestacklightnin !
comment
I suspect you are not actually using NetworkX v3.1 - this was [fixed in NX v2.8.6](https://github.com/networkx/networkx/pull/5937) and will work as expected with all matplotlib >=3.6. I suggest double-checking your environment to ensure you are actually getting the library versions you expect!
comment
> it works on in-browser systems like Jupyter Lite, whereas pygraphviz would not be possible  This is true since `pydot` is pure Python, though it's worth noting that:  1) You'd only be able to produce dot files - converting to actual image formats (e.g `.png`) would require graphviz to be installed (not sure how difficult this is for in-browser systems), and  2) It "works" in the sense that it is installable, but there are many defects related to updates to `pyparsing` that pydot never incorporated as it has been unmaintained for nearly 2 years.
comment
Thanks @lkk7 , that's great news! I'm very much in favor of keeping NX's interface to pydot around now that it's actively maintained and working again. I went ahead and tested `nx_pydot` against pydot 2.0 and indeed the original issues we had seen related to pyparsing 3.0 release have been resolved.  I opened #7204 to propose un-deprecating `nx_pydot`. Thanks for the heads up and the latest round of maintenance!
comment
Thanks for the answer @dschult . I'm inclined to close this as completed since the question was addressed. Perhaps a summary of terminology could be added to the `isomorphism` package docs, including `:term:`'s to link to the glossary.
comment
> It's just weird our tests (or us) didn't catch this  FWIW I have noticed this warning when I run tests locally, I just hadn't gotten around to fixing it yet. We should double-check to see if we have at least one job in CI (preferably on latest Python) that converters all warnings to errors (at least the ones that are not explicitly caught).  > Interesting that the syntax warning says the octal sequence is \420 when the actual string says \4200  I assume the test was going for the [unicode 4200 character](https://www.compart.com/en/unicode/U+4200), but it's hard to know for sure. The potential solution that I went with in #7159 was to turn the `\` into a literal escape (`\\`) so that it's *interpreted* as `\` in the string. Another possible solution would be to replace the whole `u\4200` with the actual unicode character. I'm not 100% sure what specifically this was designed to test, so opinions welcome!
comment
> Am I missing something here?  Yeah, I think we all are :). My hypothesis that this was *intended* to be a unicode character (that was written incorrectly) was really just a guess based on context.
comment
I'm -1 on style changes to the documentation. Networkx doesn't have a policy re: documentation style, and I'm -1 on adopting changes that imply one.  There are many places where the documentation can be improved by adding things that are missing or improving technical accuracy,  but I'm going to close this to avoid spending cycles on wordings.
comment
Thanks for following up @ekohilas   You make a good point about the documentation search, that's something I hadn't considered. In a perfect world, the search functionality would be robust enough to handle such cases, but as it stands that's clearly not the case.  The point I had tried to convey was to generally discourage subjective changes to the documentation. Without the search results as a motivation, this struck me as a simple preference for one wording over the other. TBH this may have been an overreaction on my part due to the mention of hacktoberfest, which is notorious for causing a lot of maintenence burden :)  > Additionally, could you please explain what you mean by spending cycles?  Sorry, not very clear wording on my part --- I was referring to reviews. In most cases, PRs require review & approval from at least two people, and reviews take time. I (seemingly unfairly) saw multiple PRs with "neighbour"->"neighbor" proposals and assumed they were the opening salvo of such proposals ("colour" -> "color"; "localise" -> "localize", etc.) and I wanted to head that off.  Anyways - your point about the search is a good one (maybe worth [raising with sphinx](https://github.com/sphinx-doc/sphinx/issues?q=is%3Aissue+is%3Aopen+search+label%3A%22html+search%22+)?). Feel free to re-open (perhaps folding in the changes from #4308 in here as well). 
comment
Thanks @MridulS ! I'll close as resolved since this is very likely the correct answer. If the issue persists, please reopen and provide the requested information about the networkx and python version!
comment
Any thoughts on this @EricPostMaster ? I tend to agree with @dschult that it seems like the desired behavior needs to be fleshed out more.
comment
> default behavior is deletion of all parallel edges unless key parameters are specified for each edge in ebunch.  I don't think this is correct - both `remove_edge` and `remove_edges_from` are consistent in that they remove an arbitrary edge when the key is not specified, not *all* the multiedges. Example of current behavior:  ```python >>> G = nx.MultiGraph([(0, 1), (0, 1), (0, 1)]) >>> G.edges(keys=True) MultiEdgeView([(0, 1, 0), (0, 1, 1), (0, 1, 2)]) >>> G.remove_edge(0, 1)  # Removes the "last" edge w/ key=2, *not* all edges >>> G.edges(keys=True) MultiEdgeView([(0, 1, 0), (0, 1, 1)])  # Or with remove_edges_from  >>> G = nx.MultiGraph([(0, 1), (0, 1), (0, 1)]) >>> G.edges(keys=True) MultiEdgeView([(0, 1, 0), (0, 1, 1), (0, 1, 2)]) >>> G.remove_edges_from([(0, 1)])  # Again, removes "last" multiedge >>> G.edges(keys=True) MultiEdgeView([(0, 1, 0), (0, 1, 1)]) ```
comment
>  I'm less confident that this is a good way to implement this feature.  I agree. IMO changing the default behavior like this is a non-starter. As noted in #5825 there are already ways to get the desired behavior, but the user interface is indeed opaque.  I'm not sure what the best approach is, but I'm -1 on the changes proposed in this PR.
comment
Note that if you place the example in one of the existing directories, e.g. `examples/drawing/plot_composite_feature_network.py`, it will be built automatically by CI and can be previewed via the build artifact link in CI summary (you may have to click "Show all checks" to see it).
comment
> Do we still want to push this in?  In the absence of a champion with interest in ete I'd lean towards no, but that doesn't mean it can't be picked up again if others express interest!
comment
Is this issue still relevant? @neocsbee could you provide more specific information from the paper that is at odds with the current implementation?
comment
Thanks for the input @Erotemic , I have to say that I very much agree - both re: having the more efficient option for producing edgelists and keeping the `create_using` keyword :+1: 
comment
IIUC you *do* see the problems with e.g. `dask` and `ray`, but *don't* have problems when using `multiprocessing.pool`, did I get that right?  I don't have any particularly solid ideas as to why this would be the case. However, one thing that comes to mind that is at least easy to check is whether this might be the result of a bad reaction with an underlying library, such as OpenMP. This shows up every now and then when using e.g. BLAS on HPC. If your `analyze` function *only* contains calls to NetworkX functions then it's difficult to see how this would be the case, unless `dask.compute` is using OpenMP under the hood.  Anyways, the simple thing to try would be to manually suppress threading by setting the `OMP_NUM_THREADS=1` environment variable. This of course likely won't solve your problem (as you want to parallelize the computation), but may be useful for diagnostic purposes. If it is simple for you to do so, can you try this and see if there is any difference in the "hanging" before the computation starts with your 6+ GB input?
comment
Sounds reasonable for me, thanks for taking the time to put together the summary!
comment
`shortest_path` has a default value of `weight=None`, meaning that weights are ignored by default. If you want the shortest path considering weights, then use `nx.shortest_path(G, "A", "I", weight="weight")`, which gives you the expected result.
comment
> Isn't this tricky?  YMMV - unfortunately there's no "obvious right answer" here. Clearly documenting the behavior and relying on users to be explicit is at least a good starting point IMO!  > What then is the meaning of A-D-G-I as so-called shortest path, when weight is ignored? There are several ones with only 3 edges ...  Again, referring to the [docstring for `shortest_path`](https://networkx.org/documentation/latest/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path.html):  > There may be more than one shortest path between a source and target. > This returns only one of them.  If you want an exhaustive listing of *all* possible shortest paths between two nodes, try `list(nx.all_shortest_paths(G, "A", "I"))`
comment
Any update here @wangjiawen2013 ? Without a minimal example it will be very difficult to debug this.
comment
For functions that *do* copy attributes, maybe we could consider switching to `deepcopy` as the default? I doubt many users want/expect the attributes in the union graph to be references back to the same mutable object from the input graph(s).
comment
From the discussion in #6667, it was noted that documenting this thoroughly would necessitate adding quite a lot of text to many docstrings. This may end up being a net-negative as a big chunk of the docstring would then be dedicated to a Python language feature (i.e. shallow copying by default) rather than to the functions themselves.  Perhaps we should close this one as not-actionable then? WDYT @MridulS ?
comment
This has been stale for quite some time and will likely need significant TLC to get up and running again. Are you still planning to work on this @stevenganz ?
comment
> If we do change the all_simple_paths and all_simple_edge_paths to yield the single node paths, I think we would need to deprecate the current skipping of those edges. Does anyone think we should fix this without deprecation (e.g. because the current behavior is wrong)?  Without having any real sense of how disruptive this would be in user code, my general feeling is that deprecating/warning would be overkill. IIUC, there is already an inconsistency in behavior between various functions, so user code would be getting different answers depending on which functions were used. Within this context, modifying behavior to improve consistency between functions feels more like a fix than an unexpected change!
comment
Two more cents from me: I think the best way to discuss/build consensus on this would be to have a PR that adds tests for the desired behavior across all relevant functions.
comment
> There are some unrelated tests that are failing in CI that I don't know how to fix. I already rebased onto the latest master.  I suspect this is due to the latest release of scipy (which came out yesterday). No need to worry about it in this PR, thanks for trying a rebase though!
comment
>  let me know if I should add test cases for this  Yes this should certainly be tested. Probably the easiest thing to do in this case is test the internal `_remove_terminal_nodes` function directly.
comment
> but we can pass multigraph objects in the k_core , k_shell , k_crust and k_corona like this  The issue doesn't have to do with the multigraph instances, but rather with the changing exception type itself. Here's a concrete example; the following code will run fine in networkx 3.2:  ```python >>> import networkx as nx >>> G = nx.complete_graph(5) >>> G.add_edge(1, 1) >>> try: ...     nx.core_number(G) ... except nx.NetworkXError: ...     print("Caught it") Caught it ```  However, because this PR changes the exception type from `NetworkXError` -> `NetworkXNotImplementedError`, the above try/except statement will no longer gracefully catch the exception:  ```python >>> try: ...     nx.core_number(G) ... except nx.NetworkXError: ...     print("Caught it") Traceback (most recent call last)    ... NetworkXNotImplemented: Input graph has self loops which is not permitted; Consider using G.remove_edges_from(nx.selfloop_edges(G)). ```  In practice, this means that any user code which is catching exceptions related to the `k_` functions may now result in exceptions when previously the code did not fail. Because there are many users of NetworkX, we always try to keep a look out for instances where changing code within NetworkX is likely to break the code of downstream users who depend on it!  Just to restate my opinion from above though: I'm willing to try sneaking this breaking change in, since IMO it is likely to affect only a relatively small number of users (i.e. those explicitly catching exceptions from `k_` functions).
comment
Let's keep the discussion in #7112 since creating the issue loses the context, i.e. the existing responses. Issues/Discussions is not really an important distinction in this case (contributors who watch the respository will be notified of activity on either).
comment
6f2e05d seems to be the culprit - apparently the `.. plot` directive doesn't obey `doctest_global_setup` though it looks like [`plot_pre_code`](https://matplotlib.org/stable/api/sphinxext_plot_directive_api.html#configuration-options) will do the trick. Feel free to push that up or I'll try when I have time!
comment
`-0.0` and `0.0` are numerically equivalent but may not hash the same. If you change the `-0.0` attribute in edges_s1 to 0.0 then the hashes are equivalent.
comment
One thing to keep in mind is that the example should highlight something about graph/network analysis. There is a fine line to walk between making an example compelling and making it understandable. For me anyways, making the example *more* complex is heading in the wrong direction :). That's not to say that it isn't valuable - in fact, as the scope of the example here continues to increase, it may be a better fit for the [networkx-notebooks](https://github.com/networkx/notebooks) repo, which is a more natural fit for larger examples and examples that would benefit from some narrative/explanatory text.  Also - there's no need to continue attaching images to the GH comments - all examples are run by the CI and can be previewed by clicking on the "ci/circleci: build artifact" tab in the CI check summary. This is actually better for review because it also guarantees that the image is being generated as expected (which is [not currently the case](https://1592-890377-gh.circle-artifacts.com/0/doc/build/html/auto_examples/index.html#external-libraries)).
comment
Thanks for the ping @lobpcg - I still have the sense that this is a better fit for a scikit-learn gallery rather than networkx. Since it's been 18 months since last activity and is still in draft, I'm inclined to close. I'll do so soon if there are no objections!
comment
The conflict arises from changes that were merged in #6165 related to pytest requiring the setup method of test classes to be renamed from `setup` to `setup_method`. Make sure to preserve the new method name when resolving the conflict locally!
comment
Gentle ping @mingxuan-he - are you still working on this one?
comment
@joyemang33 are you planning on continuing with this one? If so, I think we should close this and reopen a new one that originates from a different branch (i.e. not your main branch) which should help with unintended changes (i.e. the scc stuff) and will allow reviewers to push directly to your branch, speeding up the review process.  Alternatively, if you don't have the bandwidth it's perfectly fine to close this and start fresh!
comment
> Not entirely sure why the "style / format" test is failing. It might be an issue with black. Any help or suggestions would be greatly appreciated. Thanks.  The easiest way to tackle linting is to use `pre-commit`. See [the contributor guide](https://networkx.org/documentation/latest/developer/contribute.html#development-workflow) for more info, particularly bullets 1 and 4!
comment
> I'd like to note that the function I am proposing is flexible enough that a user can specify their own cluster dictionary, or they can use the default setting, which automatically calculates clusters using the community detection algorithm.  Agreed - I didn't mean that there was anything wrong with your implementation! My sense is that a clustered layout is relatively straightforward to achieve with existing functionality, and therefore a "teach someone to fish" approach might be better. For example, I'd probably approach a 1D hierarchical-clustered layout like (using the Davis club graph as an example):  ```python import networkx as nx G = nx.davis_southern_women_graph()  # Example graph, there are probably better to illustrate this! communities = nx.community.greedy_modularity_communities(G)  # Get some "clusters" # Compute positions of the clusters as if they were themselves # nodes in a supergraph, with a larger scale factor supergraph = nx.cycle_graph(len(communities)) superpos = nx.spring_layout(supergraph, scale=3) # Use those "supernode" positions as the center of each cluster centers = list(superpos.values()) pos = {} for center, comm in zip(centers, communities):     sg = nx.subgraph(G, comm)     pos.update(nx.spring_layout(sg, center=center)) # Nodes colored by cluster for nodes, clr in zip(communities, ("tab:blue", "tab:orange", "tab:green")):     nx.draw_networkx_nodes(G, pos=pos, nodelist=nodes, node_color=clr) nx.draw_networkx_edges(G, pos=pos) ```  The key concept here is the whole clusters-as-nodes in a hierarchical layout scheme. This has some nice properties:  1. It's relatively easy to reason about and code (~5 LOC for the actual cluster positioning)  2. It's extensible! For example, if a user wanted to do hierarchical clustering (i.e. clusters-of-clusters, ad infinitum) it's relatively straightforward to do so by expanding the number of "supergraphs".  Anyways, just my two cents.  > I'd be happy to update or create a gallery example. However, I have no idea how to do that and would need some guidance.  The [developer guide](https://networkx.org/documentation/latest/developer/contribute.html#adding-examples) has a brief section on how to add gallery examples which (hopefully) covers the basic pattern.
comment
I went ahead and turned the suggestion above into a [gallery example](https://networkx.org/documentation/latest/auto_examples/drawing/plot_clusters.html#sphx-glr-auto-examples-drawing-plot-clusters-py).  Thanks for bringing this up @metalcorebear ! I will go ahead and close this now that the example is in. Feel free to follow up if you'd like to push this further!
comment
> Any chance you know when it might be released? :)  Not concretely, but given the recent past I would estimate the next minor release would be sometime in early 2024
comment
> Do these doc_strings show up anywhere in the online documentation?  Good news - they do indeed! The `Filter*` classes are already included in the coreviews autosummary. For example compare the [autosummary from this PR](https://output.circle-artifacts.com/output/job/dffcf823-2949-4ac1-a918-d3abfacfd439/artifacts/0/doc/build/html/reference/classes/index.html#module-networkx.classes.coreviews) to the [devdocs](https://networkx.org/documentation/latest/reference/classes/index.html#module-networkx.classes.coreviews).  Similarly the class docs are autorendered as well: [this PR](https://output.circle-artifacts.com/output/job/dffcf823-2949-4ac1-a918-d3abfacfd439/artifacts/0/doc/build/html/reference/classes/generated/networkx.classes.coreviews.FilterAtlas.html#networkx.classes.coreviews.FilterAtlas) vs. [devdocs](https://networkx.org/documentation/latest/reference/classes/generated/networkx.classes.coreviews.FilterAtlas.html#networkx.classes.coreviews.FilterAtlas)  There's certainly some tweaking that could be done; starting with formatting with cleanup, but the overall approach is good!
comment
> Could you please review it and tell me, what further modifications it requires  The best way to do so would be to follow the procedure outlined above:   > To generate the necessary test cases, you'd want to come up with tests that fail with the current version of networkx but pass once the fix is in place.  The OP in the corresponding issue are a good place to start.
comment
>  But I’d prefer that we return an object that makes sense for the request rather than raise an exception. That way when people apply this function to a large collection of graphs, and one happens to e.g. have no edges, it doesn’t break the process with an exception. I know they can just use try/except. But it’s probably better for them to just get an empty list back… Thoughts?  I agree that this behavior is preferable from a usability standpoint :+1:   > The first and 3rd case can be handled as one case by testing for G.size() == 0 in which case my suggestion is to return [{n} for n in G]. That would be an empty list for the null graph, and each node in its own community for the no edges case. I think these make sense…???  I like this idea, assuming it makes sense in the context of what constitutes a "community" - I defer to the experts! @z3y50n does this sound reasonable to you?
comment
> This is an API change. Should we do the deprecation workflow?  I'm inclined to see if we can get away without one. AFAICT this will only burn folks that are explicitly catching `sp.sparse.linalg.ArpackNoConvergence` errors.
comment
NetworkX 3.2 has dropped official support for Python 3.8. The built-in importlib for 3.8 doesn't have all the necessary functionality for this to work. I believe there is a compatibility shim available on PyPI for older versions of Python which may work.
comment
Now that I'm at a computer - here's the backport for `importlib.resources`: https://pypi.org/project/importlib-resources/. If you need support for Python <3.9, you'll need to have this installed in the environment.
comment
> Exactly that version I've reported opening this ticket.  Sorry, I missed that in the summary. In that case I'd consider opening an issue there.
comment
Thanks for reporting @sigma67 . Out of curiosity, can you share additional info about your OS, environment manager (`venv`, `virtualenv`, `pipenv`, `conda`, etc.), Python source (i.e. a distro package manager, built from source, etc.) and installation method? This was reported in #7032 with `poetry` and `pixi`, but we have not been able to reliably reproduce. If we were able to nail down *exactly* why this SyntaxWarning is affecting some users and not others, we could add a test for it to prevent something like this happening again in the future!  > Ah yes, most likely. This will only be released in a few months likely, right?  Given that this issue has affected multiple users I think a patch release is worthwhile
comment
To summarize: between #7032 and this issue, we have warnings being raised at import time when networkx is installed with `pdm`, `poetry`, and `pixi` (though perhaps not always). We haven't been able to reproduce with either `pip` or `conda`.  This seems to be affecting at least some folks using "third party" Python package management systems.
comment
> In some configurations (that I haven't been able to track down) I can't use pytest with networkx.  I've noticed this as well - one thing I've noticed that may or may not be relevant is that re-running `pip install -e .` usually fixes the problem. Perhaps there's something related to the entry points not playing nice with editable installs?
comment
@akshayamadhuri please feel free. I think the best place to start is with [dschult's suggestion](https://github.com/networkx/networkx/issues/4225#issuecomment-705956344)
comment
Yeah the warning has shown up in the test suite, but I've never seen it at import time. Maybe something to do with either poetry, Python 3.12, or the `stacklevel` somehow being different in whichever environment which causes warnings that are normally invisible to appear :shrug:   Either way we should definitely fix and perhaps have a patch release for if other users are seeing this as well.  Thanks for the report @treykeown !
comment
Thanks for sharing your detailed work and findings @ArturoSbr ! IIUC this issue is resolved, so I will close it - please feel free to reopen if I misinterpreted!
comment
> Please update your python version before installing networkx  Or, if you absolutely need Python 3.8, stick with networkx<=3.1!  I'll go ahead and close this since it's expected - fortunately the error message from pip is raising and informative!
comment
FWIW this has come up before (can't find the issue/discussion at the moment) though at the time I believe this style of plot were referred to as [hive plots](http://www.hiveplot.com/).  I too think this is a good candidate for a gallery example - boosting visibility to external graph visualization libraries is certainly within scope, and the hive plot library seems very feature-rich and well supported.
comment
My preference is still to refer users to [hiveplotlib](https://geomdata.gitlab.io/hiveplotlib/networkx_examples.html) (or similar) rather than add this layout here.
comment
> I still feel like this layout may be beneficial for some people.  I agree! My objection isn't that this isn't useful, but that there is already a (seemingly) well-supported and very well documented alternative available in Python. 
comment
Thanks for reporting & digging into this @chrisgmorton , definitely something to keep an eye on.   > so I am wondering if you have done any performance testing with Python 3.12  NetworkX recently added some basic benchmarks in #6835 and we are working on infrastructure to periodically test & report on performance, though I'm not sure whether a lot of attention has been paid to 3.12 yet.  Also in this vein, python/pyperformance#309 proposes some basic benchmarks from NetworkX that might be useful upstream.
comment
Concurrent with the investigation of the new algorithm, it would be a huge help if we improved testing for d-separation.  Ideally we'd have tests to cover the scenarios that are missing and/or ill-defined now. This would also be the most effective way of communicating the intended semantics!
comment
> So i guess that can not contribute to that error.  Correct, the error was unrelated to your changes.  >  Will i have to wait for a comment in case there is something new ?  The problem has been fixed on `main`, so you can fix in your PR by merging `main` into your `extendability` branch. As Dan mentions above, the easiest way to do this is `git pull upstream main`. After that, you can `git push` the result and the CI will run again.
comment
> I was wondering about whether the code is going to be added in networkx library.  Thanks for the ping and sorry for the delay; the release of NetworkX v2.6 has taken up a lot of time recently and we haven't had a chance yet to switch gears back to reviewing new contributions for later releases. I aim to give this PR another review as soon as I can.
comment
Oops - looking further down the list of merged PRs, it looks like this issue has already been solved (hence the merge conflicts). I'll go ahead and close this then, thanks for the proposal @achluma !
comment
This was closed by #6992
comment
> Interesting idea. What would you want the linked docs to show? Implementation? So far, the only docs that backends have are the repositories and now the networkx docs.  Good question - I hadn't thought about this too hard, it was mostly just a reaction to the current link landing location (the backend github repo) which in general isn't particularly useful, at least not for function-specific-link. I'll admit I hadn't considered that I was asking backends to have backend-specific docs, which I certainly wouldn't want to burden implementers with! Maybe an alternative would be to show a call signature with the specified backend (with supported arguments) instead of the link.  These are just spit-balled ideas though, the current situation is definitely a good start!
comment
> We need to ensure that this function works and continues to work. So we should add tests (of the example provided, and maybe other tests as well...)  Big +1 here - a change like this should definitely also include tests, especially if there is potential for violating assumptions about node types (e.g. whether they are `min`-able). At the very least the test case from the original issue should be added!
comment
Before spending more cycles on this, you may want to check out [generate_network_text](https://networkx.org/documentation/latest/reference/readwrite/generated/networkx.readwrite.text.generate_network_text.html), which AFAICT implements a lot of these features. Note that by default `generate_network_text` will use unicode characters for richer representation, but there is also a `ascii_only` kwarg if you need to limit to ascii characters.
comment
> which is an optional attribute and should not be relied upon.  Interesting, I'm not aware of any best-practices etc. that discourage the use of `__file__`. Do you have any additional resources you could point to (aside from the already linked issue) that expands on this?
comment
Thanks for the additional info. After reading through the linked PyOxidizer issue I was struck by the seeming lack of a standardized way to handle the replacement of `__file__` in libraries. If someone wants to take this up for NetworkX they're welcome to do so, as long as the solution is grokkable and doesn't introduce new maintenance overhead. 
comment
FWIW I pushed up a few minor fixups (var-renaming, rm extra spaces) and **rebased** on main to get the CI updated without screwing up the diff.
comment
> I moved on, so feel free to change as you like.  Unfortunately maintainers are not able to edit the PR since it originates from your `main` branch. Therefore I will go ahead and close this one and re-submit on a different branch that others have the ability to modify. Thanks for the heads up!
comment
Thanks for reporting - this strikes me as a documentation issue, as indeed the FR/spring layout are only intended for 2D and 3D layouts.
comment
> Still, the open question for me: is there an algorithm to order (1D) all nodes of a graph with weighted edges so that nodes connected by heavier edges will be close to each other and with lighter edges far? I hoped the spring algorithm could help me with the problem, but it cannot.  Nothing jumps out to me as an obvious solution for a complete graph as there are connections between every node - it's not obvious to me how to embed all this information in one dimension.  One thing you could *try*[^1] is your original idea of projection: compute the layout in a higher dimension (e.g. 2D) and project it down to 1D. For example, compute the best-fit line for the collection of 2D points, then project the points from 2D down onto that line. I have no idea if this gets you any closer to what you're really hoping for so YMMV!  [^1]: strictly for visualization purposes, this is not at all a rigorous approach to the real optimization problem!
comment
I think this is related to, though not quite a duplicate of #4781 . To me, this is a sharp corner related to the fundamental flexibility of nodes/edges are specified and queried. I'm not sure there's much that can be done here without a significant disruption to backwards compatibility.  I'm inclined to close this as "won't fix" similar to #4781. Any objections? Is there a possible avenue for improvement here with at least a semi-solid idea of the impact it would have on existing code?
comment
There's definitely something strange going on with `label_attribute`. The docstring does indeed make it seem like the labels of the original graphs are stored as attributes in joined graph, but that's clearly not the case. This feels like a bug to me, but I don't know enough about the join operation to say whether this was intentional. The first step should be to determine what the behavior actually should be before we go about trying to change it!
comment
FWIW here's a minimal example of the current behavior:  ```python >>> G = nx.path_graph(3) >>> H = nx.path_graph(3) >>> joined = nx.join([(G, 1), (H, 1)])  # Joining at the "middle" node of the path >>> joined.nodes(data=True) NodeDataView({1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 0: {}}) ```  I would've expected the node attributes from the last line to have one attribute with key `"_old"` and the value set to what the node value was in the original graph. 
comment
This seems like it's related to #4895 . For instance, would we get the 10-15x speedup in `nx.selfloop_edges` mentioned in #4895 if `adj` were an attribute instead of a property? Said another way, is the AdjacencyView object creation the bottleneck for that (and other) functions/operations?  There are obviously tradeoffs involved here that affect core functionality, so it'd be nice to have some robust benchmarking in place to get a sense for what the impact would be.
comment
Thinking about this a bit more, I think there are even more dimensions to consider (or: how to make the multiedge drawing at least as complicated as the single-edges!)  For example, we might want to consider the ability to draw multiedges with `LineCollection` for  for performance reasons. It's possible to imagine drawing lines between nodes that are parallel to one another to denote separate multiedges without requiring any arcing (i.e. not necessarily using fancy arrow patches). However; how to implement, test, and switch this behavior seems like another headache: we already have problems switching between LineCollection/FancyArrowPatch edges for the *single* edge case.  Just some more food for thought :) This would definitely be a nice feature though! 
comment
> This is just thoughts put down in email. Are there better ways to arrange this code?  IMO yes, though the problem is not with the draw_multiedges function proposed here, but the fact that the `draw_networkx_edges` function is extremely difficult to reason about in it's current state.  It would be great to refactor `draw_networkx_edges` so that the two `_draw_networkx_edges_line_collection` and `_draw_networkx_edges_fancy_arrow_patch` functions weren't defined in the module itself rather than internally in the `draw_networkx_edges` function. This should make the logic for handling e.g. self-loops *much* more clear, and should make it more straightforward to plug the multiedge drawing right into the `draw_networkx_edges` function. OTOH this refactor is *probably* a bit tricky. The good news is, the edge drawing is much better tested than it was say 3 releases ago, so developers can be *slightly* more confident that they're not breaking everything in an attempted refactor. It would be a shame to block the new functionality here for these unrelated reasons though, so +1 for pursuing multiedge drawing independent of the `draw_networkx_edges` mess; though I would also strongly advocate to make the multiedge drawing function private (as has been done here).
comment
> Though this still leaves me with a small question: Is all text in the documentation manually justified (i.e. spaced to use the full 60-character width), or is that normally done automatically with a tool?  That's likely a historical artifact - there is no reason for it to be justified like this. Generally speaking the only real "rule" is to aim for a ~88 char line limit. It'd be nice to "fix" this, but since it's a style change, it'd best be done by a formatting tool.
comment
I will go ahead and close this as there's no general approach to issues related to fp precision.
comment
A quick pdb session seems to indicate that the problem is in `find_entering_edges`, presumably in the construction of the [while loop](https://github.com/networkx/networkx/blob/1de57a8ca852cf27820512720968c6cffb51a74b/networkx/algorithms/flow/networksimplex.py#L313-L337). Specifically, the inputs seem to put this loop in a condition where the reduced cost `c` seesaws between `-0.0` and `0.0` on subsequent iterations, resulting in `m` never dropping below `M`, hence the infinite loop.  On a side note, it doesn't seem that the `network_simplex` function is explicitly tested (the `network_simplex` function is only called internally in some of the tests in `test_mincost`). In addition, `network_simplex` defines quite a few internal functions/generators that are themselves not tested. This module/function seem like great candidate for a refactor to split out some of the nested functions and write unit tests for them.
comment
I will go ahead and close this as the issue is documented and there's no (t yet) a workable, general approach to issues related to fp precision.
comment
Thanks @dschult for the thorough answer! I will close this as I don't think there's anything actionable specific to `communicability`. Given how frequently floating point issues pop up, it may be worth adding a blurb to the tutorial or somewhere similar, even though fp issues are not specific to networkx.
comment
If the graph `G` is undirected, then this is expected. You can think of undirected edges as bi-directional, in which case there is no distinction between predecessors and successors:  ```python >>> G = nx.path_graph(4) >>> nx.dfs_successors(G, source=1, depth_limit=1) {1: [0, 2]} >>> nx.dfs_predecessors(G, source=1, depth_limit=1) {0: 1, 2: 1} ```
comment
Thanks for the example, I see what you mean now: I assume you are expecting `nx.dfs_predecessors(G, source=1)` to give the predecessors of node `1`, so something like `{1: [0, 9]}`. The current result does seem incorrect to me.
comment
#6856 and #6857 should address all the future compatibility issues with numpy and scipy, thus should resolve all failures with the nightly wheels.
comment
> So, we should talk about how best to handle it  My initial thought was to investigate how many of the tests we could reformat so that the output didn't contain scalar repr's. From a quick glance it seems there may be a few that can be modified, but I don't think that will work in every case. Therefore investigating alternatives is worthwhile.  I think @dschult 's proposal is the most workable, even if it requires a bit of manual config/pinning in CI. Another option might be to use `np.printoptions` to set printing to "legacy" mode for the scalars, at least for the purposes of testing. I *think* this is doable, but we'll have to look into it more (if it is possible, it should probably be added to the NEP!)  The silver lining of these failures is that the new scalar repr identifies some places in the codebase where numpy scalars are returned implicitly. Kinda neat IMO :)
comment
The currently failing test (as of b9218fb) illustrates an interesting corner case: what happens with `edge_attr=None` (indicating the user doesn't want any attributes) but the dtype of the array is structured (which is currently interpreted as "map all named attributes to edge attributes in the graph").  I *think* what we want is for `edge_attr` to override the `dtype` - in which case it looks like the code may need to be adjusted to handle this case.
comment
This seems like a reasonable request to me. I can certainly see a use-case for simply using the adjacency matrix to specify edges *without* creating edge attributes (e.g. to save memory). It doesn't look like there's an obvious way to do this, so I think this would be a nice enhancement.  Of all the proposals above I think the one I like best is to add a new keyword argument (though with a more generic name... maybe `edge_attribute`?) to indicate the edge attribute name. By default this would be `edge_attribute="weight"` for backwards compatibility, but users could also pass in `None` which would then treat the adjacency matrix as binary only indicating edge existence and not store attributes.  There are some potentially messy corners (e.g. the interpretation of `0` as non-edge and making sure everything works well with structured dtypes) but overall I think this would be an improvement!
comment
Let's keep the scope narrow so things are easier to review. I'd be +1 for a PR to address this feature in `from_numpy_array` specifically!
comment
I think this is a bug in scipy. It looks like `np.dot` semantics are not consistent between sparse arrays and sparse matrices. In both cases, `np.dot` *should* result in matrix multiplication (since both inputs are 2D arrays), but it looks like `np.dot` is element-wise multiplication on the sparse arrays:  ```python >>> import scipy as sp >>> G = nx.Graph() >>> G.add_edges_from(<cp edgelist from OP>) >>> node_list = list(range(14)) >>> A = nx.adjacency_matrix(G, weight="weight", nodelist=node_list) >>> A <14x14 sparse array of type '<class 'numpy.float64'>'         with 56 stored elements in Compressed Sparse Row format> >>> M = sp.sparse.csr_matrix(A) >>> M <14x14 sparse matrix of type '<class 'numpy.float64'>'         with 56 stored elements in Compressed Sparse Row format> # Dot on sparse matrices == matrix multiplication >>> np.dot(M, M) <14x14 sparse matrix of type '<class 'numpy.float64'>'         with 150 stored elements in Compressed Sparse Row format> >>> np.allclose(np.dot(M, M).toarray(), (M @ M).toarray()) # Dot on sparse arrays != matrix multiplication >>> np.dot(A, A) <14x14 sparse array of type '<class 'numpy.float64'>'         with 56 stored elements in Compressed Sparse Row format> >>> np.allclose(np.dot(A, A).toarray(), (A @ A).toarray()) False >>> np.allclose(np.dot(A, A).toarray(), (A ** 2).toarray()) True ```  FWIW the matrix multiplication operator (`@`) should be used instead of `np.dot` as it is unambiguous and will solve your issue here. I will open an issue upstream.
comment
> Not sure why any tests are failing as I only modified the documentation. Open to any suggestions.  The test failures are unrelated to these changes - the failing tests stem from changes in scipy, which had a new release yesterday.
comment
The main consideration is that `empty_graph` is used extensively (both within the library and user code), so this change has the potential to introduce subtle bugs that aren't caught by the test suite. NetworkX 3.0 is slated to come out in the very near future, so this PR is waiting until after that release in the interest of risk-avoidance.
comment
It looks like this PR is malformed in some way, reverting some changes from previous PRs. The easiest thing might be to start fresh by creating a new branch off of master that has only the changes related to the feature you're proposing. Feel free to ping if you have questions about how to do this!
comment
I think it may need a bit more thought to consider how best to incorporate 3D plotting within the `nx_pylab` drawing functions. The most general approach is probably to use an `Axes3D` object rather than individual objects like `Line3DCollection` from `art3d`. The question then becomes - does the `Axes3D` class have the same interface for the subset of keyword arguments that get passed through during drawing (e.g. `node_shape`->`marker` - is the marker support the same for `Axes` and `Axes3D`)?  Note that there is a [gallery example](https://networkx.org/documentation/latest/auto_examples/3d_drawing/plot_basic.html#sphx-glr-auto-examples-3d-drawing-plot-basic-py) demonstrating 3D graph visualization with matplotlib. Note that the examples uses matplotlib directly because I don't think whether/how to support 3D plotting in `nx_pylab` has been fleshed out in any sort of detail. It's definitely worth investigating if you're interested!  On a side note - if you have a nice 3D visualization, please consider adding it to the gallery!
comment
> The problem with FancyArrow class is that there is no equivalent for 3d vectors,  Thanks @cvanelteren, I think you've hit the nail on the head. The fact that there isn't an equivalent API for 2D and 3D plotting is what makes me hesitant about adding the 3D drawing support in the `nx_pylab` drawing functions themselves.
comment
#5597 looks like it's probably related too.
comment
Gentle ping @ManasviEmmadi - are you still working on this?
comment
> Is there a preferred way of finding the order in which the nodes appear in the cycles?  I can't say that this way is "preferred", nor can I guarantee that it will even always work, but the naive thing to do would be an exhaustive search of potential edges from the cycle basis in `G`:  ```python >>> import itertools >>> cycle_bases = nx.minimum_cycle_basis(GG) >>> cb = cycle_bases[0] >>> cb [0, 5, 6, 8, 13] >>> potential_edges = { ...     (u, v) for u, v in itertools.product(cb, cb) ...     if u != v  # ignoring self edges in this example ... } >>> cycle = potential_edges & set(GG.edges()) >>> cycle {(0, 6), (0, 8), (5, 6), (5, 13), (8, 13)} ```
comment
Right, but `eigenvector_centrality` works for undirected graphs as well, where there is no explicit distinction between predecessors/successors/neighbors. This is why I'm not 100% on the terminology and would like someone with a better handle on the graph model + a stronger linalg background to weigh in!
comment
I can confirm that this issue is present on `main`. Here's my reproducer (with visualization):  ```python >>> G = nx.DiGraph( ...     [ ...         ("A", "B"), ...         ("A", "E"), ...         ("B", "C"), ...         ("B", "D"), ...         ("D", "C"), ...         ("D", "F"), ...         ("E", "D"), ...         ("E", "F"), ...     ] ... ) # Draw graph reflecting DAG structure >>> for i, gen in enumerate(nx.topological_generations(G)): ...     for n in gen: ...         G.nodes[n]["layer"] = i >>> pos = nx.multipartite_layout(G, subset_key="layer") >>> nx.draw(G, pos, with_labels=True) ``` ![dsep](https://user-images.githubusercontent.com/1268991/201769426-ff1a098a-ef1b-495e-8184-6c1892ac8e00.png)  It's clear that node "F" is unreachable from "C" regardless of "D", so {"D"} is not a d-separating set:  ```python >>> nx.d_separated(G, {"C"}, {"F"}, {"D"})  # as expected False ```  Perhaps `nx.is_minimal_d_separator` assumes that `z` is a valid d-separating set? If so, then this is likely fixed by adding a check that enforces this assumption. Otherwise there's likely a deeper bug in the `nx.is_minimal_d_separator` implementation.  
comment
> I agree that strong statements in the doc_string are preferred over an extra keyword argument.  This would be my preference too. In this case though I don't think updating the docstring is enough. Given the function name I think it's reasonable for users to expect that the function would check that `z` is both valid and minimal. IMO the best approach is to explicitly check that the input is indeed a valid d-separating set. Even though this makes things more computationally expensive, it's the most straightforward way to get clear, correct results without complicating the API. In the long run, it'd be great to refactor the function so that it is capable of checking that a given input is both a *valid* and *minimal* d-separating set all in one go to avoid the duplication.
comment
@mjschwenne was this issue closed by #5394 or do you think there's more to do here?
comment
Gotcha, I just saw the purple "merged" button and thought I'd check. Thanks for the clarification!
comment
I suspect this is because `make html` also builds the example gallery, which has dependencies beyond those that are strictly necessary to build the rest of the docs. Wording suggestions for the contributor guide welcome if this fact isn't sufficiently captured!
comment
FWIW I think the reason NX doesn't necessarily recommend that is because some of the dependencies in `extra`/`example` are not always easy to install, esp. w/ the latest stable version of Python (this is why they're broken out into separate requirements files).  I would perhaps instead mention `make html-noplot` to skip building the gallery in the case where the other dependencies are not available.
comment
Seems reasonable to me, the [current contributor guide](https://networkx.org/documentation/latest/developer/contribute.html#documentation) seems to capture most of this modulo the bit about the extra dependencies - PRs welcome!
comment
I suspect this is because you're trying to build the development documentation with a non-development version of networkx (both `walks` and `girth` are not yet released). Make sure you've installed the development version of networkx in the environment where you're trying to build the docs (e.g. run `pip install .` in the source directory).  I know other projects do a check of the git hash when building docs to make sure it matches the installed version - NetworkX could do this as well, but I'm not sure it's worth the trouble (it still won't build, but the exception(s) will be much more clear)
comment
The error text still makes me strongly suspect a development environment issue - the functions it's tripping on are the only ones that are not part of the latest stable release (but are in dev). FWIW I can't reproduce locally:  ``` python -m venv nx-dev source nx-dev/bin/activate pip install -r requirements/default.txt -r requirements/doc.txt -r requirements/extra.txt pip install -e . cd doc make html-noplot  # or make html, with a few non-blocking errors from the gallery ```  YMMV with conda, or mixing pip+conda.
comment
Rounding may also give unexpected results for others who don't have floating point precision comparison issues... Generally this is something that has to be handled in user code. i.e. You can convert attributes to integers with the precision level that you care about.
comment
Yeah, this is a general "issue" with floating point representations - comparisons are always tricky!
comment
In Python, the statement `G2 = G` does not copy the underlying object, but instead binds a new name (`G2` in this case) to the same underlying object that the variable name `G` is bound to.  If you want a copy, you have to indicate so explicitly: `G2 = G.copy()`.  Closing as this is a Python language feature and not related to networkx; hope this helps!
comment
Looks great! Thanks @EfremBraun and @dschult !
comment
This was discussed at the most recent [community meeting](https://github.com/networkx/archive/blob/main/meetings/2023-04-04.md#topics) and the (loose) consensus was that it should be the responsibility of the user to know that the input graph does not contain self-loops. This fact should be documented of course, so I think this PR is on the right track!
comment
> I had a question, In a few examples I observed comments written to explain the block of code, and in some, I didn't. I see in the minor tweaks the comments have been removed. Please let me know when to include comments and when not to so I can learn and grasp the preferred style of code for future contributions.  That's a good question! Honestly, it's pretty subjective. My *personal* taste is to avoid commenting in cases where the code is "self-documenting". There is no rule though, so if you feel the comments add to the example, please feel free to re-add them!
comment
> I think that memory-wise the stack this way consumes probably 1/3 less space  Well, at least 1/3 the *number of objects*, which is not the same as memory since `depth_now` is just an integer. I was curious what this would look like in practice I tested with `path_graph(10_000_000)`[^1] and used `memray` for memory profiling:  #### On main  ![mem_main](https://user-images.githubusercontent.com/1268991/213885240-81672a23-376b-4e73-ab0b-8d11ebcf1146.png)  #### This PR  ![mem_patch8](https://user-images.githubusercontent.com/1268991/213885253-ff293710-2311-418a-ba1c-335ade5889da.png)  So that's a 400 MB reduction in total usage and 300 fewer allocations for this call (which should help with performance), but accounting for the other changes it ends up being about 300 MB saving for `dfs_edges` (out of 6.8 GB, so ~4%).  Sorry for the bad screengrab quality - to reproduce (though you may want to use fewer nodes for systems with <16GB ram):  1. Create a script, e.g. `dfs_memtest.py`     ```python    import networkx as nx    G = nx.path_graph(10_000_000)    e = list(nx.dfs_edges(G, source=0))    ```  2. Follow the [memray instructions to create the flamegraph](https://github.com/bloomberg/memray#usage)  [^1]: The reasoning here was that this graph would maximize the memory saving, as each node only ever has one child, and the nodes are also integers.
comment
>  it is actually clearer with this edit  This is inherently subjective - there is no "right" way to do it. The reason I find it more readable is that the state (i.e. the current depth) is tracked with the current node, whereas the proposed change tracks the state separately. Though again, this is entirely subjective so we're unlikely to make any progress discussing on this front :)  A minor improvement to memory consumption is a more concrete justification for the proposed change. I'd like to hear what others think!
comment
I'm going to close this - the original question has been sufficiently addressed and there are no action items. Thanks all for taking the time to discuss in such depth.
comment
Gentle ping @vakker have you had a chance to investigate the suggestion above? I suspect that the version without the function call overhead would indeed be more performant, though it'd be interesting to see what the practical effects are (if any).
comment
The only function exported from this module is `equitable_color`, which does appear to be properly documented. Some of the other functions seem like they are intended for direct use (e.g. `is_coloring` and `is_equitable`), while others definitely seem like "internal" functions that should likely not be used directly in user code (`make_*_from_*`, `procedure_P`, etc.).  Docstring improvements are always welcome, though I'd start by focusing on `equitable_color` since that's the only function that's clearly user-facing. The docstring for `equitable_color` generally LGTM but I'm sure there are opportunities to improve it.
comment
Thanks for the proposal @Naman-Priyadarshi . I'd vote not to update this, or at least not in a way that makes it more verbose.
comment
I will go ahead and close this since most reviewers are against the proposed change. Thanks for the suggestion @mlissner !
comment
I've reviewed the documentation for the functions in the `tournament` module and it looks to me like they are in good shape re: notifying the user that the input should be a tournament graph. This is admittedly a subjective assessment, so if there are ideas for concrete documentation improvements, please submit a PR!  I'm going to go ahead and close this as resolved, but again - improvements to `tournament` documentation is always welcome.
comment
From the coverage report, the untested code is when `weight` is specified (i.e. `weight` is something other than the default `None`).  So the way to test this would be to generate a simple graph with weighted edges, determine what the expected percolation centrality value is for each node both *with* and *without* edge weights, then code up a test that checks the results of the function against the expected values for `weights=None` and `weights="weight"`.
comment
Just to add: this issue has been open for a while and multiple contributors have taken a crack at it, so it's very likely it is already resolved. You should check the latest by running the test suite+coverage on `main` - see [the developer guide](https://networkx.org/documentation/latest/developer/contribute.html#testing) for details.
comment
The functions that are prefixed with an underscore (`_`) are "private" by convention; i.e. they are not intended for direct use by users. Thus they are intentionally left out of the API documentation.  Internal functions like this are often intended only for use in the module in which they're defined, so they are typically only accessible by that module. For example, if you wanted to access `_bidirectional_pred_succ` you'd do something like `from networkx.algorithms.approximation.connectivity import _bidirectional_pred_succ`.
comment
In this case I think we can safely delete the check in L109-110. I don't think it's possible to hit that due to the `not_implemented_for` decorator. Even if it were hit-able, it's a pretty trivial input validation that I think can be safely removed.
comment
I believe the coverage for the directed generators [has already been improved to 100%](https://app.codecov.io/gh/networkx/networkx/blob/main/networkx/generators/directed.py) - there seems to be overlap with this PR and #6208 which has been merged. We should re-evaluate given the latest changes - the first step would be to resolve the conflicts. Are you still interested in working on this @chimaobi-okite ? If not, then it's likely safe to close.
comment
> Anyway I'm not sure if there is a clean way of preventing users from declaring wrong filter functions. They will get what they feed in :/  Static type checking would likely help here. If we had annotations for `filter_node` and `filter_edge` that specified them as callables that expect nodes or edges as input, respectively, and must return bools then this issue would be caught by e.g. mypy.
comment
FWIW I'm -1 on adding any explicit input/output validation here. As stated above, the flexibility here is intentional, and it is incumbent on the user to ensure that the callable they are using for filtering matches the prescribed interface. It certainly is reasonable (and good practice) to validate that this is indeed the case, but my sense is that this is best done in *user* code, not upstream in NetworkX.
comment
The`strategy_*` objects are actually *generators* - in other words when you "call" `strategy_connected_sequential` it will return a generator object that will yield values when iterated over. To actually hit the code path, you have to request a value from the generator - one simple way of doing so is `next(my_generator)`.
comment
Hopefully the original question was sufficiently answered - if not please feel free to follow up.
comment
I believe this was fixed by #6654 - please reopen if I'm mistaken!
comment
Indeed - thanks for the follow-up @navyagarwal ! I see this now - I must've been on the wrong branch when I first tested the co-linearity example.  Either way I think this exercise highlights the importance of including a test for this behavior. The easiest way to design such a test is usually to write something that fails on `main` but passes on this branch. In this case, that might include:  1. Verifying that the warning is no longer raised, and  2. verifying that the (relative) position specified by the node attributes is preserved
comment
Thanks @dschult for the thorough explanation. I agree that the behavior is as intended and I think the answer sufficiently addresses the initial question, so I will close this. If there is something to follow up on @melicheradam please feel free to reopen.  There is likely an opportunity to improve the documentation surrounding how nodes in the dict-of-dict structure map to indices in an adjacency matrix representation. PRs welcome!
comment
See also: #6542 
comment
Closing as completed by #6542
comment
As the warning indicates there's certainly something wrong here, but I think the proposed doesn't really resolve the issue. It *does* result in the warning going away, but note that the node positions are not actually preserved after the `layout` call. In other words, the "correct" node layout about is actually a result of the `neato` layout program, not the positions specified as node attributes. You can verify this with a slightly more complicated example, e.g. using 3 co-linear nodes:  ```python >>> G = nx.Graph() >>> for n in range(3): ...     G.add_node(n, pos=(n, n)) ```  So the fix will suppress the warnings, but still not give the correct result.
comment
I suspect this is an environment issue - can you run `nx.__version__` inside the notebook to verify you are indeed getting the NetworkX version you expect?
comment
This looks like a bad interaction between sphinx extensions and the pydata-sphinx-theme. My guess is that one or more of `numpydoc` or `sphinx-autodoc` is mis-identifying some sections from the module docstring which is screwing up the heading levels/object tags for the rest of the docstring.  This should likely be fixed upstream, though we'll need to figure out which sphinx extension is behaving badly. In the mean time we can probably use a simple workaround to fix the docs.
comment
Is this a behavior change you noticed between NetworkX versions? In other words, did this work the way you expected in a prior version but changed in 3.0? I don't think the support for different GEXF versions is really that great on the NetworkX side of things, so I wouldn't be surprised if this was something that was just "broken" for a long time.
comment
Thanks for your interest in this @Qudirah and @PurviChaurasia . I know there are already some PRs open for this one, but I wanted to push this a little harder because I think it would be nice to get into the next release (which is right around the corner). 
comment
> Sure! Will get right to it. Let me just have a look into it and I'll let you know whatever problems I face.  Just to be clear - I've already opened #6612 to fix this. I only mentioned the above to avoid more duplicated effort :)
comment
> This issue should be closed.  Hi @Qudirah - no need to keep pinging about issues like this: this issue will automatically be closed if/when #6459 is merged. Other developers can see that there's an open PR to fix this issue, which means they should direct their effort to reviewing that PR rather than submitting a new one.
comment
I think this was resolved by #6459 so I will close it. There are likely other opportunities to refine import statements with lazy loading, but those can be fixed as they are come across (or programmatically all at once if someone were feeling ambitious...). Thanks all!
comment
Adding examples to the gallery is always welcome. Since this issue doesn't relate to anything specific I'll go ahead and close it so we can keep discussions focused in the corresponding PRs.
comment
There isn't really anything actionable in this issue so I will go ahead and close it. If there are more concrete suggestions/proposals for speeding up `all_simple_paths`, please go ahead and open a new issue or, better yet, a PR with the proposal.
comment
Hi @Qudirah - there's no need to open an individual issue for each instance where coverage is not 100%. Doing so decreases the "signal-to-noise" level on the issue tracker, not to mentions sending an email to the 281 people who watch the repository every time a new issue is opened! Code coverage is generally an open-ended potential improvement, so there's not a lot to be gained by highlighting the individual instances.  Please feel free to keep working on improving coverage! I'm going to go ahead and clean up some of the issues though.
comment
> oh. That is understood. if i keep working on them, on what issue do I attach the PR though?  There is no requirement that every PR have a corresponding issue. A clear, concise summary in the PR description is enough!
comment
Thanks @Qudirah , I'm going to go ahead and close this as any discussion related to the individual examples best belongs in the relevant PR.
comment
> Or change the code (making it not backward compatible) to raise an exception when G contains selfloops.  My sense is that raising in this case is an improvement if it cuts down on "silently unexpected" results. The fact that it's straightforward to either filter or remove self-loop edges means we're not *removing* any features, just making the requirements a little more strict!
comment
You are setting the `num_edges` attribute only once at construction time, so it is reflecting the number of edges at the time the graph was constructed. In your `__main__`, the number of edges at instantiation is 0, but then you add edges afterwards which is reflected in the `number_of_edges` *method*, but not your custom static attribute. If you want the `num_edges` attribute to track the current state, consider using a `property` instead (or simply rely on the built-in `number_of_edges` method, which already does this).  Either way, this is not a NetworkX bug, but a feature of the Python object model so I will close this.
comment
> Could you help me to understand why failed the test in style-Test the external Hyperlink?  Indeed the diff from the failed CI run is a little confusing... the reason it failed was because there was an extra whitespace at the end of the line (i.e. `_ `).
comment
> Should we also put up some disclaimer or note in our README that we are using this?  Good question - probably a good discussion topic for the community meeting!
comment
Downloading unknown zip files from the internet is generally not a good idea :upside_down_face: , so here's a reproducer to generate a graph with the given number of nodes and roughly the same number of edges:  ```python >>> import networkx as nx >>> nnodes, nedges = 4320, 564480 >>> p = 2 * nedges / (nnodes**2 - nnodes) >>> G = nx.erdos_renyi_graph(nnodes, p, seed=0xdeadc0de)  # seed for reproducibility >>> print(G) Graph with 40320 nodes and 564980 edges ```  You can then use this graph to test round-tripping with read/write graph6:  ```python >>> nx.write_graph6(G, "/tmp/example.g6") >>> H = nx.read_graph6("/tmp/example.g6")  # failure here ```  Having looked at the `read_graph6` code, the problem lies here:  https://github.com/networkx/networkx/blob/1034b689cadfc8a854745043783ba53a73cbdf36/networkx/readwrite/graph6.py#L124  The first term in the `zip` constructs a list with `nnodes**2 / 2` terms essentially corresponding to the upper-triangle of the adjacency matrix. Each term is a tuple representing the edge, so a quick back-of-the-envelope calculation gives the expected number of GB required to instantiate this list:  ```python # Use sys.getsizeof as a minimum estimate for the size of a 2-tuple of integers >>> (nnodes**2 / 2) * sys.getsizeof((0, 1)) / 1e9 45.5196672 ```  Fortunately - we can eliminate this memory footprint by using a generator instead - see #6519.  I think there are many other opportunities to improve the graph6 code, but #6519 should at least fix the memory issue reported here. Thanks for reporting @alabarre !
comment
> Should I delete this PR?  If #6490 is intended to replace this one, then yes please do. Generally speaking you don't need to create a new PR when you want to propose changes to the same code - simply push to the branch from which the PR was opened!
comment
> Any special commands to install it by hand?  Check out the [contributor guide](https://networkx.org/documentation/latest/developer/contribute.html) for instructions on how to install from source.
comment
I'll go ahead and close it - we can always reopen with the latest CI!
comment
> Maybe there is some standard documentation blurb or tag you use in NetworkX to mark changes that are not backward compatible? That would raise the attention of the caller.  We'll definitely want to add a bullet to the [release notes](https://github.com/networkx/networkx/blob/main/doc/release/release_dev.rst). For now I'd put it in the "Improvements" section - we may break some of these out into another "other changes" type of category before release.  It may also be worth adding a `.. versionchanged:: 3.0` to the docstring to call extra attention to the behavior change in the docs. I'd leave the runtime stuff (i.e. the exception message) as-is.
comment
> When would that be? Sorry, I'm not that familiar with Sphinx.  In principle you can put that directive anywhere, but in this case I think maybe putting it in the extended summary (~L328) would make the most sense. It doesn't seem like `versionchanged` is used elsewhere in the docs so maybe it's overkill :shrug: 
comment
I believe the footer is aligned with the header element, and any offset likely has to do with the logo placement rather than page layout.  Either way, the docs use the [pydata-sphinx-theme](https://github.com/pydata/pydata-sphinx-theme) which is where these things are set. I'd suggest checking out the documentation there to see if there's anything to be done about this - I don't think we'd want to add project-specific custom templating.
comment
Thanks for taking a look @U-238. I think it'd be great if we could boil down these failures to the absolute minimum (i.e. what is the graph with the minimum number of nodes and edges to reproduce this case) and add it as a test case. In it's current state I have a hard time verifying the "manual" ancestor determinations. I understand this may not be simple to do, so anyone who wants to take a stab at it I'd encourage them to do so!
comment
> Looks like the latex commands within dollar signs need each slash doubled to indicate a latex slash.  Making the docstring a raw string (e.g. `r"""`) may be another option.  There definitely is some weirdness in the formatting, but the motivation for the change is good! Here's the rendered page (as of d3bff2c) for reference: https://output.circle-artifacts.com/output/job/e1d60a12-2f31-4761-96a1-71d53775f18e/artifacts/0/doc/build/html/reference/generated/networkx.generators.expanders.paley_graph.html#paley-graph
comment
>  it's currently a blocker of moving to myst_nb from nb2plots for code block directives in our documentation.  FWIW there was a recent PR to myst-nb that may have fixed this: see executablebooks/myst-nb#375. I haven't tested it yet but we should double check!  >  But it'd be nice to continue to support making the pdf locally... but may that is difficult.  Agreed. Generating the pdf via latex should continue working, but I also agree that building the pdf on each push/PR is wasteful. Some other options might be:  1. Setup a cronjob to build the pdf every so often (say weekly or biweekly) to verify that everything still works.  2. Make the pdf-building part of the release process and only worry about it at release-time.  I think either is probably fine, but generally +1 for decoupling the build from the workflows that are run at PR open/push/merge time!
comment
I went ahead and rebased to fix the merge conflicts.  Re: CI times, I did a quick comparison of two arbitrarily selected CircleCI jobs: one [with the pdf docs](https://app.circleci.com/pipelines/github/networkx/networkx/6236/workflows/3bfa38fa-107a-40c9-8207-505c52be8092/jobs/9199) and [one without](https://app.circleci.com/pipelines/github/networkx/networkx/6240/workflows/853076f0-46fe-4e8a-9deb-70b85f2e9213/jobs/9208). To summarize, it looks like ~4 minutes total are saved, reducing the total job time from 13 min down to 9.  Let's give this a shot! If there is major demand for the pdf docs we can always add them back.
comment
@MridulS does the need for non-local go away if you use a namedtuple instead of a dataclass? If so that might be a nice alternative
comment
I will go ahead and close this one since it's been inactive for a while. If interested in continuing work here, please feel free to reopen after resolving the merge conflicts and addressing the comments from the first round of review!
comment
I will go ahead and close this one since it's been inactive for a while. If interested in continuing work here, please feel free to reopen after resolving the merge conflicts and addressing the comments from the first round of review!
comment
I can think of two things that could be done to make this more discoverable:  1. Add the suggested calls for the removed `utils` functions to the [migration guide](https://networkx.org/documentation/latest/release/migration_guide_from_2.x_to_3.0.html).  2. Add a module `__getattr__` to the `utils` module to raise the suggested replacements any time a user tries to access the removed functions.  I'm not sure option 1) would help *that* much as the migration guide isn't super discoverable in the documentation. 2) would likely be helpful, but would just require a bit more boilerplate.
comment
I too suspect this may be machine/platform/OS-specific. I've checked on Arch+firefox & chromium and Ubuntu+firefox & chromium and haven't reproduced the clipping.
comment
+1 for focusing on the separate PRs. For anything that isn't already pulled out, it might be easier to cherry-pick and resubmit that try to wrangle them in this PR. Thanks all!
comment
I'm going to go ahead and close this. If someone wants to pick it up again please feel free.
comment
This has been stale for a while now - any plans to keep working on this @popokatapepel ? I'm still -1 on the introduction of new structures for the sole purpose of encapsulating data.
comment
From a quick search on the internet I'm inclined to agree with the conclusion in #455 - it might be more appropriate to add support for more commonly used network data formats to 4CAT than to add support for yet another not-particularly-common (AFAICT) text-based format to NetworkX. For example, it seems that [TCAT](https://wiki.digitalmethods.net/Dmi/ToolDmiTcat), which [appears to have inspired 4CAT](https://github.com/digitalmethodsinitiative/4cat#credits--license), supports I/O in GEXF format.  
comment
See #6407   Generally speaking, some here are some best-practices that should help avoid problems like this in the future:  1. Make sure you are up-to-date with `main` on your development branch  2. You can search the issue tracker + PRs simultaneously to see if there's anything that looks related to the issue you're seeing.
comment
Thanks @faze-geek - yet another reason not to recommend PYTHONPATH: windows
comment
> As of today it looks like rdflib maintains connectors to networkx so we don't need to add them here :)  I agree - there's not a lot to gain by duplicating format-conversion functionality, and the RDF library seems like a more natural place for these to live. I will go ahead and close this - thanks all for the proposal & discussion!
comment
The import statement:  ```python from networkx.algorithms.community import modularity ```  is valid, so there should be no problem. I can't reproduce the error; the following works fine locally:  ```python >>> import networkx as nx >>> G = nx.complete_graph(5) >>> nx.community.louvain_communities(G) [{0, 1, 2, 3, 4}] ```  Can you provide a minimal example of the import that produces the error, along with the traceback as well as the Python version?
comment
Same with the failing lint job... see #6407
comment
Note that there's now a proposal to deprecate this function under discussion in #6186. If the decision ends up being to deprecate this function, then it's probably not worth a lot of effort to update the docstring. Keep that in mind when allocating time for your PRs - it might make sense for this one to be near the bottom of the priority queue!
comment
I am unable to reproduce this error from the reported example on `master`. What version of NetworkX are you using? Also, the traceback reports an `IndexError` - it's not immediately obvious to me how that maps to the length of the string.
comment
Ok, I've tried this one again and now I *am* able to reproduce it with `pydot==1.4.2`. It turns out the length of the name is not the only factor though, the underscores in the name also seem to be important. For example, the following works:  ```python G = nx.Graph() G.add_edge("longname"*100, 1) pos = nx.nx_pydot.graphviz_layout(G, prog="dot") ```  but the following results in the reported exception:  ```python G = nx.Graph() G.add_edge("long_name"*15, 1) pos = nx.nx_pydot.graphviz_layout(G, prog="dot") ```  Note that this works fine (on my system at least) if you change 15->14.  Looking at the code this seems to be something wrong with `pydot`'s parsing, so we should report it there as well.  
comment
My sense is that there would need to be a bit more discussion on a general policy for parallelization before moving forward with tackling any specific function - see #4064.
comment
Agreed - I think parallelization is definitely best left to dispatching and/or user code. 
comment
I'm -1 on adding an explicit testing module. Adding "helper functions" to check graph attributes during testing is an unnecessary indirection that makes tests harder to read. Using assert statements directly is both correct and more flexible.  As a matter of fact, NetworkX used to have a dedicated testing module, but it was deprecated and removed - see #5782. I don't think there's any appetite to re-introduce it.
comment
Agreed - my comment about type hinting was more that we'd get this "for free" with type stubs that reflect the documented types (I was specifically thinking of #6339). I'm -1 on the proposed change here at this stage as we don't have the stubs+definitions in place yet, so I think we can close this.
comment
Closing per the last comment, thanks all!
comment
FWIW I see a much more significant difference in import time: ~270 ms on `main` vs. ~12 ms on this branch. This is using `python -X importtime -c "import networkx"`.  Note that there is a significant difference between the *first* time networkx is imported vs. subsequent times due to caching. Try a `git clean -xdf` prior to testing the import time and you will likely see a factor of 3-4x increase in the import time for the current importing scheme.
comment
> The test suite passes, but should any of these modules be included?  The dunders aren't really excluded, right? In other words, users should still have access to e.g. `nx.__version__`? AFAICT this still works, though tab completion no longer works for the dunders - it'd be nice to recover that.
comment
It looks like you've accidentally committed an `env/` directory which has added 500+ files. I suspect this is the source of the errors in CI.
comment
+1 for adding the gallery example! Would you mind undoing the whitespace changes in `layout.py`? That's the cause of the failing CI and breaks the docstring compliance with the numpydoc standard.
comment
Thanks @nsengiyumva-wilberforce - based on the diff, I suspect there are a couple accidental commits to this branch. Did you intend to also add the line tests to this PR? It might be best to split the test updates out to a separate branch to keep the scope of this PR restricted to the documentation fixes.
comment
It looks like the changes I applied in [94bfd4d](https://github.com/networkx/networkx/pull/5972/commits/94bfd4d90a6fc2c03cd06b1a3e95051a47d24f01) were lost, in the latest push - can you look into that @kpetridis24 ? I can re-push them but I just want to be sure that the changes you think should be there are indeed there.
comment
FWIW this is still failing when the test suite is run in random order: see #5992   I will put this in anyways as AFAICT it's just a continuation of the existing problem, not a new one. IMO we should definitely prioritize fixing #5992 !
comment
Thanks for the update @MridulS :rocket: 
comment
> Thanks for the review Sir. Should I try working on something better then? Try to create a notebook example featuring Triads?  Let's wait and see what others think!
comment
> PS: I guess you meant "not opposed"  Correct! I've edited my comment.  Re: the notebook idea - I was mostly just thinking that the information was a bit dense for a gallery example. The 4x4 grid in the thumbnail is very difficult to see/interpret and just listing the triad designations is certainly informative, but a bit dry without any context. Of course, we could always start with a gallery example and expand into a notebook!  I guess the main sticking point for me is just the thumbnail with the dense grid. It's not a hard blocker I just wanted to gather opinions on whether we couldn't convey the information in a more digestible way.  Also - if we keep the example in the gallery, I think it should be moved to the `graphs` category instead of drawing.
comment
The reason tests are failing is likely because the `doctest` format that we use in the docstring examples expects the output to be *exactly* correct, including things like whitespace etc. The failures are currently due to the `triadic_census` example: https://github.com/networkx/networkx/runs/5989988593?check_suite_focus=true.
comment
Given the reference to the gallery example proposed in #5528 I'd say we should hold off reviewing this one in too much detail until that's merged!
comment
Sorry @0ddoes , this one fell off my radar - thank you very much!
comment
Gentle ping @lucasmccabe - this looks to be on the right track, the main thing that needs to be handled is updating the API to handle a user-specified weight attribute rather than baking-in the attribute name.
comment
+1 for this reorganization since the `vf2pp_helpers` package was really an implementation detail rather than anything that is to be exposed to users.  I took the liberty of removing the `vf2pp_helpers` subpkg from the `package_data` listing in `setup.py`. AFAIK that was the only other place it showed up where it should be removed.
comment
FWIW the errors here seem to be due to various misspellings in the exception checking - should be an easy fix!
comment
> I had missed https://github.com/networkx/networkx/pull/6204 when working on that (sorry...), but I do have a test that hits [those lines](https://github.com/networkx/networkx/pull/6215/files#diff-4e6352234d8b3f17771dd2ea80891796b749790a73596994000dc4bea591045fR293-R295), so they are not unreachable as claimed.  No worries @Aufinal , thanks for pointing this out. @Mjh9122 want to take a look at this one?
comment
> The only question I have is about whether warnings are the right way to handle graphs with selfloops.  Agreed - let's avoid warnings. `inverse_line_graph` currently raises at least for some subset of cases involving self-loops:  ```python >>> G = nx.path_graph(2) >>> L_1 = nx.inverse_line_graph(G)  # Works fine >>> G.add_edge(0, 0) >>> nx.inverse_line_graph(G) Traceback (most recent call last)    ... NetworkXError: G is not a line graph (odd triangles do not form complete subgraph) ```  I would vote to preserve this behavior, though raising an exception message specific to the self-loop case would be an improvement!
comment
A little extra info: I can't reproduce this in my development environment, but I *can* with a non-editable install. In other words, with `pip install -e .` in the source directory I don't see the errors, but with `pip install .` I do.
comment
> I fully intend to do it through the pre-commit hooks. I haven't yet, because that results in hundreds of errors and prevents me using the CI to check my work incrementally.  Please don't use CI to check incrementally - the way our CI is currently configured, it will launch a job for each push, so if you incrementally push 10's of times, you will create hundreds of jobs that will backlog CI significantly.  > Can you suggest an alternative? it's the standard way to suppress false positive lints  My personal preference would be to not run flake8 :). We used to have the PEP8 speaks bot and ended up disabling it as it was a lot of noise with very little benefit. My personal preference is to stick with the linting we have, as `black` takes care of most of the formatting things that matter IMO.  I think this should be discussed further before a PR is pursued.
comment
Understood, but in the past we've generally agreed that adding `# noqa :` is not necessarily an improvement. In order to push this forward, it might be better to open an issue or discussion with the proposal to add flake8 so we can get a concrete, comprehensive accounting of the history here and feedback from the core devs/community!
comment
NumPy 1.8 is incredibly out-of-date, so it's no surprise that you're seeing issues like this. This is very likely to be an issue with your environment and not related to NetworkX; therefore I will close this issue. I suggest checking out general documentation on installing Python libraries, which will be more relevant to the problems you're seeing!
comment
This should be fixed by #5938 , thanks for the ping @QuLogic !
comment
> with relative import style users can simultaneously use several versions of the library by simply renaming it. like this  True, but I don't think this would ever be recommended!  Overall I'm ambivalent on switching from absolute to relative imports in the library. If someone were truly interested in doing so (or better yet, there were an automated tool to do so) then I'd say go for it, but as it currently stands I think it would be a lot of churn with very little payoff (and in principle practically 0 difference for the end user).
comment
I suspect this is an environment management issue. The access pattern in the OP is indeed valid and should work fine with networkx 2.8.7. You can check `nx.__version__` in the notebook to ensure you are actually getting the version of networkx that you expect.  I will close this issue as I don't think there's anything to be done on the networkx side, but if the issue persists feel free to reopen with the latest info.
comment
I think my preference would be to state explicitly in the docstring that `ancestors` and `descendants` do not include `source` itself. One could even go so far as to provide an example for how a user could easily get the ancestors/descendants with the source node included, i.e. `nx.ancestors(G, n) | n`. IMO this is preferable to expanding the API of `ancestors`/`descendants`, but that's just my opinion!
comment
Agreed, thanks all!
comment
This shouldn't be a problem - `create_using` accepts either the graph class (as shown in the tutorial) or an instance of the graph class (your proposed change). Can you provide the full traceback along with which version of NetworkX you were using when you encountered the issue?
comment
Thanks @SultanOrazbayev , let's close this as inactive. If the issue persists, please reopen with a minimal reproducing example.
comment
Fixed by #6087
comment
Indeed, thanks @SultanOrazbayev !
comment
I suspect that the difference comes down to edge ordering. Have you checked that the ordering of the edges in the column of the incident matrix is consistent with the ordering of the nodes (which, in L(G), represent the edges of the original G) in the adjacency matrix of L(G)?
comment
Thanks for the ping @yixin0829 - this looks to me like it was largely resolved by #6083 , so I will go ahead and close it. However, if you have concrete ideas for improving tests or coverage please don't hesitate to open a PR!
comment
@jamespharvey20 from the posted traceback it actually seems like the cause is a simple misspelling of the function name. Try `draw_networkx_edge_labels`.  Apologies if I'm missing something obvious 
comment
Thanks for the ping @dschult , it does indeed look interesting. NumPy recently introduced a whole new random API in v1.17, so that's another angle to consider as well :). I will take a look
comment
> if the function doesn't call on core NetworkX functions or objects, then it probably shouldn't be included in the convert_matrix.py library of functions, is that right?  No, that's not the concern - since the `convert*` modules are related to converting graphs to other formats, this does indeed seem the appropriate place for such conversion functions.  My concern is more about the cost/benefit of expanding the API. The one that really stands out to me is the `to_adjacency_matrix` function. That essentially boils down to a one-liner: `my_adj_ary = xr.concat([func(G) for func in funcs], name="dim")`. My concern is this - is it worth adding specialized API to do something that is already achievable in a very "Pythonic" way? For this reason especially, it feels like the function belongs in the example rather than in the library because it demonstrates the right way to go about this, using the appropriate tools. `format_adjacency` has a similar feel: half the function is dedicated to error checking and the part that does the conversion essentially boils down to things that simply accomplished using the tools provided by the various libraries involved, e.g.  ```python nodes = list(G.nodes()) my_formatted_adj = xr.DataArray(     adj[..., np.newaxis],      dims=["n1", "n2", "name"],     coords={"n1": nodes, "n2": nodes, "name": [name]}, ) ```  Adding the extra layer of indirection for this functionality feels to me like a bit of a violation of one of the zen principles: *There should be one—and preferably only one—obvious way to do it.*  > That's the only distinguishing mark of these two functions compared to to_node_dataframe.  IMO the difference with `to_node_dataframe` is that it performs a remapping of the node/attribute data, which is a more complicated procedure that feels more atomic.  I very well may be just be thinking too conservatively re: the expanded API. I don't at all doubt the utility of mapping graphs to named nd data structures!
comment
> The API surface area provides functions that return specialized data structures already.  This is a good point. Looking at the other functions that are already in `convert_matrix`, there are some that fit the "pass-through" description. For example, `to_numpy_matrix` is essentially: `return np.asmatrix(to_numpy_array(*args, **kwargs))`. I would vote to deprecate functions like this as well (thanks for the reminder!) for the same reason mentioned above. Most of the others functions in the module have more complex procedures that translate between the Graph structures and the "other" format (DataFrame, ndarray, etc.) - I think `to_node_dataframe` fits this bill - which IMO are worthwhile. 
comment
Thanks for reporting this @caph1993 . This has been fixed in the switch to the naive LCA algorithm in #5736. I went ahead and added the case you reported to the test suite as an extra regression test.
comment
Thanks @kpetridis24 - a couple tips for this:  1. Go ahead and submit these fixups as their own PR against main instead of adding them to an already-open PR. We want to get these fixes in as soon as possible, so having them be self-contained will help!  2. There is no CI job that runs the tests in random order *on PR/pushes* - only a scheduled job - which means that the changes won't be evaluated properly in CI. To verify the fix works, you can `pip install pytest-randomly` locally and run the test suite locally a few times.
comment
I believe this was also fixed by #5972
comment
Now that #6093 is in I think it's safe to move forward expiring `literal_stringizer` and `literal_destringizer`.  I went ahead and rebased on `main` to incorporate the changes from #6093, then went ahead and resolved the testing issues related to the removal of the functions. Note that in every case, the test modifications that were required had to do with specialized conditions in the `literal_(de)stringizer` functions - nothing related to changing behavior in the gml parsing itself.  Finally, I also went ahead and included the removal of `literal_destringizer` in this PR as well, since this is essentially the other side of the coin. If we do end up having to revert in the future, it makes sense that these two functions are packaged together.  ---  PS - I had to force-push because of the rebase, so any existing local copies of this branch will now be broken. The easiest way to fix that (assuming you don't have any uncommitted changes in the repo that you want to keep):  ```bash git fetch juanis2112 git checkout remove-literal-stringizer  # Make sure your on the right branch git reset --hard juanis2112/remove-literal-stringizer ```  where `juanis2112` is the remote pointing to the fork from which the PR originated.
comment
I confirm that I can reproduce this on `main` (7fe275fd)
comment
Thanks @eriknw that plan makes sense to me. I haven't looked at it in detail, but the general structure of the PR (i.e. a graphblas namespace) also seems like a good approach - it's a simple way to expose functionality to users without having to tackle tougher issues like integration/dispatching. And as you mentioned, if a user wants to run their analyses without paying any conversion penalties, they can use `graphblas-algorithms` directly. If the `graphblas-algorithms` truly commits to mirroring the networkx API for the implemented functions (potential additional kwargs notwithstanding) that makes maintaining the `networkx.graphblas` package very straightforward :). I'd expect it would make users happy as well, since transitioning between the two should be as simple as changing an import statement!
comment
Closed by #6161
comment
@dschult 's explanation sufficiently addresses the initial question, so I will close this as resolved.
comment
Thanks for reporting - this was already covered in #6143
comment
There are many features that no longer work with pydot as that project seems to no longer be actively maintained. NetworkX's interface to pydot is pending deprecation. See #5723 for details.
comment
> Will NetworkX's continue to support the interface to pygraphviz?  Yes  > With pygraphviz there is no problem with contraction:  Great! I'm going to go ahead and close this as it's likely just related to the deprecated dependency.
comment
I'm +1 for implementing the fix+tests proposed in the OP. Investigating new algorithms e.g. for route inspection is interesting, but I'd propose to handle that in a separate issue/discussion/PR as that will likely require more discussion. Fixing the defect in `eulerize` is a high priority IMO.
comment
Agreed - this is a good idea. Another benefit of moving this is that the ThinGraph would actually be executable, which would serve as additional, practical testing of the factory function mechanism for subclasses.
comment
Ack the merges with main have gotten ugly and pulled in quite a few changes that make the diff basically useless... any chance you'd be able to rebase @MridulS ?  My one general comment is that I'd like to prepend the `dispatch` decorator with a `_`, at least at this stage, just to indicate to users (and potential backend implementers) that this is an "experimental" feature that is still being actively developed. By Python convention this would also give devs the freedom to change the interface/behavior without needing to provide any warnings in the code.  Generally I'm really excited by this idea and would definitely like to get it in for 3.0!
comment
While this is true, I don't think it warrants opening an issue since it's not very specific. I'll go ahead and close this one!
comment
Thanks @Mjh9122 , it turns out that `find_cores` is actually supposed to be removed in the next release! Your discovery via coverage is very helpful, thanks for opening the issue!
comment
The problem is that your nodes (which appear to be 2-tuple coordinates) contain whitespaces, which [is treated as the delimiter character by default for the `read_` and `write_` adjlist functions](https://networkx.org/documentation/latest/reference/readwrite/generated/networkx.readwrite.adjlist.read_adjlist.html).  You should be able to fix this for your case by choosing a delimiter character doesn't appear in your nodes, e.g. `"|"`.
comment
AFAICT this behavior isn't mentioned in the documentation anywhere, so the documentation seems like it could be improved here!
comment
The read/write adjlist functions are very simple and don't really have a mechanism for generic node serialization (i.e. converting complicated node types to/from string formats). You might want to take a look at the [parse_adjlist](https://github.com/networkx/networkx/blob/5c0b11afb4c0882a070d522ef3fa41482ba935d3/networkx/readwrite/adjlist.py#L140-L209) for further details.  At face value, it seems the text-based read/write adjlist functions are too simplistic for what you're trying to accomplish. I would recommend either:  1. Changing the problem statement a bit. For example, using tuples of floats as nodes is not a good idea *in general* as floating point values can be tricky in the context of e.g. comparison. You could instead relabel the nodes so that the nodes themselves are integers and 2-tuple coordinate data are stored on the nodes as attributes. You could then use one of the existing graph formats that supports node attributes to round-trip your data more reliably.  2. implement custom parsing for your data; for example, you could convert the data from file back into Python 2-tuples with something like `conv = lambda t: tuple([float(v) for v in str(t).strip("()").split(",")])`. Of course, you'd then have to handle the construction from adjacency in a different way...
comment
> How about adding this text to the notes for write_adjlist?  Thanks @brocla , your suggestion would do the trick IMO. Another option would be to add an example of round-tripping to the `Examples` section of the docstring, though that may be more work. I think either approach would be fine!
comment
> Can I work on what you have suggested?  Absolutely, please do!
comment
Thanks @Qudirah , it looks like a modification to the dispersion tests also snuck into this PR - based on the related issue I assume that was by accident. It'd probably be best to split out the dispersion test updates into a separate branch so that it's clear what this PR is intended to fix.
comment
I think there's a similar issue here - it looks like multiple unrelated changes have been made in the same branch. It'd be best to split out the dispersion test updates and the multigraph test updates into separate branches, since they are unrelated changes!
comment
@staillefer the existence of the node in `G` is not particularly helpful in chasing down the problem. Generally, when reporting an issue it is useful to provide a minimum reproducing example along with the traceback, as one would need the full context in order to be able to determine the root cause of a problem.  For example - the node may be in `G`, but from the traceback above I see calls that are operating on a subset of the graph, e.g. `gdf_edges = utils_graph.graph_to_gdfs(G.subgraph(route), nodes=False).loc[uvk]`. Is the node in question in `G.subgraph(route)`? We can't determine this without a full reproducing example.
comment
Thanks @staillefer this is helpful. The problem is that the `listNodes` doesn't seem to define a valid "route" in your graph, i.e. there is no edge between the final two nodes `583097017` and `583097016`.  I will close this as there isn't an underlying problem with NetworkX. You may want to consider opening an issue in `osmnx`, perhaps to propose a more informative exception message when the `route` is not a valid path.
comment
Thanks for reporting @dtekinoglu , this looks like another instance of #5694. It's been discussed at several meetings, but this issue really seems to be biting a lot of people - we should definitely get something in place for the next release!
comment
I too am confused by the `t` value you are providing - it looks like it's an adjacency matrix in the form of a nested list. This is not what `random_spanning_tree` returns (nor does `random_spanning_tree` accept list-of-list inputs, as @rfulekjames points out).  In other words, it's not clear from the example how you are actually producing results with networkx - there must be conversion steps somewhere that are missing. FWIW I cannot reproduce a non-tripartite spanning tree from the input graph:  ```python >>> import networkx as nx >>> adj_matrix = [ ...     [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], ...     [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], ...     [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], ...     [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], ...     [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], ...     [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], ...     [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], ...     [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], ...     [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], ... ] >>> G = nx.from_numpy_array(np.array(adj_matrix)) >>> # The partitions, from the example in the OP >>> part1 = [0, 1, 2, 3] >>> part2 = [13, 4, 5, 6, 7] >>> part3 = [8, 9, 10, 11, 12, 14] >>> partitions = (part1, part2, part3) >>> # A naive, brute-force check for tripartite (there's almost certainly a better way to do this) >>> def is_tripartite(G): ...     return not any( ...         [any([u in p and v in p for (u, v) in G.edges(nbunch=p)]) for p in partitions] ...     ) >>> # Check that the original input graph is indeed partitioned according to the prescribed scheme >>> is_tripartite(G) True >>> # Try to reproduce a case where the random_spanning_tree is >>> # is not tripartite >>> for seed in range(100): ...     assert check_tripartite(nx.random_spanning_tree(G, seed=seed)) ```  If at any point `random_spanning_tree` produces a spanning tree that violates the original partitioning scheme, the loop at the bottom will produce an assertion error with the `seed` that resulted in the failure. I've tried this with seeds from 0 to 10000 and haven't been able to reproduce an error. You can try this procedure on your machine (varying the seeds you are checking) - if you get a failure, please report the seed that produced it!  Note that this is not a good way to *prove* that `random_spanning_tree` is working correctly in all cases, but in the absence of a reproducer for a function that relies on randomness, it's an okay initial probe for failing cases (it only takes one failure to show something is indeed defective).
comment
> When i use this code to generate the spanning tree, one of t1 or t2 violates the partition property.  How often does this happen? Every time? Sometimes? Once? Again I am not able to reproduce, at least using another brute-force initial check for a failing instance:  ```python >>> for _ in range(1000): ...     t1, t2 = generate_random_trees(G) ...     assert is_tripartite(t1) and is_tripartite(t2) ```  There's nothing obvious in the above function to indicate whether/why there'd be a failure.  I suspect there is something going on outside of this function that is causing the behavior you are not expecting. For example, is `G` being modified in some way elsewhere in your code?  Without a full, minimal example that reproduces the behavior, I suspect it will be hard to say anything concrete about whether there may be a problem in `random_spanning_tree`
comment
> Why is that? shouldnt 2 trees with the same adjacency matrix equal one another?  It depends on what you mean by "the same" :). The connections between nodes are the same, but the nodes are being implicitly relabeled with all of the conversion back-and-forth between adjacency and graph representations. A close look at the `adjacency_matrix` docstring reveals the `nodelist` parameter, which is used to determine the *order* of the nodes. When not specified, it uses the order of `G.nodes`, which for `t1` will be random and is very unlikely to be `list(range(15))`.  However, the conversion *from* adjacency matrix makes no such distinction, and automatically maps the indices of the adjacency matrix to the node label (i.e. `t2.nodes` *is* `list(range(15))`. In other words, the exact node labels depends on ordering, and when not specified the default node ordering is not sorted when converting to adjacency representations. This means that `t1` and `t2` should always be *isomorphic* (you can verify this with `nx.is_isomporphic(t1, t2)`), but are *not* guaranteed to have the same node labels. You can of course use the `nodelist` parameter of adjacency_matrix to specify the node order, e.g. `nx.adjacency_matrix(t1, nodelist=sorted(t1.nodes))`.
comment
It's important to recognize that you're not operating on the original graph, but on the tree that results from calling `nx.dfs_tree(graph, "1")`. This is a different graph object entirely - it *is* a DiGraph (which is why `dag_longest_path` even works, as that would raise for undirected inputs). This is easiest to get a handle on with a quick visualization:  #### The original graph, `G` ![G](https://user-images.githubusercontent.com/1268991/194133239-136a10db-8ae1-4560-bc31-8beba502ed7c.png)  #### The result of `nx.dfs_tree(G, "1")` (let's call it `T`)  ![T](https://user-images.githubusercontent.com/1268991/194133459-deba815b-6fd3-48be-afe4-fb68e59b9f55.png)  From the visual inspection of the second graph `T`, it's clear that there are two valid longest paths: `1->4->5->3` and `1->4->2->6`, both composed of 3 edges. Note that `dag_longest_path` only returns *one* instance of the longest path in the case when there are multiple valid longest paths. You can use the `topo_order` kwarg of `dag_longest_path` to have it give you a different longest path by passing in a different (valid) topological order. For example:  ```python >>> nx.dag_longest_path(T)  # Gives one of the valid longest paths ['1', '4', '5', '3'] >>> nx.dag_longest_path(T, topo_order=["1", "4", "2", "5", "6", "3"])  # Different topological ordering results in a different, equally valid longest path ['1', '4', '2', '6'] ```  Note also that there are multiple valid outputs from `dfs_tree` as well! For example, `1->6->2->4->5->3` is *also* a valid DFS tree. The output here *does* depend on the order that the edges are specified in the list:  ```python # Original specification >>> G = nx.Graph([("5", "4"), ("5", "3"), ("4", "2"), ("4", "1"), ("2", "6"), ("1", "6")]) >>> nx.dfs_tree(G, "1").edges  # This is the image `T` above OutEdgeView([('1', '4'), ('4', '5'), ('4', '2'), ('5', '3'), ('2', '6')]) # Same graph G, but move the last two edges to the front of the edge list >>> G = nx.Graph([("1", "6"), ("2", "6"), ("5", "4"), ("5", "3"), ("4", "2"), ("4", "1")]) >>> nx.dfs_tree(G, "1").edges  # Different, but still valid, dfs tree OutEdgeView([('1', '6'), ('6', '2'), ('2', '4'), ('4', '5'), ('5', '3')]) ```  In summary, I think there are three main factors that contribute to the confusion in the original post:  1. The path-finding is being done on the DFS tree, not the original graph  2. There are multiple valid DFS trees - maybe you were getting a different one than you were expecting  3. There can be multiple valid longest-paths in the DFS tree, and maybe you were getting a different one than you were expecting.   
comment
@DiamondJoseph if you're comfortable doing so, it might be worthwhile to split this into multiple PRs each with a more focused scope. For example, the updates related to the missing asserts in the test suite are uncontroversial, but some of the other proposed changes are harder to review as it's not immediately clear from the diff which changes e.g. in the tests are related to which proposed changes in the code.  A good start might be to split out the changes related to adding asserts, as I think that would get merged pretty quickly!
comment
Well, the good news is I put an `assert False` in all the tests in that module and got 4 failures, so that's a good sign! I wonder if it's related to the numpy/scipy optional dependencies...  I'd like to take the opportunity to very slightly reorganize the tests!
comment
> However, I am a bit confused about the following point: > Output of nx.bridges(G) is <generator object bridges at 0x7f70733bb050> . So, it seems that it works for directed graphs as well. > However, list(nx.bridges(G)) raises NetworkXNotImplemented. > Is this expected? Should not nx.bridges(G) directly raise NetworkXNotImplemented instead?  Hmm interesting - I would've expected this to raise directly but it looks like there might be a slight behavior change here as a result of `argmap`, maybe due to `_lazy_compile`? FWIW this is a regression from NX 2.5 where the `NetworkXNotImplementedError` is raised when `nx.bridges` is called:  ```python >>> import networkx as nx >>> nx.__version__ '2.5.1' >>> G = nx.path_graph(3, create_using=nx.DiGraph) >>> nx.bridges(G) Traceback (most recent call last)    ... NetworkXNotImplemented: not implemented for directed type ``` @boothby what do you think about the `_lazy_compile` hypothesis? Is there an easy way to toggle raising exceptions eagerly e.g. in the `@not_implemented_for` case?  @dtekinoglu to answer your question - yes it'd be great to improve the documentation for the bridges module, though I think you've stumbled on a deeper issue here - thanks for bringing it up!
comment
> I think these results might be due to the recent changes to bridges.py https://github.com/networkx/networkx/pull/5397  Ah of course, thanks @dschult   I'm going to open a new, separate issue for the delayed-exception issue [mentioned above](https://github.com/networkx/networkx/issues/5486#issuecomment-1093738471).
comment
Most of the issues have been resolved, so I'm going to close this one so that no one's tempted to work on things that are already fixed :)  I opened a new issue for the remaining `root` problem. If I've missed anything here, please feel free to open a new issue or amend #5636 !
comment
Indeed it looks like you've uncovered some deeper issues!  Re: the exception - it looks like we fixed the `not_implemented_for` exception mentioned in #5486, but are still not raising a `NodeNotFound` in `nx.bridges` when `root` isn't in `G`. At first glance I don't think this is a problem with `argmap`, it's just that `nx.has_bridges` is built on top of a pile of generators, and `root in G` is never actually checked anywhere (nor is the behavior tested). I *think* adding something like:  ```python if root is not None and root not in G:     raise nx.NodeNotFound(<nice message>) ```  near the top of `nx.bridges` should do the trick.  The second issue is that it seems the `root` argument doesn't actually do what it says it does! The `root` argument is supposed to limit the returned bridges to the component containing `root`, but that doesn't seem to be the case. For example:  ```python >>> G = nx.Graph() >>> nx.add_path(G, [0, 1, 2])  # one connected component >>> nx.add_path(G, [4, 5, 6])  # Another connected component >>> list(nx.bridges(G))  # root=None, so expect all edges [(0, 1), (1, 2), (4, 5), (5, 6)] >>> list(nx.bridges(G, root=4))  # Only expect edges from component {4, 5, 6}, but get all edges! [(0, 1), (1, 2), (4, 5), (5, 6)] ```  IIUC, this means the root argument is clearly broken. Since the `root` argument is passed through from bridges to `chain_decomposition`, this means there is likely a bug in `chain_decomposition` as well.
comment
Thanks @juanis2112 , the test updates look great!  The only thing that I think this PR needs now is two new tests:  1. A test for `chain_decomposition` that it raises `NodeNotFound` when `root` is not in `G`. This should live in `networkx/algorithms/tests/test_chains.py`.  2. A final test to check that `bridges` works as expected when `root` is used with `G` with disconnected components. See my previous comment for a good (IMO anyways) test case.  After those in place I think we can remove the "draft" status and get this in!
comment
> @rossbar i believe that you applied these changes locally. Did you push it?  Yeah we made these changes at some point, but perhaps they were on the LabelArgument branch and never made it into this one? I can't tell from the history. Either way, it might be less effort to simply make the changes again manually :)  
comment
I agree with @dschult that `if node not in G` reads nicer, so +1 for that suggestion.  It's probably also worth adding a blurb to the [release notes](https://github.com/networkx/networkx/blob/main/doc/release/release_dev.rst) to notify users of the change in behavior (I'd be happy to add this if you'd prefer).
comment
> would it be a better strategy to maintain backwards compatibility by either logging warnings instead of raising an exception, or excluding known working attributes like tooltip or label from the : check?  The general problem with `pydot` is that there were changes associated with an updated version of `pyparsing` on which `pydot` depends that resulted in behavior changes. Things like #5710 are an attempt on NetworkX's end to work around these limitations, but unfortunately the changes are more extensive (for example, NetworkX's `nx_pydot` test suite is currently xfailed and has been failing since the release of pyparsing 3.0.2).   The decision to deprecate pydot is largely motivated by the fact that the project does not seem to be maintained, and the workarounds that have been attempted by NetworkX have their own associated issues as mentioned here and elsewhere.
comment
Networkx > 2.7 requires scipy >= 1.8 due to the use of the sparse array interface as opposed to sparse matrices. Upgrading to scipy>=1.8 should fix the problem.  The fact that you ran into this issue implies there may be a problem with how the dependencies are specified in whichever "channel" you installed from with conda - do you remember how the conda environment was created?
comment
I'm going to close this one as resolved as it's a dependency issue. If the issue persists or you have any additional info about how the environment was created, please feel free to reopen/post.
comment
+1 for documenting this. I think the behavior is intentional and arguably makes sense (after all, there are multiple paths via the different multiedges). I agree that the behavior might surprise users given that this yields lists of *nodes*, so the information about which specific edge was traversed is not captured in the output.  FWIW a simpler reproducing example:  ```python >>> G = nx.MultiDiGraph([(0, 1), (0, 1), (1, 2)]) >>> list(nx.all_simple_paths(G, 0, 2)) [[0, 1, 2], [0, 1, 2]] ```
comment
Given the above, I will close this - thanks for looking into this though @SkBlaz !
comment
> At least in the test graph I added to this PR, int(w * 1e6) fixes the convergence error. I will test for more cases when I can.  Thanks for checking that - this is a general problem with [floating point comparisons](https://github.com/networkx/networkx/issues?q=label%3Afloating-point+). The typical recommendation for people who run into this problem is to use integer weights instead: this allows the user to scale to whatever level of precision truly matters for their problem.  Given that I'm -1 on this specific proposed change, but perhaps we can add a caveat in the docs about float weights, similar to #5171 ?
comment
I agree that a multigraph example would be a nice improvement. The fact that `values` expects the dict keyed by 3-tuples is definitely worth highlighting.
comment
> Can you guide me on which file in the repo to be edited?  Check out [our new contributor FAQ](https://networkx.org/documentation/latest/developer/new_contributor_faq.html#q-i-want-to-work-on-a-specific-function-how-do-i-find-it-in-the-source-code) which will hopefully give you some ideas for how to navigate the code base!
comment
> Sir, however is there a function as of now, that accesses the in and out node with the edge key and gives the dict with its attributes?  You can use the `data` kwarg of the `edges` method to get the edge attributes as a dict keyed by attribute name, e.g. `G.edges(data=True)`
comment
> That second commit looks like it might be a bug in Matplotlib; do you have a small reproducer?  Do you mean the moving of when `colorbar` is called in 722978f ? I just tried this and I get a `MatplotlibDeprecationWarning`, which seems like everything is handled correctly on MPL's side:  ```python >>> import networkx as nx >>> import matplotlib.pyplot as plt >>> import matplotlib as mpl >>> G = nx.DiGraph([(0, 1)]) >>> pos = {n: (n, n) for n in G} >>> faps = nx.draw_networkx_edges(G, pos, edge_cmap=plt.cm.plasma)  # Returns a list of FancyArrowPatches (1 in this case) >>> pc = mpl.collections.PatchCollection(faps, cmap=plt.cm.plasma) >>> clrs = range(4, 5)  # Use `range` like the example >>> pc.set_array(clrs) >>> plt.colorbar(pc) MatplotlibDeprecationWarning: Starting from Matplotlib 3.6, colorbar() will steal space from the mappable's axes, rather than from the current axes, to place the colorbar.  To silence this warning, explicitly pass the 'ax' argument to colorbar().   plt.colorbar(pc) ```  @QuLogic if you think there's something more here, LMK and I'd be happy to work out a better/more specific example.
comment
Looks like this was closed by #5937 , thanks @jarrodmillman !
comment
I agree this would be a really nice addition! Re: API I'm not sure what's best, though I do think it makes sense for the "layers" to be lists instead of sets to preserve information about the visit order within each layer (IIUC).
comment
+1 for the changes - just wanted to comment here that I'd actually advocate for removal in 3.2 rather than 3.1. *Technically* deprecations shouldn't be introduced in patch releases, so the next non-patch release that would have the deprecation warning would be 3.0 which, according to the policy, would mean removing in 3.2.  It's a very minor point and I don't feel too strongly about it, but just thought I'd bring it up. If others agree I'm happy to make the change, but if others are happy with how it is that's fine with me too!
comment
> But, just to make sure I understand correctly: the stuff we deprecated before v2.8 could be removed in v3.0 because that is a major release. The stuff we deprecate now -- before v3.0 and patched into 2.8.6 -- would be removed in v3.2 because we need 2 minor releases or one major release between announcement and removal.  This is my understanding, yes. Under normal circumstances I don't think we'd want to be introducing deprecations in patch releases, but as you noted the patches for 2.8 have been *special* in some sense since we're doing a lot more backporting than ususal. Given all this *I think* the above interpretation makes sense, but as I say, I don't feel particularly strongly about it!
comment
In general this will *not* result in deterministic node positions because the default behavior is to use `spring_layout`, which is not deterministic. This behavior is [noted in the documentation](https://networkx.org/documentation/latest/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html?highlight=draw_networkx#networkx.drawing.nx_pylab.draw_networkx) (specifically, in the description of the `pos` parameter).  For some simple cases (e.g. a small path graph) spring layout *may* result in a visually similar layout when called multiple times in a row, this is not guaranteed however.  As you've noted, if you need determinism, you should compute the layout specifically with the `seed` argument.
comment
The changes caused a breakage in one of the gallery examples (see the failing circleci build). It looks like one of the `float()` conversions was for strings, rather than numeric types.
comment
I'm not an OO programming expert, so take this with a grain of salt, but it's my understanding that the mixin multiple inheritance pattern is primarily for extending methods, *not* creating a "subtype" (where concrete instantiation is important). A quick excerpt from [Fluent Python](https://www.oreilly.com/library/view/fluent-python/9781491946237/) (emphasis mine):  > 3. Use Mixins for Code Reuse > If a class is designed to provide method implementations for reuse by multiple unrelated subclasses, without implying an “is-a” relationship, it should be an explicit mixin class. Conceptually, a mixin does not define a new type; it merely bundles methods for reuse. **A mixin should never be instantiated**, and concrete classes should not inherit only from a mixin. Each mixin should provide a single specific behavior, implementing few and very closely related methods. >  > Fluent Python, 1st ed., pp. 359  This makes it seem like it is incumbent on the designer of the mixin/subclasses to not rely on instantiation from the parent class. 
comment
Thanks for the feedback @AnjoMan . Long story short, the pydot interface in networkx has had a lot of problems lately, largely stemming from the fact that `pydot` does not currently seem to be actively maintained. Given this, the current plan is to deprecate the `nx_pydot` interface entirely - see [this pinned issue](https://github.com/networkx/networkx/issues/5723).
comment
I'm going to go ahead and put this in to start getting rid of our red x's. Thanks @MridulS 
comment
Looks like there are new failures in CI fetching resources related to graphviz via homebrew, see e.g. https://github.com/networkx/networkx/runs/7003642269?check_suite_focus=true. This could be a transient failure related to bad connections though. If I had to guess what was causing these issues, my money'd be on the homebrew bump in the latest macos-11.6 version bump on github actions, i.e. see actions/virtual-environments#5757
comment
> If this isn't clear, I can provide a graphical example of the required output vs what was produced. I can also add a test to make sure this bug isn't introduced again.  If it's just the ordering of the layers themselves, then I don't think that's a bug - that ordering was never guaranteed and shouldn't affect the information conveyed by the visualization (except maybe in terms of aesthetics). If OTOH nodes are ending up in the wrong layer then yes it's a bug and it'd be good to provide a concrete example.  > This used to be the default behavior, I thought it was a bug because it broke down by graphs and I had to figure out why.  I'm not sure I understand this exactly. Maybe it would be best if you could provide a (hopefully extremely short) example that shows what the result used to be and what it is now so we can get a better of the change. Alternatively you can add a test (which would be necessary anyways) that illustrates how the behavior changed (i.e. fails currently but passes with the desired behavior).
comment
> Here's a suggested fix:  I'm +1 on the first bullet point - an extra kwarg to map the subset key to the preferred visual ordering of the layers makes sense. IMO the second bullet point seems like overkill: the kwarg would enable the user to get the exact behavior that they want. Automagically trying to determine the layer for the special case when the subset keys are numerical would complicate things for relatively little gain. I guess it boils down to how important it is to recover the original layer ordering behavior "for free" - I'm interested what others think about this tradeoff!
comment
Thanks for the example @thaumkid , here's what I get when I run it:  ```python >>> G = nx.karate_club_graph() >>> DG = G.to_directed() >>> nx.algorithms.community.louvain_communities(G, resolution=1, seed=1234) [{0, 1, 2, 3, 7, 11, 12, 13, 17, 19, 21},  {4, 5, 6, 10, 16},  {23, 24, 25, 27, 31},  {8, 9, 14, 15, 18, 20, 22, 26, 28, 29, 30, 32, 33}] >>> nx.algorithms.community.louvain_communities(DG, resolution=1, seed=1234) [{0, 11, 17, 19},  {1, 3, 7, 12, 13, 21},  {2, 8},  {4, 5, 6, 10, 16},  {23, 24, 25, 27, 28, 31},  {26, 29},  {9, 14, 15, 18, 20, 22, 30, 32, 33}] >>> nx.algorithms.community.louvain_communities(DG, resolution=0.5, seed=1234) [{0, 1, 2, 3, 7, 11, 12, 13, 17, 19, 21},  {4, 5, 6, 10, 16},  {23, 24, 25, 27, 31},  {8, 9, 14, 15, 18, 20, 22, 26, 28, 29, 30, 32, 33}] ```  So in the aggregate, it does seem like the factor of 2 is required in the directed case to recover the behavior of the undirected case. As to what's *correct* I'm not sure, that depends on the interpretation of `m` and resolution as you've mentioned and I'm certainly no expert. @z3y50n do you have any thoughts on this? 
comment
Thanks @z3y50n - at this point I think we should split the directed/undirected cases into two separate PRs, as it seems like the fixups for the undirected case are ready. I'd propose the following:  1. Undo the changes related to the directed case from this PR  2. At that point, this one will be ready for review, and we can work on getting it in!  3. Reopen #5704 with a note that the issue appears to be separate (enough) from #5175  Re: a strategy for figuring out why the directed case is still getting stuck, it might be worth zooming out from the modularity update computation and just stepping through the algorithm applied to the case in #5704 with a debugger. This process can be a bit tedious, but at the very least it should help identify where exactly the "infinite loop" manifests. That's just one idea though, a first-principles analysis is obviously preferable but if you've hit a roadblock there a little interactive debugging might be informative!
comment
Ah ok - so changing the node ordering (by adding the `add_nodes_from`) for the first test example *does* avoid the infinite loop as hoped, but also results in different communities. Is this expected @z3y50n ? If so, we can simply change the `expected` values for these tests and call it done :)
comment
> and i also test a graph with 10 edges, For some bad structures the program seems to get stuck in a dead loop,   It would be very useful if you could share a minimal reproducing example that results in the bad behavior: for example, the graph with only 10 edges that results in the dead loop. This can help us determine whether there is something in the code that can be improved.
comment
Interesting - many thanks @ginandsherry for the reproducer!  It does indeed seem that node order has an effect here. A slight modification of the initial example that results in a different insertion order has no problem:  ```python >>> import networkx as nx >>> G = nx.DiGraph() >>> G.add_edges_from([ ...     (0, 2), (0, 1), (1, 0), (2, 1), (2, 0), (3, 4), (4, 3), (7, 8), (8, 7), (9, 10), (10, 9) ... ]) >>> G.add_node(5)  # Add node that was not included when adding edges >>> G.nodes() NodeView((0, 2, 1, 3, 4, 7, 8, 9, 10, 5)) >>> pa = nx.algorithms.community.louvain_communities(G, seed=123)  # returns immediately ```  However, when running the reproducer as presented (i.e. adding nodes prior to edges) I get the stated behavior:  ```python >>> import networkx as nx >>> G = nx.DiGraph() >>> G.add_nodes_from(n for n in range(10)) >>> G.add_edges_from([ ...     (0, 2), (0, 1), (1, 0), (2, 1), (2, 0), (3, 4), (4, 3), (7, 8), (8, 7), (9, 10), (10, 9) ... ]) >>> G.nodes() NodeView((0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) >>> pa = nx.algorithms.community.louvain_communities(G, seed=123)  # hangs ```
comment
Thanks for the report - this is a known issue (see #5175 & #5704) that is fixed in #5713. I will go ahead and close this as duplicate as it will be fixed in the next release!
comment
> Is it just me or CircleCI isn't building here?  Not just you - I see it too.  @tom24d is there any chance that you have circleCI enabled on your fork? I know that this sometimes (always?) prevents circleCI from running in the "main" fork.
comment
No worries @tom24d - I think there's just something wrong with circleCI at the moment; nothing related to your PR!  I've built the changes locally and can confirm the link renders correctly. If we want a hosted preview, I'm happy to push them up to gh-pages on my fork - just LMK @MridulS !
comment
NetworkX has experienced problems with pydot for a while now, including some behavior that changed in pydot with pyparsing v3, so you might want to check the pyparsing version as well to see if that makes a difference.  Unfortunately there's only so much we can do about transitive dependencies - the goal was to raise an informative error when there was a likely pydot parsing issue, but I wouldn't be surprised if there were false positives.
comment
I vote for including them within the repo is well (if not in the wheel) - this is very convenient for quickly testing the performance impact of proposed changes, at least locally. Of course, reliable benchmarking can be quite complicated but I think there's value in having something that developers can use locally!
comment
> Also extrema_bounding will be removed in NetworkX 3.0 so don't worry too much about adding an example for it :)  If you're running on a development version of networkx (i.e. installing from `main`) then you should see a `DeprecationWarning` when you try to call `extrema_bounding` directly. "Deprecated" means that the function is planned to be removed in the future, and the warning is in place to warn users against using it so that their code continues working with future versions. Indeed we don't want to add anything to this function since it slated for removal.
comment
I think the merge conflicts here arise from the fact that this also incorporates the changes from the recent `edge_cover` examples, likely due to accidentally branching from the edge cover branch.  There are multiple ways that this could be fixed e.g. via rebasing. I'd be happy to help with this, though it would involve force-pushing and would require you to reset your local branch. If you're not comfortable with that, another option would be to create a new branch (from `main`) and resubmit the relevant `chain_decomposition` example in a new PR. LMK if you'd like me to rebase for you, otherwise feel free to fix in whatever way you're most comfortable with!
comment
Oops - it turns out this is related to the changes introduced to `covering.py` in #5549, not changes to the examples. Anyways the above still stands - the edge covering examples would be better in a separate PR so that this one focuses only on chain_decomposition.
comment
Gentle ping @simonsteinberg - do you want to give the above suggestion a try? No worries if you don't have time - we can always convert this to an issue to be picked up later!
comment
> I think we should do it a series of PRs, so it will be easier to revert stuff if that becomes necessary.  Agreed - 1 PR per deprecated function removal would be my preference, just in case a removal needs to be reverted in the future.  I gotta say though, `+7 -432` is beautiful :heart: 
comment
I think at the last meeting we discussed updating the docstring to note that these functions don't include `source` rather than adding a new keyword argument (see [this comment](https://github.com/networkx/networkx/issues/5801#issuecomment-1161598494) for a longer version of my take).  If others prefer the new kwarg instead that's find with me (I don't have a strong preference) but IMO we should make a clear decision somewhere!
comment
It looks like the problem stems from here:  https://github.com/networkx/networkx/blob/123fc2be585c6ebbf46b7fe876ba85dd34f92b2e/networkx/algorithms/shortest_paths/weighted.py#L76-L78  The use of the `get` method ignores missing keys and returns the default value of `1` instead. This is obviously not desirable in the case described above, where an error is appropriate. However, it looks like this behavior is relied upon in other contexts to set default values, so the fix isn't a no-brainer.
comment
I'm going to re-close this one as I don't think there's anything to be done about it in NetworkX. The [underlying graphviz issue](https://gitlab.com/graphviz/graphviz/-/issues/1789) is well-known and there's [quite a bit of discussion](https://forum.graphviz.org/t/clustering-gives-undeterministic-results/989) on the topic.  While the "use-pydot-instead" approach may skirt this issue, users may run into other pydot issues so YMMV - I don't think it's recommended given that the plan is to remove `nx_pydot` in the future #5723.  Aside from adding a warning to the docs somewhere in `nx_agraph`, I don't think there's any obvious workarounds we could implement in networkx itself. If there are concrete suggestions please feel free to re-open this, preferably with an accompanying PR.
comment
AFAICT this was fixed by #3136, and `nx.is_isomorphic` works as expected on graphs with mixed (i.e. non-sortable) nodes on 2b258dfd5:  ```python >>> import networkx as nx >>> G = nx.Graph([(1, 'a'), (2, 'b')]) >>> H = nx.Graph([(3, 'c'), (4, 'd')]) >>> nx.is_isomorphic(G, H) True ```  I'm going to close this as completed, though if there's still something to do here please open a new issue!
comment
The plan here is not to *replace* VF2 with VF2++, but to implement VF2++ alongside the existing isomorphism functionality. @kpetridis24 is currently working on this problem, so I think we can safely remove this from the "wishlist".
comment
It's been about ~3.5 years since this was last commented on, so I'm going to go ahead and close this. That's not to say it isn't interesting or worth pursuing! If there's still interest and bandwidth on this front, let's go ahead and open a new issue to start the discussion anew - it'd be good to have a fresh picture of the current status of the project.
comment
I'm going to go ahead an close this one as inactive. It seems like an interesting idea though, so if there's desire to do so, please follow up either by opening a new issue with more details or better yet a PR that encapsulates some of the main ideas. Note that you can open a draft PR so that you don't have to implement everything to start but can get feedback on higher-level ideas.
comment
There are quite a few different things going on here. Re: the first question - I suspect (but haven't don the work to prove) that the change is indeed due to the insertion-order guarantee of Python dictionaries introduced in Python 3.6. Seeing as the minimum supported Python version is currently 3.8, I'm not sure that issues surrounding dictionary ordering from the 3.5/3.6 transition are particularly relevant now.  Re: "randomizing" the results of `max_weight_matching` in a deterministic way, I don't think it makes sense to internally randomize a function that is not intended to be random. Since the outputs seem to depend on edge order, if you want to probe different outputs in a deterministic way, I'd probably try something along the lines of (untested):  ```python >>> edges = <your_edge_list> >>> import random >>> random.seed(0xdeadc0de)  # a new seed for each attempt >>> random.shuffle(edges) >>> G.add_edges_from(edges) >>> nx.max_weight_matching(G) ```  Now that dicts are guaranteed to maintain insertion order, the above (or similar) should be a reliable way to deterministically reproduce "random" max weight matchings.  I will close this as (hopefully) no longer relevant given that we're well past the Python 3.6 boundary, but there may be more to do here. If so, please open a new issue so we can reframe the discussion outside of the context of old Python versions.
comment
This one's been inactive for awhile and I think is sufficiently addressed (at least for the typical use-cases) with reducing the number of iterations as suggested.  For reference, here's what the original example looks like with the default number of iterations (50):  ![draw_spring_components](https://user-images.githubusercontent.com/1268991/176260729-0b9c3dc6-fcb6-4974-80c7-1e3b3a2e69d6.png)  Force-directed layout algorithms that are specifically tailored for graphs with multiple components are of course always welcome!
comment
I assume this is referring to `min_edge_cover`, in which case this was fixed by #5549 (though in a different way than expected from the above: rather than duplicate *all* the edges in both directions, it removes duplicates entirely).
comment
I'm going to close this as one as a duplicate of #4641, as there's more discussion on the topic there.
comment
I'm going to close this one as resolved as it seems like the initial question has been sufficiently addressed and I'm not sure there's anything actionable here. If there are improvements that could be made related to this problem, I'd recommend opening a new issue/PR with the proposed changes (new algorithm, documentation improvements, or whatever) and link back to this one in the discussion. OTOH if I've misunderstood the above, please feel free to reopen!
comment
This was recently changed in #5308 - the line numbers mentioned above don't match those given in the OP. Can you take a look at [the development branch](https://github.com/networkx/networkx/blob/main/networkx/algorithms/centrality/closeness.py) and verify whether you still believe there's an issue? Everything looks correct AFAICT, but I don't know specifically what's being referred to above.
comment
I'm not sure what's the best way to go about this. On the one hand I understand the convenience argument of allowing `nodelist` to have nodes that are not in `G`, but one major downside of this is that it would result in silently-incorrect results when a user *accidentally* adds a node to the nodelist that is not intended to be there.  I think @dschult 's suggestion is a good one, especially since this guarantees that the various data structures are kept in sync.
comment
This change was made in #4216, precisely to prevent the case where an accidental inclusion of a non-node in `nodelist` would result in a resulting adjacency structure with the wrong shape.  > My guess is that the original intent of the nodelist argument in the method was precisely the use-case I presented in the OP.  The docstring makes it seem like the intent of `nodelist` is to control the ordering, i.e. the mapping from node to row/column number.  > In the to_pandas_adjacency method as described [here](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.to_pandas_adjacency.html) what would be the point of being allowed to supply a nodelist if it always had to be identical to the graph's nodelist?  The two obvious use-cases are 1) to control the row/column ordering (as mentioned above) or 2) get the adjacency of a subset of nodes  > However, currently nodelist can only have a value of the nodes in the graph definition therefore it is redundant b/c it is the same a providing no list.  This is not correct, subsets of nodes are valid. Also the ordering is still important.  > The user was protected from accidentally adding a node b/c the argument was optional  This change was made precisely to prevent this problem - see the discussion in #4216  I'm personally -1 on the proposed change (i.e. reverting #4216) because I think the downside of silently getting the wrong adjacency from accidentally including nodes in `nodelist` that are not in `G` can lead to difficult-to-chase-down bugs. I also agree with the reasoning in #4216 that the "fix" to reclaim the old behavior is relatively straightforward. Even in the case where you have many graphs and a node-superset that changes, adding isolated nodes to the underlying graphs should be relatively quick and unlikely to be a bottleneck in any analysis:  ```python for g in my_graph_collection:     g.add_nodes_from(my_node_superset) ```
comment
@MridulS this would probably benefit from a revisit given the new developments re: graphblas and/or the more holistic experiments you've conducted (maybe?) with dispatching. Just pinging here in case you want link to other relevant info and/or close in favor of another preferred discussion location - whatever you think!
comment
> In consequence, I think the initialization of networks from sparse matrices should be updated, so that present zeros in the sparse adjacency matrix should also be interpreted as non-edges.  It's possible that users rely on the current behavior though. If a user has a sparse matrix and has explicitly chosen to include 0's, it would be inappropriate for the conversion function to drop those values.  In this case, this feels like something better handled in the sparse-computation part, i.e. `sparse.kron` should not (or have an option not to) include 0's in the output (keeping zeros in the sparse output seems to reduce the utility of the sparsification in the first place). Of course, you could always sparsify after the `kron` operation, which gives you the result you want, e.g. ``` result_sparse = sparse.csr_matrix(np.kron(matrix, matrix)) ``` but then you have a "dense" step in the calculation (again, this feels like a limitation of sparse.kron).
comment
I'm going to close this one as "won't fix", as I think we agree that the source of the problem originates outside of networkx's sparse conversion functions.
comment
I'm going to close this one as resolved. If the issue persists or there are concrete suggestions on how the documentation could be improved, open a new issue or PR.
comment
Thanks @dschult for correctly identifying the problem and providing the fix.  One potential improvement would be to try to add a mechanism to give more informative error messages when the matching callables fail. 
comment
I'm going to close this one as resolved by @dschult 's answer.
comment
> What's the status of this? Security concerns are not small, especially with how widely deployed the NetworkX package is.  I think if there were a contributor who was interested in exploring switching to `defusedxml` it would certainly be considered. Note that the security concerns do not originate in NetworkX itself, but through the use of the xml package in the Python standard library. In principle "fixing" the security vulnerabilities upstream would automatically handle the security issues in NX (and any other library that depends on `xml`), though I'm sure there are reasons why this hasn't been done and a third-party library has been recommended instead.
comment
I agree with @dschult 's assessment here - this seems largely like an upstream/dependency management issue. I'm going to close this as resolved by the above suggestion. I
comment
Good point - I'm not sure what the best approach is here. On the one hand, having duplicated code is obviously not ideal. OTOH, the `AntiGraph` in the examples is intended to illustrate how to properly subclass one of NetworkX's graph classes, whereas in `kcomponents.py` that same implementation is used internally (and, fortunately, privately) in the `kcomponents` code.  Due to the differing goals, I'd be less concerned about things like the implementations diverging, as the "how-to-subclass" example can still be useful even if the implementation is different than the `_AntiGraph` that is actually used in the library.  I don't have any concrete ideas or strong opinions about what to do here... anyone else?
comment
I'll go ahead and close this one as resolved, but if there's a desire for a different approach please feel free to reopen, or better yet submit a PR!
comment
I've added the milestone for the documentation component (i.e. point 1) above) which is clearly incorrect and should be fixed before the next release. The other points may require separate discussion --- in fact, it may make sense to break out @dschult 's comment into a separate issue or discussion topic, but we can cross that bridge when we come to it!
comment
The documentation update was closed by #5699. I've moved the remaining points for discussion to #5825 so we can continue the discussion there.
comment
> I do like this! Something like https://scikit-learn.org/stable/_static/ml_map.png but for network science algorithms could help direct folks towards the "right" kind of algorithm they are looking to work on :)  Agreed - the sklearn knowledge map is awesome and a great way to get a sense of available functionality in the library. Something similar for NetworkX would be great. It'd be *extra* awesome if we generated the knowledge map using... networkx :)
comment
> I could possibly make the MM made in Networkx and explored in [Gephi](https://gephi.org/) also clickable and filterable like with [sigma.js](https://www.sigmajs.org/).  Just a quick comment here - I think it'd be a priority to keep as much of the analysis and visualization in Python/NetworkX as possible (this is a networkx project after all!) I think there are plenty of options for visualization that are better tied to Python, and specifically the scientific Python ecosystem. Just my two cents before you start committing too heavily to any particular approach!
comment
Thanks @stanyas , I still think a NetworkX knowledge map is a really good idea! This issue has a lot of GSoC-specific discussion in it which is likely no longer relevant, so I will extract the basic idea into a separate issue and close this one.
comment
The NetworkX-specific components of the issue are captured in #5597, so I will close this one. Additional discussion related to NetworkX's support for the GEXF format should be done in #5597. Thanks @Jasmine-lxn for drawing attention to this issue!
comment
Thanks @MridulS and @dschult for the nice explanations. IMO the descriptions and examples are sufficient to address the original question, so I will close this one. If the issue persists please feel free to reopen with updated info!
comment
Thanks @dschult I will close this since the root cause here is the accidental shadowing of the built-in `inspect.ast` package.
comment
I went ahead and double-checked this against the original paper. The `club` attribute assigned to the nodes in `karate_club_graph` is consistent with the ground-truth from the paper (i.e. the "club after fission" column of table 1). The only difference is that the nodes are 1-indexed in the paper and 0-indexed in NetworkX.  The 16/18 community sizes appear to correspond to the cuts determined by running `NETFLOW` on the data (i.e. the "side of cut" column in Table 1).  This should hopefully clear things up and I don't think there's any info really missing in the docstring, but concrete suggestions on how things could be improved are always welcome - feel free to reopen in that case.
comment
Likely just a random connectivity hiccup - I've restarted the job and it's passed that step so we should be good :+1: 
comment
I'm too slow on the draw :) - I'll open that issue, thanks again @ben-heil!
comment
I think this would actually be a valuable addition to the [example gallery](https://networkx.org/documentation/latest/auto_examples/index.html).  It might be kind of tricky to select a good example graph that highlights the differences between the different layouts, but IMO this would be a nice improvement to the docs. I'm going to go ahead and reopen this so we can track the suggestion!
comment
I'm going to put this in and will open an issue for following up on `_hits_scipy`, thanks @MridulS !
comment
> I guess no one used this as we haven't heard anything.  This may be because we failed to set the `stacklevel` properly for the warning "/. I just tried:  ```python >>> import networkx as nx >>> G = nx.path_graph(5) >>> H = nx.subgraph_view(G, nx.filters.hide_nodes([3])) >>> H._node FilterAtlas({0: {}, 1: {}, 2: {}, 3: {}, 4: {}}, <function hide_nodes.<locals>.<lambda> at 0x7fe878f7acb0>) >>> H._node.copy() ```  on `main` and didn't get a warning, so it's likely that any users that hit this path weren't being warned.  In this case should we fix the stacklevel and wait for two more minor releases? Or do we want to just plow ahead? I don't necessarily think it'd be terrible to put this in despite the warning not being visible because it will break loudly (since we're removing functionality there's no chance of silently given an incorrect answer) and seems like it's pretty difficult to hit in normal usage. I'd be fine with either approach though!
comment
> As an extreme test, I wrote a test to go through the graph Atlas with 7 or fewer nodes and test each of them. I have to mark that as slow. But does that mean it still gets run once for every push to a PR? Perhaps I should just remove it. The other tests should cover all the cases. Thoughts?  Yes, the slow tests are run as part of the `coverage` workflow run, so the slow test would indeed run every PR/push. On the one hand it's nice to have such a comprehensive test, but OTOH that is a lot of computation time to run for each push (~95 sec on my machine)!  I'd vote for the following:  - split out the `test_triadic_census_on_directed_atlas` test into a separate PR  - Review the existing tests marked `slow` and see how many of them are actually relevant for coverage. If none are, then remove the `--runslow` from the coverage job. If only some are, then we could perhaps consider adding *another* `pytest.mark` category (maybe `pytest.mark.comprehensive`?) to indicate tests that are really slow but don't impact the coverage metric and shouldn't be run with each push (perhaps only at release time or as a scheduled job).  The first bullet will decouple the "what to do about the nice, slow test" question from this PR at least, then we can figure out how best to handle/categorize slow tests in a separate discussion. 
comment
Here's the current list of (potentially) missing functions, found using @jamestrimble 's procedure from the OP (thanks for sharing!)  ``` edge_betweenness find_cliques_recursive find_cores greedy_coloring_with_interchange is_triad _naive_greedy_modularity_communities project recursive_simple_cycles ```
comment
Thanks for the report - I can reproduce this on the development branch (96831f99). I also tried the barbell graph example you've provided with different seeds and noticed that the result was always < -1.
comment
When running the test suite locally I get a hang on `test_traveling_salesman.py::test_ascent_fractional_solution`. The easiest way to see which test is causing problems is to run the test suite with some verbosity, e.g. `pytest . -vv`
comment
It's going to be difficult to make progress here without a specific example. @joranp1 , can you pull out the example of the "simple graph" you reference from the paper? The paper is behind a paywall (for me at least) so I can't follow the original link.
comment
@joranp1 rather than posting links, it would be preferable to provide the example in the discussion directly, using NetworkX syntax. You can do this by typing code directly in a fenced markdown cell, like so:  ```` ```python <your example here> ``` ````  For instance, if the graph in question were a complete graph with 4 vertices:  ```python >>> G = nx.complete_graph(4) >>> nx.load_centrality(G) {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0} >>> nx.betweenness_centrality(G) {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0} ```  Providing the example/images directly in the github interface is vastly preferable to posting links to file transfer websites, which most users (e.g. me) won't click on due to security concerns.
comment
Thanks @lucasmccabe for the comprehensive answer! IMO that addresses the initial question sufficiently, so I will close this issue. Please feel free to comment/reopen if there are further questions.
comment
Yeah I think that makes sense, perhaps with a little preamble along the lines of `The ~sympy.SparseMatrix.charpoly` method can be used to compute the characteristic polynomial from the adjacency matrix of a graph. For example...`
comment
This seems like a reasonable request but it's not really possible to judge the implementation when it's pasted in an issue as it's much more difficult to see what this changes (i.e. it's much easier to review *diffs*) and we can't evaluate whether this breaks existing behavior because the tests aren't run.  @grusin would you be interested/comfortable submitting your suggestion as a pull request? That would make evaluating the changes much more straightforward.
comment
> I should be able to subscript into Multi(Di)Graph by pair of nodes and retrieve all the edges  The docs aren't particularly clear here, though I don't think they support the interpretation that indexing edges by node numbers will return all edges between the two nodes --- rather, indexing EdgeView (and subclasses) is intended to access specific edges (or their attributes). In the case of MultiGraphs, each edge includes a key hence the above error. I agree that the documentation could be improved here to make this more clear.
comment
> a quick way to get a list of a specific attribute value for each of the edges. I ended up doing this, is there a better way?  It depends on what you want specifically. For example, what you've done above will grab the `"weight"` attribute for every edge between two nodes (only relevant for multigraphs). Your solution is perfectly fine, though I would probably use list comprehension instead of the `list(map(` pattern:  ```python >>> e12_wts = [e['weight'] for e in G[1][2].values()] ```  If instead you wanted the attribute for *all* edges (instead of all edges between two specific nodes) then you'd iterate over the edge view (with data), something like:  ```python >>> G.add_edge(0,1, weight=7)  # Add a new edge for illustration >>> edge_wts = [d["weight"] for _, _, d in G.edges(data=True)] ```
comment
> This would handle the case when the layer identifiers are unsortable, but it will also work for ordering the layers according to a sort of the layer identifier when all layer identifiers are sortable. It does not allow the layer identifier to provide the numerical value for the position of that layer. Is that important to allow? That is how it used to work, so maybe good to have it available...  I think this is the way to go. Even though the ordering was never guaranteed, it's clear that users are relying on it (see also #5515 ) and it's perfectly reasonable to expect numeric layers to be sorted. I prefer the solution proposed above to e.g. adding a new keyword argument. 
comment
> I guess I am asking if this is the intended behavior, or maybe have a note in the documentation? Without looking at the internal code, the documentation and the examples suggest that a copy of the graph attribute dictionary is created.  The `graph_attr_dict_factory` is a *factory function* that returns a dict-like. In the particular case of the `AntiGraph` example, the factory simply returns an existing *class attribute* (i.e. `graph_attr_dict`), so yes, in this case the example is specifically designed so that the graph_attr_dict common to all AntiGraph instances. In principle this is a useful pattern as it allows the subclass designer tighter control over the memory footprint of the subclasses. The best example of why you might want to do this is the `ThinGraph` example in the [section of the Graph docstring on subclassing](https://github.com/networkx/networkx/blob/14c294b372efe1423dc2666457c98fc428ca0d02/networkx/classes/graph.py#L241-L260).  Your update to the example would indeed get you closer to the behavior you were expecting (though with some probably unintended side effects if you ever added a mutable object to the dict due to the shallow copy!) but a more idiomatic way to achieve the desired behavior would be to use the factory function pattern, which might look something like:  ```python class AntiGraph(nx.Graph):     graph_attr_dict_factory = lambda self: dict([("weight", 1)])  # Test that this behaves as expected >>> G1 = AntiGraph() >>> G1.graph["weight"] 1 >>> G2 = G1.copy() >>> G2.graph is G1.graph  # different instances produced by factory False >>> G2.graph["weight"] = 3 >>> G1.graph["weight"] 1 ```
comment
You're spot-on @dschult - self-loops are handled separately from non-self edges (prior to 2.6, they were silently ignored and weren't drawn at all for undirected graphs). This breaks the expected 1-to-1 mapping between edges and `len(edges)` containers when there is a mixture of edges + self-loops. I expect this bug affects every kwarg where edge properties are specified in a `len(edgelist)` container, e.g. `width` in the above example, which I think exhibits the same problem: `(1, 1)` has width `10` instead of the desired `1`, `(2, 2)` has width `1` instead of the desired `50`.
comment
Having looked at this again, I think the problem extends beyond the graph-with-selfloops case. There can be mapping problems when specifying a sequence of visualization properties (e.g. a list of edge colors) even with no self-loops are present. This arises from the fact that the default `edgelist=list(G.edges)` might have a different ordering than a user expects. For example:  ```python >>> G = nx.Graph([(0, 1), (2, 1), (3, 4), (2, 4)]) >>> edge_colors = ["red", "green", "blue", "black"] >>> pos = nx.circular_layout(G) >>> nx.draw_networkx_nodes(G, pos) >>> nx.draw_networkx_labels(G, pos) >>> nx.draw_networkx_edges(G, pos, edge_color=edge_colors) ```  I naively expected the `edge_colors` to map to the edges in the order they were specified in the `Graph` constructor (i.e. (0, 1)->red, (2, 1)->green, (3, 4)->blue and (2, 4)->black). However, the edgecolors of the last two edges are flipped:  ![ed](https://user-images.githubusercontent.com/1268991/155791887-b388cd0f-fc6c-4905-8b61-590887819090.png)  This is due to the fact that `list(G.edges)` returns a different ordering than the edgelist used to create the graph:  ``` >>> list(G.edges) [(0, 1), (1, 2), (2, 4), (3, 4)] ```  So what this means is - the only way you can currently *guarantee* that a sequence of properties (colors, sizes, etc.) maps to the edges you expect is to also pass in `edgelist` where the ordering explicitly matches the order of the properties! (though the self-loops ordering would likely still be broken, at least for undirected graphs).
comment
This was mostly resolved by #5407. There's still the issue of the mapping of sequences of edge attributes to the correct edges, but that's a more involved issue that is more of a discussion topic than anything immediately actionable. Thus I'll close this as resolved. If related issue pop up, please open a new issue!
comment
> I appreciate that some users may now rely upon this behavior, but I wanted to share it because it returns very counterintuitive results.  I'm not sure anyone should be relying on this behavior, this feels much more like a defect than a feature to me...  Here's another example illustrating the issue:  ```python >>> G = nx.DiGraph() >>> G.add_node('a') >>> G.add_node('b') >>> G.add_node('abc') >>> list(G.nbunch_iter('abc')) ['abc'] >>> list(G.nbunch_iter('abcd')) ['a', 'b'] ```  I'm going to mark this as a defect, but if this behavior is intentional it'd be great if a use-case could be shared (which we could potentially add to the docs) and the label can be updated.
comment
> One straightforward fix could be to check each iterator and rule out the strings. That disallows strings to be nbunch containers of single character nodes.  To my mind that isn't necessarily a bad thing, especially since the interpretation of `nbunch` can change based on the nodes in the graph (as noted above). Silent behavior changes based on object state can lead to pernicious bugs. For example, if a user *was* intending to use a string as a collection of single-character nodes, the fact that they could get a different result if a multi-character string was added to the graph as a node feels scary; take the above example but with a different interpretation:  ```python >>> G = nx.Graph() >>> G.add_nodes_from(['a', 'b', 'c', 'd', 'e']) >>> my_node_collection = 'abc'  # I intend this string to be interpreted as a container of nodes >>> list(G.nbunch_iter(my_node_collection)) ['a', 'b', 'c'] >>> G.add_node('abc')  # This change to the Graph breaks my expected interpretation of `my_node_collection` >>> list(G.nbunch_iter(my_node_collection)) ['abc'] ```  As @dschult points out, this is *very* much a corner case given all the conditions that have to be met to hit it, but clearly one that can be hit. One application that came to mind where something like this could crop up is working with [`Trie`s](https://en.wikipedia.org/wiki/Trie) where working with sets/subsets of strings is common.  I don't have a strong feeling for whether this is a strong enough motivation to change the current behavior, but I think it's worth exploring the tradeoffs if someone is interested!
comment
Having looked over this again, I don't think there's anything that can be done about this without adding some input validation code. I agree with [dschult's take](https://github.com/networkx/networkx/issues/4781#issuecomment-832207158) that this isn't desirable, especially since it would break code for users who use multi-character strings to represent an iterable of single-character nodes.  Therefore I'm inclined to close this as a "won't fix". If the problem crops up again from another use-case we should make sure to link it to this one and potentially re-evaluate whether some sort of input validation is necessary. Thanks again for reporting @j6k4m8 !
comment
Thanks for bringing this up - this is largely a duplicate of the SO post, so I will close it here as redundant. Of course, documentation improvements are always welcome, so if there are concrete ideas about adding examples, please don't hesitate to open a PR!
comment
The `remove_edge` method supports [removing edges by key for multigraphs](https://networkx.org/documentation/latest/reference/classes/generated/networkx.MultiGraph.remove_edge.html#networkx-multigraph-remove-edge). You can see the keys by adding `keys=True` to the `G.edges` call.
comment
I think this one has been sufficiently answered and the relevant info is in the docstring. This should be further bolstered by #5699. I'm going to close this but if the issue persists please feel free to reopen.
comment
AFAICT this issue has been addressed: both Katz centrality and CCPA are now available as `nx.katz_centrality` and `nx.common_neighbor_centrality`, respectively.  There may be other more specific questions, like why `katz_centrality` is implemented in the centrality package while `common_neighbor_centrality` is implemented in the link_prediction package, but this should be a relatively minor concern especially from the user's perspective since both functions are available in the top-level namespace.  If I've missed something or there are further suggestions on how things could be reorganized, please feel free to reopen this or open a new issue with specific suggestions!
comment
I think @dschult 's comment very nicely answers the question and IMO would be a nice addition to the docstring examples.
comment
> Pulled in the main branch and reran this to fix merge conflicts.  We should make a decision on this ASAP or else you're going to be doing a lot of rebasing :joy: 
comment
Python 3.11b2 was released yesterday, and from the traceback this appears to be either a Python or pytest problem.
comment
I just opened an issue with pytest: pytest-dev/pytest#10010
comment
@Lukong123 that seems like an improvement to me! :+1: 
comment
> Yes, I am proposing that the read/write functions no longer accept a string naming the path to a file, and all read/write functions deal only in file-like objects.  I'm -1 on this change: having a hard-to-grok decorator doesn't seem to me like a strong motivation for removing a very convenient, user-facing feature. I think it's also a pretty well-established pattern (at least in the scientific Python ecosystem AFAIK) for file IO functions to handle the actual file management for the user (see e.g. the load/save functions in NumPy).  Re: the decorator itself, I too found it difficult to read (a quick look at `git blame` shows that it was introduced ~9 yrs ago and, aside from formatting, hasn't really been touched since then). I'm not at all opposed to removing it and/or refactoring how/where file handling is done.  To sum up: I'm +1 on trying to remove/improve `open_file`, but -1 on any changes that remove the "convenience" support for `str`, `pathlib.Path`, etc.
comment
Thanks @danielolsen !
comment
This one was fixed in #5139. Re: `_transition_matrix` - there's definitely still some things that could be done to improve this, but one tricky thing is that the default method in many places is pagerank, which results in a dense transition matrix, so it doesn't make sense to sparsify in that case.
comment
This is an application specific problem - if you have a graph with 20,000 nodes and no edges, then `non_edges` would construct a generator that produces the edges of a complete graph with 20k nodes, i.e. around 200 million edges. @dschult is correct though - if it's possible to analyze the edges without necessarily needing to store them in a data structure, then this may be fine for your application.  I don't think there's anything actionable here so I will close this!
comment
Thanks for the report - taking a closer look at this I think the problem is in pygraphviz itself, so should be fixed there. I'm going to close the issue here and open one in the pygraphviz repo.
comment
This was (at least partially) fixed in #5049. It's probably worth revisiting at some point to see if there isn't another way to approach this that doesn't involve overriding the base autosummary template.  I'm going to close this as resolved by #5049 , but that shouldn't stop anyone from looking into this further if interested!
comment
Thanks @harshal-dupare IIRC there was some discussion about whether 1 or nan made sense, depending on the interpretation of attribute assortativity for nodes with a single attribute. 
comment
I'm going to close this one as resolved, thanks @harshal-dupare & @dschult !
comment
I'm not sure how much would be gained by re-wording the first sentence, as the remainder of the summary in the docstring is pretty explicit about what is meant by "possible triangles" and includes the formula:  https://github.com/networkx/networkx/blob/2eb274e39f712047cebf5666ee9caf2ba2e51ee4/networkx/algorithms/cluster.py#L398-L405  That said, PRs with proposals to reword the docstring are welcome.
comment
I'm not an expert so I can't comment on whether the terms "closed triad" and "open triad" is any more technically correct than "triangles", "possible triangles" and "triads". I still don't think changing the terms makes anything any more or less clear, given the specific definitions/formulae in the remainder of the docstring.  > My guess is that the terms are used differently in different functions, which are made by different people.  Unifying terminology across multiple funtions to make things more consistent would certainly be a worthy goal! If something is called a "triad" in one function but a "possible triangle" in another, that's a clear area for improvement. If you see an opportunity for unifying terminology across multiple functions, I'd encourage you to go for it!
comment
This issue has been inactive for a while, so I will close it. Documentation improvements & wording suggestions are welcome, but at this stage I think a concrete proposal (i.e. a PR) would be the next step in pushing the discussion forward. 
comment
The `greedy_modularity_communities` code has been refactored since this issue was opened - the `dq` is no longer part of the comparison. Therefore, I will close this issue - if the issue (or related) persists, please feel free to open a new issue!
comment
From the follow-up discussion in #5493 I think we determined that all of the info re: `pre-commit` was in the contributor guide, though it requires careful attention as there are multiple steps.  Given this, I will close the issue. I think it is indeed worthwhile to take another look at the contributor guide, perhaps after soliciting more feedback about pain points for new contributors. Thanks for bringing this up @Lukong123 !
comment
How did you install networkx? Any of those import patterns should be fine. IIRC the traveling salesman subpackage was only added in 2.6, so you will need networkx version 2.6 or newer.
comment
It's been a few month since the last comment, so hopefully this was resolved. If not, please feel free to open with updated info on the environment and how you installed NetworkX.
comment
I can't reproduce this - both figures from your example render correctly on my system.  Given this has to do with sequential drawing in separate figures, I suspect the problem has something to do with the matplotlib backend in your environment.
comment
I see no difference for various versions of networkx (2.5.1, 2.6.3, and 2.7) - I suspect the reason you see a difference is that you have different versions of matplotlib (and/or matplotlib dependencies) installed in the environment in which you have nx-2.5.1 vs. that which has nx-2.7.
comment
I'm going to close this as inactive - I suspect it has more to do with matplotlib and the plotting backend on the specific system.
comment
I believe this was closed by #5516 - please feel free to reopen if I'm mistaken!
comment
Thanks for reporting - the scipy.sparse array interface is only available in the latest scipy version (1.8) so if you are using an older version of scipy you will need to upgrade (e.g. `pip install --upgrade scipy`).  If for whatever reason upgrading is not possible, you can still use the `to_scipy_sparse_matrix` function, which relies on the older scipy.sparse matrix interface, though this will (should) emit a deprecation warning as NX is moving towards array semantics everywhere.
comment
I'm going to close this as resolved - if the issue persists, feel free to reopen with updated info!
comment
> Is it possible to add a parameter to draw edges based on edge attribute?  Currently this is not supported. IIRC this has been discussed before (though I failed to find any relevant links from a quick scan of the issue tracker); extracting the visualization attributes directly from the  graph/node/edge attributes is a significant departure from the way `nx_pylab` currently works. It's an interesting idea (and more akin to how e.g. Graphviz works) but it strikes me as a larger-scale project.  Re: the specific case you mention, I think there are quite a few things you can do to simplify the drawing, e.g. (untested)  ```python nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color="indigo") # How exactly this is done depends on how you are setting the edge attributes # For the sake of illustration I assumed you have an "is_bidirectional" edge attr that # is a bool d_edges = [(u, v) for u, v, isbd in G.edges(data="is_bidrected") if not isbd] bd_edges = [(u, v) for u, v, isbd in G.edges(data="is_bidirected") if isbd]  opts = {  # Options common to all edges     "node_size": node_sizes,     "arrowsize": 10,     "edge_cmap": edge_cmap,     "width": 2, }  nx.draw_networkx_edges(G, pos, edgelist=d_edges, arrowstyle="->", **opts) nx.draw_networkx_edges(G, pos, edgelist=bd_edges, arrowstyle="<->", style="dotted", **opts) ```  ... give or take an option or two. If you have many graphs that have the same types of attributes, it's generally a good idea to create a function with your preferred visualization params.
comment
This was fixed by #5514.  There's still the question of whether to raise a warning when a user calls `draw_networkx_edges` with an undirected graph and `arrows=True` as they'll see vastly degraded performance from the fact that edges are drawn with `FancyArrowPatch` instead of `LineCollection` in this case. That's a separate issue though!
comment
This was closed by #5608. Of course, further example ideas are always welcome, but at least they are no longer missing here!
comment
Since this is being followed up on in networkx/nx-guides#72, I'll close this here.
comment
>  It looks like checking isinstance(m, int) doesn't work for np.int64.  Yes, that would be expected - `np.int64` is a fixed-width integer type whereas the Python int is not. A better way to check if something is *any* integer type is to use the `Integral` abc, e.g.  ```python >>> from numbers import Integral >>> isinstance(value, Integral) ```
comment
This was fixed in #5599, closing...
comment
I'm not sure I understand the premise. This seems more like an misuse of `__hash__` than a problem in the shortest path algorithms. 
comment
Many thanks @dtekinoglu for tracking this down and presenting the issue in such a clear and concise way! And similarly @dschult for the explanation of concepts and exploration of various possible ways to resolve this. Given the discussion, I agree with [the proposed plan above](https://github.com/networkx/networkx/issues/5594#issuecomment-1120213701)
comment
> Edit: How can we install 2.8 via pip command? pip gives the following message:  NetworkX 2.8 is available on [PyPI](https://pypi.org/project/networkx/) (along with 2.6.X and 2.7.X for that matter). Do you perhaps have `pip` configured to look elsewhere for packages?
comment
Closing as resolved, thanks @dschult !
comment
> I understand the need for this but I'm -1 on adding this to our documentation as this adds another thing to keep in check and maintain.  Agreed - I certainly understand the motivation for having an environment that "just works", but IME containers can be quite the maintenance burden.  I will close this as I too am strongly -1 on incorporating an official dev container. Of course you can continue to use/maintain the dev container for your own use @silamon , and thanks for sharing the suggestion!
comment
The examples use `doctest` to test that the results are correct. `doctest` has a [specific format for testing](https://docs.python.org/3/library/doctest.html#how-are-docstring-examples-recognized). In short, it's designed to look exactly like what you'd get out of an interactive interpreter, e.g. the input on a line with `>>> ` and the result on the following line. For example:  ```python >>> 1 + 1 2 ```  The examples are failing here because they don't follow this format - i.e. the examples section ends with no output after the last `>>>` line, but in reality those lines result in output that forms the basis of the test!
comment
> I am having trouble with my check. > It says Expected: {(1, 0), (3, 2), (4, 5)} Got: {(4, 5), (1, 0), (3, 2)} > But on my code I had {(1,0), (3,2), (4,5)}.  Ah this is a bit tricky - the problem is that Python `sets` don't guarantee any particular order when printing, which is a problem for doctest. There's a [section about this](https://docs.python.org/3/library/doctest.html#warnings) in the doctest documentation, along with some recommended workarounds.
comment
>  Is this better?  Still not quite right - the return type is still incorrect; there is no local variable called `degree_view` so that doesn't really make sense, and the return type can be `MultiDegreeView or int`. On the nitty side, I think it's best to mention the default behavior first in the return description. I would again recommend taking a closer look at the suggestion in the previous review!
comment
> @dschult I agree that this is an important discussion, probably broader than this PR.  Agreed - I'm going to open a separate issue so we can have the broader discussion without distracting from the particular case in this PR!
comment
> can you tell me what impact this has on PyCharm? Does it fix the typing errors you were getting? > Also, can you try changing them all to DegreeView or DiDegreeView or MultiDegreeView or MultiDiDegreeView or int and report back whether that also fixes the typing errors you were getting?  This would be interesting to know, but my two cents is that we shouldn't worry too much about whether PyCharm's type checker works. AIUI it is a custom type inference engine that goes well beyond what is specified in the typing standards (PEP 484 and related), so failures in PyCharm's type checking are not necessarily indicative of anything being specified incorrectly on NX's end. Just something to keep in mind while experimenting!
comment
Agreed - larger discussion notwithstanding this PR fixes the `degree` Return description so in it goes. Thanks @britapiiro !
comment
I'm not quite sure I understand the question. If you mean you want a list of the strongly connected components of a directed graph in order of component size you could do something like:  ```python >>> scc = sorted(nx.strong_connected_components(G), key=len) ```  I'm not sure this is what you had in mind though... if not, it would help to have a minimal example of the behavior you're looking for!
comment
Closed by #5588
comment
> We should have a look at https://github.com/networkx/networkx/issues/1705 again  I agree - this needs discussion and an adoption plan before moving forward.
comment
Given the conclusion in #1705 I'm going to go ahead and close this. Thanks for looking at this @kpetridis24 and helping to kickstart the stagnant conversation!
comment
This is a bit tricky... if you look at the implementation of the `subgraph` method (i.e. `G.subgraph`) it looks like it returns the induced subgraph. I agree that the docstring for `induced_subgraph` should probably show the function itself being used though, and perhaps mention that the subgraph method of graphs also returns the induced subgraph.
comment
> Add tests for the new method. Could you please guide me on this?  tl;dr - I'd argue we don't need to add any new tests for this function; see below for details.  Great question! Generally yes, whenever a new function is added it needs unit tests, though I think this case is special. Since `is_planar` is really just a wrapper around `check_planarity`, we don't necessarily to add any new tests as the unit test for `check_planarity` should sufficiently cover this function as well (this is one of the advantages of calling `check_planarity` in `is_planar` instead of re-implementing it!)  Note also that the `Examples` section in the docstring doubles as unit tests thanks to `doctest` - I think those are sufficient to ensure that the API is correct (i.e. single input, single output). Nevertheless, if you do think of interesting test cases you'd like to propose, please don't hesitate to add them!
comment
BTW @dtekinoglu since you're working on this I thought I'd call your attention to a related idea to add an `is_planar` function: see #5109 
comment
Ooops, it looks like 29bf846 may have gotten committed to the wrong branch - it looks like it was likely targeted for #5544 instead. I will revert it here as it's currently breaking the docs build, but just a heads up to make sure that those changes do make it onto the appropriate branch!
comment
Thanks for the report @Anaphory - you are correct, there indeed has been a behavior changed for unconnected graphs. Your suggestions as to what to do about it seem sensible, though I'm not sure which approach we should take (i.e. document the new behavior or treat this as a regression). I'd like to know what others thing, especially @martacki & @dschult 
comment
> I think this default makes more sense. The default on arrows should make this hard to run into, but it is good to have the code explicit and the documentation say what is being done.  I'm actually not sure about this one. The behavior that this PR switched to is now `arrows=True` results in *no* arrowheads for undirected graphs. I agree that the previous behavior for undirected graphs (one-directional arrowheads) didn't make sense, but I don't know that this is quite right either. In the [accompanying discussion](https://github.com/networkx/networkx/issues/5466#issuecomment-1087982884) it was suggested to switch to `<->` for undirected graphs when `arrows=True` - I think that's a better approach than what's taken here.  I'd vote to update this along with some tests to make sure we're covering all the input combinations (directed/undirected graphs with arrowstyle=None and arrows=True/False). Thoughts?
comment
In an attempt to fix the "two PRs for one issue" problem, I went ahead and grabbed the changes from #5192 that had been agreed upon during review and committed them here, pushing up to your branch @dtekinoglu .  Make sure you do a `git pull` so that these changes are integrated locally before you try to push any other changes up!  I made sure to commit @mmuejde 's changes (see 60fe980) under their name with the `--author` flag so that they get proper credit for their contribution as well. Since this PR has the most recent & active reviews on it, the plan is to push this one forward, merge it when ready, and close #5192 once this is in.
comment
I think the piece that's missing from the current install instruction is the step to install the `pre-commit` *hooks*. In other words, installing pre-commit so that it works inside your development environment requires two steps:  1. Installing `pre-commit` itself (this is covered in the existing `pip install -r requirements/developer.txt` instruction)  2. Installing the `pre-commit` hooks. This is typically accomplished with `pre-commit install`  Step 2 is the one that's currently missing; I'd recommend adding it around line 49, e.g. something like:  ```         # Install main development and runtime dependencies of networkx         pip install -r requirements/default.txt -r requirements/test.txt -r requirements/developer.txt         # Optional: install pre-commit hooks to automatically lint code at commit time         pre-commit install ```  ... as well as adding the corresponding advice to the conda instructions.  The reason I think it should be marked "Optional" is that pre-commit can actually be a bit tricky... for one thing, it may override any currently installed commit hooks (in `.git/hooks/pre-commit`) and can lead to tricky situations if a developer tries to commit in the repo from an environment where `pre-commit` *isn't* installed, which may happen when testing minimal dependency environments or just trying to make a quick change without activating (or activating the wrong) development environment. This is all *way* too much info for the contribution guide, but I just wanted to lay it out somewhere because it's confused me before and is something we should be aware of in terms of recommended practices to users.
comment
> Neither piece is missing:  Good point - I should've read the *entire* document before commenting about what's "missing" :)  So it looks like everything needed to set up pre-commit in the development environment is already there. There could be an argument for re-organizing it, but I think keeping `pre-commit install` in it's own bullet is actually *better* (see e.g. my suggestion for keeping it "optional") because it can be a bit tricky and isn't a perfect fit for "setting up the development environment".
comment
Given that we've determined all of the information is already present in the contributor guide, I will close this PR. There's certainly an argument for revisiting how the information is organized, but that would benefit from some higher-level discussion. Thanks for bringing this up @Lukong123 !
comment
What's the motivation for this aside from the fact that it's the pattern followed by other projects? Is there a major gain or improvement from renaming the exceptions to remove the prefix? Having shorter exception names is nice and I understand that the information is redundant, but neither of these seem to be a major improvement --- at least not when weighed against the potential for breaking existing code that is catching specific NX exceptions by name.  Just curious about the background if there's any info on this topic that isn't captured in the above discussion!
comment
This was [recently discussed](https://github.com/networkx/archive/blob/main/meetings/2022-04-08.md#long-term-tracking-items) and the general consensus was that there wasn't a lot of motivation for this change.  @boothby noted that there would be a way to go about this without breaking user code by creating new, non-prefixed exceptions that inherit from the current exceptions, e.g.  ```python class AlgorithmError(NetworkXAlgorithmError) ```  then replace all internal instances of `NetworkXAlgorithmError` with the preferred exception name. That way, any user code that's catching `NetworkXAlgorithmError` will still work as expected with the new exception name.  While this approach would alleviate the concerns of breaking user code, there is still low appetite for this change without a more concrete motivation to justify the churn. If you feel strongly about this or have additional points to add to the discussion, please feel free to reopen!
comment
> We should check whether the 4th item from https://github.com/networkx/networkx/issues/5192 helps performance. The initialization of r does call G.degree twice, but that call only creates a view of the graph data structure, so it shouldn't matter much. View construction doesn't create a new data structure. Still, it does create two copies of the view rather than one. But removing that trouble may cause other slowdowns if we're not careful. So, it'd be good to run some quick speed tests.  I'll admit that I had something vectorized in mind when I saw that line, e.g. something like:  ```python deg = np.array([d for _, d in nx.degree(G)]) r = (deg**2).sum() / deg.sum() - 1 ```  but it's a good point that worrying about this may not be worth the effort. I suspect that this line isn't the performance bottleneck of the function anyways given the array operations at the end (we'd need some line profiling to confirm), so we're definitely into "premature optimization is the root of all evil"-territory from that perspective. One could also argue about the readability of one over the other, but the difference is both minimal and subjective. I'd vote to back out the change related to point 4 in the interest of avoiding bikeshedding.
comment
Thanks for the work here @mmuejde - we had a scenario where another PR was opened to fix this. For the sake of convenience, I cherry-picked the changes from this PR that had been agreed upon and pushed them up to #5458 , so your contributions are still there (and authorship was preserved) but just part of a different PR. Since the changes are in now I'll close this one - thanks for looking into it!
comment
> I'm dubious about trying to automagically figure out which method to use  I tend to agree - IMO it's much better API to have the user to explicitly choose the method. One concern with heuristic-based autoselection is future compatibility: if the threshold at which one method is selected over the other is changed at some point, then all of a sudden user's can experience changes in performance or (in the worst case) different results, all from an invisible under-the-hood change.  But 100% :+1: on documenting the criteria for choosing which method to use!
comment
`to_numpy_array` doesn't support multiple edge attributes for MultiGraphs. If you run your example on the development branch, you should see something like:  ```python >>> A = nx.to_numpy_array(G, dtype=dtype, weight=None, multigraph_weight=sum) Traceback (most recent call last)    ... NetworkXError: Structured arrays are not supported for MultiGraphs ```
comment
Did you perhaps intend to open this PR to the `networkx/outreachy` repo instead?
comment
Thanks for this! This behavior was reported in #5106 and it'd be great to have a fix for it. At first glance this looks like a nice solution - I'd like to add some test cases both from the original issue and the discussion in #5106 to evaluate those corner cases. As [I noted in the discussion there](https://github.com/networkx/networkx/issues/5106#issuecomment-1051215243) I think the problem mapping sequences of edge colors to edges extends beyond the self-loop case and was a sticking point for the solutions I tried to come up with. I wonder if we can take the turning-edge-color-into-a-mapping approach and apply it more generally not only to the fancy edges.
comment
> For example, the description of the edge_color parameter says "Can be a single color or a sequence of colors with the same length as edgelist", but the problem is edge_color could be a sequence of any size, actually. If the sequence's length is less than the length of edgelist, colors are cycled, but you can only find this out, if you will look through the code.  It's not only when the attribute lists are different sizes than the edgelist; for example, take a closer look at the pink/blue edges in the [example from your previous comment](https://github.com/networkx/networkx/pull/5407#issuecomment-1078925909) (thanks for testing those BTW!) - notice that the properties of the (2, 2) and (2, 1) edges are swapped (e.g. `(2, 2)` is supposed to be blue, not pink). This is a result of the *ordering* of the edges being different when you call `list(G.edges)` vs. the ordering of the edge list used to create the Graph. Currently, the only way to *guarantee* that you are getting the mapping you expect between edges and their viz attributes (colors, widths, etc.) is to explicitly specify `edgelist` as well.  That's beyond the scope of the issue here though: correcting the handling of the self-loop edge drawing is also important!
comment
Thanks @dschult & @mjschwenne !
comment
Agreed - I think we've had related discussions in the past (maybe in #4452 - though that seems to have mysteriously disappeared!)  One thing to keep in mind is that `Generator` is not generally backward compatible with `RandomState` --- I would advocate for a 2-step adoption process:  1. Add support for `numpy.random.Generator` in the functions/classes listed above. This can be done now and shouldn't require a ton of re-working (there are just some subtleties to handle: for example `RandomState.randint` has become `Generator.integers` instead)  2. Make the switch from `numpy.random.mtrand._rand` as the default random instance used in NetworkX when none is specified to `numpy.random.default_rng` (or similar). Since this change is backward incompatible, I think it should be done for version 3.0 then we can draw a lot of attention to the change and announce that results will differ for the same integer `seed` values between networkx 2 and networkx 3. Of course, users can still get access to the old random number generator, but they'll have to pass in an explicit `RandomState` instance.
comment
Thanks for your interest in @goar5670 - there's already been quite a bit of progress on this issue: step 1 is implemented and in the latest NetworkX release, and step 2 is being discussed in [NXEP 4](https://networkx.org/documentation/latest/developer/nxeps/nxep-0004.html).  Since this issue is largely addressed I will close it - discussion related to NXEP 4 should happen elsewhere; I'll open a discussion topic for it. Thanks for the ping!
comment
The `average_neighbor_degree` calculation is described in the [docstring of the function](https://networkx.org/documentation/latest/reference/algorithms/generated/networkx.algorithms.assortativity.average_neighbor_degree.html)  Alternatively you can take a look at the implementation itself to get a better sense of how it's computed. This is very easy to do with e.g. IPython, where the `??` will show the function implementation. However, since we're on GH:  https://github.com/networkx/networkx/blob/0ce72858168a8ece6b55f695677f4be80f144aff/networkx/algorithms/assortativity/neighbor_degree.py#L124-L138  So, does the confusion arise because what is being computed is different than your expectation, or perhaps there's a problem with the implementation? We'll need to answer this question before we can determine how best to improve things (better docs, fix a defect, etc.)
comment
The `Pypy` workflow is failing because it runs without the default dependencies. The imports of any dependencies should be moved within the calling function to prevent problems at import time. In addition to the `has_sympy` additions to the test configurations, you may also need to annotate the new tests with a `pytest.importorskip` to prevent failures during test collection when the default dependencies are not installed.
comment
>  I would argue that the old values for DiGraph didn't make sense, so we should change it (correct it?) now... not waiting for a deprecation cycle  Agreed - this seems like it's fixing "defective" behavior, so I'd say no warnings necessary. A mention in the release note (potentially with an example) would be good :+1: 
comment
Thanks @vappiah - I went ahead and just embedded the reproducing code directly in the issue, as it's much more straightforward to see what's going on.  There are two things in this example that jump out to me:  1. You are not using the `pos` with `draw` - this is why the edge labels are in different positions than the edges  2. You are not using a seed when computing the layout, which is why the results are different each time you plot  Re: `1.` - `nx.draw` computes the layout under the hood when `pos=None`. You can fix this issue by: `nx.draw(P, pos=pos, with_labels=True)`. If you want to guarantee that the positions will be the same every single time, you should provide a `seed` to the layout algorithm, e.g. `pos = nx.spring_layout(P, seed=123456789)`
comment
I'm +1 for this but was just wondering if there was a way to preview the effects before merging? This is probably one of those "doesn't take effect until it's on `main`" type of changes, but just out of curiosity - if you merge this to `main` on your fork and look at the blame in the browser, do you see the difference?
comment
> Currently this doesn't work on github! it's still under a private "public beta".  Thanks, I hadn't realized :+1:   Generally this LGTM though I think we may want to start with a smaller subset of commits. For example, `git show 0c503e31ba` (the first one on the list) has a lot of changes from `pyupgrade`, but also includes some test re-organization that got squashed into that PR. When in doubt, I'd vote to keep commits like this off the list because the downside of silently missing "real" changes in the blame outweighs the benefit of ignoring the changes from linters. Just my two cents though - we can always modify the list of commits later as well!
comment
Looking at `extrema_bounding` more carefully, I wonder if it was ever intended to be called directly? It seems like it's mostly used internally for the supported measures mentioned in the docstring and `extrema_bounding` itself is untested!
comment
I agree with @dschult - I prefer that approach over adding more conditions to the `edge_labels` check.  Also to be certain that the test is specific to this behavior, I think adding a new test is warranted rather than adding a smoketest line to an existing test.
comment
Very strange - maybe something is wrong with the `python3` package in the fedora image? It seems weird that you're getting 3.10rc2 (without specifying that explicitly).  FWIW I can't reproduce locally and CI never caught anything w/ Debian-based linux, macos, or windows.
comment
AFAIK `importlib.machinery` is part of the standard library and should be expected to be available in a Python installation. It seems like its absence might be an explicit decision in how Python is packaged in Fedora? I'm not sure there's a whole lot NetworkX can do about this, since the `SourceFileLoader` is an important component in the implementation of lazy loading in networkx
comment
I had assumed that, since this never failed in CI nor in any developer's dev env that this was a problem with the Python packages in other distros... it appears that it's the opposite - the availability of `importlib.machinery` by default doesn't seem to be intentional:  https://github.com/python/cpython/blob/df9f7597559b6256924fcd3a1c3dc24cd5c5edaf/Lib/importlib/__init__.py#L2  I think you're right @stevenengler , we need to add an explicit import of the `machinery` module. Thanks for reporting and the follow-up research!
comment
FWIW I set up docker and I can't reproduce this in any of the official images mentioned above, i.e. `fedora:35`,  `ubuntu:focal`, nor `ubuntu:jammy`. I'm not 100% convinced that this isn't some sort of python packaging problem in the images that are being used - the only evidence I've found to the contrary is the `__all__` I linked above from the `importlib` module, but that doesn't necessarily tell the whole story.  The "fix" is straightforward of course, but I'm just curious if anyone has any insight as to whether (and if so) why it's necessary?
comment
> [GitHub CI failling](https://github.com/tdene/synth_opt_adders/runs/5384343915?check_suite_focus=true) with nearly the same trace as OP, using [this workflow](https://github.com/tdene/synth_opt_adders/blob/main/.github/workflows/config.yml).  Note that in this workflow the install step is done with `python` and the test-running step is done with `python3`, which may be the source of this particular issue. What happens if you modify these lines to:  ```bash python3 -m pip install --upgrade pip python3 -m pip install . ```  NetworkX uses `ubuntu-latest` in its own CI workflows and this `importlib.machinery` issue has never cropped up.
comment
Ack... good catch!
comment
I went ahead and merged with upstream to get rid of the unrelated circleci failure so the previews can work. I will try to circle back for a review soon, thanks @eskountis 
comment
Good idea - I'll rebase on main and force push which will retrigger CI. Assuming everything goes well I'll merge afterwards
comment
@martacki your diagnosis seems correct to me - you are right that switching to a generator would be a breaking change and require a Deprecation/Future warning cycle to notify users, but this may make sense given that there is a major release coming up soon. Either way the API will change: either a new kwarg is added (as proposed here) or `greedy_modularity_communities` is changed to a generator --- the question is which approach makes the most sense in the long run.  Looking at `modularity_max` again, the thing that stands out to me is that the [communities are returned in reversed sorted order](https://github.com/networkx/networkx/blob/ba73110a26e574f896fac75dba20f73fb5a9c421/networkx/algorithms/community/modularity_max.py#L220-L223). I'm not sure how you could guarantee this with a Generator.
comment
The code + docstring changes LGTM, all that's left is adding some testing for the new feature.
comment
Thanks @martacki for the test. This LGTM and I would approve it but it looks like there is a merge conflict w/ `main` now that we have to resolve first. If you are comfortable doing so, please merge with `main` and resolve the conflicts manually. If not, I'd be happy to help out - LMK what you prefer!
comment
Requirements files are specifications for `pip` - AFAIK conda does not fully support reading from requirements files.
comment
These lines are important for correctly specifying all the constraints, so I'm -1 on modifying requriements files to make them work with `conda`. In principle we could add an `environments.yml`, but IME that's a bit of a pain from a maintenance standpoint.  For conda users, my recommendation would be to use `conda` for environment management, then `pip` to install everything in the environment.
comment
> Does pip do things like install GraphViz when you install pygraphviz?  No it doesn't; pip doesn't handle non-Python dependencies whereas `conda` handles installation of more packages. In order to install e.g. `pygraphviz` (or any library that has a non-Python dependency) with pip, users will have to have that package installed separately on their system.  > I guess one option is to have requirements files for pip and other requirements files for conda.  The way `conda` handles environment specification is with [environment files](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-file-manually). Many projects maintain both `requirements.txt` and `environments.yml` files, but this has it's own drawbacks, namely:  - it's easy for them to get out of sync  - The dependency resolvers for `pip` and `conda/mamba` are different, so it's possible that any environment specification that works with `pip` won't work with `conda` and vice-versa.  I'm ambivalent about adding and `environment.yml` file - on the one hand it probably helps windows users who want to set up a full development environment; OTOH it's another thing to maintain and AFAIK there are no good solutions for automatically keeping conda/pip requirements in sync. Adding "official" support for `conda` also opens up the whole can-of-worms regarding which "channel" packages are retrieved from, which has proved problematic for packages NX cares about, most especially [pygraphviz](https://pygraphviz.github.io/documentation/stable/install.html)
comment
Hmm I can't reproduce this - the following works fine for me:  ```python >>> import networkx as nx >>> G = nx.DiGraph() >>> G.add_nodes_from( ...     ["HP:0000001", "HP:0001510", "HP:0000118", "HP:0000002", "HP:0001507", "HP:0004322"] ... ) >>> G.add_edges_from( ...     [ ...         ("HP:0001510", "HP:0001507", {}), ...         ("HP:0000118", "HP:0000001", {}), ...         ("HP:0000002", "HP:0001507", {}), ...         ("HP:0001507", "HP:0000118", {}), ...         ("HP:0004322", "HP:0000002", {}), ...         ("HP:0004322", "HP:0001510", {}), ...     ] ... ) >>> nx.drawing.nx_agraph.graphviz_layout(G.reverse(), prog="dot") {'HP:0000001': (119.25, 306.0),  'HP:0001510': (55.245, 90.0),  'HP:0000118': (119.25, 234.0),  'HP:0000002': (183.25, 90.0),  'HP:0001507': (119.25, 162.0),  'HP:0004322': (119.25, 18.0)} ```  I get the same result with both `pygraphviz` versions 1.7 and 1.9. What are the versions of `pygraphviz` and `Graphviz` in your environment?
comment
Here's the output of `pip list` for the env:  <details>   <summary>my env</summary> <pre>   Package                       Version     Editable project location ----------------------------- ----------- ------------------------- alabaster                     0.7.12 argon2-cffi                   21.3.0 argon2-cffi-bindings          21.2.0 asttokens                     2.0.5 attrs                         21.4.0 Babel                         2.9.1 backcall                      0.2.0 beautifulsoup4                4.10.0 black                         22.1.0 bleach                        4.1.0 certifi                       2021.10.8 cffi                          1.15.0 cfgv                          3.3.1 charset-normalizer            2.0.12 click                         8.0.3 codecov                       2.1.12 coverage                      6.3.1 cycler                        0.11.0 debugpy                       1.5.1 decorator                     5.1.1 defusedxml                    0.7.1 distlib                       0.3.4 docutils                      0.17.1 entrypoints                   0.4 executing                     0.8.2 filelock                      3.5.0 fonttools                     4.29.1 identify                      2.4.10 idna                          3.3 imagesize                     1.3.0 iniconfig                     1.1.1 ipykernel                     6.9.1 ipython                       8.0.1 ipython-genutils              0.2.0 ipywidgets                    7.6.5 jedi                          0.18.1 Jinja2                        3.0.3 jsonschema                    4.4.0 jupyter-client                7.1.2 jupyter-core                  4.9.1 jupyterlab-pygments           0.1.2 jupyterlab-widgets            1.0.2 kiwisolver                    1.3.2 lxml                          4.7.1 MarkupSafe                    2.0.1 matplotlib                    3.5.1 matplotlib-inline             0.1.3 mistune                       0.8.4 mypy                          0.931 mypy-extensions               0.4.3 nb2plots                      0.6 nbclient                      0.5.11 nbconvert                     6.4.2 nbformat                      5.1.3 nest-asyncio                  1.5.4 networkx                      2.7rc1.dev0 /home/ross/repos/networkx nodeenv                       1.6.0 notebook                      6.4.8 numpy                         1.22.2 numpydoc                      1.2 packaging                     21.3 pandas                        1.4.1 pandocfilters                 1.5.0 parso                         0.8.3 pathspec                      0.9.0 pexpect                       4.8.0 pickleshare                   0.7.5 Pillow                        9.0.1 pip                           21.3.1 platformdirs                  2.5.0 pluggy                        1.0.0 pre-commit                    2.17.0 prometheus-client             0.13.1 prompt-toolkit                3.0.28 ptyprocess                    0.7.0 pure-eval                     0.2.2 py                            1.11.0 pycparser                     2.21 pydata-sphinx-theme           0.8.0 pydot                         1.4.2 Pygments                      2.11.2 pygraphviz                    1.9 pyparsing                     3.0.7 pyrsistent                    0.18.1 pytest                        7.0.1 pytest-cov                    3.0.0 pytest-mpl                    0.14.0 python-dateutil               2.8.2 pytz                          2021.3 pyupgrade                     2.31.0 PyYAML                        6.0 pyzmq                         22.3.0 requests                      2.27.1 scipy                         1.8.0 Send2Trash                    1.8.0 setuptools                    60.5.0 six                           1.16.0 snowballstemmer               2.2.0 soupsieve                     2.3.1 Sphinx                        4.4.0 sphinx-gallery                0.10.1 sphinxcontrib-applehelp       1.0.2 sphinxcontrib-devhelp         1.0.2 sphinxcontrib-htmlhelp        2.0.0 sphinxcontrib-jsmath          1.0.1 sphinxcontrib-qthelp          1.0.3 sphinxcontrib-serializinghtml 1.1.5 sphinxtesters                 0.2.3 stack-data                    0.2.0 terminado                     0.13.1 testpath                      0.5.0 texext                        0.6.6 tokenize-rt                   4.2.1 toml                          0.10.2 tomli                         2.0.1 tornado                       6.1 traitlets                     5.1.1 typing_extensions             4.1.1 urllib3                       1.26.8 virtualenv                    20.13.1 wcwidth                       0.2.5 webencodings                  0.5.1 wheel                         0.37.1 widgetsnbextension            3.5.2 </pre> </details>  My Python version is 3.10.2 and Graphviz version is 2.50.0  A couple other things come to mind:  1. `pygraphviz` 1.8 was yanked from pypi and should no longer be available. If you have 1.8 installed you should definitely upgrade (you said you tried with 1.9 as well)  2. If you're installing pygraphviz with conda, make sure you're installing from the `conda-forge` channel as the packages on the main channel are known to be unreliable. See [the pygraphviz install docs](https://pygraphviz.github.io/documentation/stable/install.html) for details.
comment
I suspect this is related to pydot/pydot#277. NetworkX has had to xfail the `nx_pydot` tests due to the pydot/pyparsing issues. You can try installing the older version of `pyparsing` in your environment, e.g.  ``` pip install pyparsing==2.4.7 pip install pydot networkx ```  Though this certainly isn't a long term solution. Unfortunately I don't think there's anything NX can do unless/until this is fixed upstream.
comment
This is looking good - one thing I noticed though is that it doesn't seem like the test suite has any tests for MultiGraphs. Since the `_edge_func` that is being removed/replaced was basically there to handle MultiGraphs, it might be good to add a test or two for the MultiGraph case just to guarantee things are still working (and will continue to work) as expected.
comment
So I think the reason it is structured this way (i.e. with the `_edge_func` indirection) is because of the difference in what is returned by the `MultiGraph.edges` when it is accessed as an attribute vs. when it is called as a method. For example:  ```python >>> G = nx.path_graph(5, create_using=nx.MultiDiGraph) >>> G.edges  # accessed as an attribute, returns keys OutMultiEdgeView([(0, 1, 0), (1, 2, 0), (2, 3, 0), (3, 4, 0)]) >>> G.edges(1)  # accessed as a method (with nbunch), no longer returns keys OutMultiEdgeDataView([(1, 2)]) ```  Is this expected behavior? The original issue would make more sense if the expected behavior of `G.edges()` were to return edges with keys in the case of MultiGraphs and MultiDiGraphs.
comment
> Basically G.edges lets the class determines whether to use 2-tuples or 3-tuples. While G.edges() gives a universal return shape of 2-tuples.  Got it - so this *is* the way it's supposed to work :). In that case, the amount of "cleanup" in `line.py` is less than I had originally thought it would be. IMO It's definitely worth getting rid of the e.g. `_edge_func` wrappers in favor of direct calls to the `edges` property/method, but given the fundamental difference in what is returned by the property/method it will still require some logic for handling the graph/multigraph case.
comment
I just realized there is already an issue open for this - linking here for context: #4179 
comment
I have a (possibly naive) question - does this behave any differently than if you were to run `find_cliques` on the subgraph containing `nodes`, i.e. `nx.find_cliques(G.subgraph(nodes))`?
comment
I really like the idea of adding image comparison tests --- thanks for the comprehensive writeup too, there's a lot of great context + ideas here.  > There is one issue, that I think should disappear when we merge with master. Namely, the new GitHub action that should point to the image artifact isn't working. I believe after merging with master we should have:  Agreed - I think this needs to be merged before the artifact redirector will kick in. Even if it takes some tweaking, the artifacts are still available (albeit with more clicks) through the circleci dashboard (as you point out by linking to them)  +1 for adding something more concrete about how to write tests to the contributor guide, which seems like a precursor to the section on pytest-mpl.  > This PR currently requires storing a baseline image to compare against in the repo.  One option might be to generate the baseline in CI (i.e. from master) then cache the results for comparison in another CI job. +1 for storing the baseline for now though - that seems like the simplest way to get started for trying this out.
comment
I ran into an issue while debugging a hang in `test_pylab` due to the recent addition of non-smoke tests (i.e. tests that assert things about axis geometries) in f559558 (#4374):  The default behavior for the `nx_pylab` functions when the `ax` kwarg is `None` (the default value) is to call `plt.gca()`. Note that axis objects created in unit tests persist across function boundaries!   ```python def test_1():     fig, ax = plt.subplots()  def test_2():     ax = plt.gca()   # This is the axis object created in test_1! ```  This crosstalk between tests can lead to unstable/unexpected test behavior, so it's important to recognize. The solution used in #4374 is to call `plt.delaxes` as a "cleanup" step in the unit tests that create axes objects. There's probably a better solution to this, and something to keep in mind as the `nx_pylab` tests are improved.
comment
Good point - there isn't really much in the way of automatic support for multiedge visualization. It would be a very nice enhancement, but would likely require quite a bit of effort to figure out how to handle the positioning of multiple edges in a generic way.  As you've pointed out, one solution is to "reduce" multiedges into a single edge. It's important to note that the suggestion above implicitly sets the edge attribute to whatever the value is for the last edge in terms of insertion order. A more generic solution to multiedge reduction might look something like:  ```python >>> from collections import defaultdict >>> all_edge_attrs = defaultdict(list) >>> for u, v, val in G.edges(data="title"): ...     all_edge_attrs[(u, v)].append(val) >>> edge_labels = {k: op(v) for k, v in all_edge_attrs.items()} ``` Where `op` in this case can be whatever reduction function you want, e.g `sum`, `min`, `max`, `np.median`, etc.  In the absence of generic support for multiedge positioning/labeling, I think a more informative error message for this scenario would be an improvement.
comment
We decided to close this one for now, we can revisit approaches for improving CI resource utilization in the future!
comment
`to_numpy_recarray` expects to be passed a structured dtype with named fields, which are used to determine which attributes to use in constructing the adjacency matrix. If you don't supply a dtype, then it's assumed that `"weight"` is the attribute of interest (though this fact is not adequately documented). This explains the exception you're seeing.  In your particular case it's not clear why you'd want a recarray instead of a `ndarray` given that your graph does not have edge attributes. You should probably be using `nx.to_numpy_array(c_50)` instead. If (for whatever reason) you *really* wanted a `recarray` (even though there are no named fields to access in the case above) you could do `nx.to_numpy_array(c_50).view(np.recarray)`.
comment
The conversation has been stalled here for a couple months, so I will close this due to inactivity. If the issue persists please feel free to reopen this and address [dschult's questions](https://github.com/networkx/networkx/issues/5201#issuecomment-980012747)
comment
I'm going to close this as I think there needs to be an MRE to make progress here. If the issues persists, please reopen with an executable example to illustrate the issue.
comment
Good question. Does the GraphML standard support containers as node attributes? If so then I'd say this is a bug, but even if not I think a more informative exception message could be raised.
comment
> Is the XML writer standard and unchangeable? Alternatively, instead of throwing errors, would it be easier to (after line 769, for example) ask that k and v be of a certain type that the writer can handle, and ignore them otherwise? Or would that just be too annoying to maintain?  IMO since there is a defined standard for this format I think it's best to adhere to it. Implementing features beyond the standard has a lot of potential downsides. For example, if NetworkX implements features that are not in the current version of the GraphML standard, but then GraphML is updated at some future date in a way that differs from NetworkX's implementation, then we have the sticky scenario of NetworkX's GraphML implementation being "wrong", but potentially having users that depend on the "wrong" behavior.  So if this feature is really desired, I would advocate for proposing it to whomever maintains the GraphML standard. In the meantime fortunately there are other graph format options that support attr dicts.
comment
Looking back at the issues history, it seems this has come up quite frequently. See also: #5058 #485 #3663 #5071.  Note that more informative error messages have already been added in #5058, so I will close this.
comment
This certainly seems like a usability improvement! My knee-jerk reaction was that this *might* be a pretty significant API change (I'm really not familiar with the current `*View` APIs) in the sense that it could have implications for things like e.g. backwards compatibility. Maybe a change like this is a nice candidate for a design document (NXEP)? Maybe that would be overkill - I'm not sure at all :)
comment
One thing about a `__str__` method is that it's not parametrized, so users would be stuck with whatever number of elements is set in the method, or we'd have to provide an external way of setting it via e.g. something like `np.printoptions` or an `rc` file. IMO these latter options seem like more trouble than they're worth. I'm not sure  ```python >>> with nx.printoptions(num_elem=5): ...     G.nodes() ```  is more convenient than  ```python >>> list(G.nodes())[:5] ```  and is certainly less flexible in any case (what if a user wanted every other node?).
comment
> Hope this helps....  It definitely does, thanks for taking the time to lay this out. Given that the `pagerank` gives a non-sparse matrix by design, that would seem to eliminate the motivation for these changes IMO.
comment
Okay, I've looked at this (and related `np.matrix` issues) again and have formed the following opinion:  It's not worth the effort to remove the uses of `np.matrix` in these cases. `np.matrix` is baked into `scipy.sparse`, so problems like this will continue to arise up any time `scipy.sparse` is used. IMO it wouldn't be worth it to ask people contributing algorithms to work around `np.matrix`, and likewise is probably not worth the effort of developers/maintainers to figure out how to work around it either. The benefits of `scipy.sparse` far outweigh the drawbacks (i.e. not having sparse representations), so my preference would be to kick the can on this issue until a more suitable solution can be found. In principle, a better solution would be to solve this problem upstream either by removing the dependence on `np.matrix` from `scipy.sparse` or by investigating and switching to an alternative sparse library that supports nD arrays.   That's my opinion anyway :) This issue proved to be very thorny during the sprint and doesn't really have a clear-cut solution as far as I can see.
comment
This one has also been superseded by #5139.
comment
I'm closing this as it's been superseded in the transition to the scipy.sparse array interface #5139 
comment
> but I think we decided on keeping all the implementations in the codebase but just removing them from the user facing API, so if someone wants to look at the code/use it they still can if they want explicitly.  Yeah this is what I thought too... so the plan in NX 3.0 will be to change `hits_numpy` to `_hits_numpy` rather than remove it entirely after the deprecation expires?
comment
> I think this is the way. In almost all use cases nothing will happen. In the complex case we'll manually promote to complex. For unhandled erroneous cases (with hypothetical esoteric edge weights) we might consider promoting to object dtype. I think I'll update the PR with this approach to see how it looks like.  I tend to agree for the case of supporting complex weights. Option 3 appeals to me because in principle you can then support more generic weights (not that I can think of many use cases for this) and there'd be a straightforward way to support more generic reductions for multiedge_weights (instead of having to map a subset of functions to their `np.nan*` versions). These considerations are out of scope here though, so I'm +1 for trying option 2!
comment
> Yet another possibility is to let the user choose a dtype and a sentinal/fill_value instead of just a dtype.  This is a great point - and in fact the function already collects info on the desired fill_value, as the user can also specify the sentinel value in the *final* array via the `nonedge` parameter. This comment got me thinking of another possibility - instead of pre-creating an array with an internal sentinel value, then modifying it in a for-loop; why not create an array using the user-specified `dtype` and `nonedge` values directly and use advanced indexing to set the values! I haven't tried it yet but I think this might result in code that is much more readable and dodges the gotchas that caused the `complex` issue.
comment
I followed the idea I got from Dan's comment above over in #5250. I haven't thought it through completely, but I do think getting rid of the iterative-update approach in favor of an indexing-based approach has a lot of advantages. LMK what you think!
comment
The `nodes` are not unique in your example: `np.random.choice` samples with replacement by default (see the `replace` kwarg), which means you can have duplicates in the nodes.
comment
> Should these same changes (docs for edges method) also be bumped to the undirected graph class?  Good question - I think this specific wording makes sense for the directed graphs because in that case it does explicitly limit to out-edges that originate from the node(s) in nbunch. In the undirected case, there is no longer a distinction between an incoming or outgoing edge, so the "incident to" isn't as much of an issue, though maybe there's a better way to say it. Maybe "edges that include these nodes" or "edges that contain these nodes"? I don't think either of those is a clear improvement over "edges incident to these nodes". Any other suggestions?
comment
I took the liberty of pushing up a few rst formatting-related changes to get rid of the sphinx warnings about header length.
comment
Whoops - I now see what you were getting at - `n` is *also* set in the loop. The question is: should this be done inside or outside of the loop...
comment
I wasn't able to reproduce locally either. I'm curious about what's going on, but likely won't have a chance to look at it before Wednesday.  On Mon, Nov 29, 2021, 10:11 AM Jarrod Millman ***@***.***> wrote:  > Hmm. I can't reproduce the error either. Maybe it is an Ubuntu issue (it > is failing on Ubuntu and I run Fedora). > > — > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/5202#issuecomment-981887829>, > or unsubscribe > <https://github.com/notifications/unsubscribe-auth/AAJVZ75BS6HFSODKNRB3HPDUOO66XANCNFSM5IYYDI2Q> > . > Triage notifications on the go with GitHub Mobile for iOS > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> > or Android > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. > > 
comment
I went ahead and merged with `main` to incorporate the recent CI fixes.
comment
> I'm happy to close the PR if it's not needed  Alternatively, you could put your proposed functionality under a `if G.is_directed()` condition.  However, looking more closely at the proposed changes I would be surprised if the `ancestors_new` function were faster than the existing implementation in general. A recursive implementation has the downside of additional call overhead - the number of calls will depend on the structure of the graph. For example, if we choose a particularly "bad" case:  ```python In [3]: G = nx.path_graph(100, create_using=nx.DiGraph)  In [4]: %timeit a = nx.ancestors(G, 99) 59.2 µs ± 294 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  In [5]: %timeit a = ancestors_new(G, 99) 80.2 µs ± 146 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each) ```
comment
Generally I'm +1 for more informative error messages. The main concern for me would be: how computationally expensive is it to provide the better error message? Are there cases where the negative cycle search could take a very long time (say a significant fraction of the runtime of the bellman_ford function on a similar graph without negative cycles)?  If there's a risk of adding significant computation time (~seconds) to a code path that only raises an exception, I'm not sure that would be an improvement - especially since the current exception is quite clear about what went wrong. Just an opinion though - it could be that others find the more informative exception is valuable regardless of cost. Better yet, maybe the negative cycle search just isn't that costly to begin with, in which case this comment is moot!
comment
Thanks for the report @stonebig , I can reproduce this as well with matplotlib 3.5.0-rc1.  Just to note, in the first image in the OP, if you zoom into the lower left corner, you will see the nodes and the edges (the tiny black dot in the image) are both drawn, but in different locations and (potentially) with different scales. I've just run the NX docs build with mpl 3.5-rc1 to look at the sphinx gallery and it looks like this is a pervasive problem, particularly with `LineCollection` which is used by default to represent undirected edges. Directed edges (represented with FancyArrowPatches) still seem to be fine.  For anyone with a little extra time looking to help track this down, one useful activity would be to run `git bisect` matplotlib in to identify the commit that's causing the behavior change.
comment
I went ahead and did the bisection which identified matplotlib/matplotlib@1f4708b310 as the source of the behavior change. I've reported there and we'll see whether this is the intended behavior, in which case we'll have to figure out how to update `nx_pylab` accordingly.
comment
From the discussion in matplotlib/matplotlib#21517, this looks like it needs a fix on the networkx side - fortunately, it is quite minor.  However we may also want to consider another NetworkX 2.6 patch to either backport the fix or pin matplotlib < 3.5 so that users who happen to get NetworkX 2.6 with Matplotlib 3.5 aren't bit by this.
comment
> Is there anything else we should include? For example, should I cherry-pick  a8b907d is purely a bugfix so would be fine to include, but it's not as pressing as the modularity fixes. I'd vote to keep it simple and just focus on the modularity fixes in this patch, but I'm fine either way!
comment
> I think this was closed by mistake. I'm opening it again. But maybe I don't understand something so correct me if I'm missing it.  Agreed - thanks @dschult .  @danielhaden if for some reason you don't want this merged please let us know, otherwise this is a nice fix and I'd very much like for it to go in!
comment
Thanks for the report - I think you're right that the problem here stems from the fact that `multipartite_layout` assumes that the partition identifiers (denoted by default by the `"subset"` node attribute) are numeric. I agree that it'd be good to support non-numeric values for the partition label.  For anyone interested, this should be a pretty straightforward fix - I'd start by looking at this line:  https://github.com/networkx/networkx/blob/92225435bbb1461391289bd5b168717e2318e8bb/networkx/drawing/layout.py#L1095  It'd also be important to add a test for `multipartite_layout` with non-numeric (e.g. string) partition labels.
comment
> while the following will work (all the nodes' names are numeric)  I recommend running this second example :)
comment
This is because there is no function with the name `independent_set` in the networkx.algorithms.approximation package. Did you mean `networkx.algorithms.approximation.clique.maximum_independent_set`?
comment
> Yes. That's what I meant.  In that case, try:  ```python >>> from networkx.algorithms.approximation.clique import maximum_independent_set ```
comment
I don't understand what's going on here - these changes have been incorporated I think. I also don't understand why the PR is originating from the `networkx:MridulS-patch-1` branch.  Did you intend to open a PR with different changes?
comment
I'm going to close this is accidental - if you had intended to submit a PR with different changes, please open a new PR. If you're not quite sure how to open a PR, please see the [contributor guide](https://networkx.org/documentation/latest/developer/contribute.html) or don't hesitate to ask questions in this thread.
comment
> It looks like the pydot tests are failing on Python 3.10.  It's not just 3.10: I'm seeing failures locally on 3.9 now and the coverage workflow, which runs on 3.8, is also failing independently. I haven't chased this down yet either, but indeed it's not related to this PR! I'm going to open a separate issue so we don't overload the discussion here.
comment
Thanks @z3y50n , the changes here all look good!  From an organizational/maintenance standpoint, I have a *slight* preference for splitting off the deprecation into it's own PR. This isn't really a big deal, the main reasons for doing so are:  1. It makes the release notes more specific  2. It makes expiring the deprecation in the future slightly easier. When the parameter will be removed in 3.0, we'll likely go back over the diff of the PR where the deprecation occurred and make sure that everything is cleaned up (changes to docs, tests, etc.). It helps if the PR contains *only* changes that are related to the deprecation.  3. On the off chance that something goes wrong and we need to revert something (not a concern for this PR), it's definitely nice to have the deprecation separated out so that it doesn't also get accidentally reverted!  It's really not a big deal, but if you were interested in doing so I think it'd be relatively straightforward to reorganize since most of the deprecation-related changes are self-contained in this case. You could try something like:  ```bash git checkout binary_operations_refactor  # Just to make sure you're on b132cf1 git checkout -b deprecate-union-name-param  # Or whatever branch name you want git checkout binary_operations_refactor  # Back to this PR's branch git reset --hard 556fb48  # Sets *this* PR back to the commit prior to the deprecation-related changes <re-add `name` back to the signature, commit the change> git push --force  # This PR *should* now be everything except the deprecation ```  Then you can open a new PR from the `deprecate-union-name-param` branch. LMK what you think!
comment
I'm still +1 for these changes, but this needs a rebase now that #5121 has gone in
comment
Closing as duplicate of #5076. Please see the discussion there for an explanation.
comment
> should changing the type of a return value from "always 2-tuple of numbers" to "always np.array of length 2 of numbers" be marked as an API change (I think yes)?  I agree - one scenario where this might catch users out is that the returned `pos` dict will no longer be JSON serializable, so if users have code where they save the generated `pos` dict to file via JSON, this change will break them.  >  And does it require a deprecation (I think no... but I'm never sure)?  I'm never sure either :) I agree with you though: this feels more like "fixing a defect" (i.e. homogenizing the layout function output type) than a change that requires a FutureWarning. I think an explicit mention in the release notes should be sufficient!
comment
I agree with @dschult that this issue is unlikely to originate within NetworkX itself. In the absence of a reproducer I don't know if there's anything that can really be done to help chase this down, so I will close it.  If there is subsequent analysis that helps narrow down the problem, or any new info comes to light, please reopen.
comment
This one looks like it's resolved - I assume the fix in osmnx has been released, though I haven't bothered to check. If it comes back, please reopen!  Related - this might be a bit extra motivation to move the geospatial examples section over to nx-guides since it's so application-specific and has many dependencies.
comment
Looking at the implementation, it's straightforward to see why this is the case:  https://github.com/networkx/networkx/blob/9ebca28d883d9c5971ab55b19bad850e71c7ad44/networkx/algorithms/approximation/vertex_cover.py#L67-L74  To me, the question is: what's the desired behavior? I very quickly skimmed the paper referenced in the docstring and the [vertex cover wikipedia article](https://en.wikipedia.org/wiki/Vertex_cover) and didn't seen any direct mention of self-loops. Are there any resources that discuss this case?
comment
Thanks for the extra info!  > By the definition of a vertex cover, every node with a self-loop must belong to every vertex cover.  In that case, I think #5104 is a viable solution  (Edited to point to correct issue)
comment
It'd be great if we could add a test for this to accompany the fix. It looks like it should be possible to adopt the snippet for checking uniformity from the original issue into a test (perhaps by seeding `fast_gnp_random_graph` with the loop variable `i`). Maybe there's a more elegant way to test it - other ideas welcome!
comment
> Will networkx release a patch for v2.5 to pin the version?  Yes, we're working on a 2.5 patch to pin the decorator version now.
comment
`decorator` now includes a test (see micheles/decorator#121) that should prevent the behavior from breaking in future patch releases, so the version pinning (#4773, #4815) should be sufficient to keep this issue closed for both the development branch and stable releases moving forward.
comment
See [the above comment](https://github.com/networkx/networkx/issues/4718#issuecomment-819489576) - decorator 5.0.6 is known not to work, it needs to be 5.0.7 or later.  This has caused a lot of problems, so we are hoping to remove the decorator dependency entirely in the next release.
comment
+1 for adding support for multigraphs. I'd recommend putting it in a separate PR though rather than adding it to #5042 - IMO it would make it easier to review if there were separate PRs for each proposal. If you want to work on it before #5042 is in, you can always open a new PR that depends on #5042 (i.e. originates from a commit on that branch) and just mark it as draft for now.
comment
Thanks @dschult - the only thing that comes to my mind as potentially "resolving" this issue would be to improve the exception message. Otherwise I'd say this can be closed as duplicate.
comment
Ok, the node checks have been added everywhere that calls `_multisource_dijkstra`, but the checks are still in `_multisource_dijkstra` itself. I think the final step is to remove the NodeNotFound checks from `_multisource_dijkstra` (if I'm understanding [dschult's comment](https://github.com/networkx/networkx/pull/5033#discussion_r698068497) correctly)
comment
Just to try to help grease the gears a bit here, I went ahead and made the change we were referring to above in d80785f. This also required slightly modifying the checks in a couple other functions to ensure that the "source not in graph" case was still handled correctly - see 80600fc. I also made some minor updates to the test comments so that they better reflect the current state of things.  @divyanx since I pushed directly to your branch here, please be sure to `git pull`. If you have any trouble (e.g. you get merge conflicts when attempting to pull because you have local changes), please ping here and I'd be happy to help you resolve any problems!
comment
Also @dschult I took the liberty of "dismissing" your initial review since there have been a number of additional changes since the original approval. I just wanted to make sure you had a chance to get eyes on the new changes, I hope you don't mind!
comment
Note that this issue also affects `single_source_bellman_ford`
comment
Closed by #5033
comment
Thanks @aaronzo . I've made a few updates to the release note (and fixed a sphinx warning) and was attempting to push the changes up to your branch but it looks like that's been disabled. If you're comfortable doing so, you can check the "allow edits from maintainers" button on the right and I will push up changes. If you're not comfortable with that (or it's not working for whatever reason), just LMK and I'll open a PR to your fork instead.
comment
@dschult just a quick double-check that your approval still stands since quite a bit has changed since the original approval. If so, we can go ahead an put this in but if you (or anyone else) would like another look that's of course totally fine!
comment
I'm not sure what the "correct" behavior is either - what does the gml standard say about this case?  From the traceback, the problem lives in the default `stringizer`, specifically around: https://github.com/networkx/networkx/blob/dfa1150a9b8cf15183d7c910ff9b67ef3d220513/networkx/readwrite/gml.py#L729  This is what prevents empty lists being used.  Looking around the module a bit more, there is a function called `literal_stringizer` that seems to behave more the way you were expecting, try:  ```python nx.write_gml(G, 'test.gml', stringizer=nx.readwrite.gml.literal_stringizer) ```
comment
> Incidentally, the link to the GML standard on the Networkx website appears to be out of date and now redirects to an alumnus page:   This should be fixed (insofar as is possible, as there doesn't seem to be an html version of the spec) in the [latest version of the documentation](https://networkx.org/documentation/latest/reference/readwrite/gml.html)
comment
> So, can we make this issue into a request to >  >     * fix the link to the GML standard This has been fixed in #4864 , though if anyone knows of an active html link (the current link is an archived page from wayback machine that only provides a pdf-version of the GML spec) this could certainly be improved. >     * add the literal_stringizer to the "See Also" section of the doc string. +1 for this. >     * maybe include `literal_stringizer` in the main namespace. I'm not sure about this one, since it is pretty specialized functionality that only really makes sense in the context of the GML module. >  > Anything else?  Makes sense to me - I'd say adding `literal_stringizer` to the `See Also` section of the `read_gml` docstring would be sufficient to close this. 
comment
Excellent catch @boothby - this is indeed what is happening with this particular data file.  On a related note, maybe `comments` should be made an optional kwarg with the ability to pass in `comments=None` to indicate that there is no comment character, rather than making the user get creative with their comment character selection. Thoughts?
comment
Yup, but building the 3.3 wheel fails due to something else in the build chain - see #5068
comment
I agree with @dschult - I think we should investigate that approach. The problem with the approach in this PR or the `fraction.Fraction` approach is that it bakes-in assumptions about what precision users need. Six sigfigs may be fine for most cases, but in the cases where the user is relying on more precision, this could potentially lead to the algorithm silently giving incorrect results.  If @dschult 's suggestion of using the scaled in the 3-tuple works, then we're back to the situation of: users can select whatever precision they desire by scaling the weight attribute appropriately.
comment
Good catch, this has already been fixed in #4996
comment
It's been a while without any follow up here so I'm going to close this. Feel free to reopen as needed.
comment
This has been dormant for a while, so I'm going to close this as resolved. If the problem persists, please feel free to reopen.
comment
I'm going to close this as it's been a while. Feel free to update/reopen as needed.
comment
Louvain community detection was added in #4929, so I will close this. For further ideas/suggestions/improvements it would probably be easiest to open a new issue/pr.
comment
> I'm a little surprised by the idea that these don't apply to undirected graphs. It seems to me that they are well defined and work just fine for undirected graphs. Is there a reason to keep people from using them that way? I generally prefer not to restrict application of a function unless using it in that context will do harm or cause confusion.  Agreed, and I was hoping to hear what others thought on this subject. My thought was that the naming of the functions was a bit confusing in the context of undirected graphs. For example, `descendants` and `ancestors` will be the same for undirected path graphs, whereas the interpretation of the output makes more sense in the context of directed graphs:  ```python >>> G = nx.path_graph(5)  # An undirected path graph >>> nx.ancestors(G, 2) {0, 1, 3, 4} >>> nx.descendants(G, 2) {0, 1, 3, 4} >>> DG = nx.path_graph(5, create_using=nx.DiGraph) >>> nx.ancestors(DG, 2) {0, 1} >>> nx.descendants(DG, 2) {3, 4} ```  More than anything I think it's just a terminological issue: descendants and ancestors felt like they implied a directionality to me, especially since they are defined in the `dag.py` module.
comment
> In any case, if it works correctly and is well-defined for the Graph case and doesn't cause any confusion by doing so, I think we should let people use it this way  +1 - in this case, would it be worth moving where these functions are defined, similar to #5016? It should be relatively easy to raise a FutureWarning for users who are accessing these functions directly from the `dag` module, and for the vast majority of users who call them from the main namespace (e.g. `nx.ancestors`) there will be no difference. OTOH maybe this is too much churn for too little benefit... treating this minor organizational point as a "wontfix" also seems sensible.  In any case, this PR still has a lot of nice improvements - @vdshk would you mind reverting the changes related to `not_implemented_for`?
comment
> If we move it, perhaps we can provide an alias for it in the old location (not advertised in the docs, but available for backward compatibility). @rossbar is there any precedence for doing something like that?  In my head it would look something like adding a `__get_attr__` to `dag.py` that raises a warning when someone imports/accesses the functions directly from dag.py. That way, when someone does something like:  ```python >>> from networkx.algorithms import dag >>> dag.ancestors(G, 0) ``` or ```python >>> from networkx.algorithms.dag import ancestors ``` They'll get some sort of `FutureWarning` stating that these functions will be moved to a new module in 3.0. The nice thing is that users accessing the functions from the main namespace (which should be most users) won't see any difference and won't get any warnings.  I agree that it's not very important, but *I think* the `__get_attr__` gives us the tools we need to clear this up without being too noisy. Either way, it should definitely be addressed elsewhere!
comment
Thanks @mjschwenne , hopefully this alleviates any workflow oddities we were experiencing - LMK if there are still problems!
comment
The fact that you are seeing similar behavior when importing multiple projects (thanks for providing links) would make me think that this may be something specific to your system configuration rather than the packaged libraries themselves. Charris' explanation in the related numpy issue is a good thing to try, though numpy shouldn't be imported by default with networkx 2.4.
comment
Can you try importing networkx in an environment in which numpy hasn't been installed? This should help isolate where the problem may be originating. e.g. something like:  ```bash python -m venv nx-testenv source nx-testenv/bin/activate python -m pip install networkx==2.4 ```  Then evaluate the total memory on import.
comment
> I've verified that without numpy import networkx does not commit any significant amount of memory.  Great, thanks for checking that. Since this isn't directly related to NetworkX I will close this issue.  Re: the numpy issue - whenever I have trouble with multiple threads the first thing I try is suppressing openblas threading via the `OMP_NUM_THREADS` environment variable, i.e. `export OMP_NUM_THREADS=1`. You may want to try this then profile the memory usage at import time for your system. If you no longer see the memory spike, you may want to consider opening an issue upstream with whichever openblas provider you are using.
comment
Yeah this has more to do with the specific environment and not NetworkX itself --- NetworkX doesn't directly depend on OpenBLAS, so any problems related to that should be reported to the relevant project (e.g. OpenBLAS or maybe the project that pulls it in, e.g. NumPy or SciPy).  That being said, see #4909 for the current thoughts/ideas on how to implement lazy loading in NetworkX
comment
Thanks for bringing this up. I'm not too familiar with this particular centrality measure so I don't have an immediate sense for what makes sense: should the CCPA result be `0` in this case or is it rather undefined? Does the Ahmad, Akhtar, Noor et. al. paper mention anything about this case (i.e. unconnected nodes)?
comment
Oops - my suggestion for parametrization was wrong... sorry about that. I've pushed up the fix and we can (finally) push the green button. Thanks @Zmeos !
comment
Is there any chance you could boil down the reproducing example, preferably to something that can be built locally rather than requiring downloading data from external sources?
comment
I think your original diagnosis is correct; an even more minimal reproducer highlights the core of the problem:  ```python >>> G = nx.Graph([(0, 1)]) >>> G.add_node(2) >>> list(nx.common_neighbor_centrality(G)) Traceback (most recent call last)    ... KeyError: 2 ```  This error path is hit via `link_prediction._apply_prediction` with `ebunch=None` (the default), which uses `nx.non_edges(G)` to create the `ebunch` that the prediction is applied over. As mentioned in the OP, this causes problems when there are multiple components. Using the above example, node 2 is not connected, so there is no path to it, resulting in the KeyError:  ```python >>> sp_dict = nx.shortest_path(G) >>> print(sp_dict) {0: {0: [0], 1: [0, 1]}, 1: {1: [1], 0: [1, 0]}, 2: {2: [2]}} >>> list(nx.non_edges(G)) [(0, 2), (1, 2)] >>> # No path from 0 or 1 to 2 >>> sp_dict[0][2] Traceback (most recent call last)    ... KeyError: 2 ```  I'm not sure what the CCPA is supposed to do in the case of multiple connected components, but some ideas come to mind:  1) Raise an exception on `not nx.is_connected(G)`  2) Compute the CCPA for each component and return a sequence of scores or a mapping of components -> CCPA value.  I think option 1 makes the most sense but am interested to hear what others think; especially if anyone is familiar with the Ahmad et. al. paper.
comment
I've added the correction to the two other module `__getattrs__` that raise this exception on access.
comment
Thanks for the report, this is related to #4966 which was fixed by #4965 
comment
#4965 was merged yesterday and has not yet been released.
comment
This is indeed a regression and the diagnosis seems correct as far as I understand assortativity.
comment
Thanks for the excellent analysis @MridulS !  I'm going to go ahead and close this as resolved given that it seems that the original question was caused more by unexpected properties of the specific graph rather than a bug in `simple_cycles`. If this proves not to be the case or a minimal reproducing example that illustrates a defect in NetworkX functionality is found, please feel free to reopen or open a new issue.
comment
The example you've provided isn't reproducible so I can't say for sure, but this looks to me like your Graph might have a lot of self-loops. Self-loops were not drawn by default in v2.5, which was considered a bug (it's never good to silently ignore information).  Can you try adding `G_dual.remove_edges_from(nx.selfloop_edges(G))` before plotting to see if that recovers the original plot?  
comment
Great, thanks for confirming!  I think you've also helped uncover a bug: I would've recommended using the `edgelist` parameter in `nx.draw` to suppress the drawing of selfloops, something like:  ```python >>> selfloops = set(nx.selfloop_edges(G_dual)) >>> nx.draw(G_dual, pos, edgelist=[e for e in G.edges() if e not in selfloops]) ```  Unfortunately that's not working for me locally, which indicates there may be a bug in how the `edgelist` parameter is handled in the branch responsible for drawing self-loops. I will open an issue specific to that behavior in a bit, so feel free to close this one if you're question has been answered --- and thanks for reporting!
comment
I'm not familiar with the SNAP algorithm, but one thing that struck me from just a quick look over was that the implementation doesn't seem to conform that well to most of the tools in NetworkX, which have a more functional flavor. Is it feasible to implement the algorithm as a function, e.g. something like  ```python def snap(G, ...):     # Perform SNAP summarization     return summary_graph, supernodes ```  From the included gallery example (thanks for that!) it seems that the algorithm lends itself to such an implementation.
comment
> Yes, exactly. It gives users the option of specifying which attributes they want to be consider when performing the summarization. Your suggested wording was much clearer, so I went ahead and applied the change.  :+1:   > I think this simpler example would readers a quicker understanding of what is happening, than in the plot_snap example.  Agreed! Thanks for condensing it down.  > I added a placeholder method in AbstractSNAP for build_original_graph and build_summary_graph:  Generally I think you'd want to use `@abstractmethod` to enforce this, but honestly that's all beside the point :). I have a strong preference to try to use pytest features like fixtures rather than OO patterns anyways as I think they generally end up being both faster and more readable, but let's not worry about it here. We can always tinker with the tests later - for now, the tests due what they're supposed: assert that the function behaves as you expect, so this LGTM!
comment
> You may be right that these thin wrappers are unnecessary.  There's obviously a tradeoff - on the one hand having our own I/O functions that wrap scipy would likely be more convenient for users, but OTOH I'm a big fan of this stanza from PEP 20:  > There should be one— and preferably only one —obvious way to do it. > Although that way may not be obvious at first unless you're Dutch.  Using the existing scipy parsing facilities feels more in line with this maxim, and in principle it shouldn't be *that* much worse for users to write e.g. `nx.from_numpy_array(scipy.io.mmread(fname))` than `nx.read_matrix_market(fname)` as long as they know to do so (which we can explain in docs w/ examples).  Bear in mind though this is just one opinion! I tend to skew towards solutions that require adding less code at the expense of user convenience, so I wouldn't undo your PR until we've had more opinions shared. It's very possible others will prefer the new module to updating docs.  > If you just want to go with a tutorial in the documentation on this, would this just be part of the readwrite section of the documentation?  I think so --- the first thing I'd try is creating a `matrix_market.rst` file in https://github.com/networkx/networkx/tree/main/doc/reference/readwrite and then adding it to the `toctree` in `index.rst`.
comment
I'm going to close this as the MM format was addressed in the docs instead in #4934 . Please feel free to raise issues/open PRs to improve the [matrix market docs](https://networkx.org/documentation/latest/reference/readwrite/matrix_market.html). Many thanks @hcars for raising awareness about MM & the scipy functions!
comment
>  I would like to take some more time to think through the implications of removing the numpy, scipy, pandas, and matplotlib dependencies.  I agree that having some time to thinking about this more would be good.  > Should we remove 2.6.0 and 2.6.1 from PyPI now?  This seems reasonable to me if it's sufficient to fix the immediate problems that scipy et. al. are experiencing.
comment
Also the test suite should be failing here for e.g. `pagerank` - it's currently not because we forgot to remove the `pytest.importorskip`s from the top of the files when we made the scipy implementation the default.  I'll go back over the PRs we had for 2.6 where we switched to a `numpy` or `scipy` version of an algorithm and double check these instances so that the tests are failing properly and we can address them via e.g. option 2. I won't be able to take a look at this *immediately* however, so +1 for yanking 2.6 until we have a bit more bandwidth.
comment
Good question @EwoutH - if you're up for it, I would recommend opening a new issue on this topic so that your question has higher visibility. At the very least I think a new issue will result in an improved docstring for this function.
comment
> BTW, just curious, what kind of feature are you looking for from Python 3.7, other than the initial commit of dropping support   IIRC the big one is that Python 3.7 guarantees that dictionaries preserve insertion order. Since NetworkX's Graph data structures rely heavily on Python dictionaries, this new language feature allowed NX to get rid of custom order-preserving code and deprecate `Ordered` versions of the Graph classes.
comment
>  We can pin to 2.5 for now, but I think it's not a good idea in the long term to stick with an old version.  I think this is just the natural maintenance burden that results of a mismatch between the goals of LTS distros and the pace of development for software packages. 
comment
I will close this one as resolved - if you'd like to continue the discussion on release cycles for scientific Python libraries more generally, I suggest [the scientific Python discussion forum](https://discuss.scientific-python.org/) or related proposals.
comment
Closing this one as duplicate
comment
> Seems decorator 4.4.2 not compatible with NetworkX 2.6.1  As of 2.6, NetworkX no longer depends on `decorator`, and should work in any environment regardless of whether `decorator` is installed (regardless of the decorator version).  I am unable to reproduce the error with networkx 2.6.1 in an environment with either: no decorator installed, decorator==4.4.2, or decorator==5.*. I suspect that something else was going on in the environment, but if you run into this again please reopen with more info.
comment
Thanks for sharing the additional context @goznalo-git   > If you want to explore many different outcomes, perhaps it is better to construct a method that explores the different results systematically rather than randomly.  I had the same initial reaction as well. Another consideration re: "randomizing" the output is that, if we were to add this feature, we would also need a way to make the randomized output reproducible. Typically this is done by providing a `seed` kwarg and using the random state decorator.  To me though, the randomization feels like it would be best handled outside the function, which can be done straightforwardly w/ shuffling as suggested by @dschult . This is a clean solution that AFAICT would give the desired behavior and has the additional advantage of not requiring an expansion to the API.
comment
> do you and @dschult think I should go on and code the TDCS and BDCS algorithms, as I intended?   I'm not familiar with these algorithms so I can't say :). From an organizational perspective though, I'd recommend opening a new [algorithm discussion topic](https://github.com/networkx/networkx/discussions/categories/algorithms) so that the discussion is framed with the appropriate context and more visible to those who might miss this thread in the PR. You could either copy/paste the relevant parts of [your comment above](https://github.com/networkx/networkx/pull/4803#issuecomment-860252245) or simply link to it from the new discussion if that's easier! 
comment
From [the description in the docstring](https://networkx.org/documentation/latest/reference/algorithms/generated/networkx.algorithms.traversal.depth_first_search.dfs_edges.html#networkx.algorithms.traversal.depth_first_search.dfs_edges) it looks like this is clearly documented and `edge_dfs` may be a better fit for the behavior you are looking for.
comment
I think circleci itself recently had a "release" where a bunch of new features were added - it looks like this PR may have just been caught up in some hiccups related to circleCI's changes - I agree that there's no reason why this should've had problems.  Hopefully things are settling down a bit now - restarting the workflow seems to have done the trick
comment
Self-loops are not included in `draw_networkx_edges` in networkx version 2.5 (see the comment on line 9).  The good news is this feature has been added in networkx 2.6, which will be released soon. You can try it out now by installing the release candidate, e.g. `pip install --upgrade --pre networkx`.
comment
No, AFAICT having the labels follow the edge arcs is not currently supported. If that's a feature you'd like to discuss, or you have any other unrelated drawing questions, I'd recommend that you open a new issue so that it doesn't get buried in this thread on an unrelated topic.
comment
It looks like you all are using conda: maybe this is a conda install and/or environment management issue? You can try the following to see if there's a problem installing networkx from conda in a fresh conda environment:  ``` conda create -n nx-testenv python=3.7 conda activate nx-testenv conda install networkx python -c "import networkx; print(networkx.__version__)" ```  Please report if you have problems with the above. 
comment
I'm going to close this one as resolved - for additional installation issues, please open a new issue and share specific info on the errors you see + your installation environment.
comment
Hi @andriu99 , it looks like this may have been submitted in error as the issue template appears to be empty. We'll need a bit more information to go on to help figure out if/what your issue is!  I'm going to close this one, but if you want to follow up, please feel free to re-open it with the specific information requested above, at the very least: 1) a description of what specifically isn't working and 2) what versions of networkx you are using.
comment
>  My question is how should this kind of deprecation be handled? Is it a deprecation or "just" an API change? I'm guessing it is an API change and so should be listed in the release_dev.rst file as such. But that we don't add warnings.. Right?  I think it depends on how strictly you want to applies the "deprecation" rules to the all API changes. The safest thing to do would probably be to raise a `FutureWarning` to warn users that the return value for the function will change, then follow the deprecation rules to determine when to actually change the return values (i.e. warn for one release, then make the change the release after that). OTOH, if the change isn't expected to be too disruptive, it's probably okay to just make the change without a warning and highlight it in the release notes.  I don't have a particular feel for which approach is more appropriate here, just thinking out loud about some alternatives!
comment
Good catch @jarrodmillman , the rst formatting issues should be fixed in 749c109
comment
Great ideas! I'm going to put this in so it's visible in the docs ASAP (I also added my name to the pedagogical notebooks project).
comment
I did a first pass through the docstrings to try to get familiar with some of the new features added in this PR. These are some very nice additions! I took the liberty of making minor changes related to rst-formatting and grammar as I was reading through. I aim to do a more complete readthrough ASAP - thanks @dschult for combining & organizing the TSP contributions into one place!
comment
> I guess I disagree with using text to designate which move to make. I think the user should supply a function -- not the text.  I didn't think of this on the first pass, but I agree that would be a better approach. It makes it much more extensible and removes the ugly case-matching code mapping strings to functions :+1: 
comment
Sounds good, I will try to take another pass today. Also I think there are a still a few comments left from the first round - you may have to click the "Load More..."` button in the middle of the comment listing to be able to see them. Perhaps you saw them already in which case feel to close them - I just didn't want to do it in case you hadn't seen them yet!
comment
I also tried the `--file` option as suggested [here](https://stackoverflow.com/questions/51042589/conda-version-pip-install-r-requirements-txt-target-lib/51043636) but it has the same problem.  One solution is to use `conda` to create & activate the environment, then use `pip` to actually install the dependencies. I think this works assuming 1) the environment is empty to start with and 2) you don't mix conda & pip for installing packages. I don't regularly use conda though so I'm not 100% sure this is recommended.
comment
> Can we just remove the conda instructions?  It would make things simpler if we removed references to environment management altogether. It's clearly important, but it's a more general development practice and new users shouldn't necessarily be learning *how* to setup virtual environments from our docs. For example: [scikit-learn's dev guide](https://scikit-learn.org/stable/developers/advanced_installation.html#building-from-source) makes a reference to creating a virtual environment (see step 3) but isn't explicit about it and avoids the whole env-management-tooling problem altogether.
comment
I can't reproduce this one:  ```bash python -m pip install flask_cors==3.0.1 networkx==2.5.1 ```  works fine. I'm going to close this for now, but if the issue persists please reopen with a minimal reproducing example (i.e. reproducing the exact error you're seeing, preferably with the fewest lines possible).
comment
@benjaminbenteke this seems like maybe this was opened by mistake - most of the commits seem to be duplicates of the work you've already submitted in #4873. Also, #1480 has already been closed, so the title is a bit confusing.  I'm going to go ahead and close this PR on the assumption that it was opened in error, but please don't hesitate to ask questions/open a discussion on what the intent was if you'd like to reopen!
comment
Nice find! This is indeed still a problem in the development version of the documentation: https://networkx.org/documentation/latest/reference/generators.html#module-networkx.generators.sudoku
comment
I can certainly see both sides to the argument. I tend to agree with @dschult that the downside of silently ignoring extra nodes passed in via `fixed` is greater than the downside of forcing users to be explicit. I think different handling of the extra-nodes case between the `pos` and `fixed` kwargs is justified in that `pos` sets an initial condition, whereas `fixed` specifies a hard constraint. I think forcing users to be intentional about hard constraints is generally safer than making assumptions about their intentions, which can lead to situations that are confusing and hard to debug. This is just an opinion though, not a blocker - if there is a strong preference for the "convenience" approach that'd be fine with me as long as the behavior was well-documented!  I also agree @dizcza that the current error message is uninformative - it should definitely be improved if we decide to stick with the raising-an-exception approach.
comment
Good catch and nice diagnosis. I also agree that it's not clear what the behavior *should* be when `normalization=False`, perhaps the linked reference in the docstring of `communicability_betweenness_centrality` would have some info on this?
comment
Agreed: +1 for pytest builtins whenever possible.  I don't have a strong opinion on `assert edges_equal` vs. `assert_edges_equal`, but I agree the former is more pytest-idiomatic and would be better.
comment
> I suspect that this was introduced in #4360; and that a solution to the performance regression may be to use LineCollection when arrows=False.  Yup, that's exactly right. The rationale behind the switch was that `draw_networkx_edges` would always return `FancyArrowPatch`es now instead of different objects depending on whether the graph is undirected or directed, thus the outputs would always be iterable and visualization properties for various edges could be set individually. This bit us in `nx-guides` as well: networkx/nx-guides#23.  I wonder if we shouldn't revisit that decision: the performance regression is so severe that it makes `nx_pylab` much less practical for "quick" visualizations.
comment
Just to put a data point on the severity of the performance regression, here's a simple benchmark for undirected graphs with 1225 edges:  **Before:**  ``` In [1]: import networkx as nx  In [2]: G = nx.complete_graph(50)  In [3]: %timeit nx.draw_networkx_edges(G, pos=nx.circular_layout(G)) 8.08 ms ± 1.69 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) ```  **After:**  ``` In [1]: import networkx as nx  In [2]: G = nx.complete_graph(50)  In [3]: %timeit nx.draw_networkx_edges(G, pos=nx.circular_layout(G)) 1.31 s ± 7.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) ```  That's O(100x) slower for a use-case that is probably quite common (drawing edges for graphs with ~1000 or more edges).
comment
I attempted to undo the performance regression but *keep* the self-loop drawing added in #4370 via reverting, but doing so gets quite complicated very quickly, and I haven't been successful.  The performance regression is really bad, but it would be a shame to lose the self-loop drawing. I'm instead going to pursue an alternative that adds a kwarg to `draw_networkx_edges` to allow the user to toggle between using LineCollections or FancyArrowPatches to try to keep the best of both worlds. Other ideas welcome!
comment
One complication is graphs with self-loops, which have to be drawn with FancyArrowPatches - I hadn't considered re-purposing the existing kwargs, but I think you're right that it's worth a look.
comment
Should be fixed by #4825
comment
> Does the refactor look correct? Since the asserts shortcircuit things when the value is False, I return False immediately.  LGTM :+1:   > Should the new utility functions stay here? We've never documented them and we are already changing their names slightly.   My vote would be to leave them somewhere non-public at least for now. The main reason being that something like `graph_equality` may not have the same meaning in the context of writing tests as it might in other contexts (my understanding is that "graph equality" in particular is a thorny concept). My vote would be a two step approach 1) refactor the tests using these new fn defitions then 2) figure out if/how best to publicly expose the functions.
comment
I also took the liberty of fixing up a few variable names - the github suggestion feature can get a bit cumbersome for these types of changes.
comment
Thanks for adding back the test @harshal-dupare :)  The CI test errors are unrelated to your changes and should now be fixable by rebasing on `main`. 
comment
I went ahead and rebased this on main to get the CI green again. Unfortunately this required a force-push, which will make pulling more difficult. @harshal-dupare the easiest way to update your branch locally (assuming your remote is named `origin`) is:  ``` git fetch origin git reset --hard origin/partialfix-for-issue-4660 ```  Note that if you have unstaged/uncommitted changes, you should stash first to ensure the hard reset doesn't overwrite them, i.e.:  ``` git fetch origin git stash git reset --hard origin/partialfix-for-issue-4660 git stash pop ```  If you have any questions or run into unexpected problems, please don't hesitate to ping me!
comment
Of course, now their are a whole host of **new** unrelated CI failures. The circleci seems like a CI config issue and the cancelled macOS jobs seem to be on GH's side. Note that none of the new failures are related to the changes in this PR.
comment
... and I just rebased again to get rid of the *new* unrelated CI failures :joy: . The update procedure will be the same as before - I just want to get the CI back to all-green so we can get this really nice patch in!
comment
Closed by #4588
comment
My preference would be to split the package reorganization and the import stuff into separate PRs. The proposed import system changes (lazy imports, mkinit, the ability to toggle, etc.) are interesting and (for me at least) it would be helpful to have them self-contained. It also would help un-stick the pkg/module reorg as I think the import changes need more attention & discussion whereas the module reorganization (coupled with following the existing import pattern with `__all__`) is a straightforward change that it seems everyone agrees with. 
comment
Thanks @Erotemic !
comment
Thanks @AbhayGoyal !
comment
Awesome, thanks @cpurmessur - the test failure just looks like a CI problem and not anything related to your changes.
comment
Thanks @lonnen !
comment
Nice, thanks @MridulS !
comment
> I've changed the PR name to better suit the final product (as mentioned, it's not really an annotation!)  It's probably worth renaming the file itself as well to something like `plot_custom_node_icons.py`; this should fix the non-alphabetical ordering of the example as well.
comment
I agree with @dschult that it would be preferable to avoid unnecessary copies of attribute data.  I also agree that it would be good to add the example to the docstring. In fact, the docstring for `transitive_reduction` doesn't currently have *any* examples, so it'd be nice to add both an example of basic usage, and an additional example illustrating how node and edge data can be transferred to the reduced graph (if desired).
comment
> the easiest way would just be to get rid of the unpacking and switch to a pure pass-through, i.e. ax.margins(margins).  Hmm, looking more closely at my own suggestion, this might not be enough since MPL implements this parsing in the signature, whereas we only have a single kwarg to work with. So to support both scalar and tuple inputs we may actually have to add a bit of parsing logic of our own (darn), e.g.  ```python ax.margins(*margins) if isinstance(Iterable, margins) else ax.margins(margins) ```  or something similar (a try/except, etc.).
comment
Ah - sphinx 4.0 came out today :). This is almost certainly a result of that - there's nothing in this PR that would cause this particular error, which I've never seen before either.
comment
>  If it is OK with you @rossbar maybe we should merge despite a failing test.  :+1: definitely!
comment
> Is this another case where we should be testing against prerelease code? Or is that even possible with sphinx_build?  It's possible, but I think projects will need to decide which "core" dependencies to test against their dev branches, otherwise things could get very noisy - I'm not sure what the best-practice would be here :)  Another thing this reminds me is that we should probably pin the upper bound on the major version number of every one of our dependencies. The only reason I've been hesitant to do that is because I don't want to fall into a situation where we accidentally don't upgrade to a new major version of a dependency because we've forgotten about a particular pin. There are tools like dependabot to help with that specifically, but I think that one in particular is a bit noisy. Again - I'm not sure what the best-practice is!
comment
> Ah - sphinx 4.0 came out today :). This is almost certainly a result of that  I spoke too soon - we are properly pinned to Sphinx 3.5.4 so I was hasty in attributing this to Sphinx 4. Either way - still +1 for merging as I'm confident the circleCI error is not related to this PR.
comment
Here are some thoughts after having looked at this issue a bit.  Removing the use of `np.matrix` in networkx involves changing both *internal* and *user-facing* code. These two categories of changes are addressed separately.  **Internal code**  This refers uses of `matrix` objects or operations within `networkx` code, but where the function/method I/O is not a `matrix` object. An  example would be the `_tracemin_fiedler` function: https://github.com/networkx/networkx/blob/c07bc09e8c9f698c8e8f0a500399272d807bb4c6/networkx/linalg/algebraicconnectivity.py#L190-L296 There are some important distinctions between the `np.matrix` and `np.ndarray` classes that need to be kept in mind. The two main ones are:  1. `np.matrix` objects are *always* 2D  2. the multiplication and power operations (which behave element-wise for `ndarray`) are overridden with *matrix multiplication* and *matrix power* operations whenever *any* of the inputs are matrix objects.  There are other subtle differences that are less likely to come up within networkx, things like:  - `matrix` objects can be initialized with a special Matlab-like string syntax  - `matrix` objects have additional properties like `.A`, `.H`, and `.I` (array, conjugate transpose, inverse) that are not available/applicable to `ndarray` objects.  Changing internal code to get rid of uses `matrix` is relatively straightforward as it shouldn't directly affect anything that the user sees. A general approach to handling the two main issues cited above is to:  1. Replace instances of multiplication with a matrix object with the `@` operator  2. Use approaches for guaranteeing the shape of arrays, e.g. the `ndmin` kwarg of `np.array` or the `np.atleast_2d` function.  **User-facing changes**  I.e. code that either *inputs* or *outputs* `matrix` objects. Changing the type of the objects expected/returned by such functions obviously has the potential to introduce changed behavior in downstream code. Therefore, I agree with @ericmjl that any changes need to be accompanied by clear and concise warnings. AFAICT, there are two such instances in the codebase (please add to this list if you find more!):  - [x] `networkx.convert_matrix.to_numpy_matrix`  - [ ] `networkx.linalg.attrmatrix.attr_matrix`  The sole purpose of `to_numpy_matrix` is to return a `np.matrix` object, so I think deprecation makes the most sense. In the case of `attr_matrix`, the output is an attribute/adjacency matrix which could just as easily be represented with an `ndarray` instead. For this case, perhaps emitting a `FutureWarning` notifying users that the output type will change from `matrix` to `ndarray` makes the most sense.
comment
I need to do a more complete grepping of the code base to ensure we've found everything that needs to be changed/deprecated. I will continue to update this issue as I find instances. I'm adding this to the 2.5 milestone to ensure we get all the deprecation/future warnings in in time!
comment
Hey @mjschwenne - this has proved to be a sticky issue. After spending many cycles on it, I had decided that I would actually be in favor of leaving internal uses of `np.matrix` in place. The reasoning here is that `scipy.sparse`, which is used heavily in linalg uses `np.matrix` by default - until that's changed upstream, any changes made in NX would essentially make our own code more complicated and less reliable for very little benefit. I have a longer take on this [here](https://github.com/networkx/networkx/pull/4141#issuecomment-774344058).  That being said, I do think we generally want to try to eliminate instances in networkx where our public functions return `np.matrix` objects. I *think* most of these have been changed or discussed, but if you find instances feel free to raise them!
comment
> Did you file an issue with scipy, and if so would you mind linking it here?  Good point; I haven't, but it's definitely worth checking through the scipy issues/mailing list and linking to the relevant discussions.
comment
Thanks @NeilGirdhar that's definitely a very relevant discussion. pydata sparse has been discussed in the past, but AFAIK no one has looked at it enough in-depth to be able to determine whether or not it is suitable. At a glance, I think some of the issues mentioned in the thread (e.g. dependency on `numba`/`llvmlite`) would also be blockers for NetworkX, or at least require further discussion.  IMO the `sparse` question is deserving of an [NXEP](https://networkx.org/documentation/latest/developer/nxeps/index.html) - I had intended to start one in the past, but have never had the time to fully survey the current state of sparse data structures in the ecosystem. The first step would be compiling information about the various libraries including their advantages, drawbacks, limitations, and dependencies.
comment
I agree with @dschult that the tests would be a really nice addition here!
comment
`np.nan` and `np.inf` are valid floats: `isinstance(np.nan, float) -> True` which sends the parser down the float branch here:  https://github.com/networkx/networkx/blob/6acdc535f885978a0729355833472abda82266f5/networkx/readwrite/gml.py#L681-L692  Note that there is no special casing in that branch to handle valid float instances that aren't numbers. Something *could* be added there, but my initial reaction is that would be brittle because of things like `float(nan) != np.nan`.  Unfortunately, the fact that these values are valid floats also means the `stringizer` branch is never hit, so defining a custom stringizer won't help you in this case.  Ideally NX's gml functionality should do whatever the GML standard says. I'm not familiar with the format and couldn't find any standards document with a quick web search. The links in our own docs don't seem to point to one either (something to update). If anyone knows of the location of a GML standards doc and can share here I'd appreciate it!
comment
No worries, it doesn't have anything to do with these changes: it's a documentation dependency issue. It's fixed now, so rebasing on `main` would fix it - feel free to do so or one of the maintainers will :)
comment
I'm going to close this as resolved by @boothby 's excellent answer (thanks!)  Please feel free to reopen if you feel there is still something here that hasn't been addressed.
comment
> Should I remove those changes? Please let me know about the same.  IMO yes, the changes related to using `callable` instead of `hasattr(..., __call__)` seem like an improvement to me. At first glance, the other changes don't necessarily seem beneficial.
comment
> But we should really change the json_graph.tree_graph code API so that instead of inputting a dict of attributes including the keys 'id' and 'children', we just use arguments id and children.  Agreed - for anyone interested, a similar change was made for the `cytoscape` module in #4284.
comment
This is looking pretty good IMO. Looking at the toc structure, I wonder if it wouldn't be better to include the user guide items in the nav bar. In other words, restructure the toc(s) so that the top navbar had the following entries: `Install`, `Tutorial`, `Gallery`, `Reference` and `Developer Guide`. The `API Changes` and `Bibliography` are probably not as commonly used as those previously listed, so could be linked to from elsewhere. My general feeling is that those categories are probably what most users are looking for, so making them prominent and immediately accessible on the main page (rather than requiring a click through the "User Guide") would be an improvement.
comment
I agree with @dschult , this is very likely an installation/environment management issue. The discussion has been quiet for a while, so I'm going to close this. If the issue persists please feel free to reopen with up-to-date info on the current state of things.
comment
A quick update here: the latest pygraphviz should [now be available on the conda-forge channel](https://github.com/pygraphviz/pygraphviz/issues/326#issuecomment-780900158). 
comment
The solution here (as of the post date) is for conda users to install `pygraphviz` from the conda-forge channel (`conda -c conda-forge`). We can update this when pygraphviz is fixed on the official Anaconda channel, but I believe that's out of our hands. I'm going to close this now, but if users continue to run into this we can consider pinning the issue or something else to raise visibility.
comment
I'm going to go ahead and close this as the discussion seems to have died down. If the issue persists or more clarification is required, please feel free to reopen with updated info on the current problem statement.
comment
It's been a while since there's been any discussion here, so I'm going to close it due to inactivity. If the issue persists (or better yet, a solution was found) please feel free to reopen with updated info.
comment
> If I try to run the example manually, I get  Ah, thanks for clearing that up - I had tried locally in `ipython` and didn't see this error, e.g. `%run examples/drawing/plot_custom_node_icons.py`, but maybe that's something special to do with my setup.  > Also, examples/graph/plot_football.py imports it this way:  yes, this should definitely be fixed, preferably in this PR IMO.  
comment
Sorry for the confusion @bkbncn , it looks like I had misdiagnosed this based on differing behavior in different interpreters. Thanks for the fix!
comment
Node clipping is a common problem with `nx_pylab` - the simplest workaround which is employed in our example gallery quite often, is to use `ax.margins` to increase the padding on the axis borders.  Here's a more minimal version of the example from the OP (note that the original example doesn't work, because some of the kwargs used in `draw_networkx_nodes` aren't recognized):  ```python >>> import networkx as nx >>> G = nx.Graph([('A', 'D'), ('B', 'A'), ('C', 'E'), ('A', 'C')]) >>> fig = plt.figure() >>> pos = nx.spectral_layout(G) >>> nx.draw_networkx_nodes(G, pos=pos, node_color='skyblue', node_size=1500) >>> nx.draw_networkx_labels(G, pos) ```  Which gives:  ![Figure_1](https://user-images.githubusercontent.com/1268991/112763679-4dfc4700-8fba-11eb-91a9-50817558199d.png)  ... where the node-clipping issue is clear. The clipping can be addressed by increasing the padding on the axis margins:  ```python >>> ax = fig.get_axes()[0] >>> ax.margins(0.1, 0.1) ```  ![Figure_2](https://user-images.githubusercontent.com/1268991/112763841-00340e80-8fbb-11eb-97bf-b9a182fc140e.png)  I agree with @dschult that the proposed changes in the OP don't quite address the issue. If I were going to try to tackle the node clipping problem, I'd start by investigating how matplotlib treats markersize during axis creation/resizing, and see if there isn't a general way that `nx_pylab` could account for the full marker extent during drawing.
comment
https://github.com/networkx/networkx/issues/3443#issuecomment-826122531  This is a nice solution, but I too would be worried about strange re-scaling problems in corner cases. Ideally this is something that would be handled upstream in Matplotlib (or through the use of mpl API) without requiring any custom axis-handling code to `nx_pylab`.  That said, this issue does seem to affect a lot of users. I think I would still prefer increasing the default axis padding in `nx_pylab`, say `ax.margins(0.15, 0.15)` which would help with some cases. The tradeoffs here are:  - `-` doesn't actually "fix" the problem, just avoids it for a larger set of cases  - `-` may result in too much edge space for images with very small nodes  - `+` avoids adding custom scaling in NX
comment
I agree with @dschult , I'm going to close this one. If issues persist, please feel free to reopen along with version info from the session where you're experiencing the problem (e.g. `import networkx; networkx.__version__`)
comment
> Thanks for your replies! manim does require decorator<5  Thanks for the clarification (though I think this is `decorator>5`) - it makes sense why this is a problem then.  > rossbar, is there a syntax for requirements.txt that would allow <5 or >=5.0.7  I'm not sure, since we have a situation where `5.0.4` and `5.0.7` work but the other `5.0.*` versions don't, I think we'd have to block specific patch versions. It's not a situation I've ever encountered before so I don't have a good feel for what makes sense. Generally I don't think it's a good idea to pin to specific patch versions, but in this case I'd be +1 to do so temporarily in support of @Groctel and other pkg managers for rolling release distros, assuming we manage to remove the decorator dependency entirely by 2.6.
comment
Thanks for the report, this was [fixed in version 2.4](https://networkx.org/documentation/stable/release/release_2.4.html#pull-requests-and-commits-merged-in-this-release).
comment
I agree with @dschult that it would be preferable to avoid unnecessary copies of attribute data.  One alternative would be to add a snippet to the docstring for this use-case, similar to #4684 for example.
comment
I'd prefer not to add a new kwarg because there's already a straightforward way to copy nodes/edge attributes over to a new graph if so desired. IMO it's definitely preferable to emphasize the existing way of copying attributes: explicit is better than implicit.  I would also be -1 on adding a new kwarg to *only* one function that has this feature - if a `copy_attributes` kwarg were to be added, it would be best if it were added to all functions where this behavior crops up.  This is just my opinion though - happy to hear other viewpoints!
comment
This does indeed seem strange: it's not clear why providing `nbunch` would change the output type to `InEdgeDataView`. At face value I also agree with you on what the expected behavior would be, though it's worth noting that the other classes tend to return `*DataView`s even when `nbunch=None`, e.g.  ```python >>> G = nx.MultiGraph() >>> G.add_edge(1, 2, color="blue") >>> G.edges() MultiEdgeDataView([(1, 2)]) >>> H = nx.MultiDiGraph() >>> H.add_edge(1, 2, color="blue") >>> H.in_edges() InMultiEdgeDataView([(1, 2)]) ```
comment
>  Though maybe what you mean is that the names of the class view objects are incomplete in their description.   This was the cause of my confusion, but the description you gave makes sense. This behavior [isn't quite captured in the docstring](https://github.com/networkx/networkx/blob/643f45705eb68068e430367847829a35152f0ca5/networkx/classes/digraph.py#L909-L914) so that could certainly be improved. That's also a good point about `.edges` vs. `.edges()`, which I've been confused by before as well.
comment
I think @dschult has identified the issue - the link to the gallery example in the OP is from the development documentation: https://networkx.org/documentation/latest/auto_examples/index.html.  The gallery for the stable release (v2.5) is here: https://networkx.org/documentation/stable/auto_examples/index.html
comment
Just to clarify - is the issue that `find_cycle` doesn't find *the same* cycle after edge removal/insertion, or are there cases where a cycle is found prior to the edge removal/insertion, but *not* found afterwards?
comment
Hmm so looking at this again, it looks to me like there may have been a very subtle bug that would persist in this implementation as well. For example, look at the following (original) code chunk:  https://github.com/networkx/networkx/blob/f97705493ac84f99aa6ab5aae3129d5dc2578e2c/networkx/generators/directed.py#L276-L279  The comment indicates that `v` should be a node that is not already present in the Graph. It seems this relies on assumptions about the nodes in G; most likely based on the default value of `create_using`, which creates a graph starting with node 0 and incrementing upwards by one. However, it's easy to violate this assumption e.g.  ```python >>> G = nx.MultiDiGraph([(2, 3), (3, 4), (2, 4)]) >>> H = nx.scale_free_graph(10, create_using=G) ```  In this case, if `scale_free_graph` hit the snippet above, the "new" node would be `3`, which is clearly already in the Graph. I'm not sure what impact this has on the algorithm as a whole (maybe this is fine?) - it's just a detail where the implementation seems to diverge from the desired behavior (gathered from the comments) for certain graph inputs. Is this a problem for the algorithm? If it's not, at the very least the comments should then be updated to reflect what can actually happen in the general case.
comment
> Hmmm.... This is getting annoying and obtuse. :}  Agreed - my fault, sorry for sending us in the rabbit hole! FWIW I think even ec9bfee was fine! I just thought it'd be good to add a note to the docstring about the non-sequential node behavior.   I'd just as soon get the original implementation in and worry about the gremlins associated with passing in a non-empty MultiDiGraph instance in another issue.
comment
The offending code appears to be in the `nx_yaml` module which has been both patched (not sure if the vulnerability is the same - see e.g. #4548) and deprecated, to be removed in the next major release. I'm going to close this as duplicate/already resolved - please feel free to reopen with more specific info if there are still things to be addressed.
comment
> I guess we should have a NXEP about it first and I didn't want to introduce a surprise functionality for 2.6  I totally agree.  > Even if a networkx object is passed pagerank_scipy will be faster than pagerank_python  Yeah I think that's the big advantage here - making the scipy implementation the default.
comment
The objects.inv is available at the top-level of the documentation: https://networkx.org/documentation/stable.  [The intersphinx docs](https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#showing-all-links-of-an-intersphinx-mapping-file) provide an example to verify that the link is correct: `python -msphinx.ext.intersphinx https://networkx.org/documentation/stable/objects.inv`.
comment
> Or you could even remove the function call get_valid_edge by putting the if/else where the function is used and shifting depth to start at 1/0 based on along_matched.  IMO this would make things much more readable: +1 for this idea!  I also second the sentiment on the test - very nice!
comment
FWIW I wouldn't get too hung up on structuring tests to cover the private functions in their current form. `_is_connected_by_alternating_paths` is basically a one-liner: `return _alternating_dfs(u, condition_1) or _alternating_dfs(u, condition_2)`. The whole purpose seems to be to map an (arguably) convenient set of parameters to the `_alternating_dfs` function. It is typically hard to write good tests for such "interface" functions - the function that implements most of the behavior is actually `_alternating_dfs`. In terms of unit testing, it would be much more valuable to have good tests for `_alternating_dfs`.  All this said, I'm +1 for getting the PR in it's current form (fixing 3306 + the nice test that illustrates a case that this fixes) and addressing the issue of more robust unit testing of the private functions in the bipartite matching module to another PR. That's a bit murkier and shouldn't block this nice improvement!
comment
This PR was incorrectly closed automatically by GitHub when the default branch was renamed from master -> main. It looks like this PR has gone stale though (last activity ~2015). If there are plans to continue the work here, a new PR will have to be created. Please feel free to do so if there is interest in continuing this work - also feel free to ping for assistance, I'd be happy to help set up a new PR based on these commits.
comment
Users should already be able to use `pipenv` to install networkx if that's their preferred environment manager, e.g. `pipenv install networkx` or, if building for development, `pipenv install -e .` from the top-level directory. AFAICT the files included in this PR simply specify a single, particular environment (though be no means the *only* valid environment for networkx).
comment
> but the idea is to supersede the requirements.txt dependency format, as it creates troubles.  I'm -1 on *replacing* the requirements files - this is a standard way for specifying dependencies for Python projects (note that `pip` (`ensurepip`) and `venv`, both part of the Python standard library) and they are universally supported by other package managers like `conda` and `pipenv`.   Introducing multiple files for keeping track of dependencies (e.g. a Pipfile for pipenv, an environment.yml for conda, etc.) creates a maintenance burden to ensure that they are always in sync with each other - it is much more straightforward to rely on the format that is supported by all the package/environment management tools.  If you are having specific problems installing/setting up an environment, please open an issue on it so we can fix the specific problem in the requirements files.
comment
Closed by #4658 
comment
This PR was incorrectly closed automatically by GitHub when the default branch was renamed from master -> main. It looks like this PR has gone stale though (last activity ~2016). If there are plans to continue the work here, a new PR will have to be created. Please feel free to do so if there is interest in continuing this work - also feel free to ping for assistance, I'd be happy to help set up a new PR based on these commits.
comment
FWIW the current behavior doesn't seem to take the direction of directed edges into account:  ```python >>> G = nx.DiGraph([(0, 1), (1, 2)]) >>> nx.is_matching(G, {(0, 1)})  # expect True True >>> nx.is_matching(G, {(1, 0)})  # expect False due to reversed direction True ```
comment
Thanks @mtreinish for the fix (and the excellent commit messages!)
comment
I'm not sure I understand what you mean by "produce the graph" and I'm having trouble parsing the provided example, partly because the formatting seems to be off and there's no indication of where the error is actually occurring. An updated, minimal reproducing example would help track down what the problem might be.  Absent that, since this seems related to uninstalling/reinstalling packages aside from NetworkX, my initial suspicion would be that it's likely an installation conflict or environment management problem. There's no reason that updating `nbconvert` should have any direct impact on networkx, so it's likely either that:  1. The environment with the updated nbconvert is not the same as the environment with the working NetworkX version, or  2. There's something wrong with the Jupyter/nbconvert/latex installation.
comment
Note that there is a permalink in the navigation column on the right that says "source code", so the link to the github is available from every page of the docs. Nevertheless, if you have a specific ideas on how the docs could be improved, please feel free to open a PR (or share here if you're note comfortable doing so)!
comment
Re: the zsh issue - my initial reaction is that it is the responsibility of shell users to recognize where their shell-of-choice differs from bash syntax. For instance - the change to `pip install 'networkx[all]'` would work for both `bash` and `zsh`, but would it work for others (`csh`, `tclsh`, `fish`, etc.)? AFAICT bash is the de-facto standard when it comes to documenting shell commands for many scientific Python libraries. Even though it's a small thing, I'd be hesitant to break that pattern. That's just my two cents though - if there are examples of how the shell-syntax-dependency problem is handled in other docs, please share!
comment
It looks like this was likely closed by #4536. If there are further performance gains to be made, please feel free to reopen this or submit a new issue/PR.
comment
+1 for the performance improvements. My initial reaction is that it might be better to replace the existing `coverage` and `performance` implementations rather than adding a new function. Any thoughts on whether that's a reasonable idea, or is it better to have a separate "fast" function?
comment
Looks like it was another gotcha related to how Windows handles temporary files. 6daa668 appears to be sufficient to fix the problem (the appveyor failure seems unrelated).
comment
Having looked at `TestAGraph`, I think things could be improved w.r.t. how `tempfile` is used. My preference would be to hold off on that until this PR is in, just so that I don't break tests in the same PR that adds them to CI.
comment
My first guess is this is some sort of environment issue. Are you running jupyter lab locally? If so, are you sure you are running it in the environment in which you've installed networkx?  Can you try the following procedure:  ``` conda create -n nx-testenv python=3.7 conda activate nx-testenv conda install networkx python -c "import networkx; print(networkx.__version__, networkx.is_path); " ```  Please report if you have problems with the above.
comment
LGTM - merging in preparation for pytest-mpl.
comment
This does seem like it could be confusing. The `Notes` section does say: *Distances are calculated as sums of weighted edges traversed*, but I agree that having something in the parameter description would improve visibility. Your wording suggestion seems reasonable to me.
comment
Sure @AbhayGoyal , no one's submitted a PR with a proposed fix so feel free to do so. As I mentioned in the comment above, the proposed wording change seems reasonable to me... we'll see what other reviewers think if you submit a PR!
comment
The functions with the docstrings that may need to be modified are listed in the top comment. I haven't looked at each closely, so I'm not sure if it's *every* docstring, or just some of them. 
comment
This will also need a rebase on master. I'd be happy to do so, but unfortunately I don't think I can since the PR is originating from @amamory 's `master` branch.
comment
> It looks like #4378 changed the behavior to exclude edges incident on nodes not in nodelist.  Ack - I thought that I had come up with a solution in #4378 that avoided this for everything except for self-loops, but apparently not. It looks like it will be necessary to revert #4378 and try to find another way to address the self-loop drawing with nodelist/edgelist. Nice catch @jarrodmillman !
comment
Thanks for the ping @AndrewEckart  - I haven't forgotten about this but it will likely take me a while to get through all the changes. One of the goals I was hoping to achieve was to see how feasible it was to get rid of the wrapping `TestEdgeList` class altogether to see if we achieve a "flatter" organization for the tests.  Another quick note - networkx doesn't use type annotations for the function signatures, so that should be removed.
comment
Closed by #4466 
comment
@stefanv good point, it says as much in the [`abc.Iterable` documentation](https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable)
comment
Thanks @jarrodmillman for the comprehensive review of the situation. This makes sense to me, my approval stands :+1: 
comment
I'm going to close this as resolved. If you'd like to follow up or have some new insight on what could be improved here, please don't hesitate to reopen!
comment
Having read through the discussion here and in #3932 I like the general approach laid out by @jarrodmillman . Having never worked with geospatial data, it seems it would make sense for I/O with shp files (and other domain-specific file formats) to live under a different umbrella. @iboates I agree with your concern about losing functionality, but having a viable alternative that is a) well-documented (e.g. with gallery examples) and b) depends only on well-established tools in the geospatial ecosystem (I assume geopandas fits this bill) should mitigate problems in the short term and be much better for both libraries in the long run. Additionally, a noisy deprecation warning for `nx._shp` gives users plenty of opportunity to push back - if it turns out that removing the shp file I/O from nx is too disruptive then we can change tacks. Also thanks for the PR @iboates , the provided code really helps frame the discussion!
comment
Thanks for the interest @IOWq750 , a couple comments:  The proposed changes are tripping the tests. You can see details by either following the link in the CI checks below, or running the tests locally.  Also, you may be interested in the discussions in #4107 & geopandas/geopandas#1592 which deal with how best to include shp support for the wider ecosystem.
comment
I think @jarrodmillman 's suggestion is a good one - my approval stands for the doc changes.
comment
Note that the circleCI build failures would probably be fixed by rebasing on master.
comment
>  If anyone has any suggestions or preferences, let me know.  The only thing I don't like about having it in the top-level geospatial README is that it drops a big wall of text in the gallery. One option to avoid this might be to move the more detailed descriptions to an orphaned document that we link to from the README. It would look something like the following:  `examples/geospatial/README.txt`:  ``` Geospatial ----------  The following geospatial examples showcase different ways of performing network analyses using packages within the geospatial Python ecosystem. Example spatial files are stored directly in this directory. See :doc:`extended_description` for more details. ```  With the remainder of the content moved to a new file, e.g. `examples/geospatial/extended_description.rst`:  ``` :orphan:  Dependencies ~~~~~~~~~~~~  `GeoPandas <https://geopandas.readthedocs.io/>`__ provides ... ```  That at least makes the additional info readily available (and keeps it tightly coupled to the gallery) without adding all of the text directly to the gallery landing page.
comment
> I like the idea that a single import of scipy allows you to be able to use e.g. scipy.sparse without having to import that sub package explicitly.  I definitely agree. I haven't looked too closely at the `__getattr__` lazy importing, but it would be great to have that pattern. I also agree with @dschult that it's an interesting thing to investigate for networkx as well.
comment
My vote would definitely be for number 3. Aside from the code-searching consideration you mentioned (a very good point), there's a strong motivation to keep namespaces to avoid collisions with builtins (this is especially important for numpy, as there are many functions with the same names as builtins). For example:  ```python >>> my_trivial_genexpr = (i for i in [5, 3, 1]) >>> max(my_trivial_genexpr) 5 >>> from numpy import max >>> max(my_trivial_genexpr) <generator object <genexpr> at 0x7fb8198bbba0> ```
comment
> But I would like to know what in networkx mechanics causes this error.  Your instincts form the SO post were correct - `nx.edge_subgraph` uses `nx.subgraph_view` internally, which creates a *read-only view* that refers to the original graph. The best place to look for more detail would be at the docs/code for `nx.subgraph_view`.  > Also, a more elegant way to sidestep this error.  Your solution from SO looks good. If you're looking for something that's more concise, you could try using the `copy` method, e.g.  ```python with open('subgraph.pkl', 'wb') as f:     pkl.dump(subgraph.copy(), f) ```  The docstring for `nx.edge_subgraph` looks like it needs to be updated as `SubGraph View` is not the correct return type. Thanks for the report!
comment
I'm going to close this for now, hoping that the above answer is sufficient. If you'd like to follow up, please feel free to reopen this!
comment
> I will make a new PR to discuss the jit examples as well as a few other things. But I think it is worth merging this sooner as I plan to make a follow up PR with perhaps more controversial, but smaller changes.  +1 on following up on the JIT discussion in a separate PR. My approval stands!
comment
Hmm I'm not able to reproduce this - how are you installing networkx?
comment
A rebase on master should fix the circleCI build error
comment
+1 for xelatex. Is that not enough to solve the unicode problems? I've never heard of "xindy" either.
comment
Looking into this a little more, it seems like the missing dependency for xindy is `libffcall`, so the options are either:    - disable `xindy` in `conf.py`, which may cause errors down the road if there are ever any index terms that contain unicode characters, or   - Add `libffcall-dev` to the CI scripts/configs responsible for building documentation.  It's a bit annoying that this requires another external dependency, but it's a small one and easy to install. I'd vote for the latter just because it reduces the chances of getting bit with LaTeX-unicode in the future.  Relevant sphinx docs on xindy: https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-latex_use_xindy
comment
My two cents: treating 3.0 as a "special case" so that deprecations added now expire in only one cycle rather than the traditional two sounds fine to me. If there end up being *a lot* of potentially breaking changes, then perhaps it's worth modifying the policy, but I think people are familiar enough with semantic versioning to already be expecting some API changes anyways.
comment
Nice catch - I don't see anything in the docstring indicating how the edges are handled in the contraction, though it does seem strange that the edge resulting from the contraction takes the attribute values of the removed edge. I'm going to mark this as a defect for now even though the behavior may be undefined.
comment
Since PyPy handles garbage collection differently than CPython, the easiest thing to do would be to skip this test if on PyPy, e.g. something like:  ```python @pytest.mark.skipif(platform.python_implementation() == "PyPy") def test_memory_leak():    ... ```
comment
I'm going to close this as well. The sentiment expressed in https://github.com/networkx/networkx/pull/4309#issuecomment-723671083 applies here as well. Since this is changes to code, the additional churn also the usefulness of `blame`.
comment
There are many ways to do this of course (I landed on `plt.bar(*np.unique([d for _, d in G.degree()], return_counts=True))`), but I like @MridulS 's suggestion to use `nx.degree_histogram` best as it illustrates networkx functionality.
comment
The degree histogram example was changed around in #4265 which I think closes this issue - please feel free to reopen/submit PRs with further suggestions!
comment
> @dschult Would it make sense to add a short note about this in doc/release/release_dev.rst?  Since this change has the potential to cause some compatibility problems for users I a release note would be a good idea.
comment
@SeanDS the reason this was closed is not because changing imports wouldn't be considered, but because there was no followup. PRs with concrete suggestions for improvements would be evaluated. That being said, @dschult 's point is very important too - there are always tradeoffs to consider (user convenience, performance, maintenance burden, etc.). Bearing that in mind, PRs with proposals to improve the import system (for all users, not just specific use-cases) would certainly be considered.
comment
> Well, as far as I know the only way to speed up imports is to stop importing so much stuff on the top level.  This is a type of change that I would be very much against for all of the reasons you listed, regardless of who proposed it. I agree with @dschult that all of the downsides you've listed vastly outweigh the potential benefit of shaving 100-200ms from the import time. There are other approaches, so if someone wants to investigate & make a case for them, they would indeed be considered!
comment
> Also, the top comment of the main module says that these function remove edges from a graph. Doesn't densify add edges to the graph? Is there a better way to describe what these functions could be used for?  Related to this, it struck me that perhaps the module name should be `densification` instead of `dedensification`? It makes more sense to me to have the module name after the "forward" procedure rather than the "reverse" (in the same way that `if positive` is usually preferable to `if not negative`). I'm not too familiar with the terminology though, so if "dedensification" is the more prevalent term in network science then please ignore this comment!
comment
Oops - I missed the linked issue. This fix might not actually work though as the `release` module is also used to populate the `setup.py` info prior to install.
comment
> Because I was not exactly sure on how pip behaves, I have also tried to run python networkx/setup.py install from the parent folder of the project root. It fails on line 32 (import release) for ModuleNotFoundError, which seems to indicate that pip runs setup.py from the root and it is not relevant to my change.  Yes, I would expect this change to cause problems for anyone trying to install `networkx` from source from anywhere other than the top-level directory. I'm not sure how big of a concern that is though. The only use-case that comes to mind is if someone has a project that depends on a custom fork of networkx and has an installation workflow that looks something like:  ```bash git clone git@github.com:<my-custom-fork>/networkx my/custom/directory/structure/networkx pip install my/custom/directory/structure/networkx ```  Of course, this is a niche use-case and isn't even the correct way to do this (pip supports installing from vcs).   I'm hesitant to add things that depend on the organization of files within the project; OTOH it's clear how the current state of affairs will cause problems. 
comment
> Also, your example still runs fine both with and without my change:  You are right - I had got caught up on the expectation that it would fail in the case where networkx was not yet installed, but didn't take into account the line you've changed is already in a try-except with fallbacks to handle that case.   I would need to dig deeper to understand what is happening at install time, but this seems to be working as expected.
comment
Note that this is a quirk of Python dictionaries (on which the graph classes are based). For example, try the following (note: on Python 3.8.5):  ```python >>> d = {float('nan') : 1}   # this is allowed >>> d[float('nan')] Traceback (most recent call last)    ... KeyError: nan ```  Dicts with nans are a strange beast - for a little context you might find this [SO post](https://stackoverflow.com/questions/6441857/nans-as-key-in-dictionaries) interesting.  IMO it would be an improvement for networkx to break with Python's dict behavior in this instance and raise an error if a user tries to create a `nan` node, since it screws up subsequent lookup/removal. It's also easy to envision how a user might hit this problem while working with numerical data with `nan`s in it (which is pretty common in data analysis).  I'm very curious about other perspectives though - are there use-cases where graphs with `nan` node(s) are useful/meaningful? What would the semantics be?
comment
> Actually, it's good to avoid any floating point numbers as keys for dicts.  I'd say this is the key takeaway from this issue.  If more problems like this crop up it may be worth considering a check for `float('nan')` during node creation on the principle of "fail early, noisily, and specifically", but the general advice of using integer nodes is better (and would catch the problem when trying to convert `float('nan')` to `int`).  I will close this as resolved for now, but please don't hesitate to comment/re-open if this gets run into again.
comment
Great, thanks for the fix @cpurmessur !
comment
This does seem to be giving an incorrect result for the directed graph you mention. The thing that jumps out to me is that the description for directed graphs in the docstring does not seem to match the implementation for directed graphs.
comment
Hmm, this looks like it might be some sort of installation or packaging problem. From the path in your example, it looks like you're using anaconda, can you try the following:  ``` conda create --name nxtestenv python=3.6 conda activate nxtestenv conda install networkx==2.5 ```  Then try to reproduce the error from within that environment. I was unable to do so.
comment
I am unable to reproduce the error you are seeing, so in order to help we'd need to know what procedure reproduces the error for you. Are you modifying the code in `site-packages` at all after installation?
comment
> Are there reasons to avoid doing this?  Not a reason to avoid it, but something to consider: `nx.info` includes the number_of_edges and the average degree calculation, which *might* take a long time for some graphs? e.g.   ```python >>> G = nx.path_graph(int(1e7)) >>> %timeit nx.info(G) 4.4 s ± 47.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) ```  N.B. @dschult I think you raised this concern before - I'm simply adding here for visibility!
comment
> is there any particular reason that G.number_of_edges (or really, G.size) is calculated on demand and not stored as an attribute on Graph and maintained whenever edges are added/removed to facilitate O(1) lookups?  I think the advantage of the property is that the class/user does not have to explicitly manage state. If the e.g. `number_of_edges` were an attribute instead of a property of the graph, then all methods for adding/removing edges would have to update the value explicitly on modification which in principle 1) makes the modification itself slightly more expensive (a minor concern) and 2) makes it very easy to introduce subtle bugs (compare this situation to ref counting for example).
comment
The trick would be loading the nodes into a Graph in the format you expect, then converting it to the format you want. Note that graphs have an attribute `adj` that will give you an `AdjacencyView` showing the mapping to each other node. There are also functions like e.g. `to_numpy_array` that will produce the adjacency matrix in the form of a 2D numpy array.  Hopefully this is sufficient to address the initial question - please feel free to follow up!
comment
There is a lot to unpack in this issue and I found it difficult to understand. I'm going to close this for now due to inactivity. Please feel free to reopen if the issue persists, but if you do - consider trying to come up with a minimum reproducible example illustrating the problem/feature that you'd like. It would also help to focus on the specific component that your question pertains to (i.e. visualization, constructing graphs, etc.) -- as the scope of a problem grows it becomes much more difficult to understand!
comment
Thanks @MridulS for the nice example! I'm going to close this as resolved. If the issue persists, please feel free to reopen with updated info.
comment
I'm going to go ahead and close this one - the original question was difficult to understand because there seems to be a lot of missing context; also, the links are dead. If the issue persists, please feel free to reopen, preferably with a reproducible, minimal example.
comment
This has a lot of similar aspects to #4209: Loading data from text files (with arbitrary format) is not really a networkx-specific task, but more of a general Python task. Once you have the data loaded in a format that makes sense for your specific application, you can use graph methods like `add_nodes_from` to construct a Graph object from the data.
comment
I'm going to close this one as the necessary changes for 2.6 seem to be sufficiently tracked. If there's something that should be added here for visibility, please feel free to reopen!
comment
Thanks for reporting this. IIRC this particular test was also failing for other platforms https://github.com/networkx/networkx/issues/4054#issuecomment-684778549. My initial guess is that this is a problem with the test, specifically the heuristic for A*, which is [given by `hash` in this case](https://github.com/networkx/networkx/blob/015a90f520a0d1f0ef85d6084619021d3d0e2cd7/networkx/algorithms/shortest_paths/tests/test_weighted.py#L200). If the results of `hash` are platform dependent, this could affect the results of the weighted A*.
comment
@antlarr-suse there is a proposed fix in #4237 based on the hypothesis that the hash function is what is causing problems. We don't test on i586, so it'll be difficult to tell whether this will fix the reported problem.  > If there's anything I can test, please tell me.  Assuming you have access to an i586 system, it would be great if you could test the proposed fix. The following procedure (+/- shell differences) should do the trick:  ```bash git clone https://github.com/rossbar/networkx cd networkx git checkout --track origin/tst/heuristic python -m venv testenv source testenv/bin/activate pip install -e .[test] pytest networkx/algorithms/shortest_paths/tests/test_weighted.py ```  Alternatively if you already have the repo set up on your system, you could modify the test in place since the change is so minor.  If you don't have the bandwidth no worries!
comment
Excellent, thanks so much for checking!
comment
@mehdinemo it looks like maybe you tried to add a test but it's not showing up in the PR - perhaps another push is necessary? Though maybe I'm just misinterpreting your last comment! Please don't hesitate to ping if you have questions about pushing this forward!
comment
Something's not right with the CI workflows and restarting them doesn't seem to be doing the trick. I'm going to close/reopen this to try to retrigger the CI with a clean slate.
comment
> Do we want the memory location for any reason?  Not that I can think of, and it's still available via the `__repr__`.  This LGTM. Are there concerns about the backwards compatibility of the change in format to `nx.info`? The only context that I can think of where this might be a problem is in doctests where a change in string formatting leads to failed tests. However, there are no instances of this in the nx doctests so it seems unlikely that it would affect downstream users.  Other than that, I can't think of any blockers.
comment
The initial setup for a CircleCI workflow to build the docs is in #4119. Still to do:  - [x] Determine whether the gdal dependency is necessary for the doc build (currently not included in #4119). If so, add it.  - [x] Add the "build artifact" (i.e. the html the results from the CI build) link to the GitHub checks (see numpy/numpy#16337)  - [ ] Replace travis doc build w/ circle ci    - [ ] Push built docs to gh-pages as ``latest`` on merge to master.    - [ ] Remove the doc building task from other CI services  - [x] Verify all members of the NetworkX org have the appropriate permissions on circleCI  - [x] Tweak settings (e.g. toggle email notifications of failed builds, etc.)
comment
Yeah AFAICT the presence/absence of `gdal` doesn't have an impact on the doc build.  Re: running the doctests - this is something we could consider re-enabling in CircleCI workflow if there is interest.
comment
>  Let's enable make linkcheck first, though. We aren't currently checking for broken links.  Agreed - I'd also like to fix the remaining sphinx warnings and afterwards enable the -e flag to escalate any new warnings to errors for the CI so that the doc build will fail if any new warnings are introduced.
comment
I think an NXEP label is a good idea since NXEP's are a sort of "special" entity in some sense and the review process has different goals than for a typical PR
comment
I rebased on master to get the circleCI config in there so that we should be able to see the rendered NXEP in the circle CI build artifact.  I'm aiming to give this another pass today or tomorrow!
comment
I finally had some time to circle back around to this and added some comments in MridulS/networkx#2.  Having reviewed the NXEP and the conversation in this PR, I would propose that this be merged with "draft" status (i.e. "draft" in the NXEP sense, not the GitHub sense) into the official docs. @MridulS and @dschult have done a great job getting this to a state where IMO it is mature enough for inclusion as a draft.  Once it's in with draft status, I think there's a nice way forward to address the motivating use case(s) raised in the NXEP. We can flesh out the plan for actually implementing changes in the `Discussion` section after that point.
comment
Thanks for the report - it looks like there is an error in the test specification mandating an `int64` dtype.  Can you verify that the fix in #4055 clears up the problem on the platform where you were getting the failure originally?
comment
Sorry, first attempt caused CI problems - the current version of the test *should* be more robust. If you have the time, it'd be great if you could verify that #4055 fixes the problem on your system.
comment
Hmm - it looks like I may have misdiagnosed (or just missed) the problem the first time around. It seems to be related to `dftrue`, which is created via `dftrue = pd.DataFrame(data)` with no explicit dtype specified. I don't know why it's defaulting to `int64` on a 32-bit architecture, but this seems like a pandas thing. Explicitly specifying `np.intp` as well for the dtype of `dftrue` will *hopefully* fix the problem.
comment
> I realized the function to get a graph from cytoscape data modifies the data that is received as argument. Is there any reason for that?  This looks like a defect to me, I don't think the input data dict should be modified. An MRE:  ```python >>> G = nx.path_graph(4) >>> data_dict = nx.cytoscape_data(G) >>> data_dict {'data': [],  'directed': False,  'multigraph': False,  'elements': {'nodes': [{'data': {'id': '0', 'value': 0, 'name': '0'}},    {'data': {'id': '1', 'value': 1, 'name': '1'}},    {'data': {'id': '2', 'value': 2, 'name': '2'}},    {'data': {'id': '3', 'value': 3, 'name': '3'}}],   'edges': [{'data': {'source': 0, 'target': 1}},    {'data': {'source': 1, 'target': 2}},    {'data': {'source': 2, 'target': 3}}]}} >>> H = nx.cytoscape_graph(data_dict) >>> data_dict   # 'source' and 'target' keys no longer present for edges {'data': [],  'directed': False,  'multigraph': False,  'elements': {'nodes': [{'data': {'id': '0', 'value': 0, 'name': '0'}},    {'data': {'id': '1', 'value': 1, 'name': '1'}},    {'data': {'id': '2', 'value': 2, 'name': '2'}},    {'data': {'id': '3', 'value': 3, 'name': '3'}}],   'edges': [{'data': {}}, {'data': {}}, {'data': {}}]}} ```  > I couldn't find that in the documentation, and I found it when browsing the code.  Unfortunately `cytoscape_graph` is also missing a docstring! Thanks for catching this as well.
comment
FWIW this should be a very straightforward fix that is suitable for new contributors. To do so would require the following:  1. Add/modify a test in `networkx/readwrite/json_graph/tests/test_cytoscape.py` that will fail with the current behavior but pass with the desired behavior (i.e. the input data is not modified when `cytoscape_graph` is called).  2. Modify the `cytoscape_graph` function defined in `networkx/readwrite/json_graph/cytoscape.py` so that the input data is not modified by the function
comment
This depends on how you installed networkx. If you installed with pip, then `pip uninstall networkx` or `pip install networkx==<version>` (where `<version>` is the version you want, e.g. `2.4`).
comment
I am going to close this one as resolved. Please feel free to reopen with more specific information if there are still problems.
comment
@cvagg you are correct that `projected_graph` does not currently accept MultiGraph inputs. Are there any references you can provide that describe the behavior you were looking for?
comment
I took the liberty of rebasing this on `master` to enable the doc-building CI service, which will make it easier to review the changes to the documentation. I will take another pass once the CI is done!
comment
I should also mention:   Git will be confused on your local machine because I changed history when I force pushed to your remote branch. So you will have to run a few commands. If you aren't familiar with what to do, please take a look at this: https://stackoverflow.com/questions/9813816/git-pull-after-forced-update  If you aren't sure after looking at that, please don't hesitate to ask!
comment
> The bayes net and the moralized graph nodes aren't in the same locations. I thought having the same layout would ensure they line up. What am I missing?  It looks like the axes limits are inconsistent between the first two axes objects, even though the positions are identical. That's an easy fix, but annoying that the user has to do it manually.  > As noted in the docstring, the junction tree isn't unique. But maybe we should (arbitrarily) make it so (possibly w/ a flag).  Would decorating with one of the seed-setting decorators accomplish this?  > Should we use graphviz for the layout?  It looks like the majority of the [examples](https://networkx.github.io/documentation/stable/auto_examples/index.html) only use `nx_pylab` tools, though there are some that use pygraphviz/pydot. My vote would be to stick with whatever has the least amount of boilerplate. It would definitely be a good idea to revisit the example gallery as the visualiztion tools are improved.
comment
There hasn't been any activity here in a while so I'm going to close this for now. Please feel free to reopen this if interested in continuing the discussion!
comment
Note that it seems #4033 removed the `with_labels` kwarg entirely, though I'm not sure this was intentional as the removal probably should've been accompanied by a warning. This probably needs a closer look.  Re: the specific issue - one workaround might be to call `nx.draw_networkx_nodes` and `nx.draw_networkx_labels` in two separate steps, e.g.  ```python >>> G = nx.dodecahedral_graph() >>> pos = nx.spring_layout(G) >>> nx.draw_networkx_nodes(G, node_size=50, node_shape='^', pos=pos, node_color='g') >>> nx.draw_networkx_labels(G, pos=pos) ```  It's an extra LOC, but it also affords finer-grained control over the configuration of the labels.
comment
I'm going to close this one due to inactivity. If the issue persists, please reopen. A minimum reproducing example would be very helpful in determining what the problem is and how it can be resolved.
comment
Unfortunately the given example isn't sufficient to reproduce the behavior. Since this has been inactive for a while I'm going to close it. However, if you have a minimum reproducing example that does not depend on loading external data, please share and reopen this!
comment
I'm going to close this since it seems it was fixed in #3698. Please feel free to reopen if you'd like to continue the discussion.
comment
It seems like the question was sufficiently answered (thanks @dschult !) so I'm going to close this. Please feel free to reopen if you'd like to pick up the discussion again.
comment
This issue has been inactive for a while so I will close it. If the question persists please feel free to reopen, preferably with a minimum example with a small graph and the desired result(s)/behavior (the previous example didn't provide any further clarity on the problem for me).
comment
Looks like this was closed by #3845. If not, please reopen.
comment
It looks like this was closed by #3854. Please reopen if I've missed something.
comment
It's been a while since there's been any activity here and it doesn't seem like there is anything that is immediately actionable, so I'm going to close this. Thanks for the question @hsiangyuzhao - if you have any followups please feel free to reopen.
comment
Thanks @dschult for the really nice, detailed answer. I'm going to close this as resolved, but please feel free to reopen if there are follow-up questions, related feature requests, etc.
comment
> It took awhile for all three to update, but they mostly seem correct now.  Agreed - I see the v2.4 docs as the top hit for these search terms everywhere now :+1:   > I am still seeing this PDF pop-up at the top occasionally:  I only see this with the linked yahoo query. I'd suspect it has more to do with yahoo than anything else...
comment
Here's a minimal reproducing example:  ```python >>> import networkx as nx >>> edge_list = ['1, 2, {"weight": 1, "color": "green"}'] >>> G = nx.parse_edgelist(edge_list, delimiter=',') Traceback (most recent call last)    ... IndentationError    ... TypeError: Failed to convert edge data ([' {"weight": 1', ' "color": "green"}']) to dictionary. ```  Side note: this seems like a good candidate for for `from None` instead of `from e` in the exception raising as the `IndentationError` is confusing.
comment
A few things come to mind:  1. verify that your entire dataset (all 2000 graphs) are loaded into memory *before* the calculation is performed. If you are loading the graphs one-by-one for each GED calculation, then disk I/O is very likely your bottleneck.  2. Comparing every graph to every other in this way scales as n^2. It might be worth taking a step back to see if you can come up with an approach that avoids this one-to-every comparison for each graph.  3. Re: parallelism - at face value this problem does seem like it could be parallelized in a straightforward way. I recommend taking a look at `concurrent.futures`, `multiprocessing`, or any one of the many libraries that bring parallel computation features to Python.  This question is rather specific and not really specific to networkx, so I'm going to close it. Please feel free to respond (even though the issue is closed) or, if you have more nx-specific questions, reopen this or open a new issue.
comment
There's nothing in the `from_pandas_adjacency` function that would modify the shape of your input `matrix`, so most likely this is a problem of the code prior to `from_pandas_adjacency` modifying the shape in the way that you expect.  More information is necessary to help chase this down. Can you provide a minimal example to reproduce the error you are seeing? For example:  ```python >>> import networkx as nx >>> import pandas as pd >>> df = pd.DataFrame(np.random.randint(low=0, high=2, size=(521, 521))) >>> G = nx.from_pandas_adjacency(df) ``` which should work fine.
comment
Thanks for the report - the `np.iterable` approach seems like a viable solution at first glance. Please feel free to submit a PR with any suggested changes!
comment
I agree @jarrodmillman, this pattern makes sense and is easy to maintain. My only comment would be to remove the `Notes` header and just have the redirection text as part of the initial docstring summary.
comment
Good catch - I falsely attributed the merge conflict. Thanks for fixing @jarrodmillman 
comment
Can you run `python -X importtime -c 'import networkx'` from the environment that you're having problems in and share the results?
comment
Once again, thanks @cgoliver for the nice enhancement and @dschult for the excellent review and intersesting discussions!
comment
Fixing up a couple last minor sphinx issues for the docs, then I will merge!
comment
Okay, in this goes - thanks @cgoliver for the cool feature and @dschult for the careful review!
comment
It's not necessary - I don't see any failures with `pytest --doctest-modules networkx/algorithms`
comment
Hmm, no I'm not familiar with `pytest-xdist` so at first glance the problem is not obvious to me. I'd have to look at it more closely.
comment
Thanks @Koukyosyumei , if you feel comfortable doing so, feel free to try it out and submit a PR. Performance enhancements are of course welcome!
comment
Nothing needs to be changed to add support for `pipenv`: the following should work:  ```bash pipenv install networkx ```  Or, to install for development:  ```bash pipenv install -e . ```  All dependencies and requirements for any specific configuration can either be added individually as needed, or with the various requirements files in `requirements` via `pipenv -r <req_file>`.
comment
This PR contains quite a few changes, but as with NumPy it's worth looking at each instance of exception chaining to be sure that the `from e` is adding valuable information. There can be cases where exceptions are caught specifically to *replace* the original exception rather than expand upon it. In those cases, it would be more clear to chain `from None`.  numpy/numpy#15986 provides a nice summary. I think a similar plan should be used here, rather than chaining *every* exception `from e`.
comment
@reethified I'm not sure how to interpret the error you're seeing. Note that the change you propose raises an AttributeError (see the CI fail log for details).  Looking closely at line changed in this PR, I see nothing wrong with it and don't see any errors when running `examples/drawing/plot_degree_histogram.py`. Did you perhaps intend to change a different file?  I'm going to close this as I can't see a connection between the reported error in the PR description and the changed file. Please feel free to reopen/open a new issue/PR with more info if you're still having issues.
comment
Perhaps you've switched from Python 2 to Python 3 over the last year? It seems that `from_graph6_bytes` expects a bytes-like object, not a string. An update to the `Parameters` section of the docstring could perhaps make this more clear.  If this is indeed a Python 2/3 issue you could try encoding your Python3 string to produce a sequence of bytes, e.g. something like `from_graph6_bytes(<your_string>.encode('ascii'))` in your script.
comment
> I believe that the error is coming from within NetworkX's code - the call to string.startswith() with is made within the from_graph6_bytes function.  This is what I was referring to before - it's confusing because the variable `string` in the function `from_graph6_bytes` is not actually a string - there is a mismatch between the function name (which implies that the input should be bytes) and the naming convention *inside* the function (argument name is string). This is likely an artifact from when the function was originally written (pre-Python3) as the naming convention would have made sense for Python 2.  Here's a quick example to illustrate the root of the problem:  ```python >>> s = '>>graph6<<A_' >>> type(s) str >>> b = s.encode('ascii') >>> type(b) bytes >>> type(b'>>graph6<<') bytes # This produces the error because the str.startswith method expects strings, # not bytes >>> s.startswith(b'>>graph6<<') Traceback (most recent call last)    ... TypeError: startswith first arg must be str or a tuple of str, not bytes # But a comparison of bytes to bytes is fine >>> b.startswith(b'>>graph6<<') True ``` And with `nx.from_graph6_bytes`: ```python >>> import networkx as nx >>> nx.from_graph6_bytes(s) Traceback (most recent call last)    ... TypeError: startswith first arg must be str or a tuple of str, not bytes >>> G =  nx.from_graph6_bytes(b) >>> G.edges EdgeView([(0, 1)]) ```  Long story short - `from_graph6_bytes` needs a bytes-like input, *not* a string.  I think updating the argument name, docstring, and maybe touching up the exceptions would help resolve some of this confusion.
comment
It seems like the conversation here has resolved things so I'm going to close this for now. Feel free to re-open if there are still things that need to be addressed.
comment
I'm going to go ahead and close this so we don't invite conversation in multiple places. Please discuss in #3978 - thanks for the contribution @iandreafc !
