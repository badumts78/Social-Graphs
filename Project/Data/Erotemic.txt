issue
Adopt xdoctest as primary doctest runner?#TITLE_END#This is based on a discussion from #4169  The basic question is might networkx benefit from the syntax flexibility provided by xdoctest?   My "The case for xdoctest in networkx" pitch is largely copied from the aforementioned thread:   I do think using xdoctest would be an overall improvement, and it would make writing / maintaining doctests much easier. I recently had to modify a PR to conform to the more restrictive builtin-doctest syntax. In the future I --- and perhaps others --- would prefer to utilize xdoctest syntax.  The main feature I'm making use of is "new-style got/want" tests, where print statements don't need to be broken up. For instance, the following works with xdoctest:   ```python     Example     -------     >>> open_to_close = {'{': '}', '(': ')', '[': ']'}     >>> seq = '({[[]]})[[][]]{{}}'     >>> all_decomp = generate_all_decomp(seq, open_to_close)     >>> node, *decomp = all_decomp[seq]     >>> pop_open, pop_close, head, tail, head_tail = decomp     >>> print('node = {!r}'.format(node))     >>> print('pop_open = {!r}'.format(pop_open))     >>> print('pop_close = {!r}'.format(pop_close))     >>> print('head = {!r}'.format(head))     >>> print('tail = {!r}'.format(tail))     >>> print('head_tail = {!r}'.format(head_tail))     node = '('     pop_open = '('     pop_close = ')'     head = '{[[]]}'     tail = '[[][]]{{}}'     head_tail = '{[[]]}[[][]]{{}}' ```  However, to refactor that to work with the builtin doctest module would require following each print statement with the text it produced:  ```python     Example     -------     >>> open_to_close = {'{': '}', '(': ')', '[': ']'}     >>> seq = '({[[]]})[[][]]{{}}'     >>> all_decomp = generate_all_decomp(seq, open_to_close)     >>> node, *decomp = all_decomp[seq]     >>> pop_open, pop_close, head, tail, head_tail = decomp     >>> print('node = {!r}'.format(node))     node = '('     >>> print('pop_open = {!r}'.format(pop_open))     pop_open = '('     >>> print('pop_close = {!r}'.format(pop_close))     pop_close = ')'     >>> print('head = {!r}'.format(head))     head = '{[[]]}'     >>> print('tail = {!r}'.format(tail))     tail = '[[][]]{{}}'     >>> print('head_tail = {!r}'.format(head_tail))     head_tail = '{[[]]}[[][]]{{}}' ```  Personally, I think the former is far more readable (you see a block of code that produces something and then you see the output as one single chunk, versus forcing humans to work like a REPL).   There are also minor issues with trailing whitespace causing got/want errors in the original doctest, the fact that any non-captured variable must provided with a "want" string, and the general issue of forcing the programmer to distinguish between lines that start a statement versus are continuations of previous statements.   For reference xdoctest is small, has no minimal dependencies, and is 100% compatible with the current structure of networks (in fact networkx is one of the main test-cases I used to ensure backwards compatibility when I wrote xdoctest), running `pytest --xdoctest-modules --xdoctest-global-exec "import networkx as nx"` will run all of the existing tests correctly.  
issue
Fix minimum_spanning_arborescence regression#TITLE_END#Attempts to fix https://github.com/networkx/networkx/issues/7279  It looks like the code for `minimal_branching` and a bunch of other algorithms access the weight attribute `attr` directrly from node or edge data `d` via `d[attr]` and does not respect the specified `default` value provided in the signature.  This PR does fix the issue by replacing these with `d.get(attr, default)`, but I'm not sure it does it correctly. Looking at the code, I'm not sure the algorithm structures ever used something like `d.get(attr, default)`, instead it looks like the "dispatch" decorator is ensuring the graph structure is ammenable to this sort of direct lookup (perhaps for efficiency?). I don't understand the dispatch structure well enough to say for sure. Advice and / or pointers to docs would be appreciated. I'm hoping this is a simple fix and not some new policy that forces the user to specify weights in a graph when there is a clear default of 1.
issue
Regression: minimum_spanning_arborescence breaks on unweighted graphs#TITLE_END#In networkx 3.2, running the following code breaks whereas in 3.0 and 3.1 it works:   ```python import networkx as nx rng = 123 num = 10 graph = nx.erdos_renyi_graph(num, p=0.2, directed=True, seed=rng) tree = nx.minimum_spanning_arborescence(graph) try:     nx.write_network_text(tree) except AttributeError:     print(nx.forest_str(tree)) ```  In 3.0 and 3.1 this prints:  ``` ╙── 2     ├─╼ 4     ├─╼ 5     └─╼ 7         └─╼ 0             ├─╼ 1             └─╼ 6                 └─╼ 8                     ├─╼ 3                     └─╼ 9 ```  In 3.2 and 3.2.1 it gives an error:  ```   File "/home/joncrall/code/simple_dvc/dev/nxmew.py", line 5, in <module>     tree = nx.minimum_spanning_arborescence(graph)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "/home/joncrall/.pyenv/versions/3.11.2/envs/pyenv3.11.2/lib/python3.11/site-packages/networkx/utils/backends.py", line 412, in __call__     return self.orig_func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "/home/joncrall/.pyenv/versions/3.11.2/envs/pyenv3.11.2/lib/python3.11/site-packages/networkx/algorithms/tree/branchings.py", line 1310, in minimum_spanning_arborescence     B = minimal_branching(         ^^^^^^^^^^^^^^^^^^   File "/home/joncrall/.pyenv/versions/3.11.2/envs/pyenv3.11.2/lib/python3.11/site-packages/networkx/utils/backends.py", line 412, in __call__     return self.orig_func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "/home/joncrall/.pyenv/versions/3.11.2/envs/pyenv3.11.2/lib/python3.11/site-packages/networkx/algorithms/tree/branchings.py", line 1237, in minimal_branching     if w > max_weight:        ^^^^^^^^^^^^^^ TypeError: '>' not supported between instances of 'NoneType' and 'float' ```  If I add the lines:  ```python for u, v, d in graph.edges(data=True):     d['weight'] = 1.0 ```  to give each edge a weight before calling `minimum_spanning_arborescence` it works on 3.2+  Going to look into a PR to fix this, but wanted to make the issue first.  
issue
Vertical chains for network text#TITLE_END#This PR is an addition to "network_text", which I use quite a bit in my DAG library ([cmd_queue](https://pypi.org/project/cmd-queue/)),  [delayed image](https://gitlab.kitware.com/computer-vision/delayed_image), and elsewhere (printing graphs to the screen makes them much easier to debug). It works very well for smallish graphs. For larger graphs it can go past the display width, which is a known limitation.  However, recently I ran into an instance where my graph was just a long chain, and it overran the width of my screen. I thought: "that's kind of annoying", it's a chain, just print everything vertically.  So that's what this does. When network text detects a chain it uses a vertical space to separate the nodes. This does cost vertical space, so it is disabled by default, but if you enable it the display can scale to much larger graphs where the bulk of it is made of chains.  Here is a before / after:  ```python         >>> graph = nx.path_graph(10)         >>> graph.add_node('A')         >>> graph.add_node('B')         >>> graph.add_node('C')         >>> graph.add_node('D')         >>> graph.add_edge(9, 'A')         >>> graph.add_edge(9, 'B')         >>> graph.add_edge(9, 'C')         >>> graph.add_edge('C', 'D')         >>> graph.add_edge('C', 'E')         >>> graph.add_edge('C', 'F')         >>> write_network_text(graph)         ╙── 0             └── 1                 └── 2                     └── 3                         └── 4                             └── 5                                 └── 6                                     └── 7                                         └── 8                                             └── 9                                                 ├── A                                                 ├── B                                                 └── C                                                     ├── D                                                     ├── E                                                     └── F         >>> write_network_text(graph, vertical_chains=True)         ╙── 0             │             1             │             2             │             3             │             4             │             5             │             6             │             7             │             8             │             9             ├── A             ├── B             └── C                 ├── D                 ├── E                 └── F ```  A more subtle case:  ```python     >>> graph = nx.generators.barbell_graph(4, 2)     >>> write_network_text(graph, vertical_chains=False)     ╙── 4         ├── 5         │   └── 6         │       ├── 7         │       │   ├── 8 ─ 6         │       │   │   └── 9 ─ 6, 7         │       │   └──  ...         │       └──  ...         └── 3             ├── 0             │   ├── 1 ─ 3             │   │   └── 2 ─ 0, 3             │   └──  ...             └──  ...     >>> write_network_text(graph, vertical_chains=True)     ╙── 4         ├── 5         │   │         │   6         │   ├── 7         │   │   ├── 8 ─ 6         │   │   │   │         │   │   │   9 ─ 6, 7         │   │   └──  ...         │   └──  ...         └── 3             ├── 0             │   ├── 1 ─ 3             │   │   │             │   │   2 ─ 0, 3             │   └──  ...             └──  ... ```    My thoughts on review:  * I think I got the implementation right, but it's always easy to make mistakes with this sort of code * I was unable to find a suitable UTF 8 character to remove the need for the "|" character to take up an entire line. I had one candidate, but I concluded it was too visually confusing. * The directed ascii only case doesn't have a great down arrow proxy. I used "v", but I thought about just keeping "|" and letting the downward direction be implicit... not sure, but my gut says "v" is the best choice.  Once this core functionality is reviewed, I'll add the tests and such.  Edit: I also noticed I had logic in to "collapse" a node if node["collapse"] was truthy. It's unrelated, but also simple and useful to manually not show subbranches of a larger graph. I suppose I could do a different PR for that, but it's pretty small, so it might be worth just to merge it here.   @MridulS @dschult 
issue
Add generate / write "network text" (formerly graph_str)#TITLE_END#This is an extension of the `forest_str` that I previously submitted. I found myself working with `forest_str` very often. I've found it very useful to be able to print out the graph in an interpretable manner.  But recently I've been finding myself working with non-forest graphs or near-forest graphs, and the existing `forest_str` function is not setup to work with them. But after thinking about it, I realized it would still be useful to look at the DFS tree of a lot of the graphs, and given a DFS tree you could munge it to denote where backedges are, thus giving a complete representation of a general graph.  The way this works is that when an outgoing edge is connects a node already in the tree, there is a special suffix written to the right of node indicating what those non-tree edges are. When a node recuses into a child that already drew one of these implicit back-edges it emits an ellipsis (or lets the vertical line trail) for conciseness.   The doctest illustrates 3 cases:  * The case of a regular forest, no backedges are produced * The same forest but with one backedge introduced between 1 and 7  * A fully connected clique.  And the unit test illustrates the case of incrementally going from a graph with no edges to a fully connected graph. This does show that there are a few cases that might be cleaned up. For self loops the current implementation seems to show the edges twice, so I'll see if I can fix that, but overall I think the idea works well and gives as good of a visualization as one might hope to get for a general graph in text form.  I coded this up as a variant of `forest_str`, and then replaced the logic of `forest_str` with a call to this new `graph_str` function as to not break compatibility.
issue
Remove unused logic in nonisomorphic_trees#TITLE_END#This PR removes a line of code that is called, but doesn't do anything.  I noticed this when looking at the code for the currently longest running tests on the CI. I was checking to see if there was any small speedups that could be made in here (nothing that I saw that really that has any major impact, there might be a way to do the first step of `_split_tree` faster with a generator, two next calls, and a try/except StopIteration). But did I notice this one line that (unless I'm really missing something) doesn't seem to do anything. Removing it speeds up the test a tiny amount (but not in a noticeable way). Either way I thought I'd make a quick PR to clean it up.  This should be an obvious review and merge. The variable `result` is assigned to but never used (it even causes flake8 to raise a F841 error, so not sure why networkx dashboard linters aren't catching it). It also doesn't seem like the line has any side effects that could influence anything. It's just calling `len` on `layout`, so unless there is a fancy `__len__` method with side effects, I'm going to say this is safe to remove.  Anyway, I'm writing way too much for a PR of this size / scope. PTAL
issue
Test if CI works on main#TITLE_END#Attempting to debug why #5602 is failing on:  * test / base (ubuntu, 3.11-dev * test / extra (ubuntu-22.04, 3.9)   If this is actually a dashboard issue and not something I introduced, I will attempt to fix it here.
issue
Reorganize minor submodule as subpackage#TITLE_END#This PR is a small self-contained reorganization to reduce the diff size in #4327. Hopefully it will be easy to review. It simply refactors the minors submodule into a subpackage, which will allow for easier integration of new algorithms related to graph minors.  Specifically the PR creates the subpackage `algorithms/minors` and moves the existing `algorithms/minors.py` inside this subpackage (renaming it to `algorithms/minors/contraction.py`). The tests were also moved accordingly.  This should not cause any change in the API because all components of the `algorithms/minors/contraction.py` package are exposed in `algorithms/minors/__init__.py`.  This PR does not have any dependencies   This PR is depended on by #4350, and #4327 
issue
Add random_ordered_tree and forest_str#TITLE_END#<!-- Please run black to format your code. See https://networkx.org/documentation/latest/developer/contribute.html for details. -->  The purpose of this PR is to simplify the review of #4327. It separates the new self-contained developed for that branch  `random_ordered_tree` which like it sounds make a random ordered tree, and `forest_str`, which makes a text-based representation of a directed or undirected forest.   After this is finalized and accepted, I'll rebase #4327 on top of master/main. Reviewing these two features should be much easier and make tackling #4327 more feasible.   This PR ~depends on #4326~ has no dependencies.  This PR is depended on by #4327 and #4350
issue
Use xetex for uft8 latex backend#TITLE_END#Closes #4325 This is a component of #4169  I finally settled on   ```python latex_engine = "xelatex" latex_use_xindy = False ```  Turns out xindy --- don't really know what that is --- was causing errors, and this setting allows xelatex to work (I think xindy is more a pdflatex thing?).   This PR does not have any dependencies  This PR is depended on by  #4294, #4350, and #4327
issue
Use a utf8 friendly latex backend#TITLE_END#The current sphinx configuration in docs/conf.py defaults to pdflatex. This is causing problems on #4169 which introduces API-level doctests with unicode characters in them. I tried several iterations of lualatex and xelatex to try and get it to work, but latex errors are never the most helpful.  I will open a PR to resolve this shortly. 
issue
Algos for max ordered common subtree embedding#TITLE_END#Closed in favor of #4327  ### Summary This PR is for two new (related) algorithms which I don't believe exist in networkx:   * (will do this in a separate PR) Maximum ordered common subtree isomorphism: Given two ordered trees: G and H, find the largest subtree G' of G and H' of H where H' is isomorphic to G'.   * Maximum ordered common subtree embedding: Given two ordered trees: G and H, find the largest embedding G' of G and H' of H where H' is isomorphic to G'. A tree G' is an embedded subtree of G if G' can be obtained from G by a series of edge contractions.  These are algorithms are restricted to ordered trees because --- at least the embedding one --- is APX-hard for any other relaxation of the input types. The subtree isomorphism probably is too, but I'd need to check.   ### Modivation  When working with pytorch-based neural networks I found that I often encounter a problem where I have some custom module that uses resnet50 as a component, and I would like to simply start from existing resnet50 pretrained weights. However, to do that the "state_dict" of the model must match the "state_dict" of the saved pytorch weights file. But because my resnet50 model is a component, the keys don't exactly line up.   For instance, the keys in the resnet file may look like this:   ```python         >>> # This means you can load an off-the-shelf unmodified pretrained resnet50         >>> # where the keys might look something like this:         >>> resnet_keys = {         >>>     'conv1.weight',         >>>     'layer1.0.conv1.weight',         >>>     'layer1.0.conv2.weight',         >>>     'layer1.0.conv3.weight',         >>>     'layer1.0.downsample.0.weight',         >>>     'layer2.0.conv1.weight',         >>>     'layer2.0.conv2.weight',         >>>     'layer2.0.conv3.weight',         >>>     'layer3.0.conv1.weight',         >>>     'layer4.0.conv1.weight',         >>>     'fc.weight',         >>>     'fc.bias',         >>> } ```  And the "model-state" for the module may look like this: ```python         >>> # And perhaps you have a model that has a state dict where keys         >>> # look like this:         >>> model_keys = {         >>>     'preproc.conv1.weight'         >>>     'backbone.layer1.0.conv1.weight',         >>>     'backbone.layer1.0.conv2.weight',         >>>     'backbone.layer1.0.conv3.weight',         >>>     'backbone.layer1.0.downsample.0.weight',         >>>     'backbone.layer2.0.conv1.weight',         >>>     'backbone.layer2.0.conv2.weight',         >>>     'backbone.layer2.0.conv3.weight',         >>>     'backbone.layer3.0.conv1.weight',         >>>     'backbone.layer4.0.conv1.weight',         >>>     'head.conv1'         >>>     'head.conv2'         >>>     'head.fc.weight'         >>>     'head.fc.bias'         >>> }  ```  Now, yes I could do (and have done) a hacky solution that tries removing common prefixes, but I wanted something more general. Something that would "just work" in almost all cases. After thinking about it for awhile I realize that this was a graph problem. I can break up the components by the "." and make a directory like tree structure.   Now, I want to ask the question, what is the biggest subgraph that these two directory structures have in common. After searching for awhile I found the [Maximum common induced subgraph](https://en.wikipedia.org/wiki/Maximum_common_induced_subgraph) problem, which is NP-hard for graphs. But we have trees, so can we do better? I found the [Maximum Common Subtree Isomorphism](https://pdfs.semanticscholar.org/1570/b5f2faa32384aa3936980a518b63f6b77412.pdf) and [Maximum Common Subtree](https://perso.ensta-paris.fr/~diam/ro/online/viggo_wwwcompendium/node168.html) but I wasn't able to follow any of these links to get an algorithm working. (although perhaps the first one is worth revisiting).   Eventually I found the paper: [On the Maximum Common Embedded Subtree Problem for Ordered Trees](https://pdfs.semanticscholar.org/0b6e/061af02353f7d9b887f9a378be70be64d165.pdf), which outlines a polynomial time algorithm for finding maximum common embedded subtrees. The paper was written in a way that I could follow it, but unfortunately I missed the detail that an embedding --- although similar to --- is not an isomorphic subgraph.  However, the algorithm for finding a  Common Embedded Subtree does still work well for solving my problem. The paper also links to external resources where they do tackle the actual common subtree isomorphism problem, but unfortunately they were all behind paywalls.   However, I do believe I was able to modify the recurrence for the maximum common embedding into one that does produce a maximum common isomorphism, and I've empirically verified that it works on a few thousand randomly generated trees.    ### Remaining Work  So, I have these two networkx algorithms for `maximum_common_ordered_tree_embedding` and `maximum_common_ordered_subtree_isomorphism`, and I'd like to contribute them to `networkx` itself. In their current state they are very messy and unpolished, but I'd want to know if the maintainers are interested in these algorithms before I do any code cleanup. If there is interest, any guidance on where these algorithms should be located would be appreciated (do these go in isomorphisms, somewhere else?).   --- EDIT:  ### Current Summary of Modifications  I'll maintain a top-level summary of modifications in this comment.   1. `networkx/algorithms/__init__.py` - expose string and embedding modules   2. `networkx/algorithms/embedding/__init__.py`  - new algorithm submodule for embedding problems   3. `networkx/algorithms/embedding/tree_embedding.py` - ⭐ implements reduction from graph problem to string problem. Defines the function which is the main API-level contribution of this PR: `maximum_common_ordered_tree_embedding`.   4. `networkx/algorithms/embedding/tests/test_tree_embedding.py` - associated tests for tree embeddings  5. `networkx/algorithms/string/__init__.py`  - new algorithm submodule for string problems   6. `networkx/algorithms/string/balanced_sequence.py` - ⭐ core dynamic program to solve the maximum common balanced subsequence problem. This is the main algorithmic component of this PR.   7. `networkx/algorithms/string/balanced_sequence_cython.pyx`  - optional, but faster cython version of balanced_sequence.py      8. `networkx/algorithms/string/tests/test_balanced_sequence.py` - associated tests for balanced sequences  9. `examples/applications/filesystem_embedding.py` - demonstrates how to solve the path embedding problem using tree embedding. This file likely needs further reorganization, or possibly a separate PR, the rest of the PR does stand alone without this.   10. `setup.py` - registered embedding and string subpackages.       
issue
ENH: Added feature delete_edge_attributes and delete_node_attributes#TITLE_END#In one of my projects I have to maintain a graph with lots of dynamic state. Its often convenient to be able to remove attributes entirely from a graph instead of having set the attribute to None. It makes debugging much easier as extra keys don't get in the way if they've been deleted.  I initially made an internal helper function to get this functionality following the discussion in https://groups.google.com/forum/#!topic/networkx-discuss/S-rIdrNNrUY   It seems like this may be something that the official library would want to support, hence the pull request.   The added functions work similarly to get_*_attributes and set_*_attributes. Except they are not delete_*_attributes. But perhaps they may be better named as del_*_attributes?  Also, there is a question of implementation in delete_edge_attributes. The implementation in my commit differentiates between the multigraph and non-multigraph inputs. However, the following code should be universal to both:  ```python     for edge in edges:         d = G         for p in edge[:-1]:             d = d[p]         del d[edge[-1]] ```  However, it seems a bit more obfuscated, which is why I went with the more explicit implementation.    Any comments? If this feature is wanted I can make any requested changes and add the appropriate tests. If not, then feel free to close the PR. 
issue
Update adjacency_iter to adjacency#TITLE_END#Closes #4338 
issue
Add Jon Crall to contributors#TITLE_END#I noticed that there is an opt-in contributors file, and I'd like to add my name for my merged PRs: #2572,  #2571, #2568, and #2554. 
issue
Behavior of `nx.union_all` when `graphs=[]`#TITLE_END#I'm running into a case where I'm passing in an empty list to `nx.union_all`, and it returns a None type.   While this is not necessarily the wrong thing to do, it is not documented.  Intuitively, I would expect the result of union with no inputs to be an empty graph, but the issue here is that you don't know what type the graph should be. Therefore I think the best behavior would be to raise a ValueError indicating that the input cannot be empty. This would make it more clear where the code is failing.   Current behavior:  ```python  >>> nx.union_all([nx.path_graph([1, 2])]) <networkx.classes.graph.Graph at 0x7f6fb15d1ac8>  >>> nx.union_all([nx.path_graph([1, 2]), nx.path_graph([3, 4])]) <networkx.classes.graph.Graph at 0x7f6fb1477ac8>  >>> print(nx.union_all([])) None  ```  Proposed Behavior:  ```python >>> print(nx.union_all([])) ValueError: Cannot union_all an empty list ```
issue
Added function for finding a k-edge-augmentation#TITLE_END#This is the second part of PR #2459. Currently this is just an initial commit that lays out the basic components of the new module.   After PR #2554 is working and merged, I'll continue work on this part.    ### Definition * **k-edge-augmentation**: given a graph G that is not k-edge-connected a k-edge-augmentation is a small set of edges that would make G k-edge-connected if they were added.  ------- From the other PR.   The second module is edge_augmentation  I only have efficient algorithms for the case where k=1 and k=2 (although in my project I just wrote a greedy algorithm for k>=3). Each function has weighted and unweighted versions. I'm not sure what the best way to handle the API for that is yet. My working solution takes in a list of available edges that can have optional weights defaulted to 1. Otherwise it is assumed that the complement of G is available and those edges have weights 1.  This module probably needs more work than just cosmetics and it might be better handled as a separate PR.  When k=1, one_edge_augmentation basically solves an MST problem, but my implementation solves it without constructing the complement of the graph. The weighted version is done in weighted_one_edge_augmentation.  When k=2, bridge_augmentation performs an algorithm described by Eswaran and Tarjan. In the unweighted case it is exact. For the weighted case the problem becomes NP-hard. I'm not sure what the policy on approximation algorithms is, but weighted_bridge_augmentation solves the problem within an approximation ratio of 2.  Lastly, I was surprised that I couldn't find a NetworkX function to collapse a set of nodes into a single node in a meta graph. There is the condense function for SCCs, but I implemented collapse for arbitrary groupings. Maybe I missed a way to do it that already exists. Currently I plan to keep it in this module as a private function, but I think it might be worth exposing in the NetworkX API.  -----  ### Tasks   - [x] get PR #2554 reviewed, finished, and merged - [x] finalize API - [x] cleanup implementations for k=1 and k=2 (and ensure they are working) - [x] add comprehensives tests - [x] cleanup documentation and docstrings - [x] add examples to docstrings - [x] whatsnew / release notes - [x] add an algorithm for a partial-augmentation - [x] add a greedy "approximation" algorithm for general k
issue
[ENH] Added functions for k-edge-connected components/subgraphs#TITLE_END#(Rewriten on 7-31-2017 to be more concise)  This is the first component of my original PR from #2459 which adds functionality for extracting k-edge-connected components (k-edge-ccs) and k-edge-connected subgraphs (k-edge-subgraphs).  Originally this PR was only k-edge-ccs, but I extened it to include k-edge-subgraphs after realizing the subtle difference (and sorting out the naming conventions) between the two.  ### Definitions   **K-Edge-Connected Component**: A k-edge-cc is a maximal set of nodes that is k-edge-connected *in the original graph*  **K-Edge-Connected Subgraph**: A k-edge-subgraph is a maximal set of nodes from G, whose subgraph is a k-edge-connected *subgraph*  ### API This PR exposes two main functions: `k_edge_components(G, k)` and  `k_edge_subgraphs(G, k)`., which both take a graph and a positive integer k and yields the nodes in the corresponding components/subgraphs.    ### Implementation  Based on the value of `k` and whether or not `G` is directed or undirected, these functions will call different underlying algorithms, attempting to use the most efficient ones available.   When `k=1` both the k-edge-ccs and the k-edge-subgraphs are equivalent, and I reuse the `nx.connected_compoments` and `nx.strongly_connected_components` algorithms for both of them.   When `k=2`, k-edge-ccs and the k-edge-subgraphs are only equivalent for undirected graphs, which I've written an efficient algorithm called `bridge_components` to find.   In all other cases I call general algorithms, which will result in the correct answer, but they may not be the most efficient (for k=3 there are certainly more efficient algorithms known).   Depending on the value of k and whether G is directed or undirected, different algorithms are chosen. So far, I've only implemented a very efficient algorithm for the case where G is undirected and k=2. Both cases where k=1 are already taken care of by existing networkx algorithms, so I re-use those. Otherwise, I call a general algorithm, which is slow but handles all cases.   ### Algorithms  For k-edge-ccs, I've implemented the algorithm in ["A simple algorithm for finding all k-edge-connected components"](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0136264), which is based on AuxillaryGraphs. This algorithm can handle any positive integer value of `k` and works on undirected and directed graphs.   For k-edge-subgraphs, I've implemented the algorithm in ["Finding Maximal k-Edge-Connected Subgraphs from a Large Graph"](https://openproceedings.org/2012/conf/edbt/ZhouLYLCL12.pdf), which is based on recursive subdivisions of global min-cuts of a graph.   For 2-edge-ccs on undirected graphs (also known as bridge-components), I've implemented a simple algorithm by Tarjan, where all bridges are found and removed, and then the remaining CCs are returned.   ### Note on the naming convention When I started this PR I was confused about the naming convention of k-edge-ccs and k-edge-subgraphs. After reviewing several papers in the literature I've concluded that the definitions given above are correct. However, I have found an inconsistency in the  [wolfram library](http://reference.wolfram.com/language/ref/KEdgeConnectedComponents.html), where there k-edge-cc algorithm seems to actually be returning k-edge-subgraphs. I'm not sure why the wolfram function is named the way it is.   ### Summary of tasks   - [x] Implement k-edge-ccs algorithms  - [x] Implement k-edge-subgraphs algorithms - [x] Add tests for k-edge-ccs - [x] Add tests for k-edge-subgraphs  - [x] Add "See Also" docstring blocks to relevant locations - [x] Add .rst documentation for the new module.   I will removing the debugging code after at least one reviewer has OK-ed the general API.
issue
[WIP] edge connectivity algorithms#TITLE_END#This PR is a work in progress. I've submitted it to gauge interest in the proposed functionality. If interest exists, I'll do the work to conform it to networkx standards and test it.   Preface ----------  In my research I've needed to compute properties of graphs based on edge-connectivity. Namely, I needed to find the k-edge-connected components in a graph, and given a graph I needed to compute a k-edge-connected augmentation.  k-edge-connected components are subgraphs of G, where it is impossible to disconnect the subgraph if less than k edges are removed.    k-edge-augmentation takes a graph G that is not k-edge-connected and finds a small set of edges that would make G k-edge-connected if they were added.   My work is primarily concerned with the case where k=2. This special case is also referred to as bridge-connected components, however I want to be able to compute the solutions for other values of k.   Work So Far -----------------  Given my problem, I checked to see if NetworkX had any out of the box solutions. I found a few tools. In the `nx.algorithms.connectivity` module I found the `edge_connectivity` and `local_edge_connectivity` functions to be quite useful. However, most other functions in this module were related to node connectivity. I concluded that NetworkX did not have this functionality (correct me if I'm wrong about that).   Therefore I implemented the edge connectivity algorithms I needed myself internal to my project. Now that the implementation is done, I want to submit it to NetworkX in case anyone else wants to use them.   I'm not sure what the guidelines for contributing are, so I haven't put much work into this PR other than copy pasting my implementations into `nx.algorithms.connectivity` and doing some minor cleanup. I haven't done any tests or even tried to import the modules yet. I'm sure it will error, but before I spend time fixing things up and writing in depth tests I wanted to make sure that this is a desirable PR.   Whats Inside -----------------  I've checked in two modules each with several algorithms.   ### module 1 The first module is `edge_kconnectivity' to be consistent with the existing module `kconnectivity'  `EdgeComponentAuxGraph` finds k-edge connected components for arbitrary k. It takes |V| iterations of max-flow, so its a little slow, but its a simple algorithm that produces the right answer.  `bridge_compoments` solves the problem efficiently for k=2. There's not much too the function. It calls `find_bridges` which essentially find edges that aren't in the chain decomposition, and then removes them. I've had a previous PR rejected because it was essentially a one liner, so I'm not sure that is also the case for this. I think its conceptually different enough to warrant its own function though.   `k_edge_components` combines the previous functions, essentially calling `bridge_compoments` if k is 2 and `EdgeComponentAuxGraph` otherwise. If more efficient algorithms are added (I'm aware of one where k=3) they could be added to this function.    ### module 2 The second module is `edge_augmentation`  I only have efficient algorithms for the case where k=1 and k=2 (although in my project I just wrote a greedy algorithm for k>=3). Each function has weighted and unweighted versions. I'm not sure what the best way to handle the API for that is yet. My working solution takes in a list of available edges that can have optional weights defaulted to 1. Otherwise it is assumed that the complement of G is available and those edges have weights 1.  This module probably needs more work than just cosmetics and it might be better handled as a separate PR.   When k=1, `one_edge_augmentation` basically solves an MST problem, but my implementation solves it without constructing the complement of the graph. The weighted version is done in `weighted_one_edge_augmentation`.   When k=2, `bridge_augmentation` performs an algorithm described by Eswaran and Tarjan. In the unweighted case it is exact. For the weighted case the problem becomes NP-hard. I'm not sure what the policy on approximation algorithms is, but `weighted_bridge_augmentation` solves the problem within an approximation ratio of 2.   Lastly, I was surprised that I couldn't find a NetworkX function to collapse a set of nodes into a single node in a meta graph. There is the condense function for SCCs, but I implemented `collapse` for arbitrary groupings. Maybe I missed a way to do it that already exists. Currently I plan to keep it in this module as a private function, but I think it might be worth exposing in the NetworkX API.
issue
[FIX] fixed divide by zero error in spring_layout#TITLE_END#Fixes #2448 where a spring layout with a single initial position got a dimension size of 0 when there was only one initial value.   My fix simply checks to see if the domain size is zero, and if it is, then it sets it to one. This also addresses the case if multiple nodes are all set to the same (0, 0) position. I added a corresponding test that replicates and tests for the original error. 
issue
[DOC] Added blurb about set_edge/node_attributes#TITLE_END#I noticed that after PR #2553 the new set_edge_attributes and get_edge_attribute API broke my codebase.  I didn't see any documentation about the new API in the migration from 1.x.x to 2.0 doc, so after finding a fix, I added a write-up describing it. 
issue
Added dag functions and fixed PEP8 errors#TITLE_END#Added transitive_reduction, source_nodes, and sink_nodes. Added corresponding tests. Fixed minor PEP8 Errors.  In my projects I've written a bunch of helper functions for networkx and I'd like to start merging  some of the more complete functions into the master branch.   The main part of this pull request is an implementation of transitive_reductions for DAGs.  I also added in helpers for source and sink nodes. I added the corresponding tests and they all seem to pass. I also cleaned up minor PEP8 formatting issues inconsistent with the rest of the file.  
comment
I'm just learning about NXEP3, so I might be missing some context, but I thought I'd offer my $0.02 on this issue.  Having a generator that  only produces nodes and edges lists is a great idea, but I'm against removing the `create_using` parameter. That is partially due for backwards comparability, but mostly because it makes it non-obvious on how to parameterize logic that uses `create_using` without using `getattr`.  My understanding of the proposal is that:   ``` nx.path_graph(3)  # works as is elist = nx.path_graph.edgelist(3)  # is the more efficient edges ```  I think having the above two methods is a great idea, and I'm 100% for it.  But instead of using `nx.path_graph(3, create_using=nx.DiGraph)`  the proposal says that should be replaced by `nx.DiGraph.path_graph(3)`. I think having the latter API is neat and makes a lot of sense, but what I think doesn't make sense is the argument that `create_using` should then be removed to prevent more than one way of doing something from existing.  If I have a function that parameterizes the type of graph I'm using, I want to pass it in as a variable. I don't want to have to do something like `getattr(nx.path_graph, cls.__name__)(3)` to use a generator in function that is parameterized over multiple types of graphs.  This is related to the topic of custom graphs which is also raised, and the syntax: `nx.path_graph.CustomGraph(3, cls)` was proposed. But doesn't this also introduce a second way of doing something? Under this proposal I could be using `nx.path_graph.CustomGraph(3, nx.DiGraph)` or `nx.path_graph.DiGraph(3)`.   So summarize my point, what is the difference between: `nx.path_graph.CustomGraph(3, nx.DiGraph)` and just keeping the original `nx.path_graph(3, create_using=nx.DiGraph)` around?  Thus, I would propose adding the new `.edgelist` attr (I love small efficiency gains like that) and also `.DiGraph`, `.Graph` attrs (I think they make a lot of sense when you _statically_ know what type of graph you want). But for cases when you need to _dynamically_ tell networkx what type of graph to use, I think the `create_using` argument is already the best way of doing that, and it shouldn't be removed.  
comment
While not the most important, a 300 vs 600 milliseconds seems like a non-trivial difference if a system is calling a small python program very often. 
comment
Pretty:  ![image](https://user-images.githubusercontent.com/3186211/98567468-f44ee680-227d-11eb-95bb-02419b15f79e.png)  Perhaps include a link to the quantummagazine article (and the original article) in the docstring? Maybe also include the main results:   In this example we demonstrate a consequence of Ringel's conjecture (now theorum?), which says: Any complete graph with 2n + 1 nodes can be tiled by any tree with n + 1 nodes (i.e. copies of the tree can be placed on the complete graph such that each edge in the complete graph is covered exactly once). We create a complete graph with 13 nodes and tile it with a tree with 7 nodes where each edge in that tree has a unique color. We rotate this tree around the complete graph to color the edges of the complete graph in rainbow pattern.   Something like that. 
comment
@pb-cdunn Nested imports do not import multiple times. There is a global `sys.modules` dictionary that maps module names to loaded modules. The second time around its as fast as a dict lookup (one of the fastest things in Python).   You are right about the not found case. This can be avoided by having some cached function like:  ```python import functools  @functools.lru_cache(None) def _scipy_module():     try:         import scipy     except Exception:         scipy = None     return scipy ```  ```python In [4]: with ubelt.Timer('foo'):     ...:     _scipy_module()     ...:                                                                                                                                  tic('foo') ...toc('foo')=0.0151s  In [5]: with ubelt.Timer('foo'):     ...:     _scipy_module()     ...:                                                                                                                                  tic('foo') ...toc('foo')=0.0000s ```  This avoids the issue with the not-found case. However, how many times are you calling this in the not found case when you don't have the ubiquitous scipy or numpy installed? And how large of graphs are those pure-python methods able to handle? Does this actually matter in the case where graph size is going to bottleneck you before file-system searches? If the answer is No, I think the current `import` style might be simpler and faster in the most common use case.   On numpy, I guess avoiding it is nice in some applications, but parallel computation on long lists of numbers is near-universally helpful. The numpy packages is also significantly smaller and more ubiquitous (a difficult feat to accomplish) than scipy. I typically put import scipy inside functions, but I think there is an argument for assuming numpy globally (because avoiding that dict lookup does still help in critical sections).
comment
> This is a graph library, not a numerics library  There are deep connections between graph theory and linear algebra. In some cases I imagine its faster to compute the answer to a graph question by converting to a matrix representation, solving the reduced problem and converting back. My point is that graphs and numerics are not mutually exclusive. I'm not saying that numpy must be assumed to exist and globally imported, but I am saying there is an argument for it. 
comment
Yeah, I agree that was a mistake. I used to do that with my libraries as well.   Sometimes, I still do like to have something close to that functionality, but I never use `import *` anymore. Instead I use a package I wrote called `mkinit` (https://github.com/Erotemic/mkinit) that autogenerates explicit `__init__.py` files that have a similar effect to `import *` (it statically introspects module attributes and includes them depending on command line arguments and special dunder variables in the `__init__.py` itself).  If you are looking for workarounds, you could try using `mkinit` to generate the `__init__.py` files, which would give you backwards compatible behavior that you could gradually move away from. In PR #4349 I create two new submodules and there is a comment that documents how I used mkinit to generate each of them: (e.g. https://github.com/networkx/networkx/blob/b6e6c4633f3c1b80c7084d8976edf8386fcf9e67/networkx/algorithms/minors/__init__.py#L20) 
comment
I did a grep for "_iter" in examples and found:   ``` subclass/plot_antigraph.py:        nd_iter : iterator subclass/plot_antigraph.py:                for n in self.nbunch_iter(nbunch) subclass/plot_antigraph.py:    def adjacency_iter(self): subclass/plot_antigraph.py:        adj_iter : iterator ```  It looks like `nbunch_iter` is still a thing? I did another grep in the repo root, and I didn't see anything else outstanding.    
comment
@dschult @jarrodmillman @rossbar   I saw this merge in the git history I was really excited that the `__str__` was finally going to give some information about the graph, but then I was a little bit disappointed when I saw the prose-like formatting of the string and that `__repr__` still doesn't give any information.   While I love that the str representation of a graph is now going to have some info associated with it, wouldn't it be better if it looked something more like `DiGraph(n_nodes=5, n_edges=4)` instead of the current `DiGraph with 5 nodes and 4 edges`?   Also, wouldn't it be nice if the repr was similar and looked like `<networkx.classes.digraph.DiGraph(n_nodes=5, n_edges=4) at 0x7f2b41f72d30>`.   I see there was some discussion of this in #4193 but I hope its not too late for me to chime in with a rather strong opinion that this formatting should change before this feature is published to pypi. When parsing logfiles and stdout having a more programming like style of summarizing information (i.e. having the parens encapsulate the info) makes it much easier to deal with. I feel like aving a prose-like string format will at best look out of place in log files and stdout and at worst cause issues with searching / parsing.  At the very least, It would be nice if the repr gave information like the way I described. 
comment
 Thanks for tackling this PR. This LGTM from a functional perspective.
comment
Perhaps this may be fixed by moving some of the logic in release.py to setup.py using some static parsing? In one of my modules, my setup.py looks in the __init__.py file and statically parses the value of `__version__`. Here is that code adapted to networkx:  ```python def parse_version():     """ Statically parse the version number from version.py """     from os.path import dirname, join     import ast     init_fpath = join(dirname(__file__), 'networkx', 'version.py')     with open(init_fpath) as file_:         sourcecode = file_.read()     pt = ast.parse(sourcecode)     class VersionVisitor(ast.NodeVisitor):         def visit_Assign(self, node):             for target in node.targets:                 if target.id == 'version':                     self.version = node.value.s     visitor = VersionVisitor()     visitor.visit(pt)     return visitor.version ```  The above function assume you are running from `setup.py`.   If the release.py code isn't actively being used in the networkx module, this may be a good alternative. 
comment
IMO:  1. It seems like the self-loop functions are too specific to belong to the base graph class. I think it would be better to call them via `nx.selfloop_edges`.   2. It took me by surprise when `G.edge` and `G.node` were changed. It is probably better to remove them entirely. They're already broken from the perspective of a `1.x` user, so may as well just throw the `AttributeError`. (On a side note, I didn't know about `G.edges.data()`, I have to say I like it).   3. I actually do like having `neighbors/predecessors/succesors`, and I'd vote for keeping them. As for telling the difference between 1.x and 2.x, isn't that what `nx.__version__` is for?  4. I'm opposed to moving `G.subgraph` to its own function. It feels like a very fundamental operation for me and it applies to Graphs, DiGraphs, and MultiGraphs. Removing this would be like removing `set().issubset`.  As for `reverse`, I sort of like it as a method, but its only valid for Digraphs, so I'm not against moving it to a function. I'm similarly ambivalent about the `to_directed/to_undirected` API. However, I'll reiterate my strong feelings on keeping `G.subgraph`. 
comment
I think its the abbreviation / full name.   What I like about neighbors/predecessors/successors comes down to semantics. When I'm coding I typically do it with IPython, and I'll type `nx.` and then try a few keywords plus tab to find something that looks like the function I'm looking for. Neighbors, predecessors, and successors are very intuitive, I think using them makes the code more readable as well.   To me, this: ``` python for node in G.neighbors(n):     dostuff(node) ```  is more clear than this: ```python for node in G.adj[n]:     dostuff(node) ```  I've used NetworkX in a labs as a TA many times, and I always urge them to use networkx as their basic data structure (after they've already implemented their own adjacency list data structure). For instance when a student is struggling to implement DFS, I walk them through it. "Now, you need to get the neighbors of the current node. How might you do that?"  ... they struggle ... "Well what about the neighbors function?" ... and then things start to click for them. This is anecdotal, but I feel like the nice semantics of networkx are one of its strongest selling points.   There is something to be said about confusion that might arise because `G.neighbors(n)` and `list(G.adj[n])` doing the exact same thing, so my position isn't staunch.  
comment
I think `G.adj[n]` is better than `G.nbrs[n]` because once you start abbreviating its a sign that things are getting more technical. (Also I've always disliked nbrs as an abbreviation for neighbors). NetworkX is really nice for addressing graph theory related tasks. From a teaching "Intro to Algorithms" perspective,  having the `G.adj` structure makes a lot of sense because it shows the students how methods like `G.neighbors(n)` are working under the hood.   So, I would always prefer the graph theory term over some other term when it comes to the underlying data structures. The more semantic and un-abbreviated names are better left for convenience methods/functions.   Anecdote: When I was formalizing my understanding of graph theory I actually learned about adjacency matrices by looking at the source code for G.neighbors (or maybe a similar function, it was awhile ago), but the point is the semantic name led me to the underlying graph theory term.   On `nbunch` vs `ebunch`, I'm pretty ambivalent. If I recall correctly, many functions that take a `*bunch` argument can either accept a single node/edge or a sequence of nodes/edges? That is an argument for leaving the `*bunch` name, so the plural nodes/edges always refer to a list of items. On the other hand `nodes/edges` are much more intuitive when specifying keyword arguments. Either way its extremely minor because argument names are so rarely exposed. If I had to vote I'd probably just keep `*bunch` due to the principle of least action, but this vote should be considered with an effective weight of ε. 
comment
@calmofthestorm Any chance this PR might be finished? This might help speed up one of my algorithms in #2572
comment
I think I can definitely reuse part of this functionality. Thanks for the ping. I'm probably going to work on my PR a bit next week. 
