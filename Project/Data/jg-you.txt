issue
[Linealg] Reorder and complete doc#TITLE_END#Tiny doc PR as mentioned in PR #3401  
issue
Add bethe hessian matrix#TITLE_END#The Bethe Hessian matrix or deformed Laplacian matrix generalizes the graph Laplacian.  It is great for clustering, and also for counting the number of communities in dense networks, see:  [Saade, Alaa, Florent Krzakala, and Lenka Zdeborová. "Spectral clustering of graphs with the bethe hessian." Advances in Neural Information Processing Systems. 2014.](https://arxiv.org/abs/1406.1880)  and [Le, Can M., and Elizaveta Levina. "Estimating the number of communities in networks by spectral methods." arXiv preprint arXiv:1507.00827 (2015).](https://arxiv.org/abs/1507.00827).  This PR implements this family of matrices and its spectrum, for undirected networks, with tests and documentation.
issue
Move LFR_benchmark to generators#TITLE_END#This addresses issue #3404 . LFR_bennchmark_graph and its tests are now in the generators submodule.  Please review how the code is organized, there's a lot of auxiliary "private" functions. I placed them right above `LFR_benchmark_graph` that uses is. It also used an ImportError to check if scipy is around. I put that at the top of the file (generators.community).  Note that I slightly modified the doc string to make them PEP compliant (summary on one line).  (Sorry for the earlier PR. I had forked off of the wrong branch)
issue
LFR benchmark in wrong location#TITLE_END#Following the convention of the module, I'd expect to find LFR_benchmark (and the whole content of that file) under networkx.generators.community instead of networkx.algorithms.community.community_generators
issue
Relocate LFR_benchmark_graph#TITLE_END#This addresses issue #3404 . LFR_bennchmark_graph and its tests are now in the generators submodule.  Please review how the code is organized, there's a lot of auxiliary "private" functions. I placed them right above `LFR_benchmark_graph` that uses is. It also used an ImportError to check if scipy is around. I put that at the top of the file (generators.community).  Note that I slightly modified the doc string to make them PEP compliant (summary on one line).
issue
Add see also links to linalg.spectrum#TITLE_END#Very simple doc PR. While prepping PR #3401 I noticed that the doc was missing see also links from the matrices definition to their equivalent in linalg.spectrum. This fixes it.  On a side note: There's a lot of doc inconsistencies in the linealg module BTW. algebraicconnectivity.py and attrmatrix.py don't follow the standard at all.
issue
Add stochastic block models to generators#TITLE_END#This is fairly straightforward PR, I've implemented the classical "binomial" SBM in `generators/community.py`.  There's a sparse switch that will create edges by jumping ahead by a random number of  non-edges, drawn from a geometric distribution. This is the default. In the dense case, I use a Bernoulli test on each edges.  Note that with this PR,  `random_partition_graph` could be refactored as a call to `stochastic_block_model` (as well as `gnp_random_graph` and other Erdos Renyi type graphs).  
issue
Maintenance: Outdated bilbilography#TITLE_END#Will drafting the `CONTRIBUTING` file (see issue #1480 ), I realized that [doc/source/bibliography.rst](https://github.com/networkx/networkx/blob/master/doc/source/bibliography.rst) is largely outdated and does not contain most of the references. It should be either auto generated from the `References` sections of the doc, or brought up to date and manually maintained with every new feature release (ouch). 
issue
Small feature proposal (feature request)#TITLE_END#The function / method `size()` is quite useful because it allows the user to sum an edge attribute "on the fly", e.g..:  ``` python G = nx.Graph() G.add_edge(0, 1, weight=2) G.add_edge(0, 2, weight=2) G.add_edge(0, 3, weight=10) print(G.size(weight='weight')) >>> 14 print(G.size(weight='width')) >>> 3 ```  I'm pretty sure that there is no equivalent for node attributes. Would it be a useful feature?  The code is quite simple. If the default attribute value is 0, then something along the line of  ``` python def sum_node_attribute(G, attribute): """Documentation string here."""     return sum([v for k, v in nx.get_node_attributes(g, attribute).items()]) ```  should suffice. A simple conditional clause could deal with different defaults.  At least it's a function I found myself writing a lot, since I started dealing with annotated graph way more often. 
issue
Write a CONTRIBUTING file#TITLE_END#Suggested in https://github.com/networkx/networkx/issues/1428, but I feel like it needs a separate issue.  It would be good if a maintainer or a core developer could take the time to write a proper `CONTRIBUTING` file that explains - the desired code style; - the preferred choice of python statements when multiple are available (e.g. https://github.com/networkx/networkx/issues/1479); - the proper way of implementing core that do not come with classes (e.g. partitions); - the desired code organization (what goes where); - the current main development targets.  And probably other things that I can't come up with just now. 
issue
Add approximation import#TITLE_END#This submodule is not accessible when one imports networkX with `import networkx as nx`. Is there a particular reason _not_ to import this submodule with the rest of the package? 
issue
Random graph optimization opportunity#TITLE_END### TL:DR - `gnp_random_graph` is faster than `fast_gnp_random_graph` for `p>0.2`, **and vice-versa**. - I propose that this limit should be hard-coded by default, with an _optional flag_ to override it. - There's some tests at the bottom.  ## Rambling  I've been looking into implementing the stochastic block model (will come in a later pull request), which eventually lead me to the source code of the [G_{n,p} network genreator](https://github.com/networkx/networkx/blob/master/networkx/generators/random_graphs.py#L124-L178), and it's [fast counterpart](https://github.com/networkx/networkx/blob/master/networkx/generators/random_graphs.py#L47-L121). After running some test, it appears that there is potential for optimization.  The regular algorithm iterates over the `N(N-1)/2` potential edges and test if they exists with probability `p`. In principle its therefore of complexity `O(N^2)`, but it has a strong dependency in `p` because no memory operation is required when a link is tested "negative". The fast algorithm is of complexity `O(n+m)` with `m` the expected number of link. It basically samples the distribution of delay between each positive test, and incorporates some fancy index tricks.  For any given `p`, the number of memory operations is -- by definition -- the same, on average. This allows us to factor out the memory cost of both methods. Choosing the most efficient method ("fast" vs. regular) is then only a matter of testing whether it's faster to sample a list of length `N**2` geometrically or normally. There is no doubt that the geometric method is faster in extremely sparse cases, but there must be a `p` such that the overhead entailed by the geometric sampler outweighs its benefits.  The outcome is _bound_ to be influenced by the environment (e.g. python version, hardware), but it's possible to come up with a rule of thumb that could be implemented directly in networkx. More precisely, for all `p` under the lower bound, `gnp_random_graph` could call `fast_gnp_random_graph` under the hood, and vice-versa.  Based on my test I believe that this value should be `p=0.2`. Because the transition point depends on the environment in non trivial ways, it should be overridable, but it could still be provided as a default. ## Test code  ``` python from __future__ import division import numpy as np import networkx as nx import time import sys import math  def mean(l):     return sum(l)/len(l)  def stdev(l, avg):     return math.sqrt(sum([ (x - avg) ** 2 for x in l])/(len(l)-1))  def main():     current_milli_time = lambda: int(round(time.time() * 200))     N = int(sys.argv[1])     iterations = int(sys.argv[2])     for p in np.arange(0.005, 0.30, 0.005):         simple_times = []         fast_times = []         for i in range(0, iterations):             start_t = current_milli_time()             G = nx.gnp_random_graph(N, p)             simple_times.append(current_milli_time() - start_t)             G.clear()              start_t = current_milli_time()             G = nx.fast_gnp_random_graph(N, p)             fast_times.append(current_milli_time() - start_t)             G.clear()          m1 = mean(simple_times)         m2 = mean(fast_times)         print("{0:.3f}".format(p) + "\t" +               "{0:.8f}".format(m1) + "\t" +               "{0:.8f}".format(stdev(simple_times, m1)) +"\t" +               "{0:.8f}".format(m2) + "\t" +               "{0:.8f}".format(stdev(fast_times ,m2)))  if __name__ == '__main__':     main() ``` ## Results ### Changing the graph size  ![Python 3.4, different lengths](https://cloud.githubusercontent.com/assets/6925003/7192080/6808816a-e460-11e4-9f0d-efdcd9348ff0.png)  The most important of all: comparison of the 2 methods for **various list length**   ```  N      |  # of edges 100     |  ~10 000 200     |  ~40 000 400     |  ~160 000 ```  **There is a small dependency in `N`**; as N grows, the optimal op decrease ever so slightly. Might be noise, might a real effect.  Nonetheless, p=0.20 seems like a good rule of thumb. ### Time dispersion of both methods  ![Python3.4_stdev](https://cloud.githubusercontent.com/assets/6925003/7192079/680837e6-e460-11e4-865e-0b5658daa0e3.png)  The time dispersion is similar for both methods, i.e. running times are concentrated tightly around the average.  ### Environment's effect  ![Python 2.7.5 vs Python 3.4.2](https://cloud.githubusercontent.com/assets/6925003/7192082/680bdd06-e460-11e4-90b7-f8d8db952880.png)  The code runs slightly faster on `python3.4`, but the speed-up is much more pronounced for the normal method, which relies on plain iterators.  ![[Python 2.7.5, different hosts]](https://cloud.githubusercontent.com/assets/6925003/7192073/64d50130-e460-11e4-8285-6000297dd9af.png)  The hosts affect the time (obviously), **and the transition point :/**. All tests are performed on the fastest host (host B).  ![Python 3.4.2, optimization flag](https://cloud.githubusercontent.com/assets/6925003/7192081/68098588-e460-11e4-8d17-defa5a02dd00.png)  The `-O` flag doesn't change anything, as expected. 
issue
Directed modularity#TITLE_END#Related to Pull Request https://github.com/networkx/networkx/pull/1448  This pull request implements the directed modularity as defined in E. A. Leicht, M. E. J. Newman,        "Community structure in directed networks",  Phys. Rev Lett., vol. 100, no. 11, p. 118703, 2008. ([arXiv link, see eq.3](http://arxiv.org/pdf/0709.4500v1.pdf)).  **Changes** - `directed_modularity_matrix` - associated documentation - associated spectrum functions - tests:`test_directed_modularity` (added), `test_modularity_spectrum` (updated)  I also fixed some typos and added an example in the  `modularity_matrix` function documentation.  You'll find that the definition of the modularity does not correspond exactly to the one of Leicht and Newman. NetworkX defines the element A_ij of the adjacency matrix as 1 if there is a link going from node i to node j. Leicht and Newman use the opposite definition.  Note that I followed the design of [networkx/linealg/laplacianmatrix.py](networkx/linealg/laplacianmatrix.py) and implemented the directed modularity matrix in its own function. It can be moved within `modularity_matrix` if requested. 
issue
Modularity matrix#TITLE_END#This pull request implements the modularity matrix (MEJ Newman, 2006, PNAS) with associated test suites and spectrum operations. I did not implement any weighted/multigraph/directed methods since there is no consensus on the definition, to the best of my knowledge.  This is my first contribution to NetworkX, comments are more than welcome. 
issue
Copyrights fix#TITLE_END#This runs `sed -i "s/2004\-20[0-9][0-9]/2004\-2015/g"` on the whole repository. 
issue
Bug with dev version of write_gml#TITLE_END#I've stumbled upon a "bug" that involves `write_gml`  ``` jgyou:networkx-master$> python2.7 >>> import networkx as nx >>> nx.__version__ '2.0.dev_20150412140538' >>> G=nx.random_partition_graph([20,20],0.6,0.1) >>> G.graph["partition"] [set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]), set([32, 33, 34, 35, 36, 37, 38, 39, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])] >>> nx.write_gml(G,path="test_path.gml") Traceback (most recent call last):   File "<stdin>", line 1, in <module>   File "<string>", line 2, in write_gml   File "networkx/utils/decorators.py", line 220, in _open_file     result = func(*new_args, **kwargs)   File "networkx/readwrite/gml.py", line 717, in write_gml     for line in generate_gml(G, stringizer):   File "networkx/readwrite/gml.py", line 636, in generate_gml     for line in stringize(attr, value, ignored_keys, '  '):   File "networkx/readwrite/gml.py", line 613, in stringize     for line in stringize(key, value, (), next_indent, True):   File "networkx/readwrite/gml.py", line 623, in stringize     raise NetworkXError('%r is not a string' % (value,)) networkx.exception.NetworkXError: set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]) is not a string >>> nx.write_gml(G,path="test_path.gml",stringizer=repr) >>> quit() jgyou:networkx-master$> head test_path.gml  graph [     partition "set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"     partition "set([32, 33, 34, 35, 36, 37, 38, 39, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"   node [     id 0     label 0   ]   node [     id 1 ```  Essentially `write_gml` fails to convert `G.graph["partition"]` to a string unless a stringizer is specified explicitly. Is this intended? Shouldn't the default stringizer be set to `repr`? Or perhaps the representation of a partition changed to something stringable (`list`)? The latter would break the API contract slightly. 
comment
I agree with @hagberg. Multiple logos could be nice! Here's an alternate proposal.  [Removed, too heavy] 
comment
@ysitu Yes 
comment
@OrkoHunter Great suggestion, looks way better with a thinner "N" @sieben  I don't really see how it could be done :/.  ![networkx_logo_thin](https://cloud.githubusercontent.com/assets/6925003/7216083/ccb0dbf2-e5bf-11e4-865e-29e6d243b346.png) 
comment
@chebee7i  It's a svg vector image. 
comment
Sorry for the spam, but here is yet another version. A (still) thinner version seems to be better! I included some favicon examples as well.  ![v3](https://cloud.githubusercontent.com/assets/6925003/7244101/4f2b41e0-e7a4-11e4-87a4-ae2a5515038e.png)  If there is interest toward making this official or temporary, let me know where I need to place the favicon, svg version, full res version, etc.  Comments are more than welcomed! 
comment
![g9167-1](https://cloud.githubusercontent.com/assets/6925003/7600341/39e8ce2a-f8d8-11e4-8f0c-25792c024dd1.png)  @SanketDG Sorry for the delay, had not checked on this issue for a while.  I feel like the "large X version" is somehow more balanced. Or it could be that I'm not good at kerning and messed up the "small x version". 
comment
![nx](https://cloud.githubusercontent.com/assets/6925003/8123144/447621ae-1092-11e5-8feb-7562d5c6805b.png)  ![nx_text](https://cloud.githubusercontent.com/assets/6925003/8123147/5b5e0134-1092-11e5-9be3-3093509a71d3.png) 
comment
And here is the svg, sorry about the slow response time.  https://github.com/jg-you/jg-you.github.io/blob/master/img/nx.svg 
comment
A good start would be a proper `CONTRIBUTING` file which explains the desired code style, the development philosophy, current targets, etc. The website "only" explains how to use git, which is not that helpful when it comes to _how_ to write your code, _where_ to place it, _what_ to code. 
comment
:+1: on a pre-2.0 release with deprecated interfaces still available. :+1:  on `1.10` too. 
comment
:+1:  It even keeps the same iteration order. Might as well change the directed version for  ``` python for u, v in itertools.product(graph, 2):     if not graph.has_edge(u, v):         yield (u, v) ```  for consistency. I wonder what are the performance implication of that second change. 
comment
@hagberg `non_neighbors` must be computed for each node. So I believe it merely hides away the complexity of the former method. 
comment
@hagberg 's suggestion is the best for both empty and complete graphs (the 2 extremes).  ``` python jgyou:~$> ipython Python 3.4.3 (default, Mar 25 2015, 17:13:50)  Type "copyright", "credits" or "license" for more information.  IPython 3.1.0 -- An enhanced Interactive Python. ?         -> Introduction and overview of IPython's features. %quickref -> Quick reference. help      -> Python's own help system. object?   -> Details about 'object', use 'object??' for extra details.  In [1]: import networkx as nx  In [2]: import itertools  In [3]: def non_edges(graph):                                                                                                          nodes = set(graph)                                                                                                          while nodes:                                                                                                                        u = nodes.pop()                                                                                                                 for v in nodes - set(graph[u]):                                                                                                           yield (u, v)    ...:               In [4]: def non_edges_2(graph):                                                                                                         for u, v in itertools.combinations(graph, 2):                 if not graph.has_edge(u, v):                                                                                                        yield (u, v)    ...:               In [5]: G = nx.complete_graph(30)  In [6]: %timeit list(nx.non_edges(G)) 1000 loops, best of 3: 213 µs per loop  In [7]: %timeit list(non_edges(G)) 10000 loops, best of 3: 65.1 µs per loop  In [8]: %timeit list(non_edges_2(G)) 10000 loops, best of 3: 131 µs per loop  In [9]: G = nx.Graph()  In [10]: G.add_nodes_from(range(0,50))  In [11]: %timeit list(nx.non_edges(G)) 1000 loops, best of 3: 1.14 ms per loop  In [12]: %timeit list(non_edges(G)) 1000 loops, best of 3: 198 µs per loop  In [13]: %timeit list(non_edges_2(G)) 1000 loops, best of 3: 454 µs per loop ``` 
comment
I don't think that's a bug; you're code is not complete. See [this example from the documentation](https://networkx.github.io/documentation/latest/examples/drawing/weighted_graph.html). You need to add a call to [`draw_networkx_labels`](https://networkx.github.io/documentation/latest/reference/generated/networkx.drawing.nx_pylab.draw_networkx_labels.html?highlight=draw_networkx_labels). 
