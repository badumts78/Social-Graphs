issue
MixedGraph and MixedMultiGraph#TITLE_END#I don't have time now, but over the summer I might look into implementing some basic mixed-type graph classes.  Right now, we have graph classes that require all edges to be of the same type---all undirected, or all directed.  This is, by far, the most common use case.  We've also had a few requests to be able to create mixed graph types.  The normal response we give is to create two graphs and analyze them separately, but this is quite awkward in a number of important cases. Specifically, I have in mind Bayesian networks.  If implemented, these would be `MixedGraph` instances whose total set of edges (directed and undirected) have no loops. There are a number of algorithms in this area that require one to traverse the graph while ignoring the direction of the edge, or in other cases, while allowing an undirected edge to be traversed in either directed manner. Generally, the undirected edges mean that the direction is unspecified, but you can't just toss in two directed edges b/c this would affect path algorithms---between any two nodes we are required there there be, at most, only one edge (hence MixedGraph and not MixedMultiGraph).  Now maybe this will end up being too specific for NetworkX, but I'm curious to see if we can come up with something nice.  Even if it ultimately just has:  `self.pred, self.succ` for directed edges and `self.adj` for undirected edges, that's probably better than two graphs---but you can already see why this will be difficult to work with since almost every algorithm we have in NetworkX treats succ and adj as the "same" (just with different interpretations).  I'm reminded of how [sparse.csgraph](http://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html#directed-vs-undirected) is quite flexible here, allowing algorithms to work with a directed graph in an undirected fashion, if requested.  More of this in NetworkX would probably be helpful for mixed graphs. 
issue
Better shortest paths for Multigraphs...#TITLE_END#Most (all?) of the shortest path algorithms do not give sufficient information for multigraphs.  In those situations, knowing the source and target is not enough---one must also know the key of the edge.  These algorithms touch _everything_ though. So I'm not sure it's wise to change them. Perhaps though, we can offer more general implementations and then have the existing implementations use them to create the outputs?  Also, its often that case that one is interested in the edges of the shortest path. In situations like these, it might even be simpler to get output of the form:  [(u,v,key,weight),...].  Without the weight, you'd have to query the graph again.  Obviously, we can't cater to every possible output format, but I think the issue with multigraphs is serious enough that we should do something. For the various outputs, perhaps we can provide some helper functions for typical cases.  Thoughts on this? 
issue
Sparsification of Graphs#TITLE_END#It might be nice to get some functions in NetworkX for sparsifying graphs. cut sparsification and spectral sparsification are two options. For example,  http://arxiv.org/abs/0808.4134  Spectral Sparsification of Graphs Daniel A. Spielman, Shang-Hua Teng  We introduce a new notion of graph sparsificaiton based on spectral similarity of graph Laplacians: spectral sparsification requires that the Laplacian quadratic form of the sparsifier approximate that of the original. This is equivalent to saying that the Laplacian of the sparsifier is a good preconditioner for the Laplacian of the original. We prove that every graph has a spectral sparsifier of nearly linear size. Moreover, we present an algorithm that produces spectral sparsifiers in time $\softO{m}$, where m is the number of edges in the original graph. This construction is a key component of a nearly-linear time algorithm for solving linear equations in diagonally-dominant matrcies. Our sparsification algorithm makes use of a nearly-linear time algorithm for graph partitioning that satisfies a strong guarantee: if the partition it outputs is very unbalanced, then the larger part is contained in a subgraph of high conductance. 
issue
Incremental topological sort and strongly connected components.#TITLE_END#Recently, I needed an incremental algorithm for detecting strongly connected components. NetworkX does not currently have such functionality. Here are two papers that describe an algorithm. An incremental algorithm for topological sorts is also described:  http://www.cs.princeton.edu/~sssix/papers/dto-journal.pdf http://homepages.ecs.vuw.ac.nz/~djp/files/tr0903.ps  Previously, we had a incremental implementation for detecting connected components, but it was never integrated into NetworkX proper. So if someone takes this issue on, we should look into adding the code in that ticket (or some modification of it) as well:  https://networkx.lanl.gov/trac/ticket/457 https://networkx.lanl.gov/trac/changeset/1464/networkx 
issue
Node attr dict values container not customize-able#TITLE_END#For subclasses of the primary graph classes, it doesn't look like we allow the developer to specify a factory for the values of `G.nodes`. The code does: ``` self._node[node_for_adding] = attr ``` instead of calling a user-provided factory to create the object and then doing `obj.update(attr)`. Similarly for the code in `add_edge` etc. This seems like an unintended omission given that we allow developers to specify a `edge_attr_dict_factory`.  Similarly, it doesn't look like we allow the developer to control `self.graph`.
issue
Node attr dict not customizable#TITLE_END#For subclasses of the primary graph classes, it doesn't look like we allow the developer to specify a factory for the values of `G.nodes`. The code does: ``` self._node[node_for_adding] = attr ``` instead of calling a user-provided factory to create the object and then doing `obj.update(attr)`. This seems like an unintended omission given that we allow developers to specify a `edge_attr_dict_factory`.  Similarly, it doesn't look like we allow the developer to control `self.graph`.
issue
Node attr values are not customizable for subclasses.#TITLE_END#For subclasses of the primary graph classes, it doesn't look like we allow the developer to specify a factory for the values of `G.nodes`. The code does: ``` self._node[node_for_adding] = attr ``` instead of calling a user-provided factory to create the object and then doing `obj.update(attr)`. This seems like an unintended omission given that we allow developers to specify a `edge_attr_dict_factory`.  Similarly, it doesn't look like we allow the developer to control `self.graph`.
issue
Note attr dict values is not customizable for subclasses#TITLE_END#(This is attempt #3 to create this ticket...hopefully it works)  For subclasses of the primary graph classes, it doesn't look like we allow the developer to specify a factory for the values of `G.nodes`. The code does: ``` self._node[node_for_adding] = attr ``` instead of calling a user-provided factory to create the object and then doing `obj.update(attr)`. This seems like an unintended omission given that we allow developers to specify a `edge_attr_dict_factory`.  Similarly, it doesn't look like we allow the developer to control `self.graph`.
issue
Node attr dict values are not customizable. #TITLE_END#For subclasses of the primary graph classes, it doesn't look like we allow the developer to specify a factory for the values of `G.nodes`:  ``` self._node[node_for_adding] = attr ``` Instead, we could call a user-provided factory to create the object and then do `obj.update(attr)`. This seems like an unintended omission given that we let developers specify `edge_attr_dict_factory`.  Similarly, it doesn't look like we allow the developer to control the object `self.graph`. 
issue
Node attr dict values container not customize-able#TITLE_END#For subclasses of the primary graph classes, it doesn't look like we allow the developer to specify a factory for the values of `G.nodes`. The code does: ``` self._node[node_for_adding] = attr ``` instead of calling a user-provided factory to create the object and then doing `obj.update(attr)`. Similarly for the code in `add_edge` etc. This seems like an unintended omission given that we allow developers to specify a `edge_attr_dict_factory`.  Similarly, it doesn't look like we allow the developer to control `self.graph`.
issue
Node attr value container not customize-able#TITLE_END#For subclasses of the primary graph classes, it doesn't look like we allow the developer to specify a factory for the values of `G.nodes`. The code does: ``` self._node[node_for_adding] = attr ``` instead of calling a user-provided factory to create the object and then doing `obj.update(attr)`. Similarly for the code in `add_edge` etc. This seems like an unintended omission given that we allow developers to specify a `edge_attr_dict_factory`.  Similarly, it doesn't look like we allow the developer to control `self.graph`.
issue
Expanded datatypes for graphml writer#TITLE_END#The GraphML writer doesn't handle NumPy floats since it only knows to accept Python floats. I'm sure we can make this a bit easier for users. 
issue
Interface for randomness#TITLE_END#In various places, we make use of random number generators. Due to our pure Python tilt, we've tended to use the `random` module. Some functions accept seeds as input, others (I have not checked) might not provide that option at all.  This briefly came up in https://github.com/networkx/networkx/issues/1341#issuecomment-76645971, and maybe it's time to dig in a bit further.  My opinion is that we should not use the `random` module. While it has satisfied our needs thus far, it does not allow enough flexibility since 1) there is always a global state and 2) there aren't many distributions provided. The ability to have different random number generator instances requires that we move away from using the `random` module.  Instead, I think any function that requires randomness should have a hard dependency on NumPy and that we should standardize on NumPy's `RandomState` API. This isn't that much of a burden since most installations can easily obtain NumPy, and a good deal of NetworkX now relies on NumPy anyway. This is also what _most_ Python packages do if they require randomness. See for example, [scikit-learn-#random-numbers](http://scikit-learn.org/stable/developers/#random-numbers). The idea is that functions/classes/methods will have a parameter, `prng` or `random_state` (as in `sklearn`) or `seed`.  We then use that to retrieve a random number generator using something identical to [check_random_state](http://scikit-learn.org/stable/modules/generated/sklearn.utils.check_random_state.html#sklearn.utils.check_random_state) (but with a better name). Then the function uses that random number generator. This will make code more robust and more easily tested. #1523 will be affected by this decision. 
issue
Normalize node variable in function signatures.#TITLE_END#For methods/functions like `G.add_node` and `G.add_edge`, we allow arbitrary attributes to be passed in at the same time. The name choice for the arguments essentially excludes them from being keyword arguments at call time.  For example, `G.add_node` does not allow passing attributes with names `n` or `attr_dict`, while `G.add_edge` does not allow attributes with names `u` or `v`, or `attr_dict`.  We probably should mention somewhere that `n`, `u`, `v`, `attr_dict` are "special" attribute names that cannot be specified in some function signatures, but can still be added via other routes.  I'm wondering if we should normalize usage and always use `u` whenever we are using `n`. This would make `u`, `v`, and `attr_dict` the only special attribute names. This frees us `n` which is perhaps a more common choice for an attribute name having to deal with "the number of". 
issue
Fix bug in find_cycle.#TITLE_END#Fixes https://github.com/networkx/networkx/issues/2323
issue
Make add_edge return key for multiedges.#TITLE_END#I think this may have been brought up long ago, but I couldn't remember.  Edit 05-2016: #278  Presently, when you add an edge to a multi(di)graph and do not specify the edge key, then we will construct one for you. The constructed key is not returned back to the user, so the user has no feedback or simple way to obtain the edge data for the added edge.  Should we return the constructed key for multi(di)graphs?   ``` >>> g = nx.MultiGraph() >>> g.add_edge(0, 1, weight=5) 0 >>> g.add_edge(0, 1, weight=10) 1 >>> g.get_edge_data(0, 1, key=1) {'weight': 10} ```  Users may not care about the details of how a particular key is assigned, but they do care about the particular assignments. 
issue
shortest paths for DAGs#TITLE_END#As was pointed out in https://github.com/networkx/networkx/pull/1118#issuecomment-41171119, the algorithm for shortest paths on DAGs is straightforward, but DAGs seem common/important enough to warrant official support. The main difference would be in the single-source or single-target portions of the code to achieve the O(V + E) running time. They could be implemented in `networkx/algorithms/shortest_path/dag.py` but exposed at `networkx.dag.shortest_path`, etc.  This would also complement other DAG algorithms like `longest_path` (#847).  What are general thoughts on this?  Aside: I still wish there was a better interface for the shortest_path code, see https://github.com/networkx/networkx/pull/762#issuecomment-8405360.  That situation makes me hesitant to (essentially) duplicate the code in `networkx/algorithms/shortest_path/generic.py`. 
issue
Update `write_gml` docstring.#TITLE_END#See discussion in #1449.  I think it would be helpful if we added a few comments to the docstring to make some of the conversion issues more transparent for `write_gml`. 
issue
Use napolean instead of numpydoc#TITLE_END#Napolean is an alternative legible docstring Sphinx extension:     http://sphinxcontrib-napoleon.readthedocs.org  that supports both the NumPy and Google docstring formats.  It also supports "Yields" sections in docstrings, whereas `numpydoc` does not (as of now). 
issue
Protect master?#TITLE_END#Should we protect master now?  You can disable merges unless the status checks pass, but you can also require at least 1 "review".  This is a new feature in GitHub. 
issue
Fix spring_layout for single node graph.#TITLE_END#Fixed #2079 
issue
TravisCI and miniconda 3.2#TITLE_END#First reported in #1340.  The TravisCI tests for 3.2 and miniconda do not seem to be working. See the log here: https://travis-ci.org/networkx/networkx/jobs/60119203 
issue
Add clique?#TITLE_END#We have methods for adding paths, cycles, but not cliques. Should we add this? 
issue
Disallow robots on networkx.lanl.gov#TITLE_END#Occasionally, Google still returns results pointing directly to lanl.gov pages, even though we are now redirecting the root page directly to the github doc site. Perhaps we should add:  ``` User-agent: * Disallow: / ```  to `/robots.txt`. The trac site is still live and helpful for historical queries. Trac does have it's own search, so perhaps we don't need it indexed? If we do, I think the following works:  ``` User-agent: * Allow: /trac Disallow: / ``` 
issue
Maximum Spanning Tree#TITLE_END#We have a minimum spanning tree, but it would be nice for users if we also had a maximum spanning tree. Yes, you can invert the weights, but that's a bit of a burden and wasteful when we can just change the sort order. Probably we can create a base function called optimum_spanning_edges() and then make the other two call it.  (Apologies for all the small issues...I can jump on them at some point soon, but I wanted to get them reported in case I forget) 
issue
Update docstrings for `cartesian_product` and `tensor_product`.#TITLE_END#The documentation for `cartesian_product` and `tensor_product` could use some updates. For example, see:  https://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.operators.product.cartesian_product.html#networkx.algorithms.operators.product.cartesian_product 
issue
Build documentation on ReadTheDocs#TITLE_END#This PR allows ReadTheDocs to build our documentation. They will be available here: https://networkx.readthedocs.org/en/latest/ once this is merged. After that, we will need to update networkx-website accordingly. There are also a number of places we need to cleanup the documentation, but that is another PR. 
issue
Add some comments/recommendations about drawing.#TITLE_END#This PR adds some comments about the state of graph visualization in NetworkX. It also includes links to Cytoscape, Graphviz, and PGF/TikZ (which as of version 3.0 has automated graph layout algorithms implemented via LuaTeX). 
issue
Update Johnson's algorithm so that graph is not modified#TITLE_END#There was some additional discussion on how to modify Johnson's algorithm #1481 so that it doesn't require a graph copy or modifying the input graph. The main point appears here in https://github.com/networkx/networkx/pull/1481#discussion_r29732490.  Here is the relevant bit:  """ Copying the graph is unnecessary. In order to compute node potentials using Bellman--Ford you do not need to add a separate node. You just assign zero distance labels of all nodes and start the relaxation loop.  The following Dijkstra using reduced costs (w[u][v] + p[u] - p[v]) instead of raw weights (w[u][v]) warrants introducing a dedicated function for that purpose. It has use cases in other algorithm such as augmenting path-based minimum cost flow algorithms. (Maybe we can use unify this with #762.) """ 
issue
Fix unit test for katz centrality.#TITLE_END#There was a random failure on TravisCI recently:  https://travis-ci.org/networkx/networkx/jobs/58959952  ``` ====================================================================== ERROR: test_katz_centrality.TestKatzEigenvectorVKatz.test_eigenvector_v_katz_random ---------------------------------------------------------------------- Traceback (most recent call last):   File "/home/travis/miniconda/envs/test-environment/lib/python3.4/site-packages/nose/case.py", line 198, in runTest     self.test(*self.arg)   File "/home/travis/miniconda/envs/test-environment/lib/python3.4/site-packages/networkx/algorithms/centrality/tests/test_katz_centrality.py", line 322, in test_eigenvector_v_katz_random     k = networkx.katz_centrality_numpy(G, 1.0/l)   File "<string>", line 2, in katz_centrality_numpy   File "/home/travis/miniconda/envs/test-environment/lib/python3.4/site-packages/networkx/utils/decorators.py", line 68, in _not_implemented_for     return f(*args,**kwargs)   File "/home/travis/miniconda/envs/test-environment/lib/python3.4/site-packages/networkx/algorithms/centrality/katz.py", line 327, in katz_centrality_numpy     centrality = np.linalg.solve( np.eye(n,n) - (alpha * A) , b)   File "/home/travis/miniconda/envs/test-environment/lib/python3.4/site-packages/numpy/linalg/linalg.py", line 381, in solve     r = gufunc(a, b, signature=signature, extobj=extobj)   File "/home/travis/miniconda/envs/test-environment/lib/python3.4/site-packages/numpy/linalg/linalg.py", line 90, in _raise_linalgerror_singular     raise LinAlgError("Singular matrix") numpy.linalg.linalg.LinAlgError: Singular matrix ```  This should be investigated and fixed. 
issue
Add function to find a single cycle.#TITLE_END#This PR depends on #1193.   - [x] Merge #1193   It adds the ability to find a single cycle, if one exists, on a graph of any type.  As for existing functionality:  `simple_cycles` does not work for undirected graphs, but is also not suitable for multidigraphs because it does not provide key data.  Similarly, `cycle_basis` is only meant for simple undirected graphs.  This new function `find_cycle` works for any graph and returns a single cycle found via a depth-first traversal of the edges.  It inherits the ability to ignore edge orientations as well, greatly extending the functionality for directed graphs.  One example where this function can be useful is when calculating the intersection of weighted matroids.  In that situation, one must add edges in a directed graph until an undirected cycle is formed.  Then you must find that cycle, and remove an offending edge.  An example usage:  ``` >>> G = nx.DiGraph([(0,1), (0,2), (1,2)]) >>> list(find_cycle(G, orientation='respect')) [] >>> list(find_cycle(G, orientation='ignore')) [(0, 1, 1), (1, 2, 1), (0, 2, 0)] ```  `G` is a DAG and so, it has no directed cycles.  So `find_cycle` returns an empty cycle.  If we allow `find_cycle` to ignore edge orientations, then it finds an undirected cycle in the directed graph.  Thus, `G` is not a directed tree. 
issue
Add DFS over edges.#TITLE_END#As mentioned in #1120, many of the algorithms for traversals are not sufficiently general for multigraphs.  In this PR, I've implemented a variant of a depth-first-search that traverses edges rather than nodes.  Unlike the standard DFS, as implemented in `nx.dfs_edges`, this version does not stop once all nodes have been visited. Instead, it continues searching until all edges have been visited.  This turns out to be especially important for multigraphs, since knowing the head and tail of an edge is not enough to uniquely identify an edge.  So this properly outputs the keys of edges when appropriate.  Note, that tweaking the existing implementation so that it return edges keys is not a sufficient fix, since it does not visit each edge.  ``` # Existing >>> G = nx.MultiDiGraph([(0,1), (1,0), (1,0)]) >>> list(nx.dfs_edges(G)) [(0, 1)]      # Feature from this PR >>> G = nx.MultiDiGraph([(0,1), (1,0), (1,0)]) >>> list(nx.edge_dfs(G)) [(0, 1, 0), (1, 0, 0), (1, 0, 1)] ```  Also, when dealing with directed graphs, one is often interested in traversals that do not respect edge orientation.  So we also add the ability to 'ignore' or 'respect' the edge orientations during the traversal.  An extra element is added to each yielded item which is 1 if the edge was traversed in a way that respects its orientation and 0 if not.  ``` # Ignoring edge orientation >>> G = nx.MultiDiGraph([(0,1), (1,0), (1,0)]) >>> list(nx.edge_dfs(G, orientation='ignore')) [(0, 1, 0, 1), (1, 0, 0, 1), (1, 0, 1, 0)] ```  So here we went from 0 to 1, then from 1 to 0 while obeying the edge orientation.  Finally, we go from 0 back to 1 again, but this time ignoring the orientation of the edge (u,v,key) = (1,0,1).  The interface to all of this could be cleaned up quite a bit if NetworkX had a formal edge object and if all edges had a unique identifier.  Alas, we are not in that situation, and so I think this strikes a reasonable balance, but I'm certainly open to suggestions.  Also the name is confusingly similar to `dfs_edges`.  Anyone have a better idea? 
issue
Merge Duplication Divergence Graph Generator (#1210)#TITLE_END#Merge #1210 with cleanup. 
issue
Update README#TITLE_END#Adds a bit more content to the README. Since the README is our landing page for developers and some potential users, I thought it might be nice to show a simple example. Also, showing the build status can be helpful. 
issue
Add find_cycles to reference documentation.#TITLE_END#Accidentally builds off #1281. 
issue
Ordered graphs#TITLE_END#For doctests (and perhaps unit tests as well---for example, unit tests for `edge_dfs` could actually be written properly), it might make more sense to use ordered graphs. This PR provides such graphs as part of the `networkx` API. 
issue
Add code to find branchings and arborescences.#TITLE_END#This is _old_ code, but it is rotting on my hard drive. So I figured it would be better to try and merge it. I don't doubt that this could could eventually be cleaned up and optimized, but it works and has unit tests right now.   Possible refactorings are that it could make use of the functionality provided by https://github.com/networkx/networkx/pull/1314.  Unique to this algorithm is that it requires keys for each edge and as part of the algorithm, edges are modified with new heads and tails, but the identity of the edge is unchanged. It would benefit greatly from a subclass of MultiDiGraph that tracked the edges by a graph-wide edge key.  Anyway, it provides code for finding optimium (min or max) branchings and arborescences. These are the directed analogs of min/max spanning trees on undirected graphs. 
issue
Update credits listing.#TITLE_END#The existing credits file in the reference documentation provided a brief summary of the particular contributions that (some) contributors have made to earlier versions of NetworkX. For many reasons, this has been difficult to maintain.  Separately, it's also often helpful, when writing grants or progress reports, to be able to point to some document that explicitly mentions involvement and/or funding support. The current credits file does not current have this.  This PR attempts to address both of the above. It also tries to make it excessively clear that the list of contributors is incomplete and potentially outdated as well. But this is easily changed by those who wish to change it. It also makes it clear that each person should individually request to be added to the list, and also, that names may be missing for a variety of reasons (anonymity, pseudoanonymity, apathy, etc).  The list is intended to be inclusive (even to future A.I.) and is not about delineating who contributed what and how much. It should be names only with a few user-determined specified links. This may include a link to a github account, LinkedIn account, or a personal website, etc.   The document includes references to various research groups at universities/labs/businesses, and also includes funding support acknowledgements.  If you'd like me to add your info now, as part of this PR, send me a PR to my personal `networkx` fork (or email me). It'd be especially nice to get a larger listing of research groups and funding sources. NetworkX has benefited from quite a large number of groups, both academic and otherwise. We should be advertising this fact! :)  A rendered example (but not sphinx parsed) of what this document will look like can be seen here: https://gist.github.com/chebee7i/f4f273d39fef5399d5fc. I will try to keep this updated with any further changes until this is merged. 
issue
Benchmarks via vbench?#TITLE_END#Is there any interest in benchmarking performance, perhaps through something like `vbench`:  1. http://wesmckinney.com/blog/?p=373.   2. https://github.com/pydata/vbench  It might be nice, but I definitely can't commit to this right now.  When we make changes to the base classes or to algorithms that touch a lot other code (e.g. copy or subgraph or shortest_path), I can see how this might be helpful.  
issue
to/from Pandas Dataframe #TITLE_END#Closes #1292. 
issue
New PR labels#TITLE_END#I saw IPython was using the following labels for pull-requests.  ``` Waiting for author Needs review ```  Might be nice to use these.  Probably, we could add something along the lines of "Needs decision" 
issue
Use wheels with TravisCI for NumPy, etc.#TITLE_END#For NumPy and SciPy, we should consider using wheels (via pip) instead of building from source---which we don't actually do at all right now because it is so slow.  Something similar to what they are doing here, perhaps. https://github.com/astropy/package-template/pull/44  This is just a placeholder. I can definitely tackle this at some point, just not immediately. 
issue
Drawing with pydot/Graphviz and inline IPython displays#TITLE_END#NetworkX is primarily a graph analysis tool and not a graph visualization tool.   However, people (myself included) continue to use NetworkX's graph drawing features.  There have been past proposals on how we might be able to provide "nicer" looking drawings (esp for directed graphs), but progress has been slow.  Graphviz is ubiquitous and provides a quick, flexible visualization solution.  The idea is to draw graphs using Graphviz and then display them using the OS's default viewer.  This is a "non-Python" solution, but it works.  We already have nx.view_pygraphviz(), but I suspect its rarely used.  And Pygraphviz on Windows is problematic enough.    In this pull request, I've revamped the pydot conversions and added a new function:  nx.draw_pydot().  This function converts NetworkX graphs to the pydot graph, draws the graphs externally using graphviz, and then displays the graph.  Additionally, one can display the graphs inline within an IPython Notebook.  To facilitate the various ways one might want to display a graph, I've added a rcParams-like option to NetworkX.  It is accessible via `networkx.nxParams`.   For pydot displays, you can set the 'pydot_show' param to 'external', 'ipynb', or 'none'.  It defaults to 'external', but when using an IPython notebook, you will want to set it to 'ipynb'.  Then, every call to draw_pydot() will display inline.  My hope is that this quick and dirty solution will serve as a nice band-aid until we get something more Pythonic.  ![pydotdraw](https://f.cloud.github.com/assets/326005/422625/2addcc20-ad2e-11e2-969d-371144421b98.png) 
issue
Generalize is_tree and is_forest to directed graphs.#TITLE_END#Also add recognition for branchings and arborescences. I'm preparing another pull request which builds on this code. I was planning to add unit tests in that PR (next week). 
issue
Add context manager for reversing a graph.#TITLE_END#This PR adds a `reversed` context manager. It is not imported in the primary NetworkX namespace, but it is available at `networkx.utils`. The construct can be helpful when one needs to temporarily reverse the edges of a graph. The context manager aspect guarantees that the graph is restored even in the event of an exception. It is no-op for undirected graphs, as this simplifies code. I think it makes more sense to have `copy=False` as the default, but I kept it consistent with `networkx.reverse`. 
issue
Fix for IronPython and types.FunctionType.#TITLE_END#IronPython doesn't support types.FunctionType, which was used by the isomorphism code to make a deepcopy of a function.  We provide a workaround in the event that a NotImplementedError is raised. This workaround increases function call overhead slightly, so there may be some performance hit, but this has not been tested. #949 #1127. 
issue
Hash randomization error#TITLE_END#Possibly, another hash randomization error:  https://travis-ci.org/networkx/networkx/jobs/25006261 
issue
Allow set_*_attributes functions to work with single attribute value.#TITLE_END#After reading @jtorrents https://github.com/networkx/networkx/issues/1162#issuecomment-43226198, I was curious about the `set_edge_attributes` functions and its friends.  It would be convenient to be able to set the attribute for all edges with a single value as input, instead of having to construct a dict. Desired:  ``` >>> G = nx.path_graph(5) >>> nx.set_edge_attributes('weight', 10) ```  This sets the weight of every edge to 10. This will only work if the attribute value is not a dictionary. If it is a dictionary, then it is assumed to match the previous behavior (keys are edges and values are attribute values).  Also, the various functions were not functional for multigraphs. I've fixed that. 
issue
Avoid insertion from front of list in bidirectional_shortest_path.#TITLE_END#This applies the patches provided in:     https://groups.google.com/forum/#!topic/networkx-discuss/ULpDth-GAGU 
issue
networkx.tree not installed#TITLE_END#Looks like we forgot to add `networkx.tree` to setup.py.  Fix is easy, but the real issue is why it wasn't caught by TravisCI.  The reason seems to be that nose is run from within the the cloned repository.  We need to install `networkx`, move into the installed directory and _then_ run the tests.  Thanks to @Autoplectic for finding this! 
issue
Revert "Merge pull request #812 from chebee7i/pydot". Addresses #812.#TITLE_END#This reverts commit e736d4b4db44f45c59039d44f92013748cbef3f4, reversing changes made to ceb31d0f4f14556d4e804868efd23c11f2e6b776. 
issue
Make make_str() always return unicode, no matter the Python version.#TITLE_END#I just noticed something about `make_str` that bothered me.  This pull request is built off @hagberg's branch for #989.  So make sure to merge #989 first, but its not strictly necessary, as merging this PR will merge both.  Consider the following example:  ``` #  -*- coding: utf-8 -*- # Example 1 from networkx.utils import make_str x = "qualité" y = make_str(x) ```  For Python 2, `y` is of type `str`, but as an encoded string. For Python 3, `y` is of type `str`, but as a unicode string.  This is consistent output type, but the interpretation is very different.  Now, consider:  ``` #  -*- coding: utf-8 -*- # Example 2 from networkx.utils import make_str try:     #2.x     x = unicode("qualité") except NameError:     #3.x     x = "qualité" y = make_str(x) ```  For Python 2, `y` is of type `unicode`, whereas in Python 3, `y` is of type `str` (which is unicode).  Now, the types are different, but the interpretation is the same.  Overall, for Python 2, `make_str` returns either a `str` or a `unicode` object.  On one hand, this makes sense since the input was different in each case.     On the other hand, its more confusing because it means that our code is possibly working with encoded strings or unicode strings.  Always working with unicode would make things easier to maintain, I think.  Additionally, this would be similar in spirit to how we handle `map`, `range` and `zip`---we always treat them as iterators so that our code treatment is uniform across Python versions.   Right now, internal NetworkX code handles "strings" differently depending on the Python version.    Below is a new proposal for `make_str`.  ``` PY2 = sys.version_info[0] == 2 if PY2:     def make_str(x):         if isinstance(x, unicode):             return x         else:             # Note, this will not work unless x is ascii-encoded.             # That is good, since we should be working with unicode anyway.             # Essentially, unless we are reading a file, we demand that users convert             # any encoded strings to unicode before using the library.             #              # Also, the str() is necessary to convert integers, etc.             # unicode(3) works, but unicode(3, 'unicode-escape') demands a buffer.             #             return unicode(str(x), 'unicode-escape') else:     def make_str(x):         if isinstance(x, str):              return x         else:              return str(x) ```  The important difference is that the `str` in name `make_str` is now interpreted to mean 'unicode', always.  Its forward looking to Python 3 just as map, range, zip, etc.  The benefit now is that all NetworkX code can be confident that make_str returns a unicode string. Note: the type will be different depending on the Python version, (e.g. unicode vs str) but we shouldn't care about the type as much as we care about what the type represents (e.g. encoded string vs unicode string).  In Python 3, examples 1 and 2 from above are the same.  unicode strings in and unicode strings out.  In Python 2, examples 1 and 2 differ but `y` represents a unicode string in both cases.  In example 1, `y` is a unicode object of length 8.  In example 2, 'y' is a unicode object of length 7.  The length 8 unicode string is probably not what the use wanted, but the user should also not expect us (NetworkX) to know how the string is encoded.  So the user should first decode to unicode and then pass it into `make_str`---in which case, the output would be the same as in example 2.  If `x` were an ascii encoded string, then examples 1 and 2 would be the same in Python 2 and `y` would be unicode strings in both cases. 
issue
Katz eigenvector test needs to seed its PRNG.#TITLE_END#It looks like there might be a seed issue with one of the tests for Katz eigenvector.  Here is the TravisCI log:  ``` ====================================================================== ERROR: test_katz_centrality.TestKatzEigenvectorVKatz.test_eigenvector_v_katz_random ---------------------------------------------------------------------- Traceback (most recent call last):   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/nose/case.py", line 197, in runTest     self.test(*self.arg)   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/networkx/algorithms/centrality/tests/test_katz_centrality.py", line 321, in test_eigenvector_v_katz_random     k = networkx.katz_centrality_numpy(G, 1.0/l)   File "<string>", line 2, in katz_centrality_numpy   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/networkx/utils/decorators.py", line 63, in _not_implemented_for     return f(*args,**kwargs)   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/networkx/algorithms/centrality/katz.py", line 300, in katz_centrality_numpy     centrality = np.linalg.solve( np.eye(n,n) - (alpha * A) , b)   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/numpy/linalg/linalg.py", line 381, in solve     r = gufunc(a, b, signature=signature, extobj=extobj)   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/numpy/linalg/linalg.py", line 90, in _raise_linalgerror_singular     raise LinAlgError("Singular matrix") LinAlgError: Singular matrix ``` 
issue
Fix tests when Cholesky solver is not available.#TITLE_END#The unit tests for the new algebraic connectivity code runs through all methods, causing a `NetworkXError` when the cholesky solver is not available. I include an explicit test in the unit test module. I also renamed the module so that it didn't conflict with the function `algebraic_connectivity`. @ysitu 
issue
Fix nondeterministic unit tests for double-edge swap.#TITLE_END#Been getting the following error from time to time:  ``` ====================================================================== ERROR: test_richclub.test_richclub_normalized ---------------------------------------------------------------------- Traceback (most recent call last):   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/nose/case.py", line 197, in runTest     self.test(*self.arg)   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/networkx/algorithms/tests/test_richclub.py", line 16, in test_richclub_normalized     rcNorm = nx.richclub.rich_club_coefficient(G,Q=2)   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/networkx/algorithms/richclub.py", line 74, in rich_club_coefficient     nx.double_edge_swap(R,Q*E,max_tries=Q*E*10)   File "/home/travis/virtualenv/python2.7.6/lib/python2.7/site-packages/networkx/algorithms/swap.py", line 95, in double_edge_swap     raise nx.NetworkXAlgorithmError(e) NetworkXAlgorithmError: Maximum number of swap attempts (120) exceeded before desired swaps achieved (12). ```  Probably need to seed the random number generator. But it doesn't look like doube_edge_swap takes a prng.  Need to be modified to something like:  ``` def double_edge_swap(G, nswap=1, max_tries=100, prng=None):     if prng is None:         prng = np.random      ....     v = prng.choice(list(G[u]))     y = prng.choice(list(G[x])) ```  For a PRNG API, we need to choose between the stdlib's random module or NumPy's RandomState class.  They are not compatible, and if we want to allow people to pass in a PRNG to various functions/classes, I don't think it's good for NetworkX to use two different PRNG APIs.  My preference would be to use NumPy's throughout, as it has much more functionality. 
issue
.travis.yml updates#TITLE_END#There was a recent update to TravisCI's build environment for Python. See: http://blog.travis-ci.com/2014-04-28-upcoming-build-environment-updates/  The restructuring was such that having:  ``` virtualenv:     system_site_packages: true ```  caused problems for Python versions which do not have a system install (e.g. anything but 2.7 and 3.3 for Ubuntu). So we had to remove the declaration and manually activate the virtual environment for 2.7 and 3.3. 
issue
SciPy creep#TITLE_END#As has been discussed elsewhere (i.e., #1076), NetworkX has slowly been relying on NumPy more and more.  Now, many of the "advanced" algorithms make use of NumPy, even though NumPy is not a required dependency.     Generally, I think the consensus is that this trend is okay (speak up if I'm mischaracterizing views). NumPy is ubiquitous among scientific users and much easier to install these days. So long as it's possible to make use of a decent percentage of NetworkX without needing NumPy, its no big deal.  (Though it would be good to for us to know precisely what and how much of NetworkX is depending on NumPy and how that percentage is changing in time....maybe even make this part of the documentation).  ---  Today, I needed an incidence matrix for a graph, and saw that it returned a SciPy sparse matrix. I believe most of this happened in #1021, and although I can't find the discussion on it, my recollection was that we had strong consensus that this would be a good change to make. The rationale was something like: many graphs are sparse and converting to a dense format is easy as pie.  However, now I'm starting to wonder...  When I install in new environments that aren't my primary development area, I almost always install Python+Numpy, but its much less common for me to install SciPy.  I've seen this same trend elsewhere in education, computer labs, etc., where Python+NumPy is installed but SciPy is not, left as a package for more advanced users only. [I'd be curious to see estimates for the number of NumPy installs vs the number of SciPy installs.]  So here I am wanting something as simple as an incidence matrix, and now I have to install SciPy.  This seems to push the bar for "what is actually usable in NetworkX using pure Python or Python+NumPy" to be even smaller.  In #1076, people seemed to think that a dependency on SciPy was even less desirable than one on NumPy.    Mainly, I just want to bring attention to the issue of SciPy creep in NetworkX.  Do we want to continue in this direction? If so, should we consider making SciPy a hard dependency?  Or is it perhaps a bit excessive to require SciPy for an adjacency matrix?  What about adding a `sparse=False` argument to most of these functions, where we default to dense representations through NumPy/Blaze and only return sparse arrays when requested? 
issue
Connected Components for DiGraphs#TITLE_END#It looks like we are missing an algorithm for calculating connected components for directed graphs.  This is strictly different from weakly and strongly connected components. So sayeth Wikipedia (http://en.wikipedia.org/wiki/Connected_graph):  > A directed graph is called weakly connected if replacing all of its directed edges with undirected edges produces a connected (undirected) graph. It is connected if it contains a directed path from u to v or a directed path from v to u for every pair of vertices u, v. It is strongly connected or strong if it contains a directed path from u to v and a directed path from v to u for every pair of vertices u, v. The strong components are the maximal strongly connected subgraphs.  So:  strongly => connected => weakly  ``` 1. weakly connected but neither connected nor strongly connected ```  ![nx_txnq_d](https://cloud.githubusercontent.com/assets/326005/2738441/bcc0cabe-c68e-11e3-9436-ff360452d578.png)  ``` 2. connected (and also weakly connected) but not strongly connected ```  ![nx_fbdqot](https://cloud.githubusercontent.com/assets/326005/2738505/a17d9088-c68f-11e3-9e9d-3fde159a4676.png)  ``` 3. strongly connected (and also weakly connected and connected) ```  ![nx_0vubdm](https://cloud.githubusercontent.com/assets/326005/2738535/305e8528-c690-11e3-8130-5bc40fb48498.png)  Is it correct that we no longer have a partition?  In the following, it seems like there are two components: (A,B,C) and (B,C,D).  ![nx_azz9bo](https://cloud.githubusercontent.com/assets/326005/2738686/7c79ad62-c694-11e3-85e5-9eeba6e7a62a.png)  Aside, I don't have time to work on this right now, but if someone had an itch for it...  [Drawings made using [nxpd](https://github.com/chebee7i/nxpd). Look for nxd3 this summer.] 
issue
Decorators require and not_implemented_for#TITLE_END#I was hoping to begin a discussion on the decorators 'require' and 'not_implemented_for'.  I originally liked these decorators, but I'm starting not to.  Let me give my reasons and we'll see if they persuade anyone...or maybe I will be persuaded back to liking them.  Generally, I feel that: 1. 'require' is unnecessary and we should use the standard traceback 2. 'not_implemented_for' is an attempt to address a larger problem, and we should think harder about that larger problem 3. 'not_implemented_for' adds undesired overhead in tight loops ### require  Regarding 'require', we use it like so:  ``` @require('numpy', 'scipy') def func(blah):     import numpy as np     import scipy as sp     ... ```  What is its function?  Essentially, it is to change an ImportError into a NetworkXError and provide a fancy error message that names the calling function and the dependency.  Some comments: - I think it is more appropriate to raise an ImportError here instead of the NetworkXError.  This is easily fixed, whether we decide to continue using 'require' or not. - The fancy error message provided by 'require' is not that much more helpful than the default traceback's error message (and users should know how to interpret a generic Python import error message anyway, since they will surely encounter it when not using NetworkX).  In other words, I don't think we need to aid the users any more than the Python interpreter and other packages do.   - The default traceback already shows which function made the invalid import call, so its not strictly necessary to add this to the error message. - Part of why we use 'require' is that not every function in a module needs to use the dependency.  Hence, we do not import the dependency at the module level.  But even with 'require', we must still do a function level import (whenever the function explicitly accesses a class or function within the required module).  Given this, we might as well drop the decorator call since the import error will raise anyway when the function imports the required module. - Some other module checks we do (that do not use 'require') include a link to the module's website in the error message.  This is helpful information, but it can also be found on the NetworkX webpage, along with the complete list of optional dependencies.  The above, if it is convincing, seems to suggest that a vanilla ImportError would be better.  As developers, we just stick to a rule which says any explicit dependency should be imported in the first lines of the function definition.  So my questions is:  Do we really gain anything substantive by using 'require'? ### not_implemented_for  The case for continuing to use 'not_implemented_for' is stronger, but I worry that it is a flawed solution to a larger issue we face in NetworkX.  Let me first bring up the performance issue, as this is the easier point to make (I hope).  Often, I need to run loops over many different graphs all of which are of the same type (eg, all are directed or not, multigraph or not).  And so, including a check on every algorithm called within this loop, on each iteration of the loop, ends up adding a lot of unnecessary checks.  In principle, the problem can get even worse, if one algorithm calls another which calls another, all of which do the check.  Now, I agree that if the algorithms are "slow" then surely the quick type check is not going to be an issue, at least for a single graph.  But my experience has been that the checks really do build up when you are looping through many graphs.  Now, one way to avoid these checks would be for the functions to be methods on the classes that they are applicable to.  However, there are too many algorithms and this would get ugly quickly.  I think NetworkX has _wisely_ tried to keep the classes barebones and simple.    Another idea would be to simply not do any checks and demand that users know which algorithms can be used with which types of graphs (the docs must be helpful here).  This is probably a bit harsh, but perhaps the idea is right and we should find some other way to inform users about which algorithms work with which types of graphs.  This is the approach I'd like to advocate for.    The NetworkX namespace, while organized by algorithm families, does not give hints as to which algorithms apply to which types of graph, and this is the "larger" issue I was hinting at.  If I am only working with undirected non-multigraphs and want to browse the NetworkX functionality relating to this graph type, I cannot do so.  Instead,  I will see the namespace grouped by algorithm families (this is helpful...its just incomplete in some sense). So here is a thought:  What if we provided separate namespaces that only contained those functions which were relevant to the graph classes that could use them.  We can work on selecting better names, but as an example:  ``` with nx.get_algorithms('digraph', multi=True) as algos:     for g in graphlist:         scc = algos.components.strongly_connected_components(g)         ... ```  And also:  ``` algos = nx.get_algorithms('graph', multi=False) algos.components.strongly_connected_components # AttributeError ```  In each module, we can "register" the functions.  ``` # Let    g = graph,        dg = directed graph,  #       mg = multigraph,  dmg = directed multigraph  @register_algorithm(['dg', 'dmg']) def strongly_connected_components(...):     pass ```  This would update some global list that is referenced by nx.get_algorithms() and then return the defined function as is.  A solution like this would require some work, but I think it addresses the larger issue we have (namespace pollution wrt wanting to know which algorithms are relevant for the graphs one is working with).  It also removes the performance concerns.  With this idea, people could continue using the standard namespace (so you aren't forced to use nx.get_algorithms()---but if you do so, you must be careful, as none of the algorithms will do any graph type checks).    This is just one idea, and I'd love to hear what people think of it or if they have other ideas....or if we should just keep the decorator. 
issue
Make nose and coveralls coverage levels match.#TITLE_END#In general, there is a difference between what nose reports in its coverage report and what the .coverage file includes (which is what is read by coveralls).  These commits make the two reports the same.  At the high-level, the changes are to: - not provide coverage for networkx.external - make sure networkx.testing does not get counted as a test module - make sure to run doctests when running nosetests from the installation directory  Also, the .travis.yml configuration was updated again, providing much more discussion about what packages are installed in which environments.  Please read those comments carefully, and comment back here.  In particular, we might want to consider pip installing NumPy for 3.3 and pypy.  The downside is that this will make those builds take _much_ longer.  Because of the various dependency issues, the current coverage levels are:  ``` 2.6  2.7  3.2  3.3  pypy 85,  91,  89,  80,  80 ```  which gives us an average coverage level of:  85.0 
issue
Shortest path with only a target specified.#TITLE_END#Presently, calling   ``` python G = nx.path_graph(5) nx.shortest_path(G, target=4) ```  returns an exception even though it is clear what the user would like. Namely, a dictionary keyed by sources with values representing paths from the source to the specified target:  ``` {0: [0, 1, 2, 3, 4], 1: [1, 2, 3, 4], 2: [2, 3, 4], 3: [3, 4], 4: [4]} ```  This pull request implements target-only calls to nx.shortest_path().  As part of it, I added a reverse() operator to NetworkX, and now, algorithms use that operator rather than using the reverse() method on the classes.  My guess is that this particular change will be the most controversial.  Some comments: 1.  Having a reverse() operator is consistent with the other operators that NetworkX provides (union, complement, etc). 2.  There does not exist a union (or complement or ...) method on the graph classes.  This actually makes it seem that there should not even be a reverse method, but since it has been there since the beginning, removing it is probably not an option. 3.  Downstream, classes derived from NetworkX classes can reimplement the reverse() method in a manner which fits their use case.  As an example, a reverse method on a DFA class might return the minimal DFA which recognizes its reverse language.  If algorithms used G.reverse(), then functions like nx.shortest_path() would not work correctly.  Although this same argument could be applied to other graph methods, it seems more likely for the reverse method than for others.  Reversing, in principle, can mean many things to different domains, whereas add_node, does not vary as much.  Comments are appreciated. 
comment
@kharris for historical purposes, can you edit your comment to clarify what _this_ means? 
comment
@jtorrents thoughts? 
comment
I think we should leave open the possibility that people with `MultiGraph`s might want a logical square. So a separate implementation, possibly with a unified interface (e.g. `power(G, method='blah')`), would be better in my opinion. 
comment
@jfinkels I guess I meant the other way around. You have a graph and want a multigraph constructed, but you want the multigraph to be populated with the logical power graph. So I think it's important to have the option to override any automatic choices `power(G, method='logical', create_using=...)` or whatever. Your latter suggestion seems good too. 
comment
This reminds me a bit of a previous discussion with the diagonals of a matrix, and how the convention differed with undirected and directed graphs. Is that discussion relevant here? I'll see if I can find it. 
comment
With #1399 in, are we moving forward with also having a `walk_power` function? 
comment
It could be nice to remove it. Are you proposing that the NetworkX API will only accept file objects, and that strings will no longer be allowed as input? You've correctly identified part of the reason for `@open_file`---and a simpler API would mean that it's not necessary.  This seems like a pretty big API change though, right?  All functions will only accept file objects, and now NetworkX will never close them.  Right now, we close file objects that we create, but do not touch file objects that are passed in. 
comment
To be clear, I'm not opposed to making the change...just wanted to clarify the ramifications.  Update: And on rereading your initial post, you were already pretty clear...I just failed to read it completely :) 
comment
Regarding std and mean...  One of the primary [motivations](http://legacy.python.org/dev/peps/pep-0450/) for including `statistics` in 3.4 is that naive implementations of mean and std have poor numerical properties. The variance is particularly [bad](http://hg.python.org/cpython/file/3.4/Lib/statistics.py#l446).  So my preference is that we not implement our own (unless you want to copy the not-so-simple code from `statistics` or add its backported version as a dependency).  As I've opined elsewhere, much of NetworkX relies on NumPy.  Also, I'd guess that the vast majority of our users make use of NumPy in their work anyway.  With NumPy being much easier to install these days, I think we should not shy away from depending on it (perhaps even explicitly). Very little is to be gained by removing a NumPy dependency in this module, and there is a numerical risk in doing so.  If we do decide to implement our own, it should be separated out as a function and called (rather than implemented in the block). 
comment
Well, I'm just one opinion...I'd wait to hear back from others too. 
comment
Since you are making use of `__all__`, is it necessary to prefix all the internal functions with a double underscore?  A non-underscored name seems fine.  I've seen a single underscore used when `__all__` is not being used. 
comment
Yeah, I'm not sure if NumPy actually does it right.  They might not---that could always change.  Generally, it seems like detail we should leave to another library. 
comment
What does that mean? 
comment
Did you want to 3D plot a graph? If so, we don't currently support that. But it's certainly possible. 
comment
I'm not sure about this, but my inclination is not to "fix" this. There is no shortage of types that will not convert for the GEXF, tuples are among them as well. I saw this: http://stackoverflow.com/questions/774192/what-is-the-correct-way-to-represent-null-xml-elements, but it's not clear to me if that would even be a valid GEXF document. 
comment
Yeah, that's a good idea. Probably something similar to what we have for `write_gml`. https://github.com/networkx/networkx/pull/1269. Let's leave this open in case someone feels inspired. 
comment
Glad to see this portion of the code getting some extra eyes on it!  @Loveuse I will try to take a look at this (#2117) in the next few days.  
comment
Oh great! I found myself needing this recently. Relatedly, I have in-progress code prepared for finding maximal spanning trees on directed graphs.  My plan was to put it all in `networkx.algorithms.tree`, and also move `networkx/algorithms/mst.py` into `networkx.algorithms.tree`.  Anyone have thoughts on this?  But to be more clear: I think a good place for this is a new module in `networkx.algorithms.tree`. 
comment
@francis-liberty, for a (very) small graph, my hope is that the algorithm would find the ideal solution, and that such a solution could be verified by a brute force algorithm (or by hand).  This would make a fine test---e.g., code up a brute force algorithm and include it in the committed module as well (knowing full well that it will never scale well).  While this doesn't verify the algorithm in totality, knowing that it works in these base cases would helpful, especially as other parts of NetworkX change.  Also, I think putting in examples from the paper would be good.  Seed the random number generator (if it depends on that) and then make the test verify the stated output in the paper.  At a minimum, the test would make sure that the returned value is consistent across NetworkX versions (and Python runtimes) and that we match published numbers. 
comment
So mostly I just meant that you want to have tests in there that verify that the algorithm is working as intended.  If you have a really small graph, then you should be able to independently verify (either by a brute force algorithm or by hand) that the algorithm is working.  Including this as a unit test makes sense.  Using examples from the paper (if they are small) is also probably good.  All I meant with different versions of NetworkX is that having such units tests (like what I just described) is precisely what allows us to make sure that the algorithm continues to work across (future) NetworkX versions and also across Python runtimes.  Short of writing the unit tests, nothing else is needed.  Let us know if you need more examples of unittests. 
comment
Glad to see this discussion happening.  @hagberg's point about overlapping partitions is spot on for me.  I'd want the data structure returned by such an algorithm to be similar to the rest of the community detection algorithms.  Also, if conversion to dict or frozensets was super important, NetworkX could provide some functions that receive the output and convert them appropriately.  They wouldn't need to be imported into the top level module `networkx`, but we could make them available `networkx.algorithms.community`.  This would save people from typing the same boilerplate conversion each time. 
comment
Very nice. Since this uses NumPy, I wonder if it would make sense to make this depend on SciPy isntead and then make use of the KDTree found in `scipy.spatial`. 
comment
Neat. So my next question is: Is it easy to get that functionality into scipy.spatial? Or is your implementation completely different? 
comment
Looks very nice! I won't have time to review this today, but hopefully we can get it reviewed soon! 
comment
We can iterate on style endlessly :) Has anyone had a chance to review the substance? Unfortunately, it will be a few days before I can even attempt that. 
comment
Related: #1550 
comment
I see the "intuition" gap, and also from an efficiency perspective. We could make the documentation clearer and provide the code @hagberg suggested as an exapmle, or maybe we could consider an `apply_defaults` argument, which defaults to `False`. 
comment
It may also make sense to provide a new argument such as `strict=False` that allows the user to turn off the flexible parsing of non-conformant gml files.
comment
If you can construct a small example that we can run to reproduce the problem, that would be helpful. 
comment
There might be some code for this already. I know I wrote some moralization code at some point...probably as a gist. 
comment
This was merged in #2241.
comment
Late to the party. :)  Just as an FYI, I attempted to document the requirements that this algorithm imposes on graphs in the module docstring:   https://github.com/networkx/networkx/blob/master/networkx/algorithms/isomorphism/isomorphvf2.py#L124.    In addition to nodes being hashable (already a NetworkX requirement), it also requires a total order on the nodes. There's also an example of how one can do a pre-computation step that will put a total order on the nodes.  Though it's been a while since I've looked at this code and algorithm, my recollection was that pulling out an arbitrary item has fairly substantial performance implications due to it re-visiting configurations that it has already ruled out.
comment
I no longer have access to paywall'd PDFs, but fortunately, I was able to find citation [1] from the docstring:  https://github.com/pfllo/VF2/blob/master/doc/A%20(Sub)Graph%20Isomorphism%20Algorithm%20for%20Matching%20Large%20Graphs.pdf  Right before Section 2.1: >Note that the algorithm explores the search graph in the SSR according to a depth-first search strategy. Using this simple formulation, a state can be reached through different paths. In order to avoid that, during the matching process, the algorithm generates useless and already generated states, a special procedure for generating a successor node is used. An arbitrary, total order relation (denoted by $\prec$) is defined on the nodes of $G_2$ which belong to the set $P(s)$. Since the node insertion order in the partial solution $M(s)$ does not influence the resulting state, the algorithm ignores any pair $(n_i, m_j)$ in $P(s)$ if this set already contains a node $m_k \prec m_j$. This simple strategy allows the algorithm to generate each state only once. 
comment
Yeah that seems about right. My feeling at the time, which was a while ago and things may have changed since then, is that most people use standard data types for their nodes like int, string, homogenous tuples, etc. For these common use cases, it is slower if we automatically try to build our own ordering since those already have a total order.   So my pref was that if you were doing something special where you had custom nodes, then you should define an ordering on them (super easy with `functions.total_ordering`), or relabel them before using the isomorphism class. If we go this route, the requirement should probably be spelled out more explicitly at the top-level functions, however.   Now, since it is an arbitrary ordering that is required, we can always make it true without affecting the result. So I see the appeal of hiding this requirement from the user. But then you have to worry about inverse mapping the results back to the user nodes. There are a number of ways to interact with this class and each outpoint would need to be handled. For example, some have asked for access to the internal mapping as the search happens. This would require retranslating those nodes at each iteration, etc. I suspect it would get ugly and hard to maintain quickly...in addition to the big perf hit. I could see an argument that only the top-level `is_isomorphic` should auto-construct an ordering though, so long as people are comfortable with the slowdown when your nodes are already ordered.
comment
I like it! And yeah, that optimization you suggested does seem like it should perform better. Odd.   Re perf: I vaguely remember all the function calls being a big deal in general, as well as the stack dives from recursion. I just noticed that someone put out a paper on VF2++ (https://www.sciencedirect.com/science/article/pii/S0166218X18300829). Looks very nice. Would be interesting to offload the isomorphism checks to that LEMON library.
comment
Opps. Typing on my phone...that's what I get. 
comment
I've desired this on occasion as well. Maybe we can meet partway and query if `create_using` is a callable, and if so, call it to create the instance to be populated? 
comment
But suppose I have a graph already with some nodes and edges that was provided to me, and then I want to use some graph generator to add more nodes (I dont't want it cleared). It seems like callables and instances are equally valid use cases. If we don't support an instance, then the user would have to write a silly lambda function to return the instance. Both approaches seem fine to me, but checking if `create_using` is callable seems pretty simple for us to do. In fact, there might even be a few places where we already do such a check, not sure though. 
comment
I might be thinking of the `weight` parameter wrt to existing code checking if it is callable. 
comment
@jfinkels those operators assume you want disjoint combinations, which may not be what is desired. I try to think of the generators as functions that add edges to the graph with a particular property, etc. In that sense, they should always be able to work with existing graphs---the final graph may not have the properties of the graph generator, but that is a decision the user made. Now whether existing graphs are supported directly or through a lambda function or similar, I could see it either way. 
comment
That could work too...then it would be up to the user to merge the returned graph with their existing graph. It isn't as efficient though. If I have a subclass and I want the generator to populate my subclass, it seems a shame to have to add all the edges twice.    With respect to passing Graphs to generators that desire to create DiGraph-like objects (and similar), I see the concern, but I'm usually of the view that we should allow users to utilize the code we have written in as many ways as possible. It could be that the code used in, say, `nx.generators.gn_graph` could be useful in some unexpected context with undirected graphs or multi-digraphs. Without `create_using`, we either require the user to rewrite the code we have already written, or they will have to populate a DiGraph and then, on a second pass, add those edges to their desired graph. 
comment
Oh I see, so these wouldn't construct graphs at all...just a generator over edge tuples. How would that work in practice? Would the generator have to build a graph internally and then yield from `G_internal.edges()`? There's no way we can assume that the generator can generate all the edges on the fly without some intermediate graph-like storage. E.g. future edges might depend on graph properties induced by previously added (random) edges. So it seems like, in many cases, we would still have to add the edges twice in the "edge generator" paradigm---once internally and then once again externally. And even then, I think we'd have to adopt the most general solution and return both a node and edge generator. 
comment
@ysitu No argument there. :) I'm liking the direction this is going. 
comment
Opps. Wrong button. I'll make sure to update to whatever we fixate on. 
comment
Oh, I just saw the comment about legacy infrastructure. Are the test failures still an issue? 
comment
(Doesn't appear so) 
comment
Seems reasonable.  I might have some time tonight.  Or was this something you'd be interested in committing? 
comment
I should mention, I think at some point it's probably better to have these more specific types of graph operations as part of a separate package, one dedicated to Bayesian networks.  I've been thinking of writing one, based on NetworkX of course.  In the meantime, I'll give this a shot, but no guarantees on if it gets included (other devs will have to comment).  If its deemed too specific, you could also keep it for your own use. 
comment
Here is code for markov blankets and moralization.  http://codepad.org/rLPDU6le I'm not sure if I'll have time to finish triangulation tonight, but if so, I'll put all of this as a pull request. 
comment
Ok, so I haven't played with this stuff too much, and I'm not exactly sure what you are trying to do, but I suspect this might be helpful.  I didn't write up anything related to triangulation (maybe later), but it seemed (from the PDF I was using to write this stuff) that triangulation is way to get at junction trees.  This gets at junction trees by constructing a maximum spanning tree of the clique graph of the moral graph:  https://gist.github.com/chebee7i/7509453.  Probably will only work decently if your graphs have a small number of cliques---otherwise, you'll need heuristic triangulation.  Based on:  http://www.mi.parisdescartes.fr/~nuel/bntworkshop2010/lecture3.pdf 
comment
Ah sorry, duties took me elsewhere. Does the `junction_tree` function in that file work for small graphs? 
comment
I wouldn't wait for it. I've got too much going on right now. You or someone else should give it a shot! 
comment
@hagberg are you referring to the currently unimplemented triangulation code or the code that appears in this gist: https://gist.github.com/chebee7i/7509453 ? 
comment
@mb3152 not sure what you mean.  I have not written triangulation code---its unimplemented :). Were you going to take a stab at that? 
comment
Also relevant:  https://github.com/eBay/bayesian-belief-networks 
comment
Good move. Also, at this point there is a mature package dedicated to this: https://github.com/pgmpy/pgmpy  If interest in this picks up again, I suspect conversions between the two libraries would be a good option instead of an all-out implementation.
comment
Jordi, I can take a look at preparing these patches today.   It looks like it should be simple, so I should finish it.   Also, the script that tests github pull requests did not seem to run each time a pull request was updated. So while helpful for the initial pull request, we'd need additional feedback anytime the request is updated or else we risk a false sense of security. 
comment
It seems there is no way to make pull requests dependent on other pull requests.  Does anyone have ideas on how this should be managed, or how other projects have done it?  In this situation, the pull request for K-shortest paths is based off the restricted shortest paths pull request.  Also, what has been decided about implementing this for the weighted case and other similar shortest path functions?  Couldn't almost every NetworkX algorithm be adapted to look over a subset of the graph's nodes or edges?  I suppose it is a matter of what people are likely to use.  As for now, only one of the four possible ways to call nx.shortest_path will make use of the ignore_nodes and ignore_edges arguments.  This is not ideal in my mind, and we should aim to support all call cases.  Every time I look at the shortest_path code, I can't help but wonder if we can find a way to make it easier to extend---there really are so many places to touch to keep things consistent.  Is there a huge difference in speed between the shortest_path and shortest_path_lengths functions? 
comment
Ok, I clearly haven't been able to find time to do this.  I'll still try to tackle this, but if someone wants to do it now, feel free. 
comment
@dschult might have more to say on views.  I can't remember where that discussion took place.  I found #335 but I swear there was more discussion than that. 
comment
In light of #793, I just reread this discussion. This definitely needs to be looked at again wrt 2.0. 
comment
These look good. Would you be able to create a pull request to get them included? It will make it easier to put comments on particular lines of the code. Perhaps `networkx/networkx/algorithms/cycle_space.py` would be a good place for them.  For the tests, you'll need to put them in `networkx/networkx/algorithms/tests/test_cycle_space.py` and follow the style of other tests.  There's a number of small code changes we'd have to make (e.g. 4 spaces for indentation), but I can send you a pull request on those, once you create your pull request. 
comment
Yep, exactly.   Fork networkx, and then add your stuff and unit tests. Run the unittests locally, and once you have it ready, commit and push up to your fork.  Github should provide an option to create a pull request to the main networkx repo. 
comment
I think all of this is now in #1067. 
comment
This is definitely a bug. `n` should be an acceptable node attribute. 
comment
The issue, of course, is that we have to use some name for the parameter to `add_node`. We could change it to `def add_node(self, _n, attr_dict=None, **attr)` or you can pass it in as: `G.add_node(3, attr_dict={'n': 10})`. Not sure if we should change or you should. 
comment
I'm not sure.   I just opened #1583 which would change the `n` in `add_node` to a `u` (but `u` as an attribute name would still be problematic). This is mostly a consistency issue, but it would address your problem (at least for networkx). As for pygraphviz, you should definitely open an issue there and link to it here. I think adding an `attr_dict` makes sense, independent of whether `n` is changed to `u` throughout the code. 
comment
@SanketDG the title says WIP? Is this WIP still? 
comment
How do self-loops fit in here?  Always allowed, in principle?  Also, it seems like you are saying single edges => binary adj and mult edges => weighted matrix.  This seems to indicate that graphs with at most one edge between nodes cannot have weights associated with those edges. Could you clarify a bit more? 
comment
Yeah, I think I'd prefer WeightedGraph and not mention multigraphs at all. Multiedges cannot always be reduced to a single edge, topologically they are distinct, giving different in/out degrees etc. Imagine multiple edges between nodes A to B and also between B to C. You want to know the mean distance between A and C, where the mean is taken over all possible paths.  Something else to keep in mind (we had a ticket on this in the past): Sometimes it is necessary to distinguish edges with weight=0 and no edge at all. One silly example is that edges with zero weight represent frictionless (but allowable) paths, whereas nonedges represent paths that literally cannot be traversed. It seems like this would demand both an adjacency matrix and a weight matrix. That said, I'm happy with having a class with just one array that covers the common case when `weight == 0 <==> no edge`. 
comment
This also makes me wonder about graphs where the edges weights represent resistance.  In those cases, infinite resistance would correspond to a 'nonedge'.  Sure, one could just invert the weights and represent conductance instead, but I wonder if there are other cases where some value other than 0 should be taken to mean "no edge".  We could push this use case out of the WeightedGraph class as well. That is, WeightedGraph exists with just one array.  Then, eventually, we also implement a class that has both an adj array and a weight array---people who need to treat the weights differently from the edges could make use of this class. 
comment
Yes agreed on implementing the easy case first. But I think asking people to use a custom dtype is a bit heavy, especially since dtypes should be agnostic on whether a particular value is interpreted as a nonedge. Simpler would be another class, down the road, that implements both an adjacency array and a weight array. 
comment
I'm rather fond of the [approach](http://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html#directed-vs-undirected) taken in `sparse.csgraph`, where the directed/undirected nature of the graph is just an algorithm parameter indicating how the algorithm should treat it. So its not something fundamental to the class at all. A benefit of this is that you can have a directed graph and easily calculate quantities on it, treating it as if it were an undirected graph.  In that set up, its not strictly necessary to have separate classes for graphs and directed graphs. Is the idea that the various classes will be friendly wrappers that set defaults for the user?  ``` class basegraph(object):     ...     def is_directed(self):         return self.graph['directed']  class Graph(basegraph):     def __init__(self, *args, **kwargs):         kwargs['directed'] = False         kwargs['dtype'] = np.int         super(basegraph, self).__init__(*args, **kwargs)  class DiGraph(basegraph):     def __init__(self, *args, **kwargs):         kwargs['directed'] = True         kwargs['dtype'] = np.int         super(basegraph, self).__init__(*args, **kwargs)  class WeightedGraph(basegraph):     def __init__(self, *args, **kwargs):         kwargs['directed'] = False         # default dtype is float         super(basegraph, self).__init__(*args, **kwargs)  ... ```  In algorithms, types are checked using `basegraph.is_directed()`, etc. So for example:  ``` def shortest_path(g, method='auto', directed=None, ...):     if directed is None:         directed = g.is_directed()     ...  g = WeightedDiGraph(...) x = shortest_path(g) # defers to g.is_directed(), and thus, treats it as directed x = shortest_path(g, directed=False) # treats g as undirected  # Of course, we don't mandate the use of the helper graph classes... g = basegraph(..., directed=True, dtype=np.int8) ```  Btw, I'm looking forward to speeding up the isomorphism algorithms. 
comment
Regarding backends, it might be useful to look at Sage.  Their graph functionality is pretty impressive and they seem to support various backends, sparse or not.  http://www.sagemath.org/doc/reference/graphs/index.html 
comment
This is tricky stuff, and unfortunately, I don't think there are good solutions available presently. You'll have to hack into sphinx and sphinxcontrib-bibtex. There has been some work done on this, and so, perhaps it is much easier now. See https://github.com/mcmtroffaes/sphinxcontrib-bibtex/issues/52.   Ideally, you'd want two types of bibliographies---one for all of the documentation and one for each docstring (e.g. at a function, method, class, and even module level).  The docstring should create a link using a unique and globally defined tag, such as [Hopcroft1969]_, and this should link locally down to the references section of the docstring. The number/reference-tag that is presented in the references section could also be a link to the global bibliography as well. Ultimately, you want to maintain one bibtex file and be able to use citations in the user documentation and docstrings in the same manner. 
comment
This is a good idea. I already have code that can do this in linear(?) time for DFAs, but these require labeled edges and determinism. It just seemed to special case to include in NetworkX. Some of the ones you describe seem like better candidates. 
comment
Good point. Do we have an implicit "contract" that the graph should be static for NetworkX? Are people using it with dynamic graphs? 
comment
Probably both are useful. Is this vector or raster? 
comment
@hagberg is the X in NetworkX related to the X division?  Do they have any special ways of visualizing the X that we might latch onto? 
comment
Do we want to move forward on this? It's been sitting a while, and I think we should be able to make a decision.   Personally, I don't think any positive decision we make has to be permanently binding---we can always change logos in the future. While the logo is literal in a sense, it also looks nice. So I am for adopting it as a logo, and if someday we come up with something different that we'd like to use, then so be it. 
comment
I like the idea of making the symbol separate from the name, and this indeed makes having a non-literal logo a bit more important. I wonder if something like a preferential attachment graph that shows a maximum spanning tree highlighted could do the trick. Maybe the spanning tree could spell networkx? That last part would probably not be legible. 
comment
This sounds like: form the transitive closure graph and then take the node-induced subgraph from it.  Though its a bit overkill since you dont need closure for the nodes that arent removed.  Are you and I doing the same thing again? I was just about to submit code for transitive closures. 
comment
Yeah, the closure is not quite right....you only need to closure for paths involving the nodes to be removed.  Well, that's what you'd need had I actually read what you wrote correctly.  I read:  > if there is a path between vertices s and t in the original graph that contains only s and t and removed vertices, then the "patched" subgraph will contain the edge (s, t).  as   > if there is a path between vertices s and t in the original graph that contains the removed vertices, then the "patched" subgraph will contain the edge (s, t). 
comment
Fortunately, the old trac site is still up: https://networkx.lanl.gov/trac/ticket/662 
comment
It sounds like you want something a bit more flexible right? If so, I'd say to go ahead and implement it.  What you describe seems well-defined (even for directed graphs). 
comment
Definitely agreed on the manipulation aspect. 
comment
Nice work. IMO, the placement of arrowheads is less than ideal. And there are a lot of z-order issues with https://github.com/rainwoodman/nxsvg/blob/master/src/a.svg. Generally, it's certainly an improvement.   Overall though, I think this is still not "nice" enough for my tastes...and I think this is something that all graph drawing in NetworkX suffers from. I had thought we were leaning towards removing drawing functionality (since we can't seem to do it well). Perhaps this pushes people to consider "proper" graph drawing programs instead (e.g. cytoscape), after exporting to a common format. Relatedly, there is a pure graphviz approach that exists outside NetworkX: [nxpd](https://github.com/chebee7i/nxpd)  So I'll go with the majority here, but I think it would be nice to settle on a set of guidelines. Are we keeping drawing functions and adding new ones? Or are we gutting them? Or .... 
comment
I don't think it's in here yet, and that's mostly because nothing else assumes that edge weights are probabilities. 
comment
I'm guessing we'd accept a PR for this. 
comment
Maybe in the form of readers to/from the METIS format? Possibly some callers that then bring the output back into Python? 
comment
Yeah, I could see that. NetworkX could probably host that repository too, which might make them formally a bit more coupled then if it associated with another github account.  We could always experiment with the submodules and optional components. 
comment
Related:   https://pypi.python.org/pypi/metis   (pure Python) https://pypi.python.org/pypi/PyMetis   (Boost.Python)  `PyMetis` doesn't seem like it fits as nicely (b/c it seems not to interop with NetworkX), but `metis` does.  @ysitu is your code mostly the same as `metis` in terms of functionality but implemented in Cython? 
comment
For AddOn...so one example would be Metis(AddOn) which configures a Cython Extension class and then setup.py grabs it and adds it?  Seems reasonable.  Btw, I saw how you currently modified setup.py.  From what I've seen, that's a pretty standard way to optionally add Cython support...so that's good.  One thing that might be helpful is to provide a way to turn off Cython extensions, even when Cython is available.  I've done something hackish and checked for a `--nocython` flag.  The other route---and its one to take if we want to keep NetworkX and its installs _primarily_ pure Python---is that no Cython extensions are setup unless: 1) it was explicitly requested via a flag as in:  ``` python setup.py install --cython pip install networkx --install-option="--cython" ```  and 2) Cython is actually available [with a hard failure if 1) is true and 2) is not true].  ---  Also, what is your thought on the completely separate route. E.g.:  ``` pip install cymetis pip install networkx ```  Then in `networkx/algorithms/__init__.py`:  ```     from .partition import * ```  where `networkx/algorithms/partition/__init__.py` does:  ``` __all__ = [] try:     import cymetis except ImportError:     pass else:     from cymetis.nxapi import *     __all__.extend(cymetis.nxapi.__all__) ```  This keeps nearly all metis-related code together and requires only the simplest code in NetworkX. I'm wondering if this scale better to many different optional packages. Curious to hear others thoughts.  If we went this route, I think the NetworkX documentation would need a dedicated page highlighting support packages that, when installed, provide additional functionality. 
comment
Ok yeah, I can see that. So then the addons are completely distinct (just shipped with NetworkX, possibly via submodules, for added convenience).  The idea then would be that Networkx proper should never look inside `networkx.addons` at all.  But things in networkx.addons will certainly look into NetworkX. 
comment
Sounds interesting. Would love to see how this might (or might not) work. 
comment
I'm generally open to this, especially dense graph representations.  Whether its using csgraph or writing Cython (possibly Numba in the future), there are some operations that can be sped up nicely. For dense though, I wonder how the API can be similar. Every time you add a node, you'd have to grow the matrices, which is maybe fine, but I'd definitely want a way to specify the size at initialization if you knew it.  My preference would be to keep this in NetworkX.  We can always reevaluate after their development. 
comment
APGL is new to me as well. Looks fun though. Note that it is GPL. 
comment
I'm personally okay with adding non-pure Python to NetworkX.  We already have NumPy as optional dependency, and lately, it seems that more and more of NetworkX is depending on NumPy arrays---so perhaps its not so optional anymore. [Granted, most people don't have to compile NumPy to get it installed, but why can't that be true of NetworkX as well.]  If it turns out to be a big burden, it might be possible to make these new features optional `python setup.py install --cython`  Ultimately, I think it depends on where NetworkX wants to be in the future.  If we push people who need performance out of NetworkX, then one possible effect is that NetworkX becomes less relevant as performance becomes important to users. Maybe that's fine, and we want to keep NetworkX less focused on performance and more about algorithms, ease of use, etc.  Or maybe we want it to be the main library used for graph analysis. 
comment
Probably. I'm just hoping that we could have both worlds: algorithms, ease of use, but also performance.  If giving up pure Python requires that, I think the benefit is well worth it, especially since package managers can handle the compilation concerns (just as they do for most NumPy/SciPy users). 
comment
> If at some point we want to Cythonize code, and I think we might want that sooner rather than later, I strongly believe this should not be part of the networkx core. networkx has a very low hurdle for diving into graph theory and it'd be a shame to raise that bar.  I'm not going to push this point further (since I won't have the time to contribute much code on this, at least in the near future), but if someone is up for it, I'd like a more detailed explanation.  There seems to be strong feelings about the pure-Python aspect of NetworkX.  I don't think its a big deal, and I don't think its even accurate since more and more of NetworkX is relying on NumPy (linalg, drawing, assortativity, centrality, link_analysis, etc).  In other words, practical, common use of NetworkX relies on a Python package that must be compiled. If the argument is  "Well most people don't compile NumPy, they get it through their distribution", then why can't that same argument hold for NetworkX? Whatever hurdle people jump through to get NumPy, that same jump will probably clear the NetworkX hurdle as well.  Maybe it's that these new features would be conceptually more difficult...i.e., too many choices on what type of class (lil_matrix, dense, dict) and so on. That's a much more convincing argument to me than the pure-Python one.  > I'm undecided where outside of networkx it would be best placed. We speak about a dense and a sparse version, so would one part live in numpy and the other in scipy? That seems disadvantageous.  I agree that a split situation would be undesirable...which means that it should probably be a completely separate package, or stay part of NetworkX, or ....    If it is a separate package, I'm guessing more and more people will start to prefer it over NetworkX, especially if it offers a really nice API (which I agree, should be natural to the data structures rather than match a NetworkX API that wasn't designed with these other structures in mind).  I'm probably putting the cart before the horse...so I'll keep most of my subsequent comments on design/API/coding related issues instead of on "where" it should live :)    TL;DR: Full steam ahead. 
comment
The flask approach sounds interesting. 
comment
Assuming we can make decent progress on this in a relatively short period of time, we could take on these "where" and "who" issues later on. At that point, we'd have a better understanding of how it fits in or not, and if we merged, it would be only a minor inconvenience for early adopters to change their import statements. A big disclaimer should be sufficient.   My naive view is that package organization can be fluid (so long as the time scales are short) and should not impede the actual coding, as much as possible. But if the time scale of development is likely to be long---possibly due to schedules or difficulties---then it might make more sense to decide now (especially if its developed outside NetworkX).  So, @Midnighter what are your immediate plans? Were you planning to code this soon, or was this meant primarily as an inquiry to test the waters?  My suspicion is that you'd be contributing the bulk of the initial code. So I'm fine doing whatever, whether that is a fresh repo on your github account or a branch on NetworkX repo. Again, I'm in the "organization can change" camp, so long as we revisit the issue sooner rather than later. 
comment
If you haven't already, you also will need to familiarize yourself with the various sparse representations. See `scipy.sparse` and `scipy.sparse.csgraph` 
comment
Yes, please! :) 
comment
I don't think it'd take much convincing. We were just caught up on the naming; being able to take the n-th power in this manner is certainly a useful operation. 
comment
@hagberg If it were not a method, but a function instead, does that change anything? 
comment
Thanks for the detailed example.  Terminology is somewhat painful in this area, but I believe what you are calling "isomorphism" is a "monomorphism" according to the VF2 papers.  And so, the current isomorphism code _expectedly_ does not find those mappings.  When a subgraph of MG is isolated, it is a node-induced subgraph.  Practically, this means an edge (u,v) in the original graph is in the subgraph iff both u and v are in the subgraph.  So if you consider the subgraph containing nodes 0 and 1 only, then it indeed has multiple edges (with labels 0 and 1)...and that subgraph is not isomorphic to the motif.  Does that make sense?  Some additional commentary appears in the module level docstring for:  ``` networkx/algorithms/isomorphism/isomorphvf2.py ```  It would be nice to extend the VF2 algorithm to handle non-node-induced subgraphs, but the published papers didn't describe the necessary modifications.  It's been a while since I've thought about this problem, and even then, I didn't put much effort into thinking about monomorphisms at that time---so it could be a simple extension or not. 
comment
Yeah I think this is on the right track, but we'd need to do more thinking on it and possibly, show that this is sufficient to handle monomorphisms.  I'll leave this ticket open since its not the first time people have wanted this functionality (but I'm going to change the title of the issue to match what is being asked for). 
comment
It seems like you did some tweaking, in part, to get under the allowed build time that TravisCI allows. How close are we to that? Is this going to be an issue that comes back to bite?  One cautionary note is that we were originally under time with RTD, then they changed their policy and we were over their new limit. 
comment
I don't have access to IronPython, so I can't test this, but it looks like IronPython might have some issue with the "types" module.  Line 23 is:  ```         return types.FunctionType(f.func_code, f.func_globals, name or f.__name__,                                   f.func_defaults, f.func_closure) ```  Can you try running:  ``` import types def f():     return True g = types.FunctionType(f.func_code, f.func_globals, f.__name__, f.func_defaults, f.func_closure) ```  Does it work?  As for sys._getframe, it is used by the decorator module (something NetworkX includes but does not maintain).  I suppose that means the decorator module does not support IronPython without the fix you linked to.  Perhaps this has changed recently.  If not, perhaps try contacting its author. I'm not sure if requiring people to make the change you did is acceptable or if we should find a workaround. 
comment
So I think we should hear from some of the other developers on this too.  In principle, I'm guessing its possible to find a way to make this work, but it would require "hacks" specific to the Python implementation.  It seems like this is precisely _not_ what we want to do. 
comment
Most users will not see deprecation warnings since Python silences them by default, just FYI.
comment
I updated your MWE to show the plots side-by-side.  The presence of nans definitely affects Kruskal's algorithm (which is what is used by `minimum_spanning_tree`) since it tries to sort all the weights. Of course, nan's are not sortable and so the resultant "sorted" list is not sorted in general.  We could remove the nans from consideration, but that also might cause bugs in people's code who didn't expect nans in their graph in the first place.  If we opt to filter them out, I think we should consider adding an option to raise an exception on the detection of a nan weight. 
comment
Looks good. Is this ready for merging? 
comment
I'm pretty sure, though it's been ages since I looked, that the code explicitly demands that the edge count between a candidate pair must be equal. And I thought there was a test for that too. So by the time the `generic_multiedge_match`, you are already assured.  Have you found an example where this doesn't actually hold? Scan through `networkx.algorithms.isomorphism.isomorphvf2.py` 
comment
I had no idea about this one. Thanks for bringing attention to it.  Does your writer use SAX? Does it pass all the unittests we have for grapml? 
comment
@dschult Thanks! :) These days, time is not what it once was for me. :-/
comment
How did improving what enters the stack affect run times? Is the difference most gone now? 
comment
Yes, I realize that, but I asked about run times, not time complexity. No big deal though. 
comment
I have a PR open that had some code to provide support for user-defined key functions..I think it was the one to make add_edge return a key. I can find it later today when I get access to something other than my phone. But I think ints are fine and if it is configurable, then people can do other stuff if they wanted. 
comment
If you want `G.add_edge()` to assign a key to each edge that is potentially informative in ways more than just uniqueness---say, insert datetime, or insert order (as a monotonically increasing graph-wide integer)---then people might find it useful for this to be configurable. It doesn't matter either way to networkx, since it is just an id, and I don't think we lose much by being flexible.  So far the main justification for why we need uuids is that it solves a problem when working with _multiple_ multigraphs. This seems to indicate that this isn't a problem that is intrinsic to the graph classes themselves.  In that sense, it seems unnecessary, but perhaps of use in particular situations---which is no different from people who might want a custom edge key function in their own particular situations. This is part of why I feel the default should stay 'int', but I'm not too torn either way.  [Aside](https://docs.python.org/2/library/uuid.html#uuid.uuid4):   > Note that uuid1() may compromise privacy since it creates a UUID containing the computer’s network address. uuid4() creates a random UUID.  These uuids are 128-bit and will increase memory footprints by a factor of 2 over 64-bit ints. If you really wanted to save some space, you could provide your custom function to use smaller ints (say `np.int16`) whenever you know a maximum number of edges that you will need to consider. 
comment
Regarding the failure of tests, if it is just an ordering issue on the results, then yes, you can use sorted(), but you might also consider using graphs with ordered dicts as your test structure. 
comment
I can take a deeper look later today.  At first glance, I'm really happy to see those failures :) as it highlights where we have made some bad assumptions about ordering.  Due to our choice of unordered dicts as the default graph class, I'm less enthusiastic about testing doctests now. Many of the examples expect the edges to come out in some particular order, which is not guaranteed. Yes, we can sort the output in a doctest, but then then the doctest is doing non-example-demonstrating things in order to pass the test. In other words, it no longer just a demonstration of the feature---I think this should be the _primary_ (and perhaps sole) goal of the doctests. Presently, the doctest has to become convoluted in order to pass. I'd rather doctests use the ordered graph classes, or we don't test docstrings at all anymore.   Bottom line for me is that making doctests pass comes at a readability cost. Even though it may be minor in some cases, users will still ask:  Why are you sorting the output? And we would have to say: Oh that has nothing to do with the example, we are just doing that so that the output is guaranteed to match what you might see on your end. After which, we should really be asking ourselves: Well, why didn't we just use a data structure that guarantees that fact and hides those details from the user?  I realize this is a bit off topic with your particular failure, but I'm interested in hearing people's opinions. 
comment
It's quite a bit of code for an example. I wonder if we could just make it clear that this stuff is experimental and not guaranteed in future versions. Getting some feedback from users might be helpful even.  I suspect the unspoken concern among many is that we've been talking about a proper solution for a long time now with what seems to be a shortage of free developer-hours. This is the closest we've come to an implementation, and so it might be good to provide a solution now.  But if someone is able to really push on the alternate data structures soon, then maybe we can hold off.  My own thoughts are that all the other data structures are nice in theory, but it's a generalization that is too big, too quickly...given our present commit activity levels. Maybe 2.0 should be a few steps up a staircase, rather than The Wall along the northern border of the Seven Kingdoms. 
comment
I'm fine either way, but if we wanted to test these out and have other people use them with ease, it's probably better that it be importable. Maybe all experimental stuff goes in:  `networkx.experimental` or `networkx.__future__`. Loosely, I had in mind that we could also toy with other class variants in this submodule...perhaps MixedGraphs, or graphs with a completely different organization (more similar to what Neo4j provides). 
comment
It not importable though. So if I sent you a code snippet using this new class, you'd have to download the gist and put it in the same directory as the snippet. While not that difficult, I suspect that's precisely enough of a barrier that most people won't use the gist. If I wanted to send you a complete snippet, it's probably better to just include the gist in my snippet...which is no longer a "snippet" at that point.  But it sounds like there is not as much interest in having this in NetworkX proper...so I'll go with the majority...whether it goes in an examples directory or a gist. Maybe one day we will get something into NetworkX that that users can use. 
comment
The named function seems cleaner to me...  ``` >>> all_edge_dict = {'weight': 1} >>> def eadf(): ...     return all_edge_dict ... >>> G = nx.SpecialGraph(edge_attr_dict_factory=eadf) ```  But if you really don't want the global...  ``` >>> def shared_edge_factory(attr_dict): ...     def factory(): ...         return attr_dict ... >>> G = nx.SpecialGraph(edge_attr_dict=shared_edge_factory({'weight': 1})) ```  If people want more flexibility, then there is always the "singleton" pattern. 
comment
Opps...I didn't take the pickling concern seriously enough :) (and good catch on my typo).  `pickle` is annoying like that, so I guess the class is a better option. Another option is to not use `pickle` at all and go for `dill` instead.  ``` python In [1]: %paste def shared_edge_factory(attr_dict):     def factory():          return attr_dict     return factory  ## -- End pasted text --  In [2]: x = shared_edge_factory({'weight': 1})  In [3]: import dill  In [4]: dill.dumps(x) Out[4]: '\x80\x02cdill.dill\n_create_function\nq\x00(cdill.dill\n_unmarshal\nq\x01U|c\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x13 \x00\x00s\x04\x00\x00\x00\x88\x00\x00S(\x01\x00\x00\x00N(\x00\x00\x00\x00(\x00\x00\x00\x00(\x01\x00\x00\x00t\t\x00\x00\x00attr_dict(\x00\x00\x00\x00s\x1e\x00\x00\x00<ipython-input-1-210506337a16>t\x07\x00\x00\x00factory\x02\x00\x00\x00s\x02\x00\x00\x00\x00\x01q\x02\x85q\x03Rq\x04c__builtin__\n__main__\nU\x07factoryq\x05Ncdill.dill\n_create_cell\nq\x06}q\x07U\x06weightq\x08K\x01s\x85q\tRq\n\x85q\x0b}q\x0ctq\rRq\x0e.'  In [5]: dill.loads(Out[4]) Out[5]: <function __main__.factory>  In [6]: Out[5]() Out[6]: {'weight': 1} ``` 
comment
Is there something to be done about that? The best case time complexity for this algorithm is O(V^2). The worst case is O(V!·V). Some graphs will inevitably trigger the worst case. If you have some suggestions, please let us know. 
comment
Should be straight-forward. If no one else jumps on this, I can tackle it over the weekend. 
comment
New to the convo...to answer that question in a satisfactory way requires that we make a decision on how to represent list/array types in graphml code, and since there is no standard, we'd have to choose arbitrarily.    One option is to follow graph_tool.  I think this is fine, but then its no longer a write_graphml() function.  Instead its a write_graphtool_graphml() function.  That's not to say that the existing write_graphml is without issues, but I think its a separate story.  Also, I have nothing against having a write_graphtool_graphml() function.  As of now there aren't too many third party extensions to consider, so having one specific for graph_tool (especially since it is a Python package) seems reasonable.   We could instead convert anything that is not a fundamental Python type to a pickled object and store that in the graphml code.  Then, we wouldn't have to worry about any sort of introspection.  This would certainly give us a fixed point for composition of read/write, but if we are going to do that, why not just pickle the whole graph and not use graphml at all?  This is a serious question.  I'm trying to understand the goal a bit more.  It seems like the route is:     graph_tool <--> graph_tool graphml <--> networkx  The first part of that route is established, and we are now concerned with the second route.  Are there other packages (possibly non-Python) that can read-in graph_tool graphml code properly and also write it again?  If so, is it correct that such functionality is specialized for graph_tool?  If the goal is simply to get from graph_tool to networkx, we can provide a pure Python solution that does not require writing to a file at all.  Maybe this is the best option?  I don't see why it is of value to drop into xml if there aren't other tools that will also be able to read the graph_tool graphml.  That said, I am still okay with idea of having a specific graph_tool graphml reader/writer. 
comment
Be bold @nicktimko (and creative)! :+1:  
comment
I will be there this year, but unsure if I will be there for the full sprint.
comment
Looks like a bug here: https://github.com/networkx/networkx/blob/master/networkx/readwrite/gml.py#L587  ``` >>> isinstance(True, int), str(True) True, 'True' ``` 
comment
My comment was utterly unhelpful. Note to self: Do not link to a line in master, only link to a line on a fixed commit.  The line I intended to highlight was:  ``` if isinstance(value, (int, long)): ```  In this case, the boolean passes through and is converted to a string. 
comment
@jg-you since you opened this issue, you should be able to attach code to it and turn it into a PR, instead of opening a new PR. See: http://stackoverflow.com/questions/4528869/how-do-you-attach-a-new-pull-request-to-an-existing-issue-on-github 
comment
Or just a link to it. The numpydoc standard is pretty standard. :) 
comment
The wiki is good for other reasons too. I've often wanted somewhere to temporarily place miscellaneous notes. For example, guidelines for developers on how NetworkX prefers handle corner cases, such as when a function is to a return a path but that path doesn't exist...or what None tends to mean in this package. After some iterations, we can always move it to documentation proper. 
comment
I wonder if we should start of less ambitiously.  For the most part, the only other type of backend that seems to be requested repeatedly is one which uses ordered dictionaries. Maybe we should provide an interface to use ordered dicts on all dict-structures, and kick the other backend stuff down the road. This would require almost no change to the API. I share @ysitu's concern that tackling this whole issue is a big undertaking for the developer-hours NetworkX seems to have presently---and one that no one has really claimed responsibility for. There's also #1076, #1080, and I don't know if we have thought through some of these other backends well enough to know whether it even makes sense to attempt a uniform API. 
comment
Sounds interesting. I'm +0 on removing G.nodes only because it's been there forever, but I could easily be persuaded.  As for OrderedDict, leaving aside hash randomization, people have wanted ordered dicts to keep the node insertion order, or edge insertion order, etc.  So it need not solve the hash randomization issue for it to be useful to others. 
comment
ANY sounds interesting, but it is an equality comparison (with a logical OR over the options) only...all other standard comparisons supported by databases are not included. I'm curious if filtering on equality is frequently requested missing feature, and if that is what motivates this. Should we make this more general to include other possible comparisons? 
comment
Right. Ok, yeah this sounds good and looks good too. 
comment
I can think of one case where a unified API won't work: [mixed graphs](http://en.wikipedia.org/wiki/Mixed_graph).  But we don't even support mixed graphs right now, and all the existing algorithms would have to be modified to support them anyway. Graphs like these come up in probabilistic graphical models under the name "chain graphs" or "partially directed acyclic graphs" (see Probabilistic Graphical Models by Koller and Friedman). 
comment
Should the same be done with `NS_XSI`? You'll also need to update the unit tests to reflect this change. 
comment
@kmanninen Any opinion on `NS_XSI`? 
comment
Golly, this piece of code has been problematic. Let me take a look!
comment
How does this relate to #1210? 
comment
Can you construct the path that you think would be optimal and see if it is actually lower? We don't have your data :)
comment
Line 539 is: `node_2 = min(G2_nodes - set(self.core_2))`  This doesn't assume orderability but hashability. The algorithm performance will drop dramatically if we cannot assume that. So I don't think this is going to be fixed. 
comment
Opps, missed that. Yes that's correct.  Feel free to read the paper here, but this algorithm does indeed seem to require orderability. Page 4, Line 2: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.5342&rep=rep1&type=pdf  I'm not sure it is worth changing the algorithm to relax this constraint. After all, every graph with unorderable nodes is isomorphic to one with orderable nodes. :) 
comment
This one is a bit tricky.  In the implementation, we are using etree and it handles all the unicode/bytes stuff for you. The important point to note is that when we call `tostring()`, it returns utf-8 encoded bytes. This means that on Python 3, you cannot use StringIO and must use BytesIO. When you do that, it works:  ``` In [1]: import networkx as nx  In [2]: import io  In [3]: G = nx.Graph()  In [4]: G.add_node('\\1')  In [5]: s = io.BytesIO()  In [6]: nx.write_graphml(G, s)  In [7]: print(s.getvalue()) b'<?xml version =\'1.0\' encoding=\'utf-8\'?>\n<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">\n <graph edgedefault="undirected">\n    <node id="\\1" />\n  </graph>\n</graphml>\n'  In [8]: print(s.getvalue().decode(utf-8')) <?xml version='1.0' encoding='utf-8'?> <graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">   <graph edgedefault="undirected">     <node id="\1" />   </graph> </graphml> ```  As for Python 2.x, this is a bit of a wart in dealing with all the unicode/bytes differences and trying to keep a single codebase with 3.x.  The culprit is that we use `networkx.utils.make_str` whose goal is to take a string (bytes in 2.x, unicode in 3.x) and return unicode.  For 2.x, we use `unicode(text, 'unicode-escape')`. This allows people to have escaped unicode in their byte strings. For example:  ``` # Python 2.x >>> s = 'Sk\xf6ld'  # unicode-escaped bytes >>> print unicode(s, 'unicode-escape') Sköld >>> unicode(s, 'utf8') # fails >>> print repr(unicode(s, 'unicode-escape').encode('utf8')) # utf-8 bytes 'Sk\xc3\xb6ld' ```  So here we made a decision that strings in Python 2.x were actually unicode, rather than utf-8.  In your case, when 'unicode-escape' encounters the `'\\1'`, it has trouble doing the conversion and returns the empty string.  The quick way around this is to pass in the node as unicode: `G.add_node(u'\\1')`. Then you'll get the behavior you want.  Or use Python 3.  Or we can rethink whether we want to assume bytes in Python 2.x are unicode-escaped bytes. 
comment
Thanks for the report @joebockhorst! 
comment
@rstmr nice find.  You might be the first person to use this particular feature, especially for multidigraphs, ...in many years.  Would you be able to submit a pull request with this fix implemented, along with a unit test? No problem, if that is not a possibility. Just let us know. 
comment
This was just to simplify the use case where `attr='weight'`; that is, a single attr instead of a list. In hindsight, we probably should just do something like:  ``` if nx.utils.is_string_like(attr):     attr = [attr]     value = [value]     op = [op] ```  and then proceed with only one implementation. If that makes sense to you and you agree, let's do that in #2124.  The only reason we might keep it as it is, is that there will be some performance hit with the extra looping that you have to do in the case that `attrs` is a list. 
comment
The main reason was because that is how NetworkX preferred to test for a string.   However, the rationale for the particular implementation of `is_string_like` has more to do with performing a test that does not rely explicitly on type-checking.  See:  https://www.safaribooksonline.com/library/view/python-cookbook-2nd/0596007973/ch01s04.html  for a more thorough description---in particular, take note of the issues with detecting when `attr` is a `UserString`. Also, keep in mind that this `is_string_like` function was written many, many years ago at a time when the Python world wasn't as clean/consistent as it is now. :) 
comment
Generally, looks good. We will need a unit test for this new algorithm before we can merge it though. Can you add one? 
comment
Question for others: Why do we even provide the option to specify `u` and get back `harmonic_centrality[u]`? The amount of code, either in the implementation or for the user, is small in both cases, but I feel like including it in the implementation is undesirable for two reasons:  1) It needlessly "complicates" the scope of the implementation. 2) It's potentially misused by users who repeatedly call `harmonic_centrality(G, u)` for different `u`.  Would it be better if we just made the user type:  ``` python hcall = harmonic_centrality(G) hc = hcall[u] ``` 
comment
I'm sending you a PR which adds the changes we discussed and also fixes the unit tests. Once you merge it in, we can merge this PR into NetworkX. 
comment
Thanks for the contribution! 
comment
Oh Read! Nice. I haven't read that book, but Read has some other work about orderly generation of graphs which I'm sure this book is based on to some degree (pun intended). The generation order is a bit arbitrary in some cases if I recall correctly but well-defined and designed to avoid having to perform an isomorphism check of new candidate graphs against all previously generated graphs.  If you can find this paper: "Every One A Winner; or How to avoid isomorphism search when cataloguing combinatorial configurations", it might have the info you need. I have a copy on my machine at home, so let me know if access is difficult. 
comment
Because it's not clear that what was proposed is desired. Even if it were conceptually desirable, testing against None should not be done with !=. 
comment
Looks very nice! I can take a better look later today. 
comment
A couple quick comments.   In `get_date`, you consider two possible attributes `'date'` and `'datetime'`. It might be more flexible if the constructor to `GraphMatcher` and `DiGraphMatch` accepted as an input the desired date attribute. Then users can use whatever attribute name they want.  Also, the constructor uses `d=timedelta()` as a default delta. What do you think of renaming this to `delta` and making it a required attribute? The interval that determines compatibility seems essential to the algorithm and not something the library should pick for the user. [It also should be documented in the `__init__` how this choice affects the isomorphism task.  The code does conversions from a string to a datetime instance. Any thoughts on whether we should just require that the attributes be datetime instances already? This potentially provides a speedup, since conversions won't happen repeatedly. It also allows users to add timezone information to their datetime objects---this might be crucial if the datetimes span day light savings periods, etc. It also takes away the hardcoding of a particular datetime format (e.g. `'%Y-%m-%d'`) 
comment
Yeah, let's do it. We can iterate on it, if anything is discovered later. 
comment
https://github.com/networkx/networkx/issues/1654 https://github.com/networkx/networkx/pull/1376  Can you explain how these proposals solve the problems you mentioned?  I think the first option makes the API confusing---if it is a multigraph and we want to say that explicit keys are required to get multiedge functionality, then the API should just make the key be required. But doing so is overkill, since one often doesn't care about the particular key that was assigned.  The second option could work, but my opinion is that uuid is overkill. Either way, it should be customizable. I also think an always increasing global integer could work just as well, while being more memory efficient.  In this case, the key signifies the total number of edges that have ever been added to the graph.  ``` G = nx.MultiDiGraph() G.add_edge(0, 1)  # key = 1 G.add_edge(0, 1)  # key = 2 G.add_edge(1, 2)  # key = 3 G.remove_edge(0, 1) G.add_edge(3, 4)  # key = 4 ``` 
comment
Yes, so the argument is: if one is in favor of uuids (which I am not), then one should like auto-incremented, global integers more since they use less memory, are faster to generate and handle too. They also don't put potentially sensitive information inside a graph that you might distribute, while uuid1 does. 
comment
My broader point is that if the user doesn't care about the meaning, then global ints are better than uuids, and they still have some meaning that is at least relevant to the graph. If the user wants to have keys that are meaningful in other ways, we should allow them to provide a generator. 
comment
Yeah, that is a pain and it is one reason why I advocated in #1376 that the autogenerated key should be returned by `add_edge`. The key is required to query the graph for a particular multiedge. If we provide a mechanism to autogenerate that required key, then it seems reasonable to me that we ahould return it. Or not provide the mechanism at all, but that seems rather awkward for users. 
comment
My opinion is that we do both: return the key and expose the generation mechanism :)    As to what is returned, I could see either the key or the tuple. No strong opinion. In #1376, I provided a custom subclass that I use for primarily edge-based algorithms. It has an extra dict that associates keys to (u, v, data) tuples. This enables quicker access to and iteration over edges. 
comment
By luck, I found my original ticket on this point! #278  We decided at that time not to implement it. I still needed it, so my own implementations of `MultiDiGraph` have had this feature for some time. In that sense, I am not particularly tied to the decision this time around either, but I'm chiming in nonetheless. :) 
comment
Maybe make sure the networkx that is being imported is indeed the conda networkx. 
comment
The change in `branchings.py` should be ok to do. None of the code actually uses `attr_dict`. 
comment
Has this always been failing? That would strike me as odd, since no one has brought it up before.  To the intended behavior, can you explain why you think it is important that two nodes have the same attribute dictionaries?  If I have X number of attributes on node A and Y number of attributes on node B, but my isomorphism tests only cares about the 'weight' attribute, then it seems the only property that should matter is that `'weight' in set(X).intersection(set(Y))` and that the values on those 'weight' attributes are "equal". 
comment
And in fact, the 'weight' attribute need not even be in the attribute dictionaries, since this function accepts a default value for whatever attributes you care about. 
comment
No worries! I'm just happy that someone is looking at and using this code.  The more eyes, the better. 
comment
Won't the key error raise in the if/else clause immediately before your modification? It seems to me the code should do a quick:  ``` if not (source in G and target in G):     raise Something ```  or something similar at the very beginning. I'm unsure of whether a NoPath vs KeyError is appropriate here. KeyError makes more sense to me, but perhaps others feel more strongly about it then I do. Maybe we should have a new exception UnknownNode. 
comment
@alexnikleo UnknownObject doesn't provide a contextual interpretation. We know that a specified node is not present in the graph. Do you think that `NetworkXInvalidNode` is more informative? If so, let's try to create that instead, and then, update most of the functions in this module. So in the case that u and v are both `None`, then we should immediately check that they are both in the graph. If not, we raise the exception saying which of `u` or `v` or both is not a valid node in the graph. 
comment
Also, you will need to update the api/release notes found in the `doc/source/reference` 
comment
@alexnikleo are you still interested in this PR. Let us know if not, so we can proceed. 
comment
If it is large, are you able to make something smaller (possibly by removing nodes that you know are not relevant) and paste it here? If not, put it in a [gist](https://gist.github.com/). 
comment
That's great. Sorry for the confusion. Additional discussion now appears here: #1445  
comment
My fuzzy recollection is that See Also is something that is not part of Sphinx-proper but was added by [numpydoc](https://github.com/numpy/numpydoc). So maybe something has changed in recent versions of Sphinx. 
comment
Opps. 
comment
I was about to fix this.  But Java does  not actually accept 0 and 1 as boolean values, nor does it accept True or False. So, maybe we should not allow them? Still looking up the spec for gexf.  #1385. 
comment
http://en.wikibooks.org/wiki/Java_Programming/Literals#Boolean_Literals says that only `true` and `false` are allowed in the language. I guess string parsing is different---not sure how GraphML is meant to be interpreted, but the string parsing seems the likely way.  For gexf, http://www.w3.org/TR/xmlschema-2/#boolean seems clear that only lowercase true/false and 0 and 1 are allowed. 
comment
Ugh, just tried a rebase. Didn't work like I wanted. Hang on. 
comment
You can click the red "X" for more details.  I didn't check them all, but some of them look to be test failures unrelated to your changes.  However, the file you submitted is not even a Python file and it is not incorporated into the package. If this is unfamiliar to you, we can help out. However, I am not seek how this is a parallel implementation. Can you explain that? 
comment
Ahh, so it is not a algorithm that exploits parallelism (e.g. threads, processes etc), but rather, a single-threaded algorithm that functions in a way that explores many options simultaneously. Confusing name, is that name standard? 
comment
@woodElec, my guess is that is because the original graph is traversed for each v in G, and the multiple traversals repeat work already computed. Can you try to make use of `all_pairs_shortest_path` and then compare? 
comment
Unless we actually implement this in a parallel way, calling it a parallel algorithm seems to be misleading. If someone codes up a version that uses a parallel version of all-pairs shortest path, then I think the shoe fits. 
comment
Node information is preserved (if the node is the head or tail of one of the edges). 
comment
Do we just need to update the unit test? 
comment
Looks like we were too slow in merging this. If you can rebase or update, that'd be great! 
comment
My opinion of pep8 is that is should mostly be followed, but that we should be practical about it.  Using `u` and `v` from the tail and head of an edge is _now_ standard in NetworkX. While I can see that it makes the code more readable to use something like `from_node` and `to_node`, it is also a bit ugly for one-liner list comprehensions. I'm also with @Midnighter on indentation in those cases. 
comment
And actually, I don't think we can even review this PR: "Sorry, we could not display the entire diff because it was too big."    Note also that other projects, of which NumPy and Matplotlib (iirc) are included, have a policy where PEP8 changes are only applied on source files that were touched in the process of making other changes or adding new features. The result is a slow progression to PEP8 compliance. A big PR like this has a high probability of creating merge conflicts with existing PRs. 
comment
It looks like >99% of the changes are just whitespace issues around operators and commas etc, of which I'm generally in favor of. So I'm generally ok with the changes and think it's probably safe from the code perspective to merge this, but I'm just not sure if we _should_ merge this. Potential conflicts with existing PRs are a concern for me, and my general preference is that we do these changes in the process of editing files for non-PEP8 reasons, rather than all at once. 
comment
@jfinkels Btw, please don't be discouraged. We definitely appreciate the efforts (here and also in #1347 etc) that you are making to `networkx`, and are looking forward to future contributions! 
comment
I can see the argument for moving the tests out of the package. Happy to go with the majority on this one. 
comment
I just thought of another reason to keep them (at least for developers). It is often the case that you develop code and run the subset of tests related to your changes reguarly. This is a pain if the tests are in another directory. 
comment
 Though I could just have two terminals open...man, life is tough. Geez. 
comment
No worries and keep up the good work. Your help is much appreciated! 
comment
I fixed part 1). 
comment
And the redirect seems to be working fine now.  Thanks for the report! 
comment
I believe that this is giving the correct answer.  The subgraphs that are constructed are node-induced subgraphs. So when you test if a subgraph of c is isomorphic to g, then note that the node-induced subgraph on nodes (1,2,3,4) is isomorphic to g.  On the other hand, there is no node-induced subgraph of h that is isomorphic to g since the edge (2,3) will always be present.  This is the distinction between a monomorphism and an isomorphism. See also #948 and http://stackoverflow.com/questions/459799/whats-the-difference-between-subgraph-isomorphism-and-subgraph-monomorphism  Note that presently, we do not support searching for monomorphisms.  If you or someone wanted to work on that, that would be fantastic! 
comment
Going to close this now. 
comment
I tried looking in the history. #501 seems relevant. Maybe @rafguns contributed the code? @hagberg might remember. 
comment
I just looked at http://bugs.python.org/issue3332. While I understand why they don't consider it a bug, I agree with the bug advocators that it makes doctests _confusing_ for users. We end up having to throw a lot of non-example-demonstrating-code into the doctest.  Is there some way that we could configure how doctests are executed/verified? It would be nice if we could get it to do actual dictionary equality whenever the output looks like a dictionary, etc. Adding line comments such as `#doctest: object_equality` or `# pragma: nocover` are not an option, in my opinion, as it only serves to confuse the user who doesn't care about the fact that developers want to test all their docstrings as part of the build process. 
comment
As linked above, see also some other relevant comments on this issue:  https://github.com/networkx/networkx/pull/1350#issuecomment-73762339. 
comment
If we want to keep doctests clean and simple but also tested, there is another option to consider. It could be a well-defined, but perhaps large, task for a GSoC project. #1326   Idea: Convert all doctest style examples to use the [IPython directive](http://matplotlib.org/sampledoc/ipython_directive.html). Here is an example:  ``` python .. ipython::     In [1]: x = 'hello world'     # this will raise an error if the ipython output is different    @doctest    In [2]: x.upper()    Out[2]: 'HELLO WORLD'     @doctest float    In [154]: 0.1 + 0.2    Out[154]: 0.3 ```  When rendered, the user will see:  ``` python  In [1]: x = 'hello word'  In [2]: x.upper() Out[2]: 'HELLO WORLD'  In [3]: 0.1 + 0.2 Out[3]: 0.30000000000000004 ```  or something similar. This will pass the doctest however, since we have transparently instructed sphinx to compare floats (and it knows not to test for exact equality). We can quietly seed random number generators as well. When hiding seeds, there is still an issue that the user, when pasting, might not see exactly what is in the example, but we can at least guarantee that it will work via this method. There are a few projects that use this format for their documentation---[pandas](http://pandas.pydata.org/pandas-docs/dev/dsintro.html) is included in this group.  Now, this is primarily used for documentation and not for docstrings. One concern with doing this in docstrings is that docstrings are not typically rendered (at least from the IPython terminal), and so users would see the `@doctest float` stuff. This is clearly undesirable. So one "technical" problem that someone could work on is to pass all docstrings through to Sphinx during build time and store the rendered output as the docstring. Overall, this will clean up the docstrings and it will guarantee that we have working, tested doctests. 
comment
Regarding unit tests, we should make sure this issue figures out a nice way to handle the ordering issues mentioned in this comment: https://github.com/networkx/networkx/pull/1356#issuecomment-74567160 
comment
Can you update the api/release notes too? 
comment
@jtorrents Good idea on actively suggesting that people update the credits. In addition to the release notes, I'll start mentioning that more regularly. 
comment
@jtorrents How common is it to want to calculate node connectivity between all pairs?    If it is quite common, I'd lean towards including a simple function that does this for the user.  One general complaint of Python libraries (i.e., in comparison to R), is that sometimes they tend to be too low level.  The example you provided is >10 lines right?  It involves the use of itertools, auxillary digraphs, and residuals. Users who only want node connectivity between pairs and don't care to learn the NetworkX implementation will thank you for being able to calculate it in one line. 
comment
I'm fine with either. The NumPy array is more efficient, but it is a dependency. So dict works. 
comment
Merging in 3 min... :) 
comment
Is this good to go? Or did you have further stuff to do? 
comment
It might be hard to back them out now.  [Aside: My editor automatically removes trailing whitespace on lines...though that's usually easier see as 'noise'.]  I glanced briefly at this and I like the idea. Implementation passes the smell test, but I haven't studied it in detail. 
comment
Also, I know @dschult has spent time with this code. Would be good to hear from him. 
comment
Regarding the maintenance concern, would it be addressed if all these were made into functions instead? nx.copy would be the primary implementation, handling all classes and methods are wrappers around it. Similarly for reverse, etc. 
comment
+1 from me. @ysitu can you rebase or merge in master? Looks like there might be some conflicts. 
comment
Sounds good. Hopefully you get a "yes!" 
comment
I think that was a next consideration after this PR. 
comment
It looks like it could use a bit more editing.  But I suspect that duties took the author elsewhere, so we might have to tackle them. 
comment
Since this is renaming functions that already existed, it will be important to mention this in the release notes. Can you update those? And feel free to add yourself to credits.rst if you'd like and haven't already. 
comment
It looks like each graphviz_layout is independent of the others. Are these supposed to be chained, trying the next until something succeeds? 
comment
Something that I just noticed: The function signatures for `nx_pydot.graphviz_layout` and `nx_agraph.pygraphviz_layout` are not the same. We could probably make them match, but now I'm wondering if we should require the user to decide which `graphviz_layout` function should be called. In other words, we keep this as is? 
comment
It looks like we do this for all the drawing functions now, even in `nx_agraph`. For example, `nx_agraph.to_agraph` has a function level import for `pygraphviz`.  If we don't want run time errors, we could make `drawing.__init__` try to import the various modules (pydot, pygraphviz) and if it fails, then the corresponding submodule is not imported. This should allow the submodules (`nx_agraph` etc) to assume that the needed package is available. 
comment
Ah ok, that looks familiar. :) I'll make this a PR shortly. 
comment
Here's another thought...if we do this conditional import, then if pydot/pygraphviz is not available, the user will get an `AttributeError` when doing a call such as: `nx.to_agraph`. So this is still going to be a runtime error from the user perspective, esp if the call happens inside some function they have defined.   I wonder if this might this be more confusing. E.g. the documentation suggests this function should be available, but it does not seem to be. Also, the error message no longer makes it clear why the exception arises---namely, that `pygraphviz` is unavailable.  Contrast this to the situation when the exception is raised by the function when called. Here we explicitly tell them why it fails and that an optional package is needed. 
comment
Yeah it seems like not doing any imports in `nx.drawing` and requiring the user to explicitly import `nx_agraph` or `nx_pydot` could be more clear. This means these functions will no longer be available at the top level namespace.   With 2.0 coming around, I wonder if this is the time to simply remove the drawing subpackage and make it an add-on with a required dependency of both (or maybe at least one of) `pygraphviz` and `pydot`. 
comment
Ok let's do that. We can probably close this, and then link to it in a new issue that will turn `networkx.drawing` into `networkx.addons.drawing` and `networkx.addons.layouts`. I'm off to the Jurassic ...  ```                                 _.-.                                /  99\                               (      `\                               |\\ ,  ,|                       __      | \\____/                 ,.--"`-.".   /   `---'             _.-'          '-/      |         _.-"   |   '-.             |_/_   ,__.-'  _,.--\      \      ((    /-\   ',_..--'      `\     \      \\_ /                   `-,   )      |\'                     |   |-.,,-" (                     |   |   `\   `',_                     )    \    \,(\(\-'                 jgs \     `-,_                      \_(\-(\`-`                         "  " ``` 
comment
That's how I prefer it too as well, and I don't mind a bit of duplication in the `requirements.txt`. It's not like we're managing a stadium full of dependencies. 
comment
While ugly, it might be possible to support this through a "global" configuration parameters.  All copy functions would check for the value of nxParams['memo'] which would default to False.  Advanced users could set it to True and then all such copy algorithms that check the 'memo' keyword would be usable.  This way you don't add noise to the call signature of functions, but you still remain flexible for this use case. 
comment
Would that address @brianthelion's concern?  If I understood correctly, the issue was all the functions in NetworkX which make copies that do not use the `memo` parameter.   
comment
@brianthelion you mentioned that you perform manipulations on the copied graph. Are these manipulations restricted to the topology of the graph, not making use of any node/edge data?  I'm just wondering what the graph structure should be?   All nodes will probably be integers. For multigraphs, the edges should maintain their keys.  What about the node and edge attributes? Do you want these available on the new graph?  I think I'm possibly missing the point still.  ``` mapping = dict(zip(G.nodes(), range(len(G)))) Gcopy = nx.relabel_nodes(G, mapping, copy=True) ```  Now you can pass `Gcopy` to everything and use `mapping` to translate. We could also just modify `nx.convert_node_labels_to_integers` to return the mapping it used as well. Then it'd be a one-liner. 
comment
The link was correct, we just needed to update readthedocs after releasing 1.10. 
comment
These all sound great. While I will be sure to chime and help out during the GSoC, I can't commit to being a mentor right now, as I'm expecting an especially busy summer. But I will definitely be around and will lend a hand! 
comment
@jtorrents @Midnighter 
comment
One other fun project could be to push on #1076. 
comment
@Midnighter isn't NetworkX already a single source base for 2/3? 
comment
Both @bjedwards and I have visited the networkx gitter room: https://gitter.im/networkx  That seems more than fine for an IRC equivalent. No promises on occupied that room will be in general, but live conversation is often helpful. 
comment
Aha. I'm guessing @hagberg do that, or create a second room that is open. 
comment
@MridulS Live chat would be nice, but there is no way that all devs would be able to particpate. Given that the discussion will take place at a slower time scale and will also require time to think/reply on some of the issues, the non-threaded format of github comments and gitter is also not ideal. So my estimate is that the best place for this discussion would be on the mailing list.  That said, if you want to just chat and bounce ideas around initially, let us know...perhaps some of us might be able to join on gitter, but no guarantees (we are all quite busy folks). I've created: https://gitter.im/networkx/GSoC_2015. This room is open to all, so chat there too. 
comment
@MridulS is that list final? I don't see NetworkX on it. So maybe we didn't get accepted? Or are we under one of the umbrella organizations? 
comment
From the log, I don't understand why it even considers the lanl address. We certainly don't intend for that to be happening. 
comment
I'm going to guess that this is some caching issue (maybe proxy) or possibly the index for devpi 
comment
I wasn't aware of pydot2. We can probably add that to the list we already search. See also: #933, #1334, #1389 
comment
@mfitzp If you make one of these other pydot libraries available, then I suspect it will just work. As of now, NetworkX tries the following modules (in order, breaking once it finds one): ['pydot', 'pydotplus', 'pydot_ng'].  The whole situation is _still_ a mess... 
comment
Also, could you add a unit test that covers this case, similar to what you wrote here: https://github.com/networkx/networkx/commit/1c067638f57292b42a0536d3444cc6e656174e78#commitcomment-11258642 
comment
Another option is to downgrade sphinx manually on readthedocs. I tried doing this on my fork, but rtd recently imposed some build time limits and we exceeded them. I haven't had time to look further, but if someone wanted to investigate, see my fork's master branch. 
comment
The only changes required were in the `requirement.txt` and `conf.py`.  Specifically, we explicitly use an older version of Sphinx via pip and go back to using the napolean extension:  https://github.com/chebee7i/networkx/commit/2e2e40e200253f0986f3baa32e59a00b9e1190f0  However, I wasn't able to take it further due to a build timeout, and I hadn't spent any time since then trying to get it to build faster. I wonder if this is just due to it being the a build with no cache. More details here:  https://github.com/rtfd/readthedocs.org/issues/1767 
comment
Also relevant for anyone wanting to try out your own builds:  https://github.com/networkx/networkx/wiki/readthedocs 
comment
Just had another thought. I believe that pip now supports github links to install:  http://codeinthehole.com/writing/using-pip-and-requirementstxt-to-install-from-the-head-of-a-github-branch/  So maybe we do this: 1. Fork our own branch of Sphinx 1.3.2 2. Apply the one line fix in https://github.com/sphinx-doc/sphinx/pull/1892 3. Point our `requirements.txt` to our (temporary) fork. 4. Build happy.  This might side-step the build timeout issue since Sphinx 1.3 seems to build faster than 1.2. 
comment
Added! 
comment
Thanks for tracking this down! 
comment
Good catch. I think we do a single-pass right now. This will require two passes but probably should be done. 
comment
I def don't think we should be converting non-numeric types to numeric...and worrying about the numerous ways of representing bools is a pain. But numeric attributes seem like an easy use case to handle specially.   I'd imagine the cases where one wants mixed numeric types is near nonexistant, and so if we see both int and float, there seems to be an obvious answer in terms of what the user would like during conversion...very similar to what most (friendly) CSV readers when a column contains numeric looking data but with only some having decimals in them. I guess we'd at least need to allow this inference to be be able to be disabled. 
comment
@gdbassett, if there were an interface that allowed you to specify the type for each attribute, would that be sufficent? Or would you want it automatically inferred? 
comment
Possibly, it looks like casting rules have changed in NumPy and our TravisCI configuration is now picking up the latest version. 
comment
It's a shame that sphinx's one-line solution cannot be monkey-patched in...the fix is in some very long function. 
comment
I think it is manually done or periodically (but not on each commit).  A port to use ReadTheDocs would be a nice task to complete if someone had the time. 
comment
I think they use napolean which is now part of Sphinx proper, and it supports both Google docstring and numpydoc. 
comment
https://github.com/networkx/networkx/issues/1221 
comment
Can you enable the readthedocs GitHub hook for the networkx repo? I've got the initial configuration ready at readthedocs.org. All maintainers will have access to it if they give readthedocs access to their Github account. 
comment
Wish I could make it, but SciPy is definitely a fun conference. 
comment
For some reason, my memory tells me that we had previously come to some sort of soft plan that 2.0 would try to avoid being a major wall we had to climb. We already have tons of new functionality, as is. If we focused 2.0 on renaming the `_iter` functions, then we can postpone many of the other changes for a 3.0.  3.0 doesn't have to be that far away. IMO, it is much easier for people to adapt code to backwards incompatible changes when the number of changes are small and fairly insignificant. Lumping them all into one big release might be asking for trouble. If I can find it, I will link a similar discussion that happened in NumPy. Also, we don't have to go to 2.0 right now, we could just as well do a 1.10. 
comment
Big changes to the API still seem like they could be part of a GSoC proposal, but the GSoC need not be tied to the release of 2.0.  IOW: 2.0 might be released with partially completed project. 
comment
That (pushing changes for 2.0) seems reasonable to me, but let others chime in too.  Regarding the timeline, we haven't stuck to those deadlines in the past---usually being overdue. I don't think it would be a huge concern if we were early either. 
comment
Can we use `d.viewkeys()` etc, since we are 2.7+ now? 
comment
Ah right, `viewkeys in 2.7+` <--> `keys in 3.x`. 
comment
Yeah, I wouldn't mind a 1.10 release or something similar. Presumably, there will be people who will be quite slow to migrate to 2.0, so it'd be nice to get as many features out with the old API as possible. 
comment
We're gonna have a fun time updating all the PRs and issues that were tagged as 2.0. Haha. No big deal though. 
comment
Yeah, that part can be confusing, but the version number should not be interpreted as a decimal---afterall, how would one interpret 1.2.1. Periods separate integers (possibly with alpha, pre, or whatever also attached to them). 
comment
Yeah 2 seems fine. I'll definitely be participating in the discussions. 
comment
We should be careful about making edges() return a "view". I know I have a ton of code right now that _modifies_ the data dict from edges(data=True). The entire point is to change the graph. So I'm sure others are doing similar things. 
comment
Err...I guess a view wouldn't change that fact. Continue on... 
comment
I don't see how we can make edges() provide a `__len__` without first iterating through all the edges that are to be returned. 
comment
Is this ready to merge? Please consider adding yourself to `doc/source/reference/credits.rst`. 
comment
Yeah, I also think we should make this jump in 2.0. 
comment
Yeah, I can see the value in doing it earlier.  Do we have a third "yay" to start implementing this?  Here is where we find out how good of test coverage we have. :) 
comment
I think it was a half-baked idea :) But we quickly agreed that it would be a second class citizen (e.g. functions only, and not part of the main graph classes). 
comment
What is the standard procedure here? There are two PRs that we want to cherry-pick (#1597 and #1601) into the 1.10 release. I can do that, but should I create a new branch RC2 and put them there, or should I add them to the RC1 branch? 
comment
Yeah, I think that makes sense. I think tags are independent of the branches, since they are tied to a revision. So I will checkout the tag for 1.10rc1, then push up a branch `networkx-1.10`. From there, I'll cherry-pick the desired commits. At some point, we can tag for 1.10rc2 or whatever. 
comment
I think we are good for the 1.10 release. We are going to have to keep this branch alive for a while, b/c I suspect we'll have to keep support for the 1.x branch for a while (and maybe even put out bugfixes for it, but hopefully not). 
comment
I'd be fine with that. If we ultimately decide against namespaces, we can always remove `networkx.addons` in 2.0 or later. The perturbation required to networkx seems fairly minimal, and even with `networkx.addons`, we still have the option to _not_ use namespaces on a package by package basis. 
comment
@OrkoHunter but there are obvious weird parts about namespaces that don't seem to have a great support. Suppose, I install networkx as `pip install -e .` Then, suppose I install networkx-metis as `pip install .` and networkx-lemon as `pip install -e .`  What happens in these cases? Could we even uninstall properly? In one of the other conversations, I linked to an issue that seemed to indicate that editable installs were problematic for namespace packaess. If we went with an external package, then there would be no issues.  When it comes down to it, what really is gained by using a namespace? Is it just that the addon looks as if it is more "official" since it is in `networkx.addons`? If so, I don't know why official-ness should take precedence over compatible installs etc. 
comment
Note, this is only non-consuming if the iterable passed to `peek` is not already an iterator:  ``` In [1]: x = {1,2,3}  In [2]: y = iter(x)  In [3]: next(iter(y)) Out[3]: 1  In [4]: list(y) Out[4]: [2, 3] ``` 
comment
Interesting that the for/break approach was so fast! Perhaps this alone is why it might be nice to abstract it? 
comment
Any interest in changing this to use the for break paradigm instead? 
comment
Ok, I'm fine staying with the existing `next(iter(S))`. 
comment
Any chance we could test if S is already an iterator and raise an exception if so? In that case, the operation consumes, which is probably not what was desired. 
comment
Opps...I thought I commented on this, but I must have forgotten to click "Comment". Here goes again:  We need to make sure we are good for 2.x/3.x. But @dschult's comment suggets:  ``` def is_iterator(it):     hasnext = hasattr(it, '__next__') or hasattr(it, 'next')     return iter(it) is it and hasnext ``` 
comment
Same. 
comment
Aside, I think @hagberg's suggestion to use `isinstance` would work. The abc classes in `collections` implement `__subclasshook__`, which enables `isinstance` to work as desired in this case. The only issue is that the abc module is not written to be simultaneously 2/3 compatible and so it only tests for `__next__` xor `next`. 
comment
Thanks! 
comment
Seems like a helpful enhancement. 
comment
I think we should update the documentation and the comment in the code to reflect that nbunch can be used to restrict which nodes are considered. 
comment
Separately from whether `OrderedGraph` is another possible solution, I just want to make sure I understand the concern. Is it that we are unsure if this function should search only the descendants of nodes in `nbunch` or if it `nbunch` should be extended so that all nodes not in `nbunch` are tacked onto the end, in some arbitrary order. If that is the concern, it seems simple enough to add a parameter to control that.  Constructing a subgraph first will definitely have bad performance...just a consequence of Python I suppose. 
comment
I suspect it was just an initial, reference implementation. 
comment
Thanks, I'll look into this.  It works fine without the `node_match` function, so something is probably wrong with the semantic checks. 
comment
Oh it looks like your example had a typo.  You have: `weitht` rather than `weight`. This caused the node match to inherit the default weight of 0.  
comment
Should this be closed? 
comment
The 2-tuple seems fine. 
comment
Also, testing `nbunch in self` is not going to work since nbunch (without preprocessing) might be a list. Probably going to need a try/except. 
comment
I think it's only an issue when `nbunch` is actually a bunch of nodes:  `nbunch=['A', 'B']`.  `if nbunch in self` will raise a `TypeError` that needs to be handled. 
comment
Ahh, it is OK because we do a try/except in `__contains__`. Otherwise, it would not be ok:  ``` >>> d = {} >>> [1,2] in d TypeError ``` 
comment
Aside, I think it is a bit unintuitive why we do a try/except in `__contains__`. Using unhashables with the `in` operator in Python _should_ return a `TypeError`, not `False`, in my opinion.  That would keep NetworkX consistent with the rest of Python. 
comment
Yeah it is probably too late to change now, but I think it warrants a comment in `degree` or other similar functions.   When we test `nbunch in self`, it looks as if we are testing to see if there is a node that is equal to a list in the graph, but that is not really what we are _after_ when we do that query. Rather, we are trying to determine if `nbunch` is a node that is in the graph xor if `nbunch` is an iterable of nodes.  If the latter, there is no expectation that the iterable of nodes is also a node in the graph. So while I think this works just fine, I think the code looks is out-of-sync from what we are trying to achieve at a high level, and so, a comment could make the code easier to interpret. 
comment
Buuuut...it's also just a NetworkX idiom that people can get used to seeing (as I have). 
comment
Also, it would be great if you could add a unit test that makes use of non-default weights. 
comment
It is noticeable but pretty minimal, right? Waiting 1.5 milliseconds is not exactly taxing. 
comment
I'm not even sure we guarantee that other vertices are comparable.  You can have an integer node and also a string node---that comparison is invalid. `None` is not supposed to be a valid node, but we do not enforce it. 
comment
Can you explain a bit more how you are receiving the error then? When `write_gml` is finished, the file should be closed---assuming everything is implemented correctly. 
comment
Essentially, the decorator does:  ``` python         try:             result = func(*new_args, **kwargs)         finally:             if close_fobj:                 fobj.close() ```  If you pass a string in, then `close_fobj` is `True` and so it definitely closes the file. I don't see how what we are doing could cause a failure every so often. 
comment
@jtorrents, as a developer you might consider setting up your own readthedocs build for NetworkX.  https://github.com/networkx/networkx/wiki/readthedocs  This will allow you to test out commits on your own networkx fork and see how they build on readthedocs. 
comment
Also, they seem to be generated on the readthedocs latest:  http://networkx.readthedocs.org/en/latest/reference/algorithms.approximation.html http://networkx.readthedocs.org/en/latest/reference/algorithms.connectivity.html 
comment
I think we can move over. Will you be able to keep the existing, older versions of NetworkX documentation available? I think we can only do 1.10 forward on readthedocs since it builds documentation from a particular commit.  The documentation.rst file has already been updated:  https://github.com/networkx/networkx-website/blob/master/documentation.rst  All future versions will point to ReadTheDocs. Since 1.10 hasn't yet been released, that link is current broken.  The links to older versions of the docs need to stay active. That should be possible correct? And we will never need to rebuild them. 
comment
The versions (like 1.9) that are on networkx.github.io should be fine and unaffected by us moving future documentation builds to readthedocs. So as soon as we release 1.10, we should re-push networkx-website to networkx.github.io. 
comment
@ysitu I think I read somewhere that the behavior might depend on the order in which imports are done--this is why both packages are supposed to declare themselves as a namespace package. So perhaps the reason it is working for you is because it is encountering networkx-metis before networkx? 
comment
Thanks for looking into this! 
comment
What about renaming `_optimal_spanning_edges` to `kruskal_mst_edges`. The idea would be that users could call either of:  ``` python kruskal_mst_edges(..., minimum=True) kruskal_mst_edges(..., minimum=False) prim_mst_edges(..., minimum=True) prim_mst_edges(..., minimum=False) ```  Then, I would get rid of `kruskal_mst` and `prim_mst`, and have only:  ``` maximum_spanning_tree(..., method='kruskal') minimum_spanning_tree(..., method='kruskal') ```  When `'prim'` is passed in, they do the obvious and call `prim_mst_edges`.  I would prefer to remove `maximum_spanning_edges` and `minimum_spanning_edges`. This just bloats the code, and people who want "lower" level stuff can choose to explicitly call `kruskal_mst_edges` or `prim_mst_edges`. 
comment
I guess I was thinking they are not even necessary any more. People who want the edges only can call one of the more specific functions `prim_mst_edges` or `kruskal_mst_edges`. So the recommended API for users is one of `minimum_spanning_tree` or `maximum_spanning_tree`. We will also expose the other two functions `prim_mst_edges` and `kruskal_mst_edges` but they aren't front and center. All four should have docstrings. 
comment
The min/max spanning tree is not unique, so the various algorithms could arrive at different ones. Just make sure that it is indeed finding _a_ maximum spanning tree.  Also, I'm not sure that the doctest output is reliable across different Python hash seeds. This is a bit problematic. See #1356 for more discussion. The doctest is supposed to demonstrate the max span tree (not just the weight), but we can't do so b/c the algorithm is no longer deterministic. This is yet another situation where the doctests would benefit from using `OrderedGraph` instead of `Graph`, even if the user might see something different on their end.   I'll have some comments on the code later, including the docstrings. But for the method, you should put something like METHODS = {'prim', 'kruskal'} in the global scope and make sure that the passed in method is in that set. If not, raise an exception. The docstring should say something ot that effect.  ``` method : str, {'prim', 'kruskal'}     A string specifying the method used to find the spanning tree.      This should be 'prim' for Prim's algorithm and 'kruskal' for Kruskal's algorithm. ```  Or something similar. Others feel free to chime in. 
comment
Opps sorry, just that the value for `method` is in that set:  ``` if method not in METHODS:     raise ValueError('Unknown method: {}'.format(method)) ```  Still not sure if this argument should be called 'method' or 'algorithm'. Haha. 
comment
Ok, let's do `algorithm`. 
comment
I sent you a PR which makes this change and does some refactoring. 
comment
Oh yeah, definitely. We should aim for 100% coverage in that file. 
comment
This needs to be added to `api_2.0.rst`. It should clearly state the API change and provide a migration path from the prior API. 
comment
We need comment from others. @hagberg @dschult   But also, while your addition to api_2.0 is sufficient, it isn't helpful to people who want to know how to migrate old code to new code.  Perhaps adding something like:  Calls to `minimum_spanning_edges(G, 'weight')` should instead use `kruskal_spanning_edges` or `prim_spanning_edges` as in `kruskal_spanning_edges(G, minimum=True, weight='weight')`.  Feel free to spice that up as you like. 
comment
Generally, I think we should move towards providing more information about changes between versions. Look here to see how pandas does it. http://pandas.pydata.org/pandas-docs/stable/whatsnew.html#whatsnew-0152-api 
comment
Oh yeah, that's right. I meant `kruskal_mst_edges`. The 'm' is generic right? It is an abbreviation that can be 'minimum' or 'maximum'. :) 
comment
We put the `maximum_spanning_arborescence` code in the tree subpackage too, so this was just to make it similar. If it comes up again, we can move them all out. 
comment
Anyone else want to comment on this set of changes? @jfinkels @ysitu  
comment
That's the change we wanted feedback on the most.       1) My initial thought was that most users called the `*_spanning_tree` functions rather than the `*_spanning_edges` function. So we wanted to target the common case.     2) The `minimum_spanning_edges` and `maximum_spanning_edges` functions would be really simple wrappers around `kruskal_mst_edges` and `prim_mst_edges`. I'm certainly not opposed to adding them back in...but it seemed like it just created more options for the user to choose from without really helping that much.  It also creates yet another slightly different function with its own docstring that we had to keep updated.     3) Whatever you can get from `minimum_spanning_edges`, you can also get from `minimum_spanning_tree`. You'll just suffer a slight performance hit.     4) People who don't want to suffer the performance hit can choose to make use of the `kruskal_mst_edges` or the `prim_mst_edges` functions. These were considered "advanced" functions and users who wanted them should be able to deal with 1) that they had a different interface and 2) that they had to choose the algorithm directly. These are exposed at `nx.algorithms.tree` only.  It'd be very simple to add those two functions back if these points are not convincing, or if we think there is still sufficient value in providing them to the user.  Taking this further, it seems like we could also provide a very generic `optimum_spanning_edges` and `optimum_spanning_tree` function which takes both an `algorithm='kruskal'` and `minimum=True` parameter. This is potentially two more functions to be exposed. 
comment
Let's go ahead and put them back in. We won't need to export `kruskal_mst_edges` or `prim_mst_edges` anymore. 
comment
Lets do the first option. Min and max are common names and will be familiar to the user, and so, they are probably preferred over optimum... 
comment
@SanketDG you are a super trooper! This is looking great. Last thing I see is that the examples in the `_edges` functions are not pep8 spaced: e.g. `>>> G=blah` to `>>> G = blah`.   And the failing TravisCI. 
comment
The first docstring example in `minimum_spanning_edges` still needs spacing. Also, `api_2.0.rst` mentions "method", but we now use 'algorithm'.   Perhaps we should also take this time to update to a more appropriate name:  `release_2.0.rst`. The point of these files is to document release notes, in addition to API changes.  For the release notes, what do you think of:  ```   In addition to minimum spanning trees, NetworkX now provides functions for    calculating maximum spanning trees. The new API consists of four functions:      `minimum_spanning_edges`,  `maximum_spanning_edges`,    `minimum_spanning_tree`,  and `maximum_spanning_tree`.   All of these functions accept an ``algorithm`` parameter which specifies the    algorithm to use when finding the minimum or maximum spanning tree.    Currently, Kruskal's and Prim's algorithms are implemented, specified    as 'kuskal' and 'prim', respectively.    The ``algorithm`` parameter is new and appears before the existing ``weight``    parameter. So existing code that did not explicitly name the optional ``weight``    and/or ``data`` parameters will need to be updated. For example,        >>> nx.minimum_spanning_tree(G, 'mass')  # old       >>> nx.minimum_spanning_tree(G, weight='mass') # new     In the above, we are still relying on the the function being imported into the     top-level  namespace. We recommend the following instead, but do not have     immediate plans to deprecate the other approach::         >>> from networkx.algorithms import tree        >>> tree.minimum_spanning_tree(G, weight='mass') # recommended ```  That is only a suggestion. Feel free to modify/add/del. Minimally, it needs some formatting cleanup. 
comment
Looks good. Let's get a final OK from anyone who wants to chime in. I think the only remaining question is whether or not we want to import the `_edges` functions into the top-level namespace. 
comment
Any other comments on this? @hagberg @dschult @ysitu @jfinkels.  Should the `*_edges` functions be in the top-level namespace? Or should we consider these to be "advanced" functions and force the user to fetch them at `networkx.algorithms.tree`. Note that we are already recommending users to call the `*_tree` functions from the `tree` submodule, but they are still available in the top-level namespace. 
comment
I was hoping the discussion would clarify what we should do. :)  @dschult regarding the `*_edges` functions...do you think these are functions that most users will use? Or are they more likely to be utility functions that we and other algorithm writers might make use of?  If the latter, then I think we might have an argument for not importing the `*_edges` functions into the top-level namespace---afterall, users can just do:  ``` >>> T = nx.minimum_spanning_tree(G) >>> edges = T.edges() ```  It will be a bit slower than calling `nx.minimum_spanning_edges`, but the API is simpler from the user perspective. Thoughts? 
comment
Ok, @SanketDG let's go ahead and bring them into the top-level. This will minimize the amount of modifications people have to make when upgrading. However, let's continue to emphasize/recommend that people use the submodule access in the release notes. After that, let's merge this thing (finally). 
comment
Closes #1604  
comment
@dschult Does that sound ok to you? 
comment
We may have to take PyPy off TravisCI...or come up with a simpler set of tests. We seem to regularly hit their time limit. 
comment
@MridulS check your gitter. 
comment
Here we go! Merge time. We can tackle issues as they arise. 
comment
It looks like the wheelhouse we were using stopped providing Scipy for everything except py27. So we're going to have to find another source or disable testing of dependencies for everything except 2.7.  Perhaps we should consider using (mini)conda for TravisCI instead of wheels now?  http://conda.pydata.org/docs/travis.html 
comment
Fixed in #1594. 
comment
Weird. Travis finished, but Coverall's didn't get the info. I've restarted the two relevant jobs within that build. Maybe it will go this time. 
comment
Would it make review easier to create a new branch and push it upstream. Then you coukd submit multiple PRs to it. Once ready, we could merge back into master. 
comment
Sure thing. I'll base it off current master? 
comment
It is up now:  iter_refactor 
comment
Let's close this now then. Look for multiple PRs to https://github.com/networkx/networkx/tree/iter_refactor from here on out. @MridulS feel free to re-open if necessary, but I suspect all the info you need is on your own fork. 
comment
Thanks for looking into this. What I had in mind was a fairly comprehensive discussion of the issues involved, along with examples. This would be in `doc/source/reference/readwrite.gml.rst`. E.g. the documentation might show what happens if you use `stringizer=json.dumps` or `stringizer=pickle.dumps`, and some of the difficulties one encounters when dealing with different Python versions and also when re-importing back into NetworkX. Using `pickle.dumps` might work, but it somewhat loses the benefit of going to a language-independent format in the first place...but again, all this depends on the use case.  For the docstring, I commented on that last few sentences. I think what could be helpful, instead, is just to mention something along the lines of users needing to give some thought into how the exported data should interact with different languages and even different Python versions. Re-importing from gml is also a concern.  For particular use cases, there are perfectly fine solutions, but it is not a decision that NetworkX can make for the user. Possibly, we could point them to the documentation. 
comment
That's probably ok, but it should be changed to an actual http link: http://networkx.github.io/documentation/latest/reference/readwrite.gml.html, and only once we get something in there. Presently, there is no such discussion.  Actually, I'm not sure what the best practice is for linking to user documentation, especially with respect to versions. It would be nice if there was a way to automatically have a version appropriate link, but that might be asking for too much. Maybe others have ideas. 
comment
Whatever you feel is best. If you think you might want to work on the extended discussion, we can leave this open and wait. If not, we can merge this now but keep #1454 open. Let us know. 
comment
Fixed in #1566  
comment
Agreed that it would be nice to just return the graphs, but there are performance concerns. Building a graph is much slower than just returning the nodes in the graph, and sometimes you really only need the nodes.  As for breaking backwards compatibility, I think we should resist the urge to turn 2.0 into a massive (more massive than necessary) backwards compatibility breakage mess.  We need to make it as easy as possible for users to migrate or we will have the Python 2.x/3.x all over again.   While I could be persuaded, I think this change should not happen in 2.0, which is already breaking enough compatibility. We could announce a deprecation warning in 2.0 that says in 2.1 (or 2.2) the behavior of `nx.connected_components` is set to change and instruct the users how they can adjust their code now to prempt the change. 
comment
In #1476, I just realized that degree() returns a dict, while nodes() returns a list. We'll definitely need have a very clear migration guide for people.  Also, is it weird to say that the *_iter functions are deprecated when there is no valid migration route for them in the same NetworkX version? The behavior of the non-iter function is completely different and will also change in the next version.  Essentially, it seems like we want to say "Hey these functions are going to be removed, but don't bother changing your code yet, because if you do, you're just going to have to change it again in the next version." 
comment
For the `*degree_iter()` functions. Should we special case singletons? I'd be a little inconvenient to have to type:   `next(G.in_degree("A"))` just to get the indegree for node A.  I'm not sure it is even possible to special case it---if the degree functions become generators, we can't have yield and return simultaneously. 
comment
Sure, but returning a generator means we'll have to type `next(G.in_degree("A"))` to get the indegree for node A. Maybe we are okay with that. 
comment
Doh, ok yes I see now. 
comment
Looks good. @hagberg @dschult or @jtorrents should be able to create that repository for you. Question:  It says they need to install "networkx-metis" but the actual pip install command is for "metis". Should these be in agreement? 
comment
Is it actually necessary for the addons to have a compatible license?  Honest question.   If these addons were separately installed and not officially part of NetworkX, it seems no different than if NetworkX had imported NumPy which, on some particular computer, had been compiled to make use of Intel's MKL. In principle, users can put whatever they want in the package that NetworkX will try to import. It just has to implement an interface that the NetworkX code has been written to expect. 
comment
If we are talking about the wrapper code, then sure.  What I meant was that the wrapper code need not have the same license as NetworkX itself.  So if necessary, the wrapper code could be Apache licensed to match METIS, and NetworkX could still import it. Right? 
comment
Fixed in #1544 
comment
Yep. 
comment
I haven't had time to read through this in detail, but I know the historical trend has been that we haven't used a lot of magic methods (where a particular algorithm is automagically choosen for you). So my guess is that we'd probably have to add a new function that could serve as the new public API rather than modify either of the two existing ones. 
comment
Is there some helpful name that could be reapplied to each? This is a bad suggestion, but it is along the lines I was thinking of:  ``` # Exposed at top level. # If user wants a particular method, then they must call it explicitly. gnp_random_graph # automagic  # Exposed only at nx.generators gnp_random_graph_systematic gnp_random_graph_geometric ```  I understand your suggestion...the asymmetry bothers for some OCD reason. Haha. 
comment
Also, I just realized the scale on the y-axis: ms.  While the difference clearly exists, is a few milliseconds worth worrying about? I'm just trying to understand where this becomes _very_ helpful. Is it for values of p much larger than .25? 
comment
Unless I am misreading the top graph, for N=400, the difference between the two algorithms doesn't look to be more than 2-3 milliseconds. Is the y-axis mislabeled? 
comment
Seems helpful. Could you add a unit test that covers this case? You should be able to modify one of the tests that appears in `networkx/readwrite/tests/test_shp.py`. 
comment
I just want to point out that I had some old code that no longer works because of this change.  I'm now wondering why this change was needed. This assumes that if you pass in an integer matrix and want a MultiDiGraph then you _must_ want parallel edges. It seems just as likely that you might want the elements of the matrix to be weights within a MultiDiGraph.  I think I'm of the opinion that we shouldn't guess what the user wants and just do the original conversion.  Parallel edges is _new_ functionality and it should be requested from the user. Having to call MultiDiGraph(from_numpy_matrix(A)) is a bit wasteful. 
comment
I can put in a PR that says if you want the old behavior you must make sure that the incoming matrix is a float matrix.  Or we can add a new keyword `parallel`, which if `True`, interprets the elements of the incoming matrix, when ints, to mean the number of parallel edges. Happy to do whatever we decide. 
comment
Ideally, an API should present the user with choices that are relevant to the task, rather than force the user to think about dtypes which are, arguably, not as relevant to the task. This is why an explicit `parallel_edges` keyword makes more sense to me.  But perhaps all this is suggesting that the construction you want (parallel edges) is just fundamentally different from weighted edges and should be its own function? 
comment
Can you add a note in the api_2.0 release notes? 
comment
Looks helpful! 
comment
Also, feel free to update the `credits.rst` file with your information.  I'll merge this in the morning if you haven't by then. 
comment
Opps. @jfinkels If you fix the documentation issues, please also update the release notes and submit another PR. If you don't get to this in a few days, I will open another ticket for both points. 
comment
Sorry for this, but I just realized you were using "vertex" terminology. While completely understandable, it is counter to the convention in NetworkX. Could you switch to using "node" instead? 
comment
Also, does it make sense to expand the `contracted_node` code to take an arbitrary number of nodes to contract? I think I've written a similar function in the past. For some reason, I thought (but I could be misremembering) that   ``` G2 = contract_nodes(G1, [u, v], node=(u, v)) G3 = contract_nodes(G2, (u, v), w, node=(u, v, w)) ```  was different from `contract_nodes(G, [u, v, w], node=(u, v, w))`, but I was also manipulating edge probabilities at the time. 
comment
Ok. With respect to the node name, I think what you have is fine. I could also see how one might want both nodes dropped and a new node manually specified, but no one has chimed in. So let us be driven by desire/interest and keep what you have---similarly for the node attributes. Anything else on this PR (besides fixing the last unit test)? 
comment
Can you also update the release notes? 
comment
Might the user ever want to specify this threshold? I'm wondering if it makes sense to make it a function parameter. 
comment
Looks good.   There is also an `Other Parameters` section for less-commonly used parameters. I'll leave it up to you if you want to make that change. If you do, can you also get rid of the newlines after the section titles? Otherwise, I'll merge later. 
comment
What version of NetworkX are you running? 
comment
And btw, we've been supporting 3.x for some time now. 
comment
Fixed in #1510  
comment
Note, not all 2.0 functionality has been implemented yet.  I would do the following instead:  ``` git clone http://github.com/networkx/networkx.git cd networkx pip install -e .  # Don't forget the period (.) ```  This installs NetworkX similarly to what you'd get with a symbolic link. Any changes you make or pull from the git repository will seamlessly appear in your installed version. 
comment
Does it let you try to resolve them (e.g. you just add your changes and commit again)? You'd just need to make sure your additions to api_2.0.rst and combined probably with some of the other new additions. 
comment
Sure give it a try. 
comment
We're slowly moving towards better compliance.   It's mostly the older portions of the codebase that are not compliant. While compliance is nice, it is pretty low priority for us, since it's only marginal improvement in most cases. We'd rather that time and effort go into things like new features, etc.   So generally, our approach has been to do some cleanup whenever you touch a function as part of some other issue, new feature etc. We've merged PRs that touch a few things, like this one. We've rejected PRs that try to touch everything at once via autopep8.  As for compromises, yes as necessary. :) We'll stick to pep8 when it makes sense, but it's a guideline, not a rule. I'm not sure that the exceptions are clear and rigid either. There is a bit of art to coding. 
comment
Also, please update the release notes. 
comment
Go for it. 
comment
Does this address everything in #1307? 
comment
Apologies for the delay....  For the import errors, this seems to be a bit of a nose quirk. `setupClass` is run immediately to determine if any of the method tests should be ran. So if NumPy is not available, then it will skip. However and confusingly, this does not seem to imply that `np` will be globally available. In fact, it seems that only methods which match the regex for testing will have the same namespace that includes `np`. So I think you'll have to add an explicit `import numpy as np` in your `__init__` method.  Once you get that fixed, is this ready to be merged? 
comment
Great. If you have the patience to do a few more edits, we can get this merged. If not, let me know and I'll take care of the rest.  Could you edit:  `doc/source/reference/api_2.0.rst` and add a short comment about this PR. Also, consider editing `doc/source/reference/credits.rst` to add whatever information you feel is relevant (your personal info, acknowledgments for any support you've received, any affiliations etc.).  I had a few other comments that you'll see shortly. 
comment
Ok great, I will merge this manually. 
comment
There were a few nit-picky docstring changes I wanted in there. I'll send you a pull request shortly. But a few more edits will be necessary. I've updated the unit tests to cover the corner case, and the current implementation fails it. 
comment
Sent: https://github.com/Overriders/networkx/pull/2  Once you merge that, let's see if people have other suggestions. Also, consider adding yourselves to `networkx/doc/source/reference/credits.rst`. 
comment
@jfinkels Any other comments, perhaps relating to `is_empty`? 
comment
@theosotr  Yes, that should work as well:  ``` def is_empty(G):     return not any((node for node, neighbors in G.adj.items() if neighbors)) ```  Or as @hagberg suggested,   ``` def is_empty(G):     return not any((node for node, degree in G.degree_iter() if degree)) ```  `degree` does slightly more than necessary...but no one would notice:  ``` # effective degree_iter() call for n, nbrs in G.adj.items():     yield (n, len(nbrs) + (n in nbrs))  # return tuple (n,degree) ``` 
comment
Well, I think that's probably the simplest we can get:  ``` def is_empty(G):     return not any(G.adj.values()) ```  In keeping with our previous 2/3 conventions, 2.x users will have a performance penalty but 3.x users will be just fine. @theosotr, if no one chimes in again, could you make this change? 
comment
Thanks! 
comment
I haven't had (and in the short term, won't have) the time to properly review this, but the ImportError you are receiving is because Python 3.x uses absolute imports.  You'll need to change your line from:  ``` import coloring_with_interchange ```  to one of the following lines:  ``` # relative import from . import coloring_with_interchange    # absolute import from networkx.algorithms.coloring import coloring_with_interchange ```  Hope that helps! 
comment
Added a few minor comments on code style only, not on implementation. 
comment
For testing, you should pass in a seed, so that the tests are deterministic.  If we wanted, we could have some other tests (that could be run occasionally, or when people want longer tests) that would try a bunch of random graphs.  `small_world` should be adjusted to allow the user to pass a seed as well. Because of the way `nx.gnp_random_graph` is defined, you won't want to pass this seed to it. Instead, you'd have to set the seed of `random` before you enter the for loop.  Generally, I think that seeds are not flexible enough...as we see with `small_world`. Instead, functions should accept an object which supports a `random()` method. This would allow people to pass in the `random` module or a `numpy.random.RandomState()` instance. Users and algorithms then have explicit control over the generated random numbers.  (Aside: I'd prefer the NumPy API for prngs, but we don't have a hard NumPy dependency). 
comment
The tests are failing (at least in 3.3) before the test is run (and during the import of networkx). With NumPy optional, `networkx/linalg/__init__.py` cannot just import `linalg_clustering.py`.  Either put a try/except in the init file or move the top-level import in `linalg_clustering.py` to the a function level import.   Also, the tests do not have an explicit check on scipy, so you'll want to do that. That should fix 2.6. 
comment
You might b able to add:  ``` # doctest: +SKIP ```  to each line of the doctest that you want to skip.  We could also remove the >>> so they are skipped altogether.  Alternatively, we can add the `ordereddict` module to NetworkX dependencies for 2.6 only.  For setuptools, something like:  ``` if sys.version_info[:2] == (2, 6):     install_requires.append('ordereddict >= 1.1') ```  Then, we'd have to supply our own:  `networkx.utils.OrderedDict`:  ``` try:     # 2.7+     from collections import OrderedDict except ImportError:     # 2.6     from ordereddict import OrderedDict ``` 
comment
Ah, I should have known you were talking about unit tests and not doctests.  Looks good btw. 
comment
Call me crazy, but I actually like option 1.   Presumably, people will continue to use the base classes by default and using something else will be a bit more of a specific use-case.  In that scenario, requiring an extra line of code to declare the special class seems particularly _clear_.  There is also nothing preventing us from making particular special classes available at the top level (such as OrderedGraph).  Option 2 seems confusing and I'm tempted to say we should push users away from even thinking about it---I mean, what value would there be to switching to an ordered dict _after_ already populating a graph with some nodes?  Option 3 works, but it seems messier to me somehow.  I guess my inclination is to see the various modifications as _different_ classes, rather than the same class with _different_ factories.   Comparing these specialized classes when they have different factories could be tricky---and so, if they were all different classes, it might provide a cleaner separation. 
comment
I like the backend idea.  I'm curious though if you can change the base class after instantiation.  We might have to pass the backend into the `__init__` method.  One downside to the backend route is that you won't be able to optimize lookups to the baseclass. Methods would have to do:  `self.__class__.__base__.node()` or `self.node_dict_ctor()` if we stored it during init. However, this may or may not be a problem (we should profile to see if speed really is adversely affected...seems like it'd only affect adding/removing nodes/edges anyway.).  If it is, then we can use a metaclass approach to build the main classes with different backends...this would allow us to make the access calls faster with the cost of making the implementation more abstract. 
comment
Something else we should keep in mind...  Do we want Graph with an SQL backend to be of the same "class" as a Graph with a dict backend?  In principle, the backend should be completely replaceable and so I want to say "yes" they can be the same class.  However, it also seems that the backend does cause very real differences in how users will interact with the classes.  For example, people want to use OrderedDict precisely because they want to take advantage of the ordering.  So I'm not sure that these qualify as "behind-the-scenes" backends.  If that is true, then maybe we don't want a graph with the SQL backend to be of the same class as a graph with a dict backend.  For example, what if someone writes code (outside NetworkX) that uses properties of the backend (like OrderedDict) but then someone passes in a Graph which doesn't have that backend.  It seem like people will want/need to be able to compare backends.  One way to do this is to make graphs with different backends correspond to different classes (the metaclass approach).  Another is to provide an API for users to compare the backends directly.  As for another example that would be fun to include:  Provide an edge dict which prepopulates an attribute called 'linestyle' to be 'dashed'.   Currently, the only edge attribute that exists on every edge is 'weight'.  These backends will also allow people to make other attributes exist on every edge. 
comment
So where are we with this issue? Is it that we need a clearer statement of what the NetworkX API should be? 
comment
Can you elaborate on this "However in both files this happens on, the line before "node [" is the "]" the parser was expecting."   I'm not quite understanding what you mean.  It sounds like you are saying that the parser is looking for the close square bracket _before_ the open square bracket, which doesn't make much sense. 
comment
Can you provide the line of GRAPH_FILE that seems to be the problem?  Even better would be if you could prune most of the lines prior and after line 229508, keeping only what is necessary to have a well-defined graph that still cannot be parsed. 
comment
Would it be possible to modify the graph file to remove the proprietary data?  I'm guessing the bug is not sensitive to the particular data (strings, floats).  We don't need the IPs, DNS, etc.  So you could wrote a script to replace those lines...  ``` DNS "string.some.domain.net."   --->  DNS "hello" ```  and this would be done so that every DNS line is the same. 
comment
Travis error looks like a fluke.  JSON doesn't support NaN or Infinity either. It might be nice to provide autoconversion for NaN and infinity to string types, but that's probably another ticket. 
comment
I'm +1 for this. Requiring a roundtrip to be equivalent to the original is asking too much I think, since JS objects are not equivalent to dicts.  So the int->str conversion is okay in my book. Referencing G.graph shouldn't be an issue since the object is only read, not modified, during serialization.  I think maybe json.dump() should be passed `allow_nan=False` since NaN is not valid in JSON, Though, strangely, `json.dump` does not raise an error on infinity, even those are also not supported in JSON. either. http://stackoverflow.com/questions/1423081/json-left-out-infinity-and-nan-json-status-in-ecmascript 
comment
@jdrudolph Any thoughts on NaN and Infinity?  @ysitu Your work in #1269 makes me think I was too hasty in saying we shouldn't care about roundtrips. While it is true that many people won't care about roundtrips, that doesn't mean that we should make it impossible, if people desired, to do them. Undoubtedly, some people will _want_ to do roundtrip conversions. So as in #1269, I now wonder if we should instead provide converter arguments with sensible defaults. 
comment
Ok, did you want to add a couple remarks to the documentation? Or is this ready for merging? 
comment
Very nice. I won't be able to check this out in the next few days, so hopefully someone else can. If not, and since the tests pass, then I'll merge after a quick code review (mostly just style, syntax). 
comment
Are the changes you made "odd", e.g., convoluted in order to avoid the IronPython bug? If so, it should be documented in the code that you are taking a particular action due to IronPython issues and how that path resolves it.  Also, it looks like there are still issues, so maybe those changes didn't fix it?  https://travis-ci.org/networkx/networkx/jobs/37445007#L3246 https://travis-ci.org/networkx/networkx/jobs/37445007#L3275  If there is a legitimate IronPython bug that requires lots of finagling to get around, maybe it is not something we should worry about---and just wait for IronPython to fix it? It wouldn't be the first time that we've had issues with IronPython's behavior not matching CPython. 
comment
@lpand Your opinion on #1269 would be nice to hear as well. 
comment
Addressed in #1269  
comment
We could probably inherit stability if we used ordered dicts as the internal data structure. #980 #1181 
comment
But I suspect the idea is that this should be easy across many environments and users. It's not always the case that you have control over how a process or script is run. Imagine telling users that in order to get the correct behavior that they had to set an environment variable before running a script. I also feel like the hashseed sits "outside" Python, and it seems like a hack. 
comment
Though, I guess you could provide a script that ran the script and set the environment variable...but that's even more hackish in my opinion. A pure Python solution is a reasonable request. 
comment
Check out some of the comments in #1181. It is the iteration over the dictionary that stores neighbors. 
comment
Opps. Sorry, I'll take care of that. 
comment
True, but `create_using` allows people with custom graph classes to instantiate the graph. So long as their custom graph class is undirected, multigraph-like, then it will be fine. If a user passes in something different, then they get what they asked for :) So I'd guess that anyone doing something odd is either 1) doing it with purpose, or 2) can figure out their error by reading the docstring, which clearly states that it is for undirected multigraphs.  Update: But I'll add checks for multigraph and directed. 
comment
Also relevant: 45b6257d9d03ce037b.  [I should've put those last few commits in another PR.] 
comment
They are already all merged. So it's all finished! 
comment
Nice implementation @ysitu! I can't verify correctness, but the code is clean and very understandable. So I wouldn't expect gotchas if another developer had to check work on this code in the future. +1 to merge from me. 
comment
Thanks. Nice catch! 
comment
I'd be okay with slowly removing the redundancy. Especially when the default is `None`, descriptions are more helpful as the function may go through a decision process to determine default behavior. 
comment
I like the removal of pyparsing. Haven't had time to really check this out though. 
comment
I've still only glanced at this...but can you explain a bit more. In #1048, the default stringizer is the `__str__` method on each object.  As this can be customized on a per class/type basis, it does not treat all objects the same regardless of the type. In this way, it seems equally as flexible as using a `stringizer` function that does type checking, only it defers to the object itself when deciding how it should be stringified.  Is the argument that people will often use tuples or something similar and might want it stringified in a way other than what `tuple.__str__` implements? That is, the approach in #1048 asks the user not to use tuples but to use a subclass with a reimplemented `__str__`, whereas `stringizer` let's them choose on the fly? 
comment
Am I correct that this would supersede #1261?  As I said earlier, I like that pyparsing is no longer a dependency. But the parsing code is a bit involved. Would it have been significantly simpler if we had continued to rely on pyparsing?  If I remember right, I think the previous implementation had some unicode issues as well...so this presumably fixes those.  So one question...would it make more sense (in terms of keeping things easy for the typical use case) to set the default stringizer to repr (and the default destringizer to ast.literal_eval)? If not, I think it might be helpful to readers if we mentioned what a common (de)stringizer pair might look like.    Mainly, I'm just looking for the simplest path for users. I can imagine most users just want to generate the obvious GML equivalent code and don't want to have to think about converters, etc. Most are probably also accepting if you can't roundtrip the conversion (so using repr/None for a stringizer/destringizer will probably common).  Also, can you comment on how this handles attribute lists that are of length one? This was mentioned at the beginning of #1261 as a point of improvement. 
comment
@lpand got it. So I'm wondering if you could instead write one of these stringizers that does precisely what you were thinking of hardcoding into your implementation.  The advantage of this implementation is that the converter decisions are hot swappable. We could either make your stringizer the default or provide a mapping of common stringizers that users might find useful...certainly we can mention some prebaked stringizers in the documentation.  So I definitely like and appreciate your focus on trying to do the most practical/likely conversion to GML. What I'm hoping is that we can have that but also maintain flexibility for people who might want to do more, such as embedding an image (as that Mathematica example showed). 
comment
@ysitu I meant whatever the 2nd bullet of #1261 meant :) 
comment
@ysitu, given @lpand's busy schedule, I'm curious to know if you think there anything else that this needs?  In particular, should we set default (de)stringizers? We should merge one of these soon... 
comment
Ok, but am I reading the code correctly? It looks like, presently, there are no default (de)stringizers. 
comment
Ok great. Well, I'll put a 24 hour mental timer on this. If no one else chimes in before then, I'll merge it. This is a generalization of the proposal in #1261, and is flexible enough to handle more complex cases too. So that's a win-win. 
comment
Simple enough. Does this fork differ substantially from pydotplus? What's the motivation for YAPP (yet another pydot package)?   While we can add this, note that we are trying to import in order, so people might complain that their pydot fork has lower precedence than other pydot forks. This whole pydot situation is a mess... 
comment
Ah ok, that's good to hear. Maybe pydot_ng will be the winner! I don't have an issue merging it, especially since it's just another item to the list. Did anyone else want to chime in? 
comment
@prmtl  can you make that change too?  I'm not sure why we went with `__import__` at all. It might have been because I had forgotten that we weren't supporting 2.6 anymore. 
comment
Probably #1356 as well. 
comment
Yeah, that or the merger should add it at merge time. 
comment
I'm going to merge this now. If we missed anymore, we can add them later. 
comment
Yeah it seems like if you want to draw a subset, then you create a subgraph and then call the standard draw function. 
comment
Ah. That is pretty neat, and very useful.  ![karate_clip](https://cloud.githubusercontent.com/assets/326005/6364015/12899750-bc66-11e4-9b6e-69aede969e39.png) 
comment
+1 from me. Somehow I completely misunderstood what this PR was doing.  Your example clarified quite a bit. 
comment
Yeah, just waiting to hear back from others. 
comment
Could you add a note to the release notes? `doc/source/reference/api_2.0.rst`. Also, consider adding any information you'd like to `credits.rst`. Otherwise, this is probably good to merge. 
comment
I've sent you a PR with some refactorings. Please review it. 
comment
Funny that github included discussion from an unrelated commit. Oh well.  Assuming this passes, was there anything else you wanted to put into this PR? 
comment
Yes, your contribution is much appreciated! Looking forward to more in the future. 
comment
Did this get merged? 
comment
Yep. Merged in f634c4f6de8db8ab42f191af09bb328a6ec78623 
comment
Fixed in #1319. 
comment
I like this more, much cleaner. 
comment
@dschult Can you rebase? I think we should merge this in. Given recent activity, I suspect this is going to be the biggest feature for 2.0. 
comment
Thanks. 
comment
Good catch. Indeed, `doc/make_gallery.py` does not get covered by the unit tests. 
comment
Fixed in 965640e7399c669980243d8b162c4521339f294f. I don't believe the doc generation is covered. 
comment
Do you know if it is necessary to escape the backslashes (in general). I wonder if we should put an r in front of the docstring.   r"""docstring""". 
comment
It looks like you'll have to update some of the unit tests so that they require scipy, or something like that. Also, some tests were failing due to print statements. 
comment
Also, be sure to update the release notes. And consider adding yourself, a research group, or funding sources to `credits.rst`, if desired. 
comment
No worries, still looks clean. If I was doing something with git that isn't recommended, let me know :) 
comment
I like this. Code is clean and it's standalone. Let's merge now, and you can submit another pull request with additional features. The directed extensions would be nice. 
comment
Thanks! 
comment
Can you update the release notes for 2.0? 
comment
Technically, it should be four spaces for indentation as per the standard. But I don't like pull requests to be overly picky...so I'm going to merge this when I get home...we can always update on future edits. If you beat me and make a commit, so be it. Otherwise, it is a lesson for future PRs. 
comment
Something is up with GitHub (it didn't close this when I merged it). This was just merged in f9c09ca4ed669f2723a7c708d269e729fbb19446. Closing this now. 
comment
Can you add a short note to the release notes?  `doc/source/reference/api_2.0.rst` 
comment
Great. The last thing is that need to modify the function signature (add spaces) and make the docstring look like all other docstrings we have. Minimally, you'll need a Parameters, Returns, and Examples section. 
comment
I fixed the final error in 8ed04af956dd7f0c068b88c15f483dc13ab57c7b and then merged.  Thanks. 
comment
Can you add this to the release notes? Also, consider adding yourself, research group, or funding sources to `credits.rst`. 
comment
This pull request does more than what you described.  In https://github.com/networkx/networkx/pull/1360, we decided that we did not want to have the `u=None` option, but this PR adds it back in. Once that is cleaned up, we can merge this. 
comment
This PR is a bit messy. I had to send you a PR since your fork wasn't updated and the nodes parameter was still not completely removed. Once you accept that PR (https://github.com/Scinawa/networkx/pull/3), then we'll see if it makes sense to merge this...if it's too ugly, I'd rather just deny and do the very small needed change manually. 
comment
@Scinawa ping. Can you accept https://github.com/Scinawa/networkx/pull/3? 
comment
Looks good. I think for the cutoff, it would be helpful to users to say what a default value of `None` means. 
comment
Can you also update `doc/source/reference/api_2.0.rst`? After that, it's probably good to merge, but I'll leave it open for a bit. 
comment
Ok, going to merge this. We can tackle anything people find later. 
comment
Yup. It's a disconnect between Sphinx and Markdown :) 
comment
There are libraries for that. See `six` or `futures`. However, we've opted to use list(range()) in the codebase. 
comment
I may have overstated a bit, I think we mostly just use `range()` and so Python 3 users will get the lazy evaluation while Python 2 users will not.  In cases were we need the list immediately, then we use `list(range())`. 
comment
The most obvious culprit would be the use of `nx.utils.reversed()`, but I'm not sure why that would cause this since that function has been tested to produce the same results as calling `G.reverse()`. 
comment
We discussed this a bit in #1269. In particular, see https://github.com/networkx/networkx/pull/1269#issuecomment-61442401. So the intention was to require the user to make an explicit choice. When using `read_gml`, you must make the corresponding choice. @ysitu may have some additional comments, but I think this is "as designed". 
comment
Seems like a consequence of storing "non-simple" Python code in strings.  For comparison, note what json does:  ``` >>> import json >>> json.dumps({1,2,3}) TypeError: set([1, 2, 3]) is not JSON serializable ```  We act similarly in that an exception is raised (provided no stringizer is provided). It will work if choosing repr, but you don't get 2/3 compatible code for roundtrips. We should probably add a note in the docstring highlighting this fact.   Any stringizer that does some conversion (e.g. from sets to lists) will not have enough information (unless you do something custom) during decoding to convert the lists back into sets. So I wonder if it makes more sense in this particular situation to convert the partition into a list of lists?  Alternatively, would pickle.dumps() be a good stringizer for you? Seems like overkill and I'm not sure what the 2/3 compatibility concerns would be. 
comment
Yeah, ok. Thanks for the discussion. Helpful. 
comment
Actually, I'll create another ticket. 
comment
Thanks! 
comment
I'll merge this, but the discussion in #1355 is our general guideline here. PEP8 changes should accompany other non-PEP8 changes. 
comment
I've had similar thoughts and was never able to decide what should be done. The present approach requires that the user provide node names with graphiz encodings in mind. Your proposal definitely makes it more user-friendly. The question, I guess, is whether we want `write_dot` to do something vanilla or put a cherry on top too. 
comment
I think we preferred using the decorator since the error messages are normalized. With the top method, the error message must be re-written each time (typos, different words, etc). 
comment
Sure, but it's not what I'd call high priority or anything, and it seems to be more in the "maintenance" category, closer to PEP8 fixes, etc. I'm not sure if that makes for a better project proposal or not. 
comment
Sounds good! 
comment
Is this just failing on the miniconda environments? 
comment
Whether the red crosses appear or not on PRs depends on a tolerance level that is adjustable from Coveralls. The project maintainer has control over that. 
comment
@MridulS What do you mean by "tested on miniconda only." As far as I can see, exactly half of the TravisCI runs do not use miniconda. 
comment
Got it....there is an `if` statement that runs coveralls only when we do miniconda runs. So that explains it, as the path search is wrong. I've pushed a fix to my fork. Once Travis runs it, we can see if that fixes it.  ``` python fixcoverage.py ".*/networkx/" "$TRAVIS_BUILD_DIR/networkx/"; ``` 
comment
That seems to have fixed it. As for what we see on PRs, we just need the options configured here:  ![untitled](https://cloud.githubusercontent.com/assets/326005/6741303/6467fdc8-ce54-11e4-8c4a-40f5dfa8bb9f.png)  That's an independent task, so I think this particular PR is good to merge. 
comment
I think you'll find it at the bottom of the page here: https://coveralls.io/r/networkx/networkx  I see it for my fork here: https://coveralls.io/r/chebee7i/networkx 
comment
Nice catch. @dschult, why didn't the unit tests pick this up? 
comment
We seem to come across situations like this frequently, where the solution is not a one-liner but also not that complicated either. Maybe we should promote example recipes (organized into sections etc) as part of the documentation. I think tossing them in the examples folder is also a possibilty, but users might be less likely to find and/or understand them without some contextual discussion. 
comment
What if we added an `all_edges=False` parameter? 
comment
Add, mean, max, min...all seem valid. I think we need to provide a way to specify how the decision is made or raise an Exception if if nx.Graph(G) is called and G has multiple edges. 
comment
I'll add my vote to not adding a keyword to `G.neighbors()`.  That said, I can see the value of being able to specify an attribute and getting all the  `(nbr, eattr_dict[data])` tuples, potentially with a user-specified combinator. This is a common query and if we can remove the boilerplate of having to deal with multiple graph classes, etc, then I think it might be worth it, but perhaps as a separate function not at the top-level. `nx.algorithms.neighbor_combinator(G, n, data=None, combine=min)` ? 
comment
@dschult If we do the function route for helpers and _+1_ features, we can have separate implementations for each class (if necessary), just as we'd do in methods. There will be no performance penalty if core functions query for the appropriate function ahead of time. Yes, it does seem a bit messy on one hand, but there are also benefits: all variants of the function appear in a single (small) file making maintenance easier.   ``` __all__ = ['funcname']  def common_stuff():      pass  def funcname__graph(...):     common_stuff()     ...  def funcname__digraph(...):     common_stuff()     ...  def funcname(...):     func = get_funcname(G)     return func(...)  def get_funcname(G):     # Core algorithms call this once to avoid inner loop penalties.      directed = G.is_directed()     multigraph = G.is_multigraph()      if directed and not multigraph:         return funcname__graph     .... ```  Sometimes a single docstring will suffice for `funcname` instead of 4 different docstrings. When that is not possible, we can programmatically construct customized docstrings from a template---which also reduces docstring repetition. 
comment
Why isn't the base of the arrowhead perpendicular to the line of the edge connecting the nodes? 
comment
What would the counts be useful for? It seems like it is a one-off lookup to OEIS:  ``` python def n_graphs(n):     counts = '  1, 1, 2, 4, 11, 34, 156, 1044, 12346, 274668, 12005168, 1018997864, 165091172592, 50502031367952, 29054155657235488, 31426485969804308768, 64001015704527557894928, 245935864153532932683719776, 1787577725145611700547878190848, 24637809253125004524383007491432768'     counts = list(map(int, counts.split(',')))     if n >= len(counts):         raise ValueError("The count is too damn high.") # This is a joke     else:         return counts[n] ```  There are many variants of counting (self-loops, directed, etc). Now, I can definitely see the value in having the graphs themselves, but this is a tough computational problem when n is large. 
comment
I think it would be helpful if we familiarized ourselves with how various graph databases, such as Neo4j, deal with sql-like queries on graphs. We might as well bootstrap our design. 
comment
I'd second @mad4alcohol thought. I suspect the Eclipse IDE is using your system Python (`/System/Library/Frameworks/Python.framework/Versions/2.7`). Could it be that when you use IDLE (python), Terminal or iPython, that you are using a different Python?  One easy check would be to do the following from a Python terminal session:  ``` import networkx print networkx ```  If the location of the displayed file is different from `/System/Library/Frameworks/Python.framework/Versions/2.7` then you'll need to configure the Eclipse plugin appropriately. If it is the same, then we'll have to dig deeper. 
comment
Which post? 
comment
There's a number of these Python 3.x `pydot` around. For example, I'm including one in [nxpd](https://github.com/chebee7i/nxpd). Has the community settled on pydotplus as being the "official" replacement? If not, I'd rather wait a bit longer. 
comment
@pfmoore True that there doesn't appear to be anything on PyPI. Here are two others that I know of, which are Python 3 compatible:  https://bitbucket.org/prologic/pydot https://github.com/nlhepler/pydot  This PR is pretty straightforward, so its not going to be a big maintenance issue if merged. Anyone else want to chime in? 
comment
I haven't checked this out yet, but it's not surprising. Some of the 2.x/3.x interop in NetworkX needs fixing. But I really think we should do this with `six` rather than add piecemeal code in various places....but maybe there is a quick, nonintrusive fix to this. 
comment
Nice, yeah so `is_string_like` actually requires that your nodes be string-like by trying to add a string to it. 
comment
@josch it looks like support might get added in #1268. 
comment
Looks right. 
comment
When writing to files, we can attempt to sort the nodes/edges, but NetworkX doesn't require that nodes be sortable.  Maybe a sort=True parameter?  I wonder if we could just move to OrderedDict without causing too many issues. 2.7 and 3.x have it. For 2.6, we make ordereddict a dependency. 
comment
Is it the edges or nodes or both that get out of order?  If its only the nodes, you could also make it accept an ordering on the nodes.   `node_order=None` being the default...which then just grabs `G.nodes()`.  But if edges are getting out of order as well, then this won't work. 
comment
Just wanted to point out that in Python 3, the `cmp` option to `sorted` no longer exists.  So to be forward looking, maybe it would be better to make the user provide key functions instead.   http://python3porting.com/preparing.html#keycmp-section  So for each node, node_key just returns something that is sortable. For edges without attribute data, it seems that those can be sorted using the node key information:  (u_key, v_key) or (u_key, v_key, edge_key). 
comment
I must be the odd one here :)  I'm still partial to building your graph (or converting your graph to) a class that uses ordered dicts. 
comment
Nice job, especially with picking up Sphinx and numpydoc. 
comment
Thanks for the info. If you are able to submit a PR to fix this, that would be greatly appreciated! 
comment
"Pull Request", the github way of contributing a a fix for the issue you reported. Definitely not necessary though. Reporting of issues is much appreciated. 
comment
The documentation is part of the source code. It does have a bit of a learning curve though, you need to familiarize yourself with [Sphinx](http://sphinx-doc.org/) and [numpydoc](https://pypi.python.org/pypi/numpydoc) and get all of that setup locally on your machine. 
comment
Thanks will do! 
comment
This might require a new release to 1.9.2. 
comment
@idella is the build process broken because these are missing? Or just annoying? 
comment
Ok, then maybe it is not necessary to do a bug release just because of this. We will get it included though. 
comment
Is that relevant to #1241 and not this PR? 
comment
Opps. dominance vs dominating. :) So yes, it looks like that line should be added to `algorithms.rst`. 
comment
[Looks](https://github.com/networkx/networkx/issues?q=milestone%3Anetworkx-1.9.1+is%3Aclosed) like most (all?) are labeled now. 
comment
I believe so! 
comment
Looks good. I'll update the docs for `draw_networkx` since we seem to be deferring to documentation there. 
comment
Yeah I just noticed that. So where in the documentation does it say that labels are not drawn by default? That is, before #936. I'm now wondering if we should update `draw_networkx` instead and move this if block there. 
comment
Seems weird that it is different from `draw_networkx`. Oh well. Thanks again! 
comment
Maybe it should be extended so that if `labels` is a kwarg, then `with_labels` is automatically set to True? This seems more in line with what the user actually intends---as there is never a time when it is useful to pass in labels and have `with_labels` be `False`. When `labels` is None, then if the user wants the "default" labels, then they must explicitly pass `with_labels=True` (this is the current situation and should remain). 
comment
Ah yes, I suppose that is true. You might not remove it from kwargs and so you want some way of turning it off. What you proposed sounds good to me. 
comment
This is `networkx`. You might ask your question on the [matplotlib mailing list](https://lists.sourceforge.net/lists/listinfo/matplotlib-users). 
comment
Duplicate of #1254 
comment
Duplicate of #1254 
comment
How does this demonstrate that the wrong version was installed?  Can you provide the output of:  ``` from __future__ import print_function import sys import networkx print(sys.version) print(networkx.__version__) ```  The second output should say 1.9, according to the first screenshot. 
comment
Oh I see. Yeah, looks like that should fix it. I wrongly interpreted that those print statements were his code, not ours. Whoops. 
comment
Oh yeah, I think 1.9.1 would be good. 
comment
@ysitu are you asking me or @hagberg?  I'm assuming me.  I guess I wasn't aware of this plan...the merging I was doing into master was done independent of any plan for a 1.9.1 milestone...just as a "let's merge these things" approach. So that might be my mistake. Are we able to checkout a previous commit and tag/release from there? 
comment
Thanks looks great! 
comment
Was this closed accidentally? 
comment
😲 Wow. Very nice. 
comment
Our test matrix doesn't cover everything.  Should we make it so that for Python 3.4, we install NumPy, SciPy, matplotlib from source (via pip)? It would take a lot longer...but we'd have at least one Python testing against the latest versions of the main packages we (optionally) depend on. 
comment
Leaving directed for another PR, what else is left to do/decide before this can be merged? 
comment
TLDR: I vote for including it.  My personal opinion is that open source software packages are too fussy in deciding whether to use other open source packages. If a mistake is made (wrt licensing), it's not as if it was done in bad faith. So if someone upstream makes an issue of it and has weight to throw around, then we change it. No big deal.  We are not lawyers and the legal status of linking and derivative works, especially as it relates to Python's run-time compiled modules, is still undecided.  Due to project relevance, we are also probably some of the least likely projects to face litigation <knocks on wood>.  IMO, simply typing: `import X` where X is some GPL or LGPL module does not mean that NetworkX is "linking" to X or is a derivative work of X.  The meaning of X is determined at runtime.  I can, on my computer, put anything I want in my PYTHONPATH with the name X that supports the same public API that NetworkX is expecting---and that thing I put in my PYTHONPATH need not be the GPL'd or LGPL'd package X. So it doesn't logically follow that "import X" means we are "using" GPL/LGPL'd code.  This remains true even when we distribute wheel files for NetworkX (of course, this would get messier if the NetworkX wheel included, say, compiled Cython libraries).  Interesting discussion in this thread...here are two views similar to what i just described:   Grant Edwards: https://groups.google.com/forum/#!msg/comp.lang.python/zYZGRRqs_Mk/cMJkm9gfi-0J Alex Martelli: https://groups.google.com/d/msg/comp.lang.python/zYZGRRqs_Mk/kxt6qtLjR_oJ 
comment
Merging... 
comment
Yep.  It looks like this may have been brought about by changes in the default encoding of ElementTree.  This might even behave differently for 2.x and 3.x, so we'll need to be sure.  http://bugs.python.org/issue8047 
comment
Personally, I'm -0 on the 'require' decorator.  See my reasoning here:  #838 
comment
While I agree a redesign would be nice, I'm still puzzled about the claimed memory leak.    Previously, there definitely was a memory leak, but that was fixed years ago (see b83a37a6d in 2009) by removing the `__del__` methods.  As @ysitu mentioned, the Python interpreter has no problems resolving circular references for classes that do not implement `__del__`. See also: http://stackoverflow.com/questions/10962393/how-does-pythons-garbage-collector-detect-circular-references  Do you have some way to demonstrate that this is indeed leaking memory?  Perhaps you could show that the objects are accumulating in `gc.garbage`?  Aside: having 10,000 DiGraphMatcher instances all at once with large graphs is definitely going to take up quite a bit of memory and need not be due to a memory leak. But perhaps it is. 
comment
To the issue of messiness: design is hard, and redesign is very rarely a pretty process. So the messiness of the PR isn't an issue to me.  There have been far simpler pull requests (some by me) which have been hard to follow as well.  :)  I think the main focus here should be on good design, with backwards compatibility issues being an after-thought. That's not to say that backwards compatibility isn't important...it just tends to influence the design.  Once a happy design is found, we can always create a separate pull request which adds a compatibility layer back in. 
comment
Regarding branches, this already is a branch on @jtorrents's clone. So I'm not sure how a branch on networkx/networkx would be significantly different. We should feel free to pull changes from him and directly send pull requests to his clone. The discussions being public can be helpful. For simpler comments, maybe a direct pull request to @jtorrents keeps it cleaner? 
comment
Same. 
comment
@ysitu, have you set up an HTTP server?  Go to the build directory and run:  `python -m SimpleHTTPServer`. Then open the browser and go to `http://localhost:8000`.  It's been a while since I've built the docs, so I can't remember if Sphinx provides its own command to serve the built docs. But the above should work, if not. 
comment
The most likely situation, in my mind, is that this won't get many more eyes on it...certainly not at the level of detail that has already been put in.  Are you guys (@jtorrents and @ysitu) at the point where this is ready?  If so, you have my vote to merge it.    [If it passes a smell test and all the unit tests continue to pass, I lean for "merge". :) ] 
comment
So ends the most-involved PR evaarrrrr.... 
comment
Ready to merge then? 
comment
Btw, should we get in the habit of updating the release notes in major pull requests?  It might cause slightly more conflicts when merging, but they are easily resolved.  The thought would be that incrementally updating a list of bugfixes, what's new, etc, might make it easier to release. 
comment
Is it worthwhile producing a migration guide? Or would that be too complicated?  Old:  ``` # did that ```  New:  ```  # do this ``` 
comment
Also, could/should we change the link on the front of the github page.  Right now, it refers to networkx.lanl.gov instead of networkx.github.io. 
comment
Great. That should probably decrease the unexpected referrals shown here: https://github.com/networkx/networkx/graphs/traffic. 
comment
In general, I stick with the module-name-must-be-unique rule. Any import statement makes the imported submodule part of the namespace. So a statement like:  ``` from .conflicting_name import conflicting_name ```  will always override the module. This is probably undesirable, as it can sometimes be helpful to navigate into the module. Once this is done, it might not even be possible to get access to the submodule, except possibly through an explicit use of the `__import__`  builtin or by using `imp`.  But your error is different, suggesting that the module has overwritten the function.  Is that even possible?  It sounds like the function name was never imported into the namespace.  ---  Also, why do many of our `__init__.py` files have:  ``` from networkx.algorithms.submodule import * import networkx.algorithms.submodule ```  The second import statement seems unnecessary.  It doesn't import any specific submodule into the namespace and instead adds `networkx` to the current namespace.  This is, in part, why:  ``` import networkx networkx.algorithms.networkx.algorithms.networkx.algorithms.networkx ```  is valid code.  If we were to get rid of all those secondary import statements (and if all submodules made proper user of `__all__`), then `networkx.algorithms.networkx` would give an AttributeError. 
comment
If we import anything from the submodule, then the submodule will already be available without having to also manually import it.  Example:  http://codepad.org/ePJvYGzf 
comment
Yeah seems good to go! 
comment
The other common fix is to use an `__all__` inside `networkx/algorithms/flow/__init__.py`.  One problem with deleting `utils` is that everything in `utils.py` is no longer accessible (manually importing it again doesn't work either).  Presumably, no one will need those functions, but who knows.  It's quite a bit more boilerplate though: http://codepad.org/7oTfU82b Note that there is a possible function/module name conflict with `preflow_push`. 
comment
Let's do it! 
comment
Doctests can be painful when they are tested.  Nose provides a feature to skip a test on a per line basis by adding `# doctest: +SKIP` to the end of the line. That's fine when its a single line but when its a whole bunch of lines, it seems to default the purpose of the docstring---which is to demonstrate usage (not validity) and look clean for users.  Filling it with +SKIP on a lot of lines might even be confusing.  An alternative is that you use things like NumPy's `isclose` function, but that also defeats the purpose of the docstring, making it more difficult to parse.  I somewhat prefer not to test docstrings, but that has its own problems as well.  In documentation (that exists outside the docstrings), if we make use of the IPython Sphinx directive, then you can do things like:  ``` .. ipython::     In [133]: import numpy.random     @suppress    In [134]: numpy.random.seed(2358)     @doctest float    In [135]: numpy.random.rand(10,2)    Out[135]:    array([[ 0.64524308,  0.59943846],       [ 0.47102322,  0.8715456 ],       [ 0.29370834,  0.74776844],       [ 0.99539577,  0.1313423 ],       [ 0.16250302,  0.21103583],       [ 0.81626524,  0.1312433 ],       [ 0.67338089,  0.72302393],       [ 0.7566368 ,  0.07033696],       [ 0.22591016,  0.77731835],       [ 0.0072729 ,  0.34273127]]) ```  And that will test for approximate equality. The rendered output looks clean, something like:  ``` .. ipython::     In [1]: import numpy.random     In [2]: numpy.random.rand(10,2)    Out[2]:    array([[ 0.64524308,  0.59943846],       [ 0.47102322,  0.8715456 ],       [ 0.29370834,  0.74776844],       [ 0.99539577,  0.1313423 ],       [ 0.16250302,  0.21103583],       [ 0.81626524,  0.1312433 ],       [ 0.67338089,  0.72302393],       [ 0.7566368 ,  0.07033696],       [ 0.22591016,  0.77731835],       [ 0.0072729 ,  0.34273127]]) ```  So this is another option (but it exists outside docstrings and would only be tested when documentation was built). 
comment
(Moving forward, we should be using the `.format` approach anyway.)  But aren't those statements mostly equivalent?  What's an example value of `label` that won't work? 
comment
Aha, it fails anytime label is a tuple.   So `% (label,)` should work as well.  In fact, I think that's a common gotcha---the recommendation is to never use `% label`. 
comment
The first commit implements the proposed change.  The subsequent commits address another issue that this ticket revealed.  If your nodes were tuples, then the generated GML code produced labels that were not quoted.  Something like:   `label  (1,2)`  Now, the code forces the node to a str, possibly using a custom classes **str** method. Quotation marks are must also be converted to HTML entities---the code does this now. 
comment
This is ready for merging I think.  It fixes both #1048 and #1061.  Will merge in a few days if no one objects. 
comment
This is looking great, and definitely a welcome contribution. There's been some interest in this too (mailing list). 
comment
> An algorithm should not return a generator. A generator creates an invisible dependency on the graph that lingers even after the algorithm has ended. An inadvertent user will be surprised when the generator raises an exception after he/she modifies the graph.  We use generators and iterators all over the place as a way of delaying calculations. There's a (fairly obvious) implicit rule that, unless stated otherwise, the behavior of every algorithm in NetworkX is undefined if the graph is modified during the calculation.  E.g.:  ``` # Python 3.x G = nx.path_graph(5) for u, v in G.edges():     G.remove_edge(u+1, v+1)     print u, v ```  This is also a potential issue if an algorithm is implemented as a class and the user goes in and modifies the stored variables of the instance. That it is possible does not, in my mind, mean we should not use such features of the language.  I think lazy calculations offer a lot of flexibility. Suppose you don't need all outputs of an algorithm, but only need outputs until some non-trivial criterion is satisfied. Being able to break out early, or cease calling next() is a big win. 
comment
Ready to merge, I think. 
comment
I'd say go for it (in another PR).  Is this and #1148 ready to be merged?  Update: Hahah, I had merged this already! 
comment
Thanks! 
comment
Yep...I ran into this too.  I have some code that fixes this, but it was meant to be part of a bigger pull request for finding optimal arborescences on directed graphs. I can get the recognition parts in for this issue, but unit tests for the added functionality might have to wait. 
comment
This is mostly a Python 3.x error right? I ran into this recently with some other code.  ``` >>> 1 < '1' # works in 2.x but not in 3.x True ``` 
comment
Can we convert the node to an integer before pushing, and convert back to its original form on popping? Two more dicts. 
comment
Yeah, it seems like it would...it just seems messy.  Python idioms always warn about comparing with "is", which is exactly what we'd be doing here (even though those reasons aren't in play here, I think). The other thing is that the codebase has never relied on using on id() for equality---we always use hash equality or comparison equality.  Are we positive there are no operations in the codebase that create copies of nodes and store them instead? [Though, I guess even if they did, this would only be an issue if `single_source_dijkstra` were doing  things across different graphs.]  So it's probably fine, I will just need to get used to it. 
comment
As much as I like parameter names to be self-documenting, I hope we can avoid the overly long name `undirected_self_loop_factor`.  Maybe `selfloop=2.0` and then describe what it means and when it applies, in the docstring? But I'm not really invested in the name, either way. 
comment
Looking great. 
comment
Any interest in using:     https://github.com/snide/sphinx_rtd_theme  It's now the default theme used by readthedocs.org 
comment
Looks good. IIRC, it's a fluid layout too, so it works on mobile phones.  We can use this without moving to readthedocs as well. 
comment
Unless someone objects, I will merge this tomorrow, same bat time. 
comment
Out of curiosity, what version of Jython was this not working on?  At least from this thread:  https://mail.python.org/pipermail/python-3000/2008-March/012565.html  it seems that 'from .foo import *' is allowed. 
comment
The changes look good. I'm +1 on merging. 
comment
Looks great. Ready for merging, I think. Hold till Monday? 
comment
Yeah seems good. 
comment
We can merge this once those changes are done. 
comment
Would it be worthwhile to mention in the docstring the O(n+m) time for DAGs? Is this the preferred algorithm when working with DAGs? Providing some guidance to the user could be helpful. 
comment
Looks great! 
comment
I believe this has already been fixed in #1048 (which is awaiting approval).  But perhaps not for `read_gml`. 
comment
Actually, looks like I fixed it for node labels only, not for any other attributes. And `read_gml` needs to be adjusted appropriately as well. 
comment
Duplicate of #1054  
comment
Ahh...conventions, always a fun game.  Looks good though! 
comment
I think it comes up because TravisCI has changed their environment very recently. See #1130. I'm wondering if they upgraded their Ubuntu version and so now, apt-get installs a different version of SciPy than it used to. 
comment
Opps. We aren't using a git submodule are we? 
comment
Yeah odd. Maybe a side effect of their new changes. I've got one going here that looks like it will finish: https://travis-ci.org/chebee7i/networkx/builds/24115882 
comment
Looks like a network error...I've gotten 3 successful builds (on all python versions) now. So unless this keeps happening, I'm guessing it was transient. 
comment
Hi, I believe this is issue #949.  It sounds like nothing has changed.  Maybe try to contact the developer of the decorator module? 
comment
Yes, please let us know if that is enough to make it work. Sounds like it might be.   As to why copyfunc was used, I can't remember now, but I suspect it had to do with copying docstrings (but there are certainly other ways to achieve that goal). I probably just copied the answer here: http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python without giving it much thought at all. Hah!  From reading that SO question, I think the main motivation was that I didn't want to increase the function call overhead. The VF2 code is recursive and the match helpers are called frequently on inner loops.  Perhaps it was a premature optimization. 
comment
Download this file: https://github.com/chebee7i/networkx/archive/nocopyfunc.zip  And try to install that version of NetworkX. This is from a branch I made that applies that patch. e5884efd5 
comment
> @chebee7i The problem is that IronPython does not have types.FunctionType as you commented in #949.  Whoops...@ysitu thanks for correcting me.  Indeed, the decorator module was related the sys._getframe and not related to IronPython's lack of support for `types.FunctionType`. 
comment
Ok, I'm going to do a try/except and merge this change in. 
comment
Functionally, It's not going to be any different from what you have through the zip file. So its up to you. 
comment
Note: I think attr_matrix handles this.  ``` def floyd_warshall_numpy(...):     ...     if G.is_multigraph():         edge_func = lambda u,v: min([d.get(weight, np.inf) for d in G[u][v].values()])     else:         edge_func = lambda u,v: G[u][v].get(weight, np.inf)     A = nx.attr_matrix(G, edge_func, rc_order=nodelist)     .... ```  I think its important to use np.inf rather than np.nan since for multigraphs, we are taking the min of all weights between two nodes.  Alternatively, as @hagberg suggested, something (essentially) equivalent to the above could be added into to_numpy_matrix(). 
comment
@wwh-phd glad you found edge_attr.   That `edge_attr` accepts function is an undocumented feature.    Ok, and I see the issue with initializing the matrix with zeros.  However, the way that `attr_matrix` is currently defined, it `+=` the existing matrix element.  So initializing to np.inf will not work.  We could change this, but I'm not sure what a good solution is yet. 
comment
So one way that attr_matrix could be modified to handle this is if we use a masked array.  First, we mask all elements.  Then, any element we touch, we unmask it and modify it.  At the end, we can convert all remaining masked values to the user desired value.   This allows users to specify that elements corresponding to non-edges are treated differently from edges which did not have the desired attribute.  Potentially, this could be useful outside `floyd_warshall_numpy`.  Let me give this a try (I'll attach to this issue). 
comment
I ended up just adding an option to `to_numpy_matrix()`.  I wanted to avoid some subtle differences in how `attr_matrix()` and `to_numpy_matrix()` combine edge values. Separately, I'll try to normalize their behaviors so that `attr_matrix()` is strictly more general. 
comment
@wwh-phd Does this work for you?  If so, I'll merge. 
comment
Great, I'll just leave it open for you to merge when ready. 
comment
Good catch.  I want to try something else which addresses the issue with nans.  I'll push those up next.  If we don't like, we can just revert anything after 52011fa. 
comment
Ok, take a look at these.  Big difference is that we now use masked arrays to handle nonedges.  This allows us to deal with nan values on real edges properly.  Also, `weight` can be any callable now.  This allows you to have a different default value.  ``` weight=lambda attrs: attrs.get('weight', 3) ```  Acceptable `multigraph_weight` options are now strings, and they need not ignore nans.  ``` multigraph_weight='nansum' multigraph_weight='sum' ```  To handle this, it is necessary that the initial value of the matrix be configurable.  For the provided weight operators, we give sensible defaults.  For a custom weight operator, the user must specify `multigraph_initial`.  Overall, this should be much more general now.  I will write tests for this if we like the improvements. 
comment
Yeah sounds like a good plan.  We could avoid numpy.ma and just use our own mask, but that would also have performance issues.  And using todense() might be better in general. 
comment
Should we consider making the docs available on [rtd](https://readthedocs.org/)? They'd be rebuilt on each commit. I've had good experiences with rtd, and setting it up easy. 
comment
Nice, I'll probably hold off on RTD...two locations for the documentation might be confusing. 
comment
Are the pages the same?  I was under the impression that they were different versions of the documentation. Also, the root already redirects to the github.io pages...its the other stuff that still gets scanned.  So maybe it would be good to modify the robots.txt to stop caching and crawling. 
comment
All tests pass and nested for loops are avoided. I'm +1, but will wait for another to chime in. 
comment
Aha! I recently committed a fix to a test that was occasionally failing (the fix was to sort the items).  Good to know what the cause was. 
comment
Some of the problems relate to how `nx.generate_adjlist` is written.  There are two loops in there...  ``` for s,nbrs in G.adjacency_iter():     for t,data in nbrs.items(): ```  Since both loops are over dictionaries, the iteration order can vary with hash randomization. That is not normally a problem, but in this particular function, we are constructing a string and yielding as we go---so the generated string is affected by hash randomization.  The obvious solution would be to sort the nodes (and the neighbors of each node), but that would assume that the nodes _can_ be sorted.  IIRC, NetworkX doesn't mandate that nodes be sortable.    Should this function be "upgraded" to require that nodes be sortable?  This would allow roundtrips, but roundtrips aren't absolutely necessary.  Perhaps people with unsortable nodes still want to generate adj lists as strings, and don't care of the string itself might differ with hash randomization. 
comment
If we do not want `generate_adjlist` to assume sortable nodes, then only the unit tests will need to be updated. Currently, they assume you can roundtrip: G -> adjlist -> G'  and that G and G' should be equal.  But because of hash randomization, that assumption is incorrect.  One easy fix would be to use `is_isomorphic` instead of demanding that the edge tuples be equal. 
comment
Aha, I had forgotten about those functions. Very nice. Same fix is needed for some of the tests in `networkx/readwrite/tests/test_adjlist.py` and probably anywhere else we aren't using `assert_edges_equal` when we should be. 
comment
Also, since we don't have a hard dependency on `scipy`, maybe it would be better to make another function? 
comment
This runs without error for me. ![figure_1](https://f.cloud.github.com/assets/326005/1806966/87760a3c-6cc9-11e3-95fa-000b1bb9d51f.png) 
comment
Looks great! 
comment
This seems to bring up the issue of views again too.   https://github.com/networkx/networkx/issues/335  The conclusion there was that views are not faster, but perhaps speed should not be the only consideration. 
comment
Having options is usually the better route.  As for what should be the default (copy or not), probably, we should keep the default unchanged. 
comment
Also, for future Python 2/3 compatibility, I recommend we add the `six` module as a dependency and start using it instead of putting version checks in various places throughout the codebase. 
comment
Yeah fortunately, the codebase hasn't needed to do many checks.   So probably we can get by without it. 
comment
Are gexf graph formats backwards compatible (independent of whether or not this particular 1.2draft is backwards compatible)?  If not, should we be clearer about what version of gexf this reader can handle? 
comment
How does the `@attr` work?  It looks like it just adds an attribute, but its not clear why that means it is skipped by nose in the way that we run `nosetests` on travisCI.  Wouldn't it be clearer to just create two classes one for numpy and one for scipy.  Then for each, something like:  ``` @classmethod def setupClass(cls):     global numpy     try:         import numpy     except ImportError:          raise SkipTest('NumPy not available.') ```  Then, every test method on that class is skipped. 
comment
Ah I see. Makes sense.  So we're not passing any attrs in via the command-line.  Most of our skips are done via something like that code snippet.  Any chance you can modify it like that?  Generally, I think having separate classes (since the implementations are different) might be cleaner.  You are free to subclass too.    ``` class TestPageRank(object):     # Looks like everything minimally requires NumPy     @classmethod     def setupClass(cls):         global numpy         try:             import numpy         except ImportError:            raise SkipTest('NumPy not available.')      def test_empty(self):         pass  class TestPageRankScipy(TestPageRank):     @classmethod     def setupClass(cls):         global scipy         try:             import scipy         except ImportError:            raise SkipTest('SciPy not available.')       def test_empty(self):          # override ```  Now, maybe we should be using attrs.  That's another pull request either way though. 
comment
So I think the `setup_module` in `pagerank_alg.py` is for doctests within that module.  And for those, I think we're just saying its an all or nothing---we'll test the doctests if the environment has both numpy and scipy, otherwise not. 
comment
As for the guard on line 410, I think we are slowly removing those and just letting the default ImportError raise.  So I think its okay to remove that one.  Looks like this is good to go.  Up to you if you want to remove the guard there.  I'll leave this for @hagberg to review. 
comment
I saw you forked.  Did you want to submit a pull request?  If not, I'm happy to make the change for you. 
comment
But what is the purpose of having a line:  ``` *network ```  in a Pajek file if there is no name listed with it?  Also, although `*network` appears in the example [here](http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/draweps.htm), it is not mentioned in the [PDF](http://vlado.fmf.uni-lj.si/pub/networks/pajek/doc/PajekMan.pdf) manual anywhere that I could see. 
comment
Do you mean an edge without an edge value?  Or a pajek file with only edges specified (and no nodes explicitly specified).  I can adjust pull request as we desire. 
comment
I'll go ahead and merge this fix (even if it seems a bit odd to provide a name without a name). If we think we need to allow "only edges", we can make another ticket.  Since no one has asked for this as of yet, its probably not a high priority. 
comment
The functions are the same---we provide two ways of accessing it.  I've updated the docstring to make this clearer. 
comment
I can take a look at this later if no one jumps on it before then.  I agree that the correct "answer" really depends on the type of construction you want.  [Here](https://gist.github.com/chebee7i/6251876) is a quick and dirty translation of the line graph construction I use in my own work with multidigraphs. If people like this construction, we can clean it up and make it handle all graph types too.  In the output below, the new nodes are (from node, to node, key).  For non-multi digraphs, the keys wouldn't be there. ![nx_dqavzo](https://f.cloud.github.com/assets/326005/976955/13698b68-069a-11e3-85e5-e061ac427f04.png)![nx_zujbdl](https://f.cloud.github.com/assets/326005/977084/92c63472-069c-11e3-9a8d-a3932946dd63.png) 
comment
I don't think multi(di)graphs require changes to the definitions.  The reason being that the definitions are defined on edges---having additional edges will not change that definition any more than would adding non-multiedges to the same graph. So each edge in a multi(di)graph has a key associated with it, but that key is ignored when determining edge adjacency.  Interestingly, it seems that "standard" definition for directed graphs is not the obvious analogous definition from undirected graphs.  The main difference being that with undirected graphs, you pop the current node (an edge in G) from the list of all nodes and then you connect any nodes which represent incident edges.  For directed graphs, the standard definition (e.g. that used for de Bruijin graphs) is that you do not pop the current node.  Consider a multigraph with the following edges: (A,A), (A,B), (A,B). Its line graph has the following edges: (AA, AB0),  (AA, AB1)  Consider a multidigraph with the following edges: (A,A), (A,B), (A,B) Its line graph has the following edges: (AA, AA), (AA, AB0), (AA, AB1) The point being that each of the 3 edges represents one of the 3 possible length-2 paths.  It seems to me that you _could_ define line graphs for digraphs in an exactly analogous way (and that would correspond to the code you posted on SO).  For some reason though, the emphasis with digraphs has been on the line graph representing all possible paths, and thus, it has deviated.  We could perhaps provide:  line_graph() and line_digraph().  line_graph() pops the current edge when determining adjacency in the line graph, while line_digraph() does not.  To allow people to use both definitions, we simply allow both functions to work with graphs _and_ digraphs.  So in principle, you could construct the line graph of a multidigraph, and also the line digraph of a multidigraph (and these would not be the same thing).  To make it clear what I am proposing:  ``` edges: [(A,A), (A,B), (A,B)]  g is a MultiGraph with those edges. line_graph(g)    is of class g.__class__ with edges:   (AA, AB0),  (AA, AB1) line_digraph(g)  is of class g.__class__ with edges:   (AA, AA), (AA, AB0), (AA, AB1)  g is MultiDiGraph with those edges. line_graph(g)    is of class g.__class__ with edges:   (AA, AB0),  (AA, AB1) line_digraph(g)  is of class g.__class__ with edges:   (AA, AA), (AA, AB0), (AA, AB1) ``` 
comment
Great.  I don't think there should be any ambiguity with self loops, but we'll see I suppose.  I'll go ahead and give this a try with plenty of tests so at least its clear that the functions are doing what they intend.   Once I get code, I'll try attach the pull request to this github issue (http://stackoverflow.com/questions/4528869/how-do-you-attach-a-new-pull-request-to-an-existing-issue-on-github). 
comment
I had to define some auxiliary functions to hide some of the API differences between multi-and non-multi (di)graphs, but the result is that the main construction code is fairly readable and intuitive (I hope). 
comment
Opps. Looks like rest of the comment got mangled.  Repost. 
comment
Let me take a look at this before we merge #812.  This might be an issue since I think there isn't an official pydot that working for Python 3 yet.  @dcmoyer is the repo you posted "ready" or still in progress. 
comment
Before you submit a pull request, can you fork https://github.com/chebee7i/networkx and checkout the "pydot" branch.  This has updated code for converting NetworkX graphs to pydot.  I'm guessing some changes will still be required for encodings, so I'd like to get them incorporated there instead.  Then, submit a pull request to that branch on that fork.  See #812 for more details. 
comment
Also, for reference purposes, it would be helpful if you could post a minimal example that demonstrates the problem. 
comment
Without an example demonstrating the problem, I just took a guess and assumed it was something related to having a node with a unicode name.  I've tested that such a graph works fine with the code in #812 in both Python 2.x and 3.x.  So I'm going close this once #812 is merged. 
comment
So I suspect the encoding issue you ran into is related to the version of pydot that you were using.  Some changes were made to to pydot regarding how it writes to files and reads from them.  So I'm unclear how to proceed on this.  And given that the official pydot doesn't support Python 3, we really need to think about what version of pydot to support. 
comment
I have pulled out the pydot drawing code and made it available as a separate module. Check out this:     https://github.com/chebee7i/nxpd  It includes its own version of pydot and so is able to say with confidence (in principle at least) whether the node names should be encoded to utf-8 or not.  It uses the version of pydot you linked to and does, in fact, remove the encoding portion. 
comment
We definitely need more information, as the following example demonstrates there is no issue with DiGraphs having a single node:  ``` g = nx.DiGraph() g.add_node(0) assert len(g.nodes()) == 1 assert len(g.edges()) == 0 assert g.number_of_nodes() == 1 assert g.number_of_edges() == 0 ```  So if you can provide a minimal example (something we can run ourselves) demonstrating the issue, that would be great. 
comment
I'm going to close this ticket and respond on Google Groups, as this is not really an 'issue' but a request for help.  If you are looking for your post, try this link:     https://groups.google.com/forum/?fromgroups=#!topic/networkx-discuss/_vvuBLLGQGM 
comment
Looks good to me.  Can you fix the first line of the docstring on parse_graphml?  It refers to a 'path' not a string. 
comment
This is very useful!  Happy to see it. 
comment
@jni: Can you comment on how these differ from using:  ``` nx.shortest_path_lengths(G, source='A') nx.shortest_path_lengths(G, target='A') ```  The above commands work for all graphs (undirected and also directed non-DAGs). When you have a DAG, I think the interpretation of the keys of the returned dictionary is the same (unless you are only returning immediate descendents, etc). 
comment
The issue is that there is a conflict.  I'll resolve it and merge. 
comment
All tests pass for me.  I'll go ahead and merge it.  Probably, there are other algorithms where similar improvements could be achieved, especially with respect to the "in" operation.  Also, I will add a Wiki page which shows how to run tests on a pull request. 
comment
Actually, it doesn't look like the wiki is enabled for the NetworkX repository.  Where should I put this information? 
