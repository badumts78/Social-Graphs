issue
Added new generator function equivalent to simple_cycles#TITLE_END#This pull request implements a new generator function which is equivalent to simple_cycles (called simple_cycles_generator). The idea sparked when trying to speed up the function in https://github.com/networkx/networkx/pull/874 and the use of a generator function as a means to to avoid memory errors for graphs with lots of cycles was also suggested in https://github.com/networkx/networkx/pull/888.  The original algorithm is recursive and the transformation into a generator function preserves the recursive structure. (For an example of a recursive generator structure, see http://stackoverflow.com/questions/6694404/help-understanding-how-this-recursive-python-function-works/6694827#6694827).  Any comments on the code are most welcome.   Also, I would like to add the filtering options implemented in https://github.com/networkx/networkx/pull/888 to this new generator function but I am not sure how it works regarding the work flow. Here, I just implemented the generator function mimicking the original algorithm, should I create another branch for the generator function with filtering options? or is it maintainer work to do this kind of merging?  
issue
Main changes to add node and edge filtering capacity to simple_cycles#TITLE_END#I modified the simple_cycles algorithm because I was getting a MemoryError due to the size of the result array (I was studying (almost) complete digraphs which are the worst case for simple cycles since (almost) all of cycles exists. The biggest complete digraph for which all cycles can be found in a 32-bit machine is an 11 node one; a 12 node one raises a MemoryError). However, I did not require all simple cycles but only some: the ones going through specific nodes or edges: that is the main idea behind this modification . Thus, I  modified simple_cycles accordingly, so that now you can filter the results either by providing a list of nodes AND/ OR a list of edges. The node filter is negative (cycles containing the defined nodes do not appear in result) and edges are positively filtered (one cycles containing _all_ the defined edge(s) are included in results).  So, the outcome of this modification is a refined search of the simple cycles within a graph, either by avoiding specific nodes or by filtering cycles containing specific edges.  Consequently, in both cases, the size of the result is lower due to the filtering, enabling to analyse bigger graphs or to make quicker searches (providing a node list speeds up the calculation since the search bypasses the predefined nodes; the edge filtering does not reduce cpu time).  The code seems to work for digraphs. I also added some code for the case of undirected graphs to generalise the edge definition. I am however not sure whether simple_cycles is supposed to deal with undirected graph, if not, please discard the modification https://github.com/Friedsoap/networkx/commit/e7edff2cc54587b092b5207b1cf3f0c2fff14fe6 
comment
Hi there,  I understand and appreciate the worries about speeding up the simple_cycles function. However, it would be nice to maintain the original Johnson algorithm since it adds "academic" robustness (even if it is not the default script). So, it would actually be very nice to have two different functions for simple_cycles, one fully based on the Johson algorithm and a new one with increased speed based on different methods.  Also, I am not sure whether it is a related issue but I am using simple_cycles on a 55 nodes digraph (which is almost complete) and simple_cycles spits a memory error (posted in the google group networkx-discuss: https://groups.google.com/forum/?fromgroups=#!topic/networkx-discuss/6FiQy-cxsBM ). The networkx version is 1.7 and othe specs are in the error post. 
comment
Dear all,  I made some tests. Just a reminder that finding all simple cycles of a complete digraph _is_ the very worst case of finding simple cycles since all simple cycles are present. The trend of number of simple cycles I found is about (nbr_nodes-1)!_3 (I know that the number of Hamiltonian cycles of a complete digraph is (nbr_nodes-1)!. The (nbr_nodes-1)!_3 number is an approximation I found after calculating the simple cycles for complete digraphs from 2 to 11 nodes.   Both simple_cycle.py and the new code suggested above (simple_cycle2.py) have similar cpu times (but I guess this is because it is the worst case for finding the simple cycles). Both manage to find the simple cycles of a complete digraph up to 11 nodes. From 12 nodes onwards, it raises a memory error.   Thus, the biggest simple cycle list  that simple_cycles.py is able to calculate is for an 11 node complete digraph which contains 10 976 184 cycles.  According to the approximation of cycles described above, the list containing all simple cycles of a 12 node complete digraph contains about 120E6 cycles (of which one third are hamiltonian cycles, i.e. 12 nodes long, and the rest simple cycles of smaller length); but it seems that this is already too much for python or the module.  This is however against python's max capacity (unless my back-of-the-envelope calculation is wrong). According to http://stackoverflow.com/questions/855191/how-big-can-a-python-array-get  '''According to the source code, the maximum size of a list is PY_SSIZE_T_MAX/sizeof(PyObject*). PY_SSIZE_T_MAX is defined in pyport.h to be ((size_t) -1)>>1 On a regular 32bit system, this is (4294967295 / 2) / 4 or 536870912. Therefore the maximum size of a python list on a 32 bit system is 536,870,912 elements. As long as the number of elements you have is equal or below this, all list functions should operate correctly.'''  The question is then: if the size all cycles of a 12 nodes complete digraph should be below 536E6 elements so why does simple_cycle.py run out of memory? (see below for a quick code to replicate the error)  The memory error is raised when appending to the final results list The error is:   File "/usr/lib/pymodules/python2.7/networkx/algorithms/cycles.py", line 161, in circuit     result.append(path + [startnode]) MemoryError  So how can this limitation be overcome? Would it be possible to split the results list every, say, 10 million elements (i.e. cycles)?  I will also explore the possibility of using cycle_basis, but I believe it is not the way to follow in my case. I ran the test above on a complete digraph to show the worst case.  What I do in my case is to find all simple cycles between the economic sectors which are usually represented by a complete digraph. However, I subtract those cycles according to some weighting algorithm and need to iterate it over the remainder of the original array to subtract the remaining cycles, that is why I need to recalculate the remaining cycles within that graph (which then is not a complete digraph any more because some arcs are zeroed after some iterations). I believe I really need to use the simple_cycles.py module. The issue is that although I can aggregate the sectors economy not to exceed the calculation capacity of simple_cycles.py (i.e. reduce the economy to 11 sectors), this is really hindering the possible analyses since this level of aggregation is too coarse compared to the original data set which contains about 55 sectors.  Thanks a lot,  Aleix  ========= code replicating error ============= import networkx as nx nbr_nodes=12 working_array_digraph=nx.DiGraph(np.ones((nbr_nodes,nbr_nodes))) all_cycles = nx.simple_cycles(working_array_digraph) print('''... There are {0} simple cycles in a {1} node complete digraph '''.format(len(all_cycles),nbr_nodes)) 
comment
Conclusion on the MemoryError: the results array is too big to include all simple cycles of a complete digraph of more than 11 nodes for a 32-bits machine.  Pseudo-solution and enhancement of the simple cycles code: filter the search by node and/or edge; the results array will be smaller. The search will also be quicker if nodes are skipped. See pull request https://github.com/networkx/networkx/pull/888. 
comment
Dear Dan and Érika,   It seems that transforming the simple_cycles function as a generator one is the proper way to solve the memory issue and was already under consideration by Érika in this pull request (thanks Dan for drawing my attention to it; at the time I read this entry I had no clue what a generator function is...).   I am working on transforming the simple_cycles into a generator function keeping its current structure (i.e. keeping its recursive structure). When I am done I will open a new pull request for that since it is not about speeding the function (as this thread originally aimed to) nor about filtering nodes nor edges (as thread https://github.com/networkx/networkx/pull/888 aims to do)  Thanks a lot,  Aleix 
comment
Hi, I am up for the testing, see https://github.com/networkx/networkx/pull/890 for a suggested method.  Regarding testing the 12 node complete digraph run: if you are not storing the results under any variable, both generator functions should be fine for any node size. If you are using a 64-bit computer, you might still store all results in RAM for a 12 node complete digraph but I bet you won't be able for a 13 node one :-)   Best, Aleix 
comment
Hi Dan, well done! 2 things:  - why is the non-recursive (i.e. iterative) function quicker than the recursive one? Do you think it is worth to run some time tests? - I think it is not practical to cut the very last number of a cycle. On the one hand it might save some memory/time but it can because non-practical if you want to search for a specific arc within a cycle: then, you will need to somehow add the last node to explicitly reproduce the last arc. E.g. cycle [0,1,2] contains in fact the arcs [0,1], [1,2] and [2,0]. If I need to find the last one, I will need to add a function to rebuild the last node for each cycle which I believe it is not practical. 
comment
Hi Dan,  OK for Python being slower while using recursive methods, as you suggest in https://github.com/networkx/networkx/pull/874 it would be nice to make some speed comparisons for complete and sparse graphs with both codes (the non-recursive generator you suggest here and the recursive generator in https://github.com/networkx/networkx/pull/889. To make the comparisons we should also agree on the method.  How about:  ``` generator=simple_cycles_generator(digraph_to_analyse) t0=time.clock() [cycle for cycle in generator] t1=time.clock() print(' It took {0} cputime to find all cycles'.format(t1-t0)) ```  Regarding the added node closing the cycle at the end of each cycle array: To find whether an arc is contained in a cycle I am using a generator function that rolls over the cycle providing each arc providing a "window" of 2, as suggested in http://stackoverflow.com/questions/11131185/is-there-a-python-builtin-for-determining-if-an-iterable-contained-a-certain-seq. I am afraid I do not use the built-in options of networkx for that nor I am familiar with them. So, I do not convert the cycles into arcs, I just "read" them on the fly and compare each window with a specific arc; I guess this is quicker than converting the cycle into separate arcs and then check whether any of those is equal to the one you are looking for. This will not be possible if cycles are provided without the last repeated node (or the window generator will need tweaking).   Regarding the induced RAM saving: I guess this was critical for the original simpe_cycles function returning a list with all cycles, however now that the function is a generator function, I am not sure whether people will store the results for later use, so the RAM saving is not as critical as it was before. On the other hand, if networkx users are used to represent cycles  without repeating the initial code, it might be worth using that notation for the sake of consistency. I am divided... maybe we could add an option like: repeat_initial_node=False by default but if set to True the cycles are generated repeating the initial code... Anyway, I am fine with both options, I am just giving some reason to think about.  Best,   Aleix 
comment
I did not forget, I will (hopefully) run the tests this week. 
