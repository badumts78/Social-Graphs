issue
Adds NVIDIA Corporation to list of supporters#TITLE_END#Followup PR for [this comment](https://github.com/networkx/networkx/pull/7823#pullrequestreview-2600659527).  This adds NVIDIA Corporation to the list of supporters.  cc @MridulS @dschult 
issue
Add config option to disable the cache warning message, update warning to include instructions on how to disable#TITLE_END#As discussed by @dschult , @eriknw , and @rlratzel at PyCon US 2024, the current behavior of warning users when they enable caching to inform them that changing graph attributes after the converted graph is cached will result in a stale cached value seems adequate for now and possibly longer-term.  However, to improve the UX, the following updates were proposed:  * Add a config option to allow users to disable the warning.   * This will let users easily disable only this particular warning, without the need to add code (if they use the config env var or other front-end option such as .yaml, .ini, etc.)   * This defaults to `False`, so users still have to knowingly disable this warning and therefore are more likely to understand the limitations of caching.  * Update the warning message to include instructions on using the above config option to disable the warning. 
issue
Updates documentation to include details about using NetworkX with backends.#TITLE_END#As discussed in the NetworkX community dispatching meeting on [August 14, 2024](https://hackmd.io/rqs_pWMxSLmICXCpI3w-Ug?view#August-14-2024), this PR adds documentation on "the wide variety of topics related to using NetworkX with backends".  These updates include: * Updates to the detailed backends docs for the latest changes to dispatching for NX 3.4 * Separate the content for Configs into its own section * Add text to the tutorial section to explain the basics for using backends * Add a "Backends" section, accessible from the top navbar, that lists various backends known to work with NX 3.4 * Add text to the Install section to briefly explain backends, how they're installed separately, and where to learn  more.
issue
Recognize `networkx.Graph` subclass instances and allow them for networkx fallback and auto dispatch of generators#TITLE_END#This PR: * updates the dispatcher to recognize graph types that are `networkx.Graph` subclasses and allows them to be passed to networkx functions as a fallback if a backend does not support the function call. * Allows dispatched functions that return graphs to be automatically dispatched if the function has an attribute that indicates it returns a graph that's known to be a compatible subclass of `networkx.Graph`.  Note that the changes here are primarily to the dispatcher policy to allow backend graph instances to be used much more freely if they abide by the `networkx.Graph` API contract, which should give backend developers plenty of freedom to implement as they see fit, and seems more pythonic as well.  These changes are additive though and should not break existing code. These changes provide much greater performance for no-code-change workflows that consist primarily of networkx functions that are supported by a single backend. By allowing graph generators supported by backends to be auto-dispatched, the backend graph can be used in no-code-change use cases without the initial conversion penalty. Backend graphs that are compatible with `networkx.Graph` can also be used to fall back to networkx if the backend does not have full coverage of the networkx API.  Related issue: rapidsai/cugraph#4456  When combined with minimal updates to nx-cugraph to make it a compatible with `networkx.Graph`, the following demo was run: ```python edgelist_csv = "/datasets/cugraph/csv/directed/cit-Patents.csv" edgelist_df = pd.read_csv(edgelist_csv, sep=" ", names=["src", "dst"], dtype="int32")  with Timer("from_pandas_edgelist"):     G = nx.from_pandas_edgelist(         edgelist_df, source="src", target="dst", create_using=nx.DiGraph)  print(type(G)) print(f"{G.number_of_nodes()=}, {G.number_of_edges()=}")  with Timer("pagerank"):     pr = nx.pagerank(G)  with Timer("coloring"):     c = nx.coloring.greedy_color(G)  with Timer("coloring (again)"):     c = nx.coloring.greedy_color(G)  Timer.print_total() ```  No backends used: ``` (base) root@beec0a6f2028:/Projects/cugraph# python t.py  from_pandas_edgelist... Done in: 0:00:51.344383 <class 'networkx.classes.digraph.DiGraph'> G.number_of_nodes()=3774768, G.number_of_edges()=16518948  pagerank... Done in: 0:01:10.098742  coloring... Done in: 0:00:13.701762  coloring (again)... Done in: 0:00:13.760541 Total time: 0:02:28.905428 ``` nx-cugraph backend used - nx-cugraph does not yet support `coloring.greedy_color()`, note the first call to coloring includes the conversion to a networkx Graph, but the second uses a cached conversion: ``` (base) root@beec0a6f2028:/Projects/cugraph# NETWORKX_BACKEND_PRIORITY=cugraph python t.py  from_pandas_edgelist... Done in: 0:00:00.775263 <class 'nx_cugraph.classes.digraph.DiGraph'> G.number_of_nodes()=3774768, G.number_of_edges()=16518948  pagerank... Done in: 0:00:03.823638  coloring... Done in: 0:01:08.104769  coloring (again)... Done in: 0:00:11.660575 Total time: 0:01:24.364245 ```
issue
Adds initial debug logging calls to _dispatchable#TITLE_END#This PR adds debug logging for dispatched NetworkX functions.  ```python import networkx as nx  G = nx.karate_club_graph() result = nx.betweenness_centrality(G)  print(f"node with highest BC value is {sorted(result, key=lambda k: result[k])[-1]}") ``` ```bash bash$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python example.py node with highest BC value is 0 ```  Debug logging is enabled using the standard python `logging` API: ```python import logging  import networkx as nx  logging.basicConfig(level=logging.DEBUG)  G = nx.karate_club_graph() result = nx.betweenness_centrality(G)  print(f"node with highest BC value is {sorted(result, key=lambda k: result[k])[-1]}") ``` ```bash bash$ NETWORKX_AUTOMATIC_BACKENDS=cugraph python example_debug.py DEBUG:networkx.utils.backends:using backend 'cugraph' for call to `betweenness_centrality' with args: (<nx_cugraph.classes.graph.Graph object at 0x7fc3dd53cb80>, None, True, None, False, <random.Random object at 0x56016e34f090>), kwargs: {} node with highest BC value is 0 ```  Debug messages are not shown when functions are not dispatched: ```bash bash$ python example_debug.py node with highest BC value is 0 ``` 
comment
FYI we just merged https://github.com/rapidsai/nx-cugraph/pull/50
comment
> So, if a user sets the nx.config.backend_priority as ["parallel", "networkx", "cugraph"] and runs a python script with nx.leiden(G,...) call(and other nx calls too) then they could run their code without modifying it. But, it is misleading because it gives the impression that networkx has leiden implemented but it does not. So, I think the code should be changed by the user in this case. And the user would anyways have to change the code, because their code might run with this backend_priority but without any backends it will throw an error.  This is a good comment for a topic larger than what's covered in this PR, and I'm thinking we should have had a NXEP on "backend-only" APIs or "NetworkX as a standard graph analytics API" or some other name for this topic.  My thoughts about supporting backend-only APIs are: * They allow NetworkX to define a standard interface for functions which backends can use for their implementations, rather than have multiple backends implement the same function with different/incompatible interfaces.  The API standard defined by NetworkX allows users to have compatibility across backends, just like they do for functions NX currently implements. When/if NX implements the function (no longer making it "backend only"), the user's code continues to work on systems without backends. * Having NX define the function signature also allows NX to provide a docstring, examples, and possibly tests which make it even easier for backend authors to share their implementations, and easier for users to adopt them. * Zero-code change becomes more of a relative term for backend-only APIs, meaning a user's code is portable across systems that have a backend installed that supports the backend-only function.  So if cugraph and parallel both implement leiden, then users can have code portable to CPU-only systems using nx-parallel, and GPU systems using nx-cugraph, without code changes. When/if NX implements leiden, then their code will work everywhere without changes. But I agree that backend-only functions do stretch the meaning of zero code change, and maybe we wouldn't call it that for cases involving backend only APIs.  > So, if a user sets the nx.config.backend_priority as ["parallel", "networkx", "cugraph"] ...  I'm a bit confused by this use case though. Is this saying the user has installed nx-cugraph, but doesn't want to use it for anything except functions not implemented in NX (leiden)? I'm just wondering if that's more of an uncommon corner case, and the recommendation for that is to set `backend_priority=["parallel"]` and then set `backend="cugraph"` for leiden in their code (assuming nx-cugraph is the only backend implementing leiden), which is similar to what I think you also suggested for this case.
comment
This is a good change, thanks!  I also really like the additional debug log messages.
comment
I'm glad we're having this discussion.  I agree with many of the points brought up by @dschult and @jim22k, but I'd take it a bit further: 1. If `backend=backend_name` is given then use that backend (and fail if can't) 2. Step through the `backend_priority` entries -- if a backend cannot handle the input graph, it raises `NotImplementedError` and the next backend in the list is tried. Maybe "networkx" is an implied/default last entry in the list.  And that's it.  Since "many" backends (3 of the 4 publicly-known backends that I'm aware of) have graph types that are either NetworkX graph types or drop-in/duck-type compatible NetworkX graph types, I'm thinking it's feasible to just formalize that as a requirement for backends.  Supporting a library's types seems like a reasonable requirement for a backend to said library IMO.  Furthermore, if a backend wishes to be used for a type other than its own (including NX types), it's free to convert it itself (using either `from/to_networkx`, or any other means that makes sense for them), and cache the conversion as a `__networkx_cache__` entry if it wishes and if that dunder is supported by that type.  This gives backends the opportunity to optimize as they see fit ("backend-to-backend" conversion).   This seems like it would simplify core NetworkX dispatching tremendously - NetworkX doesn't care about backend types, conversions, and caching anymore, it just passes graphs to functions in priority order.  Of course, it would require changes to some backends and NetworkX, but I think it would simplify the code, docs, tests, etc. in the long run.  We'd have to get there from here incrementally in a backwards-compatible way.  But I don't mean to block this PR if the community wants it in, plus this PR does solve problems faced today (for `nx-cugraph` use cases, possibly others), so these suggestions can be applied to a later discussion for future PRs if necessary. 
comment
> A reader might consult the NetworkX documentation for a summary of the "dispatch" decorator. But there is no mention of it in the [Introduction section](https://networkx.org/documentation/stable/reference/introduction.html) or the [Algorithms section](https://networkx.org/documentation/stable/reference/algorithms/index.html).  It sounds like having an overview paragraph on the Introduction page, probably right after the [Data Structure](https://networkx.org/documentation/stable/reference/introduction.html#data-structure) section, could have helped here.  I think making dispatching a more advertised, first-class concept/feature for NetworkX will go a long way in helping under-documented or not-optimally-named types be less mysterious.  That said, I do appreciate good names and know how hard they can be to come up with, which brings up the next part...  > The name of the decorator makes the problem worse. The name dispatch reads like a verb in the imperative form: "Dispatch this thing!" That creates an expectation that the decorator performs an action, known as "dispatch", and changes the behaviour of Python code in the decorated function. However, in the common case where no backend is present, I believe the decorator has no effect. Thus the expectation set by the imperative verb is incorrect.  @eriknw and I were discussing this and we both think `@dispatchable` would be better than the current name.  This reads like an adjective, describing that the decorated API is _capable of being_ or _may be_ dispatched.  I suppose it's accurate to read it as a noun too since the decorated API is a type, more specifically, it's a special kind of callable.
comment
Thanks!  This is great.  I'll work on an example to demonstrate the speedup when multiple algo calls are made using the same Graph.  It should be significant based on benchmark comparisons with and without conversion cost.
comment
> Right now, this only applies to when networkx graphs would be converted to backend graphs before running the networkx algorithm, so it would be accurate to call it should_convert_and_run, ...  I don't think we should restrict it to cases where a conversion would be done.  I think we should allow backend developers the freedom to communicate that they may be too slow, buggy, experimental, etc. for specific operations/corner cases even if the graph is already converted.  Of course, the backend should also be able to see if a conversion would be required or not and use that to influence its decision too.  The "reason" string is crucial here for a good UX (and I think it should be required).   clarification: if `backend=` is specified, then `should_run()` will be ignored since `backend=` overrides everything.
comment
@eriknw and I met offline and discussed that this PR could be merged with the policy in place to only consider `should_run()` if a conversion was required, but a future update could allow `should_run()` to be used in other situations that a backend may know of that results in sub-optimal performance or behavior even if the graph is converted.    Merging it now will allow `should_run()` to be used today for at least a subset of use cases (which could possibly be the majority of use cases), then expanded later to be used for more cases. 
comment
Thanks!  This is a good topic for dispatching, especially as we work toward getting caching in for NX 3.3  > This changes a behavior: if a function is known to mutate an input graph, then this does not automatically convert an input graph to a backend graph.  Conversion can still happen by using backend= keyword.  Can you explain the reason for the behavior change?  My understanding is that a function that mutates the input graph - as a side effect or otherwise - will not result in those updates being propagated to the NX Graph if dispatched, and therefore dispatching the call will result in different behavior (ie. a difference that breaks user code).  What I'm not clear on is what the expectation is if a user forces the conversion/dispatch using `backend=`.  Is this just use-at-your-own-risk, or are backends expected to properly mutate the NX Graph in that case?  I'm assuming it's not the latter otherwise we'd then allow the automatic conversion/dispatch.  Either way, I think if a user sets `NETWORKX_AUTOMATIC_BACKENDS=...` then this new behavior will be surprising.  Here's a couple of options: * backends that can't mutate the NX graph should return False for `can_run`, which means being strict about treating Graph mutations as part of the contract for the function that backends have to honor.  I suppose if that was decided, then we wouldn't need `mutates_input=` and we'd have to enforce the proper mutations with tests.  (this feels like the most work, but the right decision IMO). * somehow tell users we're making this decision for them: warning, logging, throw an exception by default which can be overridden by a config option, something else.  
comment
Chatted offline with @eriknw (Erik, please correct me as needed): * The bigger goal is to not break user code, so the change here to not auto-dispatch will avoid that completely since backends are not currently assumed to mutate the input graph * There's still an element of surprise for users using `NETWORK_AUTOMATIC_BACKENDS=`, but this is lower priority than preventing the breakage of user code   * A future effort will be made to prevent this surprise too, and there's overlap with this and [should_run](https://github.com/networkx/networkx/pull/7257)   * Introspection features ("tell me what you're going to do and why"), logging, and config are all topics/upcoming features related to solving this problem * We want one of the overriding principles to be "if I tell you to do this then don't decide for me" and therefore `backend=` will still allow a user to force dispatch to a particular backend.  This allows power users, backend developers, etc. to dispatch even if graph mutations aren't supported.
comment
I've made progress on some mockup scripts that demonstrate various dispatching use cases that I think can be supported incrementally over time.  They cover config, auto-conversions, fallback scenarios, multiple backends, dynamic graph updates, etc. and I think they could influence this PR. I'm hoping to make the mockups available this week and go over them at the next community call, or earlier if possible, with at least @eriknw and @MridulS .  Can we go over those before merging this? It might prevent some API churn if we decide we want to support things differently.
comment
> Can we go over those before merging this? It might prevent some API churn if we decide we want to support things differently.  We had a good discussion on this topic recently.  We also brought it up at the last dispatching community call (some notes [here](https://hackmd.io/rqs_pWMxSLmICXCpI3w-Ug?view#August-31-2023)).  I think the outcome was that we need to continue the discussions and add an NXEP, but in the meantime, I don't think we should block this PR (assuming we can change things related to it in the future if necessary, which I think we can?).  I will add a link to the new NXEP for this once it's available.  cc @aaronzo @MridulS @jim22k 
