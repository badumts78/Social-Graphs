issue
Fixed Rich Club Boundary Case#TITLE_END#Address #1867  
issue
Community generators#TITLE_END#This adds the community graph generators added in the GSOC 2011 project. It should serve as a starting point for adding some of the other community detection code in that work.  I've looked over the code, but its been a while, any comments would be appreciated. 
issue
Adds parallel betweenness example#TITLE_END#Adds an example on how to calculate parallel betweenness. Addresses #585. 
issue
Error on undirected graph for strongly_connected_component functions.#TITLE_END#This modifies strongly_connected_components and its variants to only operate on directed graphs. This is more consistent with its definition. Also adds tests to ensure the correct assertions are raised. 
issue
Strongly Connected Components should return an error for undirected graphs#TITLE_END#Because strongly connected is only defined for directed graphs, when called on undirected graphs it should return an error. This behavior will be consistent with all weakly connected component algorithms. 
issue
Correctly initialize the distances in floyd_warshall_predecessor_distance.#TITLE_END#Report distance 0 for the distance from a node to itself. Also adds a test to be sure this is correct. 
issue
Waxman Graph Documentation Fix#TITLE_END#The waxman graph documentation was incorrectly documented. Changed the positive exponential to a negative exponential 
comment
What a blast from the past. I love that this code (or at least the idea of the code) lives on. 
comment
I know this has been up for a while, but I'll make a few comments if the authors are still interested in getting it incorporated.  The algorithm seems sound, though I haven't read the original paper in depth. I'll make a few inline comments on the code. I think this could probably be included in `networkx/algorithms/centrality/knotty.py` instead of just `algorithms`. Your test script looks good. It could be converted into the type of tests we have for other centrality measures, if you take a look in `networkx/algorithms/centrality/tests`. 
comment
It looks like you use `numpy` for `gml` reading and writing, but `networkx` has a `gml` reader. Would it be possible to include some simpler examples? 
comment
Thanks for adding this Moritz. Sorry I've been slow getting this integrated.  > - spectrum.py and laplacian_spectral.py are disorganized and seem to serve similar purposes. Can we reorganize it?  Yes we should. Adding spectral graph theory type algorithms to NetworkX turned out to be a bit more of a big deal than we anticipated. There was also some question about requiring numpy for this vs. providing iterative methods. I think the consensus we came to was that if one wants to do spectral graph theory, then it is a reasonable assumption that numpy is available. We can always raise an exception otherwise. The other alternative was to use scipy_sparse methods if possible, and reasonable.  > - similarly quality.py and community_quality.py isn't all the code community related?  Yes, these both contain community quality measures. I think there was a little bit of a fork between Aric and I's efforts, but I don't think there is overlap. We can simply move all the functions in `community_quality.py` to `quality.py`. Could probably do some renaming as well, e.g. `community_performance` could just be `performance` 
comment
@MridulS, About the same as it was 2.5 years ago?  #1035 added some of the community graph generators in December of 2013, but I haven't had much time to work on the other stuff.  @harrymvr initially we were avoiding the scipy.sparse dependency, or at least tried to implement it in such a way that we could use either. I am not sure what the current thoughts requiring scipy.sparse. My thought is if you want to do spectral graph theory, it's probably not unreasonable to assume that you have scipy.sparse installed. 
comment
I think in the end there was somewhat of a consensus to have communities represented as a list of sets. This works for most community detection algorithms, even those that allow for overlapping communities. It's fast, and makes a lot of the post community processing fairly easy. If you want to create dendograms for some sort of recursive partitioning, this usually requires converting the sets to 'frozensets' so their hashable.   The other option it a unique data structure for a community partition. I messed with this idea. It seemed a little overkill at first, especially with basic partitioning algorithms, but perhaps it's the way to go for more complex algorithms.  It looks like #1092 provides for a weighted partition? This one place where the list of sets structure isn't perfect, and maybe a unique class would be useful.  
comment
Overkill though it may be in simple algorithms, the more I think about it, the more I think it might be something to consider. We could attempt to create a class which appears and behaves like a `list` of `sets`, but actually has greater functionality.  Make indexing and iteration act like a list returning sets, have the `__repr__` appear as a list of sets, but then allow things like naming communities, allowing for weighted membership (with the `__repr__`) changing appropriately, and methods for quickly checking affiliation, subgraphs, creating affiliation graphs, and other functions. I [started on this at one point, but gave up in favor of the simpler solution](https://bitbucket.org/bedwards/networkx-community/src/5f88c6ae0db47fd3a5b4a52988093b448f083ca8/networkx/algorithms/community/with_big_class/communities_class.py?at=default)  I think there are maybe a few questions that we maybe haven't answered adequately. 1. How will users utilize community detection algorithms in NetworkX? If its common for people to just find communities and then report or analyze them statically with things like modularity then simpler is better. If there is going to be manipulation, storing of attributes, altering of the graph based on the community structure maybe a custom class is what we need 2. What will make it easiest for developers to contribute new community detection and general 'community/networkx' algorithms? A list of sets is a pretty easy concept, but perhaps an algorithm needs to quickly access one community. Accessing a specific community would be average O(N) with N being the number of communities, but if we did something more like a dictionary (which I did in the implementation above) this is O(1).   Deciding this would probably make our lives easier when the next pull request for a community detection algorithm comes along... 
comment
Just wanted to chime in. I am +1 for an iterable object that yields objects that support set operations. I would argue for `set` instead of `frozenset`.  Most algorithms will create communities iteratively, meaning that most would require the construction of a `set` only to have it recast to a `frozenset` before yielding. Though I suppose we could leave those details up to whoever writes the algorithm.  In some cases having hashable communities might be useful. If information about the communities is also provided by the algorithm, or if information is to be added later it would be useful to return a dict keyed by `frozenset` communities. But I think @hagberg 's suggestion to convert it to a dict of labeled communities (through a provided function or by the user) is straightforward.  
comment
One other option @hagberg and I considered when I first started the community work was a new class `Community`. This could have some of the built in functionality like affiliation representations. I messed with the idea at first but we decided it was probably a little more than we needed. A rough implementation can be found [here](https://bitbucket.org/bedwards/networkx-community/src/5f88c6ae0db47fd3a5b4a52988093b448f083ca8/networkx/algorithms/community/with_big_class/communities_class.py?at=default).  It built heavily on the `Graph` class as it was at the time. I am not necessarily advocating for this, but it is an option... 
comment
I messed with this a bit when the paper first came out. Note I haven't played with this code in quite a while so it may or may not work. Also no promises on whether this is an accurate reflection of what Lui et al did. I think I vaguely remember it working for some of the networks in the original paper, but really I am just copy and pasting.   One scholarly note [this paper](http://www.researchgate.net/profile/Baowen_Li/publication/51947433_Controllability_of_Complex_Networks_with_Nonlinear_Dynamics/links/0c960533c6fb7c6178000000.pdf) notes that "Interestingly, for one-dimensional nonlinear nodal dynamics, any bidirectional network system can be fully controlled by a single driver node, regardless of the network topology." Also if you consider transport networks where each node has a self loop under linear dynamics this is trivially controllable by a single 'control node'. I'd be curious about more current work on this.  I could potentially turn this into a pull request, but I probably won't have time to do it for a few weeks...  ``` python import networkx as nx  def to_bipartite(G):     """Converts Directed graph G to an undirected bipartite graph     H."""     if not G.is_directed():         raise nx.NetworkXError("G must be a directed Graph")     if G.is_multigraph():         H = nx.MultiGraph()     else:         H = nx.Graph()     for n in G:         H.add_node((n,'+'))         H.add_node((n,'-'))     for (u,v) in G.edges_iter():         H.add_edge((u,'+'),(v,'-'))      return H  def control_nodes(G):     H = to_bipartite(G)     M = nx.max_weight_matching(H,True)     matched = set(v for (v,sign) in M.values() if sign == '-') | \               set(u for (u,sign) in M.keys() if sign == '-')     n_D = set(G) - matched     if len(n_D) == 0:         return [G.nodes_iter().next()]     else:         return list(n_D)  def controllability(G):     return len(control_nodes(G))/float(len(G)) ``` 
comment
My summer is pretty busy, and I haven't done much on controllability since the initial paper(2011) came out. Even then, I was mostly just messing around.  Contributing is actually quite easy. If you haven't already check out the [Developer's Guide](http://networkx.github.io/documentation/networkx-1.9.1/developer/index.html). If you submit pull requests, I would be happy to review them. 
comment
@hagberg No problem! I'll try to paraphrase my comment. I am +1 for having alternative graph implementations in a separate module. I think this has organizational value, and we could provide the suggested (and partly implemented) big graph types in this library. NetworkX 2.0 might be a good time to do this. Additionally, at the same time we could think about moving all the drawing functions out as well. This would start a small, but hopefully useful `scikits` eco-system for networkx. I know the scipy guys have moved away from the idea of `scikits`, for example `statsmodels` no longer bears that name. 
comment
@argriffing, I would think we could have the packages live under the `networkx` github, but with different subsets of maintainers.  I think the consensus was that the separate namespace `scikits.whatever` was not the best implementation approach as python packaging continued to evolve. With `pip` it's somewhat easier to install packages, and providing a specialized namespace for `scikits` became unnecessary. So now we have `sklearn` etc. 
comment
This also fails one of the clique tests on `pypy`. This is strange as the test should sort the cliques and the order of the cliques... 
comment
I think the plan was to provide a power method version, a numpy version, and a sparse matrix version, then import based on what is available.  I think part of the consensus was if you are doing spectral graph theory, you like have numpy or scipy installed. So we could potentially make these modules require numpy and not provide a power method version.  Moreover, I don't think numpy is as big of a dependency hassle as it used to be, so I actually wouldn't be opposed to just making it a requirement for networkx, but that is a horse of a different color. 
comment
@jfinkels Awesome! 
comment
If we're desperate and @hagberg, @dschult, @jtorrents, or @chebee7i can't commit, I'd be willing to be a mentor. I'm probably not the best choice as I haven't been particularly active in the last few years. 
comment
What about keeping the implementations separate as @jtorrents suggests, but also provide a method (from networkx from the add-on?) to blow away the underlying implementation. Something like  ``` python import networkx as nx from networkx.external.addons import lemon  G = build_a_graph_with_appropiate_attrs() # Uses Networkx result_nx = nx.network_simplex(G, demand='demand', capacity='capacity', weight='weight') # Uses Lemon result_lemon = lemon.network_simplex(G, demand='demand', capacity='capacity', weight='weight')  nx.use_addon(lemon.some_function_or_data_structure) # Now the uses Lemon result = nx.network_simplex(G, demand='demand', capacity='capacity', weight='weight') ```  This might be more convenient if I have a large codebase and want to replace all instances of the networkx algorithm with some other add-on library.  I don't know how hard this would be, and how much machinery could be taken care of by networkx, and how much might need to be supplied by the add-on developer. 
comment
I agree with @Midnighter that silently replacing functions is not my preferred usage. Is it possible that `nx.use_addon(lemon.some_function_or_data_structure)` could replace the relevant functions, then reload networkx in such a way that we get the correct behavior in functions that use the replaced functions? Maybe monkey patching like that isn't a good idea... 
comment
I assumed that since it looks like we have more active/senior developers acting as mentors I did not register. Can you guys handle it from here on out? 
comment
@jtorrents I have access to Mathematica 10. I don't know a ton of Mathematica, but I tried your example out, and Mathematica doesn't look like it works for `k>2`. First the python  ``` python import networkx as nx def torrents_and_ferraro_graph(): #Copy and pasted ...  G = torrents_and_ferraro_graph() nx.write_graph6(G,'t_and_f.graph6') ```  What I tried and got in Mathematica  ``` Mathematica g = Import['t_and_f.graph6'] KVertexConnectedComponents[g,3] KVertexConnectedComponents::ngen: The generalized KVertexConnectedComponents[Graph[<99>, <200>], 3] is not implemented. >>�������������1�������CalculateScan`UnitScanner`Private`boxes�A�������•ê����p¢ï����•ê������������omponents/Iconiz@��� ```  For `k=2` it returns some output.  ``` Mathematica In[11]:= KVertexConnectedComponents[g,2] Out[11]= {{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99}}��� ```  If you have a different (maybe smaller?) graph I could try it on. 
comment
These results are interesting. I am a little blown away that the transition point doesn't seem to depend on the size of the graph, the version of python, or the host. One thing I am curious about is in the first figure it seems like `gnp_random_graph` is scaling rougly linearly in `p`, which I guess we should expect given that adding an edge probably takes longer than not adding an edge. `fast_gnp_random_graph` does not, which is strange because I feel like it _should_ scale linearly with `p`. Does `fast_gnp_random_graph` catch up for larger values of `p`?  Can you give a few more details? What are the two hosts: OS, Architecture, etc... 
comment
@jg-you I didn't notice the log scale, whoops. I see that it sits in the range [0.1,0.2], but I am surprised this is consistent with different values(orders of magnitude apparently) of N. Before seeing this I would have assumed that the switch point would vary depending on N.  @hagberg I think that `gnp_random_graph` would actually scale with `p`  in the function  ``` python ... for e in edges:     if random.random() < p:         G.add_edge(*e) return G ```  I am assuming the call to `G.add_edge(*e)` takes longer than not calling it. Large `p` values would mean more calls and longer running times.  
comment
It looks like this isn't skipping tests when numpy and scipy aren't available correctly. Not quite sure what is wrong though. Anyone else have ideas? 
comment
Looks like this is choking on `numpy` in version 3.3 and `pypy`, and `scipy` for version 2.6. This makes me think you need a `setup_module` function as is available in [algorithms/bipartite/spectral.py](https://github.com/networkx/networkx/blob/7d9682a07dcae30acab3c4841e33d31f727a3fb2/networkx/algorithms/bipartite/spectral.py).  
comment
I think this is a great idea. @hagberg might be being a little generous about the quantity and quality of work we got done, but I appreciate the sentiment.  I know the organization administrator for the PSF(Terri Oda), I can send and email and see what the status is for this year.  
comment
I'll get in touch. @ysitu can you send me an email address that I can give to Terri? You can send it to bjedwards (at) gmail. 
comment
@ysitu, nothing yet, checked spam. Can you try again? (Why is there no PM feature on github?) 
comment
Still nothing. Maybe it's my gmail, try bedwards (at) cs (dot) unm (dot) edu 
comment
@ysitu Sorry, still nothing. 
comment
Just sent the email. Hopefully we can get involved this year.  @ysitu let me know if I can help. I'm probably too removed from development at this point to be any use as a mentor, but having been on the student side of the project, I'd be happy to lend any advice. 
comment
I'm sort of with @hagberg on this one. This seems like a lot of cruft to add in for what seems like a single use case. NetworkX has tried to be as general as possible and not introduce constraints(like orderable node types), and I think trying to shoehorn that into a writer is a bad idea. I think there are also another option, subclass or rewrite the `GraphMLWrite` class as a custom writer.  @josch, if you know a priori the type of your nodes, rewriting `GraphMLWriter.add_nodes` and `GraphMLWriter.add_edges`, seems easy enough. It's the general case that is a pain in the ass.  Also can you turn of hash seed randomization programmatically? Or is it read at startup? 
comment
This looks awesome. My spectral graph theory is a bit fuzzy, but I think the Fielder vector should be defined for directed graphs. There is a `directed_laplacian_matrix` function. I don't know if `spectral_ordering` makes sense on directed graphs, and am assuming it doesn't make sense on `algebraic_connectivity`. 
comment
@ysitu, @argriffing, if I am remembering my Fan Chung right, the way we have the directed laplacian defined the article linked is correct in that:  > all eigenvalues of L are nonnegative, with 0 being a simple eigenvalue if the graph is strongly connected.  Even if the graph is not strongly connected, there are version of the laplacian (which I think we've defined correctly), which still have this property. I'll try to dig around and confirm this. Can anyone access Fan Chung's "The diameter and Laplacian eigenvalues of directed graphs? Every source I go to won't load for some reason, but that paper might have the answer... 
comment
@argriffing, @ysitu, it seems like you are right that the normalized Laplacian alters algebraic connectivity in a fundamental (though maybe not useless) way according to [this cstheory.stackexchange](http://cstheory.stackexchange.com/questions/5439/effect-of-different-graph-operations-at-algebraic-connectivity-of-graph-laplacia). Though I haven't dug through the literature they (sort of) cite.  [The paper @argriffing found](http://www.tandfonline.com/doi/abs/10.1080/03081080500054810) seems to indicate that if you define the Laplacian  as D-A, with D being the out degree, it is possible to calculate the algebraic connectivity of a directed graph, though it requires a bit more extra work, eg finding   > a Q is an n by n-1 matrix whose columns form an orthonormal basis of...[orthogonal complement to e = (1,...,1)^T]  I guess my conclusion is that this is not quite well defined enough to include here, ie that is you can totally ignore my suggestion that we try to do this for directed graphs. Maybe if there is a demand for it later, and someone can give it a go... 
comment
You should use:  ``` @require('numpy') def linalg_clustering ```  when using numpy or scipy. At least I think this is still standard practice.  
comment
Section 6.3 of Chung's _Spectral Graph Theory_ there is one example. When constructing a certain class of expander graphs she states:  > (Thus, e.g., (0,0) is joined to itself by 2 loops - note that here we consider that a loop adds 2 to the degree of a vertex)  One thing to consider is that if `weight=None`, we could consider the matrix returned to simply be an incident matrix, with only {0,1} indicating whether an edge exists. If `weight` is provided it should be the weight of the self loop. Then if no weight is provided, Chung's definition of Laplacian in Section 1.4 would be   L(u,u) = d_u - 2*(# of self loops)  with d_u = degree(u). 
comment
This looks like it is equivalent to [Hierholzer's algorithm](http://en.wikipedia.org/wiki/Eulerian_path#Hierholzer.27s_algorithm)? If so you could find and add that reference back into the documentation.  
comment
This looks interesting. I read this paper not too long ago. They define a number of dispersion measures including some odd normalizations like (disp(u,v) +b)^alpha)/(emb(u,v) + c) for various values of (a,b,c), and a 'recursive' dispersion measure. Would it be worth it to include these? 
comment
@dschult They use a few different distance measures including: 1. Your definition 2. d(s,t) = 1 if they are within graph distance r, where r=3 seems to give good results, and 0 otherwise 3. d(s,t) = 1 if s and t are in different connected components in  G_u -{u,v}, 0 otherwise 4. d(s,t) =1 if they are in different communities using the Louvain community detection method (implemented in NetworkX!), 0 otherwise 5. Use NetworkX spring layout method on G_u and then use the Euclidean distance (again using NetworkX)  I am a little surprised they used NetworkX, even on small graphs, python might be a little slow if they are using really large facebook data sets. I wonder how much of this work was done in NetworkX, perhaps they already have it implemented, maybe we should contact them. 
comment
@hustonhedinger This is looking pretty good. Looking over it again, I have a few other comments. I might be being a bit nit picky, so if anyone else has input... 1. I wonder if a small refactor might be helpful. Since dispersion is naturally defined between two nodes (u,v) should we treat it like we do shortest path? So define it as         ``` python    def dispersion(G, u=None, v=None, ...):    ```        and     1. If both `u` and `v` are undefined return 'all pairs'    2. If one of `u` or `v` is a valid node, return dispersion for whichever node to the rest    3. If both are defined just return the dispersion of `u` and `v` 2. I think the return should be  ``` python if normalized:     if embededness:         dispersion[v] = total / float(embededness) # otherwise we might have integer division problems     else:         dispersion[v] = total     else:         dispersion[v] = total ```  If the user is calling it with `normalized=True`, I am not sure I see a reason to label the output. 1. I wouldn't mind the inclusion of `alpha`, `b`, and `c`. If you make the defaults, `1.0`, `0.0`, and `0.0`, respectively and change the normalization to   ``` python if embededness + c != 0:     dispersion[v] = ((total + b)**alpha)/(embededness + c) else:     dispersion[v] = (total+b)**alpha ```  you'll get the same value as you currently have. This also allows users to more accurately recreate the results in the paper. Also using `float` defaults means you don't have to cast total or `embededness`.  1. Change the name to just `dispersion`. That's what they use in the paper, and I don't think there are any conflicts in the code base. This way if we want to `recursive_dispersion` later we can. 2. Why include a `exclude_nodes` keyword? Did they do this in the paper, I can't recall. If a user wants to exclude nodes they could easily use `subgraph`. 3. I wonder if we could pre-emptively make this a bit easier to add different distance functions, like take a `distance_function` keyword argument, that defaults to what you have now. Thinking about that might require a serious refactor, so maybe that's not the best plan. 4. One more comment about unpacking tuples, inline in the code.  Sorry this ended up being so long, I do think this is pretty great and want it in the code base.  EDIT: Sorry for the wierd formatting I github markup defies my best efforts sometimes. 
comment
@hustonhedinger   > however did not refactor for distance_function... We can do this down the road easily, but for now, unless there are multiple options, it just looks messy...  I agree. I think it would require constructing `G_u` via subgraph, which could slow things down substantially. If there is a demand for it at a later date I suppose it could be done. Looking over the paper, it seems that this definition predicts relationships very well (when normalized). Let's stick with it.  Two inline comments, and this.  Updated docstring (with LaTeX!) untested, but should be right:  ``` python """ An implementation of 'dispersion' as defined by Lars Backstrom and Jon Kleinberg [1]_.      Where a link between two actors ('u' and 'v') has a high dispersion when their mutual      ties ('s' and 't') are not well connected with each other.      .. math::        $disp(u,v) = \sum_{s,t \in C_{u,v}} d_v(s,t)$      where `C_{u,v}` is the set of common neighbors of `u` and `v` in the subgraph `G_u`      induced by `u` and the neighbors of `u`, and `d_v(s,t)` is a distance measure between     `s` and `t`. In this implementation `d_v(s,t)` is equal to 1 when `s` and `t` are not directly      linked and also have no common neighbors in `G_u` other than `u` and `v`, and equal to      0 otherwise.      Parameters     ----------     G : graph       A NetworkX graph     u : node, optional, default : None       the source node for the dispersion score (e.g. ego node of the network)     v : node, optional, default : None       the target node for the dispersion score if specified     normalized : bool, optional, default : True       If True normalize using the formula        ..math::        $norm(u, v) = \frac{disp(u,v) + b)^alpha}{emb(u,v) + c)}$        where `emb(u,v)` is the embededness of `u` and `v`.     alpha : float, optional, default : 1.0       Normalization constant     b : float, optional, default : 0.0       Normalization constant     c : float, optional, default : 0.0       Normalization constant      Returns     -------     dispersion : dictionary        If u or v is specified, returns a dictionary of nodes with dispersion score         for all "target" nodes        If neither u or v is specified, returns a dictionary of dictionaries for all nodes         'u' in the graph with a dispersion score for each node 'v'.        If both `u` and `v` are specified returns a float.      Notes     -----     When not specifying 'u' or 'v' on larger networks can take some time, worst case O(n^3).     Typical usage would be to run dispersion on ego network (G_u) with u specified       References     ----------     .. [1] Romantic Partnerships and the Dispersion of Social Ties:         A Network Analysis of Relationship Status on Facebook.         Lars Backstrom, Jon Kleinberg.         http://arxiv.org/pdf/1310.6753v1.pdf """ ``` 
comment
Yikes, one more thing, we'll need some tests. Check any of the tests folders for ways to construct them. Some simple ones might be: 1. Make sure it returns the correct size dictionary, or float in the case of a single value. 2. Make a small test graph and make sure they appropriate values are returned (by hand). Figure 2 in the paper has a small graph that should be easy. 3. Test to make sure values are valid (ie > 0). Maybe generate a small connected random graph and test the return values of that. 
comment
Did you add a test file? That and the python 2.6 fix should just about do it. 
comment
@hagberg, this looks ready to merge. I think I have commit rights on this repo, but haven't done so in a while. I know you do most of the merging, but I would be happy to start for stuff I shepherd through like this. 
comment
I think @hagberg has this fixed in #1070. Just needs to be merged. 
comment
Looks like this failed because the test for `planted_partition_graph` uses the this function, and uses the `seed=42` for testing. Should be able to just update the number of edges in `test_planted_partition_graph` from 197 to 218. 
comment
I think you are correct. I think the more pythonic fix might be:  ``` python if p1==p2:     probs.append(0) else:     d = sum((abs(b-a) for a,b in zip(p1,p2)))     if d <= p:         G.add_edge(p1,p2)     probs.append(d**-r) ```  Can you make a pull request?  The description in the original article potentially indicates that self-loops should be present on all nodes:  > For a universal constant `p`, the node `u` has a directed edge to every other node within lattice distance `p` (local contacts) .  Distance 0 would seem to fall under this definition. 
comment
> However, self-loops cannot be present in the network because random long-range connections are chosen with probability proportional to the lattice distance d raised to some negative power. d would be zero for a self-loop such that only self loops would exist in the network if they were allowed.  Correct, I was thinking self-loops would qualify as _local_ links, not long distance ones. 
comment
Why would we do this only for undirected graphs? It would make sense to me to either return distance 0 for directed graphs as well, or the length of the shortest path back to the node, either way there seems to be a bug. 
comment
Looks like the code currently does:  ``` python for (u,v,d) in G.edges(data=True): ... dist[u][u] = 0 ... ```  So potentially assigning dist[u][u] multiple times but never assigning dist[v][v].  Making a pull request... 
comment
After reading what I could find in the docs on`pylab.sci()`, I am still unsure exactly what it does, but in the uses I've seen don't seem to relate to ours. Are we sure this isn't going to introduce some odd subtle bug down the road? 
