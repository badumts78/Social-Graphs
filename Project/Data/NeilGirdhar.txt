issue
Add type annotations for Graph and DiGraph#TITLE_END#See #3988.
issue
Please consider adding type annotations to networkx#TITLE_END#I'm still using this wonderful library!  It would be really nice to have type annotations, which help with linting and discovering bugs early, and act as documentation.  Something like: ```python Node = TypeVar('Node')   class Graph(Generic[Node]):     def add_edge(self, u_of_edge: Node, v_of_edge: Node, **attr: Any) -> None: ``` and so on would be really nice.  `attr` could be nicely annotated if https://github.com/python/mypy/issues/4441 is ever implemented.
issue
NodesDataView is not always a set, but promises that it is#TITLE_END#I was just looking at filling in some annotations on the typeshed, and I ran into a consistency issue with some definitions.  `NodesDataView` is only a set when its `data` element is false.  Otherwise, it violates the set interface.  This would be very confusing if it were used as a set in generic code.  I suggest making a copy of this class: `NodesDataViewSet`, which covers the special case when it is a set.  The `NodesDataView` can then make a more conservative promise of being a `Container`.  You can share code between these two classes by having a common private base class.
issue
Mixins incompatible with Graph#TITLE_END#Using a Mixin with `Graph` is currently perilous: ```python class Specialized(Graph, Mixin): ...  s = Specialized()  # Mixin.__init__ is not called!! ```
issue
Fix multiple inheritance with Graph#TITLE_END#* Make Graph.__init__ call super in case it's used in multiple   inheritance * Add Graph._create_attributes to factor out common behavior in   Graph.__init__ and DiGraph.__init__  Fixes #5865  This is a preliminary pull request in an effort to simplify the giant #4014 to make the review simpler.
issue
Remove numpy matrix#TITLE_END#networkx crashes when run with `warnings.simplefilter("error")` because numpy emits:  > PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.  `numpy.matrix` should not be used anymore.
issue
When are we dropping Python 3.6 support?#TITLE_END#If networkx is part of the "Scientific Python ecosystem", how does it stand on [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html)? > This NEP recommends that all projects across the Scientific Python ecosystem adopt a common â€œtime window-basedâ€ policy for support of Python and NumPy versions. Standardizing a recommendation for project support of minimum Python and NumPy versions will improve downstream project planning.  They argue that projects should drop Python 3.6 support on their next release after June 23, 2020, among other things.  Thoughts?
issue
Add support for Python 3.9 when dependencies are ready#TITLE_END#Just an issue to track progress of support Python 3.9, which was released yesterday.
issue
Remove usage of deprecated np.int#TITLE_END#np.int is just int--it is not a numpy type.
issue
Removed internal use of deprecated np.matrix type.#TITLE_END#See #3107.
issue
Updated migration guide and release notes.#TITLE_END#Updated migration guide and release notes for topological sort changes.
issue
topolgical_sort, lexicographical_topological_sort#TITLE_END#Rewrote topolgical_sort as a generator: - It no longer accepts reverse or nbunch arguments - It is slightly faster.  Added lexicographical_topological_sort, which accepts a key. Some tests for topological sort were removed because they relied on CPython's particular dict element ordering.  Added utils from more_itertools: - These were necessary for working with the new generators. - "pairwise" is equavalent to zip(some_list, some_list[1:]) -- for   generators. - "consume" consumes an iterator. 
issue
Use dict comprehensions#TITLE_END#Now that networkx has dropped Python 2.6, it should be safe and faster to use dict comprehensions instead of applying the dict constructor to generators.  networkx/algorithms/approximation/kcomponents.py:        return dict((node, all_edge_dict) for node in networkx/algorithms/approximation/tests/test_kcomponents.py:    labels = dict((v, k) for k, v in rlabels.items()) networkx/algorithms/approximation/tests/test_kcomponents.py:    labels = dict((v, k) for k, v in rlabels.items()) networkx/algorithms/connectivity/tests/test_kcutsets.py:    labels = dict((v, k) for k, v in rlabels.items()) networkx/algorithms/connectivity/tests/test_kcutsets.py:    labels = dict((v, k) for k, v in rlabels.items()) networkx/algorithms/core.py:    node_pos = dict((v,pos) for pos,v in enumerate(nodes)) networkx/algorithms/flow/tests/test_maxflow.py:    excess = dict((u, 0) for u in flowDict) networkx/algorithms/flow/tests/test_maxflow_large_graph.py:    excess = dict((u, 0) for u in flow_dict) networkx/algorithms/flow/utils.py:        flow_dict[u] = dict((v, 0) for v in G[u]) networkx/algorithms/shortest_paths/weighted.py:    d = dict((u, inf) for u in G) networkx/algorithms/traversal/breadth_first_search.py:    return dict((t,s) for s,t in bfs_edges(G,source)) networkx/algorithms/traversal/depth_first_search.py:    return dict((t,s) for s,t in dfs_edges(G,source=source)) networkx/drawing/nx_pylab.py:        labels = dict((n, n) for n in G.nodes()) networkx/readwrite/graphml.py:        data=dict((k,v) for (k,v) in  G.graph.items() networkx/relabel.py:                               dict((v,k) for k,v in mapping.items())) 
issue
Allow indexing using Ellipsis#TITLE_END#Sometimes, I need to update a shared variable having dynamic shape.  I do this by calling:  ``` function([], [], updates=[(x, inc(x, tensor.inc_subtensor(x[index], 1.0)))]) ```  When the shape of `x` is a vector like `(4,)`, then `index == slice(None)` updates the whole vector.  Using `index == ()` gives a crazy exception (that should probably be fixed).  When the shape of `x` is a scalar like `()`, then `index == ()` works fine whereas using `slice(None)` gives the reasonable error:  > ValueError: The index list is longer (size 1) than the number of dimensions of the tensor(namely 0). You are asking for a dimension of the tensor that does not exist! You might need to use dimshuffle to add extra dimension to your tensor.  In numpy, I can deal with both cases by letting `index = Ellipsis` (or `...`).  Unfortunately, theano doesn't accept it, telling me:  > /opt/local/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/tensor/basic.py in as_tensor_variable(x, name, ndim) >     206         except Exception: >     207             str_x = repr(x) > --> 208         raise AsTensorError("Cannot convert %s to TensorType" % str_x, type(x)) >     209 >     210 # this has a different name, because _as_tensor_variable is the >  > AsTensorError: ('Cannot convert Ellipsis to TensorType', <class 'ellipsis'>) 
issue
Make successors consistent with edges#TITLE_END#For multigraphs, `successors` coalesces non-unique target nodes, whereas `edges` reports non-unique edges separately.  I think they they should be consistent, and they both should report the multiple copies separately.  ``` In [22]: import networkx as nx  In [23]: a = nx.MultiDiGraph()  In [24]: a.add_edges_from([(1,2), (1,2)])  In [25]: a.successors(1) Out[25]: [2]  In [26]: a.edges(1) Out[26]: [(1, 2), (1, 2)] ```  I think returning each of the edge copies separately is better than changing `edges` to coalesce them.  If the user asks for the data on the edges, then the edges returned will have different pieces of data, so you won't be able to coalesce them. 
issue
topolgical_sort, lexicographical_topological_sort#TITLE_END#Rewrote topolgical_sort as a generator: - It no longer accepts reverse or nbunch arguments - It is slightly faster.  Added lexicographical_topological_sort, which accepts a key.  Some tests for topological sort were removed because they relied on CPython's particular dict element ordering.  Added utils from more_itertools: - These were necessary for working with the new generators. - "pairwise" is equavalent to zip(some_list, some_list[1:]) -- for generators. - "consume" consumes an iterator. 
issue
Make topological_sort stable when the graph exploration order is specified.#TITLE_END#Fix topological_sort so that it is stable if "nbunch" â€” the graph exploration order â€” is provided.  This is done by using nbunch both when exploring top-level nodes and when recursing. 
comment
> I spent some time looking at this and couldn't find any obvious, non-invasive way to make mypy happy so I went with the solution in https://github.com/networkx/networkx/pull/5127 and adde  I annotated them here: https://github.com/neilgirdhar/networkx/tree/annotations  ```python from typing import MutableMapping  MapFactory = Callable[[], MutableMapping]  class Graph:     node_dict_factory: ClassVar[MapFactory] = dict ```  I understand that the current design of using callable class variables may have been chosen as an optimization.  However, the real problem is this is awkward design.  And setting those class variables as member variablesâ€”with the same nameâ€”in the constructor is extremely awkward, and should be removed if possible.  The ordinary way to accomplish this is with ordinary methods that subclasses can overload.  The use of callable member variables seems to me like over-optimization, but perhaps there are examples where it makes a significant computational speed difference.  If so, those examples should at least be added to testing to verify that they are still relevant in modern Python versions.  ```python from typing import Hashable, MutableMapping  class Graph:     def node_dict_factory(self) -> MutableMapping[Hashable, Any]:         return dict() ```
comment
Could you add ``` # MyPy .mypy_cache ``` to .gitignore?
comment
> The reason I went with a mypy-specific configuration is that the pyproject.toml is a more general config file that deals with far more options than just mypy.  The reason I suggest pyproject.toml is that you will likely ultimately end up with a pyproject.toml file, and so you may as well just start with it now.  mypy.ini is antiquated: It has its own peculiar syntax.  The toml file has one syntax that's shared for all configurations.  Even if you don't end up using pyproject.toml to replace setup.py for a couple years, you can use it to configure other things too, like pylint, pytest, and isort.  > Yes, probably fine as is. I'd probably have written it as...  That's what I would have done too, FWIW. ðŸ˜„ 
comment
> I know from a few years ago that there are implications to adding a pyproject file.   That looks like a build system issue with an old build system.  Nowadays, poetry is quickly becoming the standard in the Python community.  I'm pretty sure poetry supports such isolation.  > It may be best to add the pyproject.toml in a different PR that also handles our build dependencies etc. though.  I think it's better to use pyproject.toml as a catch-all configuration file, and preferring it to having each tool using its own configuration file.  Doing this now just prevents churn.
comment
> Also, the mypy.ini is 3 lines long, so no big issue to port it later :)  Yeah, but I think it should probably be something more like ```toml [tool.mypy] warn_redundant_casts = true warn_unused_ignores = true  [[tool.mypy.overrides]] module = ['scipy', 'scipy.sparse', 'scipy.optimize', 'scipy.linalg', 'scipy.sparse.linalg',        'pandas', 'osgeo', 'pygraphviz', 'pydot', 'ogr', 'lxml', 'lxml.etree', 'PIL',        'matplotlib', 'matplotlib.pyplot', 'matplotlib', 'matplotlib.collections',        'matplotlib.colors', 'matplotlib.patches', 'matplotlib.path', 'matplotlib.cm', 'scipy.stats',        'scipy.linalg.blas', 'scipy.spatial', 'scipy.special'] ignore_missing_imports = true ``` That way ignore missing imports is restricted to projects that don't have annotations, and errors otherwise.  Also, warn unused ignores is going to be really useful going forward as mypy is developed.  As far as I know, there's no succinct way to do this with mypy.ini
comment
This looks amazing so far.  Any progress on getting this pushed?
comment
Woohoo!  Congrats and thank you for finishing this up!
comment
We shouldn't be taking a seed parameter at all.  We should be accepting an instance of `random.Random` or if None using `random`.  This would allow running simulation for example with a random number generator and getting repeatable results.  You don't want to have to come up with seeds for every time the rng is used.  You want to seed it once and pass it to every function that needs an rng. 
comment
@jeffzemla The problem with passing in seeds is that you might have many other pieces of code that use random numbers.  Are you supposed to seed all of those random number generators at the start of your simulation?  When you want to restore the simulation to a particular point, you need to save all, say 5, seeds?  That is really poor design.  That's the reason that all of the scipy and numpy functions that take random numbers accept a random number generator.  It is the de facto Python way of introducing randomness, and in my opinion, it's good design.  Also, a seed limits you as to what kinds of random number generators you can use.  Not all random number generators have a state that can be described using a single number. 
comment
@jeffzemla  Instead of passing the rng, you generate a seed from an rng, which you then use to initialize another rng?  That is very roundabout way of just passing the rng.  I don't undersand your second point.  What is hard about reading code that passes an rng?  The order of calls will always matter even in your system of creating seeds â€” the order in which you create them changes them and changes things.  I personally think this is a poor design. 
comment
By the way, if you want to inspect or serialize a rng, you can always call its `get_state` member. 
comment
@jeffzemla It is one extra indirection without value.  If you want a magic number in your code (why you want that I have no idea), you can still do: `rng=RandomState(1234)`.  If I've ever had a fixed seed, it was usually in exactly one place in the code, rather than mixed in everywhere, which is what you're suggesting. 
comment
@dschult Passing an object that exposes the RandomState interface is the de facto standard set by numpy and scipy.  Why would networkx depart from that approach?  Instead, networkx should be passing the rng to functions so that they can be chained together, e.g.  ``` from numpy.random import RandomState from networkx import uniform_random_intersection_graph, gn_graph  rng = RandomState(123) x = gn_graph(100, rng=rng) y = gn_graph(200, rng=rng) i = uniform_random_intersection_graph(n, m, p, rng=rng) ```  Doing that with seeds is very convoluted: you end up needing a seed for each operation, which initializes a new `RandomState` three times (yuck).  Also, it makes networkx hardcode the random number generation algorithm, which is unjustified.
comment
@hagberg  Right.  But at the end of that issue, is just a pointer back here.  Am I missing something?  What was wrong with @chebee7i 's suggestion?
comment
@dschult Oh!  I see it now.  I looked at the code too quickly.  Sorry if my message came off badly.  Your code looks great.
comment
If you don't mind, just to make it clear for me, I'll sum up my understanding:   `create_np_random_state` would be used internally to turn an argument into a `np.RandomState`.  (It is currently called `create_random_state`.)  A new `create_random_state` would be added to create an object with `random`'s interface.  Users would continue to pass in either `random` objects or `np.RandomState` objects depending on the function.  If this is right, then I think that's pretty good design.  The only question is whether to just make everything use `np.RandomState` or try to keep some functions working with `random` to keep users from having to pull in `numpy`.   Let's call these two ways of doing things "single generator type" and "two generator types".  I understand the desire not to depend on numpy.  However, it is often the cases that someone will want to pass the *same* random number generator to all functions, so if you go with the "two generator types" system, then it would be nice to provide an adapter so that an `np.RandomState` can be used as a `random` generator in those functions that expect a `random` generator.  It should be possible, but if it's not then I suggest you go with the "single generator type" system.
comment
One reason to want to use a single generator is when doing any kind of simulation.  You often want repeatable running of that simulation.  You might also want to save the state and reload in.  In the two generator version, you would actually need to save and reload two generators in order to accomplish this.  I like client code to be as simple as possible, so just keeping one generator is just simpler.  Another argument against using `random` is that using it means that you have to have possibly expensive `random.setstate` and `random.getstate` calls bracketing all use of `random`.  That's going to complicate networkx's code.  I see that's not being done [here](https://github.com/networkx/networkx/blob/d565cec4a29c56ab28406d676d9734ef52a869d4/networkx/generators/duplication.py) for example.  That means a user that wants repeatable results might be forced to do that in his code.
comment
Here's my proposal: Keep using `create_random_state`, but in those methods that only need the simpler `random` interface, pass in a flag like `simpler_interface`.  Then `create_random_state` would create something based off of `random` for them:  ``` import random from numbers import Integer  # All methods clobber random's state.  Use np.random instead to preserve state. class SimpleRandomState:      from random import (         random as random_sample,         uniform,         triangular,         betavariate as beta,         expovariate as exponential,         gammavariate as gamma,         gauss as normal,         lognormvariate as lognormal,         vonmisesvariate as vonmises,         paretovariate as pareto,         weibullvariate as weibull,         shuffle)      def randint(low, high=None):         if high is None:             return random.randint(0, low + 1)         return random.randint(low, high + 1)      def choice(self, a, size=None, replace=True, p=None):         if (size is None or size == 1) and p is None:             return random.choice(a)         if replace:             return random.choices(a, weights=p, k=size)         if p is not None:             raise ValueError         return random.sample(a, k=size)   _simple_random_state = SimpleRandomState()   def create_random_state(random_state=None, simpler_interface=False):     """     If random_state is True, returns either a     * numpy.random.RandomState instance, or      * a SimpleRandomState instance.     Otherwise, returns a numpy.random.RandomState instance.      Parameters     ----------     random_state : int or RandomState instance or None  optional (default=None)         If int, `random_state` is the seed used by the random number generator,         if numpy.random.RandomState instance, `random_state` is the random         number generator,         if None, the random number generator is the RandomState instance used         by numpy.random (if simpler_interface is false) or random (otherwise).     """     if simpler_interface:         if isinstance(random_state, Integer):             random.seed(random_state)             return _simple_random_state         if random_state is None:             return _simple_random_state     try:         import numpy as np     except ImportError as e:         if simpler_interface:             msg = '%r is not an integer'             raise ValueError(msg % random_state)         else:             raise ValueError("numpy is not available.") from e     if random_state is None or random_state is np.random:         return np.random.mtrand._rand     if isinstance(random_state, np.random.RandomState):         return random_state     if isinstance(random_state, Integer):         return np.random.RandomState(random_state)     msg = '%r cannot be used to generate a numpy.random.RandomState instance'     raise ValueError(msg % random_state)  ```  Now, in code like [this](https://github.com/networkx/networkx/blob/2b04833ed070513c72c98592b22e277928226627/networkx/algorithms/centrality/betweenness.py#L197), you can replace: ``` random.seed(seed) nodes = random.sample(G.nodes(), k) ``` with ``` rng = create_random_state(random_state, True) nodes = rng.choice(G.nodes(), size=k, replace=False) ```  This * supports passing in a `numpy.RandomState` as well as an integer seed (called `random_state`), * doesn't depend on `numpy`, and * is more explicit. 
comment
> I'm digging into this again and it looks like the simpler_interface argument is essentially making two create_random_state functions. By that I mean that the methods available from np.mtrand._rand and random._inst differ. So if you want to use the random interface for your code you call with the simpler_interface set. If you want the numpy interface you use the default simpler_interface. It's generally not possible to allow the user to pick the RNG so we're back to using two RNGs.  I think you misunderstood.  The `simpler_interface` argument supports passing a numpy interface.  Please look again.  Even though the methods in `random` and `RandomState` differ, the class `SimpleRandomState` creates a facade so that most of the `RandomState` interface is exposed using functions in `random`.  This means that all code can be written using numpy's `RandomState` interface.  Code that does not need to depend on numpy would just use a subset of that interface, and it would not require numpy if the `simpler_interface` flag is passed.  > I'm leaning toward your example's approach of allowing contributors to use wither of two RNGs (random and numpy) and users will need to supply state for both   That's unnecessary.
comment
I think the misunderstanding might be because there is no `else` clause to `if simpler_interface:`.  If the caller doesn't pass `None` or an `Integer` for `random_state`, then it _falls through_ and checks to see if you passed in a `RandomState`.  In any of those cases, it returns some object that implements enough of the `RandomState` interface to work with the calling function.  This is why it's not â€œessentially two `create_random_state` functionsâ€.  It is one function where the two cases share some behavior.  I definitely should have updated the docstring, which I have now done.  > The returned value is different and can't be used interchangeably. So they are effectively two separate functions.  The returned value can be used interchangeably because they implement the same interface.  Also, even if you call it with `simpler_interface=True`, you don't always get the `random` package.   You can still pass in a `numpy` `RandomState`.  > Also, we won't need lots of getstate and setstate with the random package.   It's the user code that might want to preserve the state so that execution is consistent.  The way things are written now, it is overly complicated to accomplish that.
comment
> The RandomState returned does NOT have the same interface. That's the problem we are facing.  That's fixed by using a facade like I did in `SimpleRandomState`.  > One example is that randint(1,3) for [`random`] has 3 possible outcomes while for [`RandomState`] it only has 2.  Good catch!  That's fixed in my code now.    > Maybe I need to look at the APIs more closely, but I believe that the numpy versions returned arrays of values while random do not.  The numpy versions return scalars when size is none.  > We are essentially creating a 3rd random number generator API (some sort of lowest common API between the two.  Yes, you're on the right track there, but you only need to make one look like the otherâ€”as I have with `SimpleRandomState`.
comment
> But it's not fixed. The facade either has to implement the entire API or we end up with some 3rd API that is a least-common-api approach. For this example facade, we lose the ability to use the size to return an array of values. So now -- to make our code work for either RNG, we must have size=None. Every contributor that uses numpy.RandomState will not be able to use the size argument.  It is fixed.  You only ever call `create_random_state` with `simpler_interface=True` in those cases where you rely only on the `random` interface.  You don't need the `size` parameter in those cases.  Contributors that rely on the full `RandomState` interface will not set `simpler_interface` to true, and they can therefore use the full interface.  > Merging those two goals seems to mean that a "seed" becomes two integers -- one for each package.  This is poor design.  I feel like my solution might be clarified by looking at a particular method?  If you would like, why don't you pick one that you think would be particularly problematic and we can work through how this works for each object type passed as `random_state` (`NoneType`, `int`, `RandomState`)?
comment
> Are you saying that any code that "relies only on the random interface" has to use the "simpler_interface"? Those two interfaces are different. You are saying that contributors can't use the random interface.  The simpler interface does everything that the `random` interface does.  You might need to slightly adapt your algorithms.  I gave an example of the kind of necessary changes underneath my proposal.  > I'm confused. How is this helping? It seems like any contributor of code has to use the numpy interface which the simpler_interface then translates if needed. We might as well require all contributors to use the numpy interface. I want contributors to be able to use whatever package they prefer...  Why?  - The numpy interface is just as easy to use as the random interface. - The simpler interface mechanism means that code that only relies on what the `random` interface provides does not depend on `numpy` even though the code uses the `numpy` _interface_.  What is the benefit of letting contributors write things using the particular quirky interface that `random` went with?  Even in your suggestion of having a â€œ3rd random number generator APIâ€, you would be deviating from the `random` interface.  > [By the way -- thanks for sticking with me on this. I'm not agreeing yet but I feel that it is helping and we're making progress even if it does take a bit for me to catch on.]  No worries, my pleasure.  > I think your proposal is to have cases 3) & 4) rewrite the code to use the simpler_interface and case 2) isn't supported.  Yes, exactly.  Note that case 2 and case 3 are currently unsupported.  > I'm hoping this: allows contributors to use what they want while allowing users to use RandomState for everything if they want to. Users would not be able to use random for everything.  Yes, this alternate proposal is definitely reasonable.  My instinct was to go the other way because I like the numpy interface better.  So the big advantage of your proposal is that existing and future `networkx` code that uses the `random` interface does not need to be adapted to use the `numpy` interface.  What I like about my proposal is that it means that if there are any algorithms that could use the simpler interface, but were written to use the `numpy` interfaceâ€”their requirements can be downgraded by a simple flag.  It also means that there is consistency throughout the library: everyone is using the same interface.  The confusion of having `randint` work differently depending on which interface you used is definitely worth avoiding.  It can be very easy for someone who asks for the wrong interface to have a subtle bug.  _Both of our proposals_ have the benefit that if someone wants consistent execution, they can accomplish it by initializing a single `RandomState` object and passing it to every function that wants a random number generator.  I hope we can agree that is really good design.  I know that you spend a lot of time working on `networkx`, but there are many more users than library-writers.  I believe that the user's life should be made as easy as possible.
comment
Totally agree.  You're right that if that last bullet point is too much work, we should go with your proposal.
comment
The docstring looks good.  I think if you want to keep `seed` for backwards compatibility, you should deprecate it, call `warnings.warn` and add `random_state` alongside it.  [Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/deprecation.py#L262) is the nice solution `tensorflow` used with their deprecated parameters.
comment
I agree with most of the motivations and the initial comments, but please don't accept `seed`.  Accept `random.random` or `numpy.RandomState`.  This allows you to plug in any random number generator with the same interface.  In your code, you can default to using `random.random` or `numpy`'s default `RandomState`. 
comment
This might become a bigger issue with the next release since `attr_dict` has been removed in a lot of places.  I suggest using more descriptive names than `n`, `u`, and `v`.  E.g., `node_name`. 
comment
Not to make this longer, but looking through other networkx code, I noticed a lot of `dict(zip(`, which is almost always unpythonic.  E.g.,   | Original | Should beâ€¦ | | --- | --- | | `dict(zip(range(m+1),range(m+1)))` | `{x: x for x in range(m + 1)}` | | `dict(zip(range(0,lena),[0]*lena))` | `{x: 0 for x in range(lena)}` | | `dict(zip(nodelist,map(float,expA.diagonal())))` | `{x: float(y) for x, y in zip(nodelist, expA.diagonal()}` | | `dict(zip(ordering,range(n)))` | `{x: i for i, x in enumerate(ordering)}` | | `dict(zip([n for d,n in dv_pairs], range(first_label, N)))` | `{x: i for i, x in enumerate((n for d, n in dv_pairs), start=first_label)}` |  Some are fine, e.g.  `pos = dict(zip(G, pos))` (looks ok to me) 
comment
Any luck with this? 
comment
Fair enough. 
comment
I agree with Joshua that there should be one way to do things.  I just want to point out that calling `G.edges` is different than say `G[u]` with multigraphs.  Multiple edges show up multiple times in `edges` (even if they are the same), whereas multiple target nodes show up once in `G[u]` (when are they are the same).  If possible, I think `successors` should return multiple target nodes if there are multiple edges to be consistent.  I added bug: https://github.com/networkx/networkx/issues/1757 
comment
Isn't `has_predecessors(u)` equivalent to `in_degree(u)==0`? 
comment
Good point.  I'm not against adding convenience methods :) 
comment
As a user, I am +1 on returning "iterables" (â€”not just iterators of course!) 
comment
One of the difficulties with generators as opposed to returning lists is that a caller might accidentally change the graph used in the call while they are processing the iterator.  It might be nice to raise a `GraphChangedError` in that case.  For example, I reimplemented `topological_sort` as an generator.  I could intercept `KeyError` and replace it with `GraphChangedError`.  Also, it's possible for the a user changing the graph while the generator is being processed to fool the algorithm into thinking that there's a cycle, which is unfortunate. 
comment
@dschult good point.  It's unlikely anyone will want to catch this error, so `RuntimeError` should be best.  ``` In [1]: a = {1:2, 3:4}  In [2]: for x in a:    ...:     del a[x]    ...: --------------------------------------------------------------------------- RuntimeError                              Traceback (most recent call last) <ipython-input-2-7b84a6b1aa7f> in <module>() ----> 1 for x in a:       2     del a[x]       3  RuntimeError: dictionary changed size during iteration ``` 
comment
In my change, I added `pairwise` and `consume` from `more_itertools` to `utils.misc`.  I noticed that some of the iterable code in the branch is using `len(list(x))`.  It would be better, I think, to use `more_itertools.ilen` and not creating the list. 
comment
It looks like that function sometimes returns a random 14 bit integer?  Do you need it to guarantee uniqueness? 
comment
Right, I agree with your reasoning.  I think I just misunderstood the documentation.  I thought it returned 14-bit numbers.  It looks like you get at least 64-bit numbers, which are essentially unique as you wanted. 
comment
You can write this as `(x for x, y in bfs_edges(G, reverse=True))` :) 
comment
However, there is a lot of cleanup possible in the file `breadth_first_search` in terms of optimizing the functions `bfs_successors` and `bfs_predecessors`. 
comment
You're right.  My guess is `bfs_edges` should be changed to output pairs `(node, children)`.  This would make `bfs_successors` faster, and solve your use case.  You can pass `bfs_edges` through `itertools.groupby` for now if you want a simple implementation of your iterator. 
comment
It's not up to me :) 
comment
@harlowja Check this out: https://github.com/networkx/networkx/pull/1693  Looks like they are working on this general problem (but not your specific problem). 
comment
They are making all methods return iterators, so I guess they ultimately want bfs_predecessors to also return an iterator.  I think it wouldn't hurt for you to write the patch for that file.  There are no significant changes to it in their patch so far. 
comment
I am also +1 on six.  I noticed that too when reading the code. 
comment
@hagberg I agree with your point about minimizing code churn.  However, the "hacks" in six are [extremely well-tested](http://pypi-ranking.info/alltime).   
comment
I have to disagree that the changes in https://github.com/networkx/networkx/pull/1709 are "no simpler or clearer".  At the very least they are 30 lines fewer.  I agree with you that we should "be thinking in Python3", which means that things like `basestring` and `long` and `unicode` should not appear in the code. 
comment
The lines of six don't count in my opinion.  They don't count as sources of potential bugs because they have been tested by millions of users.  And they don't count as contributing to complication because they are common between the many projects that use six and so as users we only have to learn them once.  I don't like dependencies either, but `six` might as well be a standard library given how much it's used.  Anyway, like you all, I am looking forward to day that python 2 is dead :) 
comment
I never said we should ignore python 2.  I said that we should use six because it's standard and that I agreed with Aric that we should be thinking in Python 3.  I was just trying to close my comment with a conciliatory remark. 
comment
I've already written this, but I'm too busy with work right now to push it.  https://github.com/networkx/networkx/pull/1662  I'll try tomorrow if you can help me with the rebase. 
comment
oh, you don't have to do it for me.  I just want to know how to do it so that I know for next time :) 
comment
Ok, I'll try that now.  If you don't mind, I'll post any problems here. 
comment
(and thanks) 
comment
okay, I need to find an irc clientâ€¦ 
comment
@dschult  I think this is safe to close. 
