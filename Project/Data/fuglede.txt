issue
Update documentation of minimum weight full matching#TITLE_END#In #4057 , we removed the requirement that the input for `minimum_weight_full_matching` be a complete bipartite graph. This simply updates the documentation of the function accordingly. In the process, we include a test showcasing the possible `ValueError` that will arise if the input graph admits no full matching.
issue
Fix a few typos in documentation of implementations of maximum matching#TITLE_END#The documentation currently refers to "mate", which is not defined anywhere. It could seem like that's simply an earlier name for the output directory, and we update the documentation accordingly.
issue
Remove completeness condition from minimum weight full matching#TITLE_END#When first implementing `minimum_weight_full_matching`, we were still only requiring a SciPy version of 1.1.0, which meant that we had to require that the bipartite graph was complete. This requirement was removed in SciPy 1.4, and now (with 8d8aa86eede1bccc160af73e823492357196b341) that NetworkX specifies a minimum SciPy requirement of 1.4, the requirement can be removed here as well.  The only gotcha is that we can no longer use `biadjacency_matrix(...).toarray()`, which would leave unexisting edges with a value of 0, when what we need is actually `np.inf`; we handle this by explicitly creating an all-`np.inf` array, then populate it with the weights of the edges we do have.
issue
Fix documentation issues for exceptions in a few places#TITLE_END#This takes care of a couple of function in which the documentation of raised exceptions had gone bad. For instance, in [`flow.gomory_hu_tree`](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.flow.gomory_hu_tree.html#networkx.algorithms.flow.gomory_hu_tree), the documentation currently looks as follows:  ![Screenshot from 2019-11-09 20-10-49](https://user-images.githubusercontent.com/6169306/68533763-0f1a1280-032d-11ea-8669-82847feee4e6.png)  We take care of all cases where this particular issue showed up; that is,  ``` grep -r --include \*.py ": Exception" ```  outputs nothing after the fix.
issue
Fix a typo in docstring for get_edge_data#TITLE_END#This closes #3550.
issue
Fix a few documentation issues for the bipartite algorithm reference#TITLE_END#In https://github.com/networkx/networkx/pull/3527 we added `nx.algorithms.bipartite.minimum_weight_full_matching`; this introduced a few issues in the reference documentation which we fix here:   - The function was missing from the general overview.  - The documentation itself had some rendering issues due to backslashes    not being properly escaped.  - The reference to Karp was not formatted properly.
issue
Implement minimum weight full matching of bipartite graphs#TITLE_END#This provides an implementation of the minimum weight full matching problem, also known as the linear assignment problem, for complete bipartite graphs by deferring the calculation of the assignment to `scipy.optimize.linear_sum_assignment`.  This closes #3520; note that there, I referred to the matchings as "perfect"; since we also support bipartite graphs in which the two partitions have different cardinalities, "perfect" becomes a bit of a misnomer, so here we use term "full" instead; this follows the terminology used in Karp's *An algorithm to Solve the m x n Assignment Problem in Expected Time O(mn log n)* (doi:10.1002/net.3230100205).  I've marked the PR as "WIP" mainly because this is my first contribution, so I'm sure it has some rough edges. All suggestions for improvements are taken positively. One thing I can see myself:  * As mentioned in the code comments, there's an improvement coming to SciPy 1.4.0 that simplifies the solution here. NetworkX currently requires only 1.1.0; is there a way to ensure that the constraint is removed once 1.4.0 becomes an extra requirement? One could add a check on the SciPy version number but that seems hacky and unrobust. * I added myself as a contributor. I don't know what sort of precedence exists in terms of the size of contribution, but I'll remove it again if this is too awkward.
issue
Minimum weight perfect bipartite matching#TITLE_END#I'm wondering if there would be appetite for an algorithm to solve the problem of finding minimum weight perfect bipartite matchings in bipartite graphs: Let *G* = ((*U*, *V*), *E*) be a weighted bipartite graph. We then want to find a perfect matching, i.e. one of cardinality min(|*U*|, |*V*|), so that the sum over all weights in the matching is minimal. This is also known as the [linear assignment problem](https://en.wikipedia.org/wiki/Assignment_problem).  There are algorithms for solving several related problems in NetworkX already, but I didn't seem to be able to find one specialized to this problem. Some related algorithms are the following:  * The linear assignment problem is a special case of a minimum cost flow problem, obtained by adding a source incident to all vertices in *U* and a sink incident to all vertices in *V*, and letting the demand for these two vertices be min(|*U*|, |*V*|) with appropriate signs. This could then be solved using `networkx.algorithms.flow.min_cost_flow`. A specialized algorithm would take advantage of the bipartite structure to improve performance. Moreover, as far as I can tell, `min_cost_flow` assumes that all weights are integral, which we wouldn't have to here. * On the other hand, `networkx.algorithms.bipartite.maximum_matching` can detect if a feasible solution exists, i.e. a matching with the desired cardinality, but it doesn't take into account the weights. * `networkx.algorithms.matching.max_weight_matching` takes weights into account but operates on arbitrary graphs and does not require that the matching be perfect in the bipartite case.  A large number of different algorithms exist to solve the problem. Assuming that the inclusion of an algorithm is relevant, the first thing I'm wondering is the following: SciPy already has an efficient implementation of an algorithm to solve the problem in [`scipy.optimize.linear_sum_assignment`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html), and since NetworkX already depends on SciPy, it would be only a few lines of code to simply make use of the SciPy implementation. However, as NetworkX is generally focused on bundling Python implementations of its algorithms, simply making use of the SciPy implementation might be going against the spirit of the library, and it would be preferable to include a Python version of one of the algorithms instead?
comment
Some immediate thoughts, skimming through the references you provided, and with the caveats that I don't have an overview of the history, and that I can't really talk as an expert on either of NetworkX and SciPy:  * How far would you want it to go? If something like `SparseGraph` existed, would the idea be that it plugs in directly to all existing algorithms? That seems like it'd be a mouthful of a task, and with the algorithms being pure Python, performance benefits are likely limited, so that's probably not what you want. For the algorithms I've worked with, what would suffice would be 1) convert the appropriate `Graph` type to a sparse matrix (usually `csr_matrix`), 2) run the desired algorithm on that using the relevant `sparse.csgraph` method, 3) do all the book-keeping to convert results back to something meaning for the original graph; no additional types needed, just a bit of adapter code. A concrete example of this is [`bipartite.matching.minimum_weight_full_matching`](https://github.com/networkx/networkx/blob/19cd86c254ebf4a889fddc171ba166187fb72890/networkx/algorithms/bipartite/matching.py#L495), which does essentially nothing but defer calculation to `scipy.optimize.linear_sum_assignment`. As a bit of an aside, [there's currently a PR](https://github.com/scipy/scipy/pull/12541) for adding a solver of the same problem to `sparse.csgraph`, and once that's in place, it would be natural to use that in NetworkX as well. So that's a kind of boring "don't do anything" answer. I'm sure you have use cases that suggest doing more than that? One use case the above approach does not cater to is what happens when you want to do something more online like "run the algorithm, change a few weights in a graph, run the algorithm again, change a bit in the graph, rinse and repeat". In those cases, the adapter code could end up being a bottleneck, and having more fine-grained control over the data structure with something like `SparseGraph` would solve the problem (ignoring that many problems come with specialized online algorithms that you'd use instead if you _really_ want performance). What I'd normally do myself in such a situation, though, is just look at what's going on under the hood and remove unnecessary adapter code by hand. To me, the power of NetworkX being pure-Python is that diving in is always a breeze.  * So indeed, talking as a user, I've often enjoyed the low barrier-to-entry with NetworkX being pure-Python. Yet I remember being surprised myself that the de factor Python package for graph analysis had only a limited focus on performance, when that's often what I was looking for myself. In [one of the threads](https://github.com/networkx/networkx/issues/4089#issuecomment-662051098), you talk about porting things to SciPy. It would be a shame if that means removing good pure-Python implementations, but there should be room for more ports/contributions to `csgraph`, and in particular, there'd probably be room for Cythonizing some of the more common algorithms from NetworkX and making them available here through thin wrappers. But, they're careful about scope creep: Citing a post by @rgommers (who might want to weigh in on the discussion here as well?) on the [SciPy-Dev mailing list last year](https://mail.python.org/pipermail/scipy-dev/2019-July/023601.html):  > We [SciPy] indeed don't want to do everything that NetworkX already does, however if it's a well-known algorithm that would fit in with what we already have and focus on performance (which NetworkX does not do) then I think there's scope for more.  * Somewhat more concretely, the [discussion on the handling of explicit zeros](https://github.com/networkx/networkx/pull/1080#issuecomment-38421519) can turn into a bit of a pain point: In general, SciPy provides limited guarantees that an explicit zero is not removed during various transformations of sparse representations, and even just converting from one representation to another can cause an explicit zero to vanish. That's an issue when you're talking about zero-weight edges which tend to be the exact opposite of missing edges. In practice, this means worrying about implementation details.
comment
(Same caveats above regarding my own knowledge of the libraries, while at the same time, I'm probably saying stuff you already know:)  Regarding speeding up pure-Python algorithms in particular, your typical `csgraph`-object is just a `scipy.sparse.csr_matrix`, nothing more, and nothing less, and there's only so much you can do with such a thing. That is, what you really have is an adjacency list that's well-represented in memory, and where it really shines is in any operation where you do things like "for _v_ in some subset of your nodes: for all neighbours of _v_". That's of course enormously common, but only really helpful if the indexing is the bottleneck of your algorithm. Now as it turns out, once you're done tweaking the performance of any C/C++/Cython network algorithm, at some point those lookups will become the main performance bottleneck, so the `csr_matrix` is indeed useful in general. But at the same time, if the lookups are just one part of a Python implementation of some algorithm and if that's all you improve the performance of, I'd perhaps be a bit surprised if that gave you any huge relative benefits in general. So that would suggest that the fast converters would be the more useful. But I'm not sure; do you have an example of an algorithm where most of the work boils down to looping over adjacency lists?
