issue
Is the closeness centrality being normalised correctly?#TITLE_END#The code to compute the [closeness centrality](https://networkx.github.io/documentation/development/reference/generated/networkx.algorithms.centrality.closeness_centrality.html#networkx.algorithms.centrality.closeness_centrality) of a node `n` [is given by](https://github.com/networkx/networkx/blob/master/networkx/algorithms/centrality/closeness.py)  ``` sp = path_length(G,n)  # all shortest paths from n to all other nodes. totsp = sum(sp.values())  # the sum of all shortest paths. if totsp > 0.0 and len(G) > 1:     closeness_centrality[n] = (len(sp)-1.0) / totsp     # normalize to number of nodes-1 in connected part     if normalized:         s = (len(sp)-1.0) / ( len(G) - 1 )  ## FOCUS HERE         closeness_centrality[n] *= s else:     closeness_centrality[n] = 0.0 ```  I am struggling to understand why, in the normalized case,  ``` closeness_centrality[n] = (len(sp)-1.0)*(len(sp)-1.0)/( len(G) - 1 ) / totsp ```  Shouldn't the normalized case be given by   ``` closeness_centrality[n] = ( len(G) - 1 )/ totsp  ```  So it corresponds to the formula given in the documentation?  (corresponds changing `closeness_centrality[n] *= s` to `closeness_centrality[n] /= s` in the source). I can PR this.  
