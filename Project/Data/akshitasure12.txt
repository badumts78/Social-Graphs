issue
Add Lexicographic Breadth-First Search#TITLE_END#As discussed in last week's meeting, we were considering adding Lexicographic Breadth-First Search (Lex-BFS). I just wanted to confirm — are we still planning to include this functionality? If yes, I’d be happy to work on the implementation.
issue
Use `pytest.raises` as a context#TITLE_END#This PR ensures that exceptions are raised inside the `pytest.raises` block, so that pytest can catch them instead of the exception being raised *before* the test starts monitoring for it.  ### Motive:  In nx-parallel, the `_apply_prediction` function is run across multiple cores. Unlike NetworkX’s standard implementation, we cannot return a generator expression because generators are not picklable and cannot be transferred between processes — that is, we cannot return the generator from the child process back to the parent process. To handle this, `_apply_prediction` returns a fully consumed chunk instead of a generator expression.  In the test below, if `self.func` raises an exception, it means the exception occurs when the function is called, not when the result is consumed: ```py G = nx.complete_graph(5) pytest.raises(nx.NetworkXAlgorithmError, list, self.func(G, [(0, 1)])) ``` This can cause the test to fail to catch the exception correctly because pytest starts monitoring only when `list()` tries to consume the generator. By wrapping the entire call in a with `pytest.raises` block, we ensure that pytest is already watching for the exception when the function is called, not just when its output is consumed.
issue
Optimise harmonic centrality#TITLE_END#In the current implementation of harmonic centrality, we compute `nbunch.intersection(dist)` for every source node. This creates a temporary set at each iteration, which is unnecessary and slightly inefficient since we can achieve the same logic with a conditional check during iteration over `dist.items()`.  ### What I changed I replaced: ```py for v in sources:         dist = spl(v)         for u in nbunch.intersection(dist):             d = dist[u]             if d == 0:  # handle u == v and edges with 0 weight                 continue             centrality[v if transposed else u] += 1 / d ``` with: ```py for v in sources:         dist = spl(v)         for u, d_uv in dist.items():             if u in nbunch and d_uv != 0:                 centrality[v if transposed else u] += 1 / d_uv ```  ### Benchmarking results I ran the following benchmark command to compare performance: ```sh asv compare f750ce5b a9371f78 ``` [EDIT]:  The most notable performance improvement is observed for `wheel_graph(100)` but other benchmarks also show modest improvements.  | Change   | Before [f750ce5b] <main>   | After [a9371f78] <hc>   | Ratio   | Benchmark (Parameter)                                                                                                                                                                    | |----------|----------------------------|-------------------------|---------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | -        | 1.76±0.06ms                | 1.58±0.01ms             |    0.89 | benchmark_harmonic_centrality.HarmonicCentralityBenchmarks.time_harmonic_centrality('wheel_graph(100)') [Akshita-MacBook-Pro.local/virtualenv-py3.12-numpy-pandas-scipy] | |          | 2.63±0.1ms                 | 2.41±0ms                  | 0.92    | benchmark_harmonic_centrality.HarmonicCentralityBenchmarks.time_harmonic_centrality('directed_wheel(100)') [py3.12]                   | |          | 795±5μs                    | 768±2μs                   | 0.97    | benchmark_harmonic_centrality.HarmonicCentralityBenchmarks.time_harmonic_centrality_node_subset('directed_wheel(1000)') [py3.12]     | |          | 70.3±0.6μs                 | 65.6±0.08μs               | 0.93    | benchmark_harmonic_centrality.HarmonicCentralityBenchmarks.time_harmonic_centrality_node_subset('wheel_graph(100)') [py3.12]         | |          | 19.3±0.2μs                 | 17.9±0.04μs               | 0.93    | benchmark_harmonic_centrality.HarmonicCentralityBenchmarks.time_harmonic_centrality_single_node('wheel_graph(100)') [py3.12]         | |          | 149±0.9μs                  | 142±0.2μs                 | 0.95    | benchmark_harmonic_centrality.HarmonicCentralityBenchmarks.time_harmonic_centrality_single_node('wheel_graph(1000)') [py3.12]        | |        |            ...             |            ...            |   ...   | ...
issue
Added the function perfect_elimination_ordering.py along with necessa…#TITLE_END#This PR adds a function that handles perfect elimination ordering.  ## Changes :  - A new function perfect_elimination_ordering to the `networkx.algorithms.chordal` module. - Respective unit tests under test_perfect_elimination_ordering in `tests/tests_chordal.py`  ## Related Issues :  Handling issues #7052, #6873 
issue
optimise `is_reachable()`#TITLE_END#As per the discussion in the recent networkx community meeting, these are the major changes introduced in this PR:  - converted the networkx graph object to a numpy-based matrix with the help of `nx.to_numpy_array()`. - replaced the use of `is_path()` with direct matrix-based checks to identify two-hop neighbors. - mapped node labels to integer indices and used index-based access throughout the function.  [edit] : Heatmap for reference ![image](https://github.com/user-attachments/assets/95eb32c4-fffd-41a0-a3d8-e59657546246)  
issue
Refactored the working of chordless_cycles to handle self loops#TITLE_END#This PR addresses an issue in the `chordless_cycles` function where multigraphs with self-loops were not being correctly handled. Added test cases under `test_chordless_cycles_multigraph_self_loops` that verify : 1. Self-loops as single-node chordless cycles. 2. Parallel edges in a multigraph as two-node chordless cycles.  ``` def test_chordless_cycles_multigraph_self_loops(self):         G = nx.MultiGraph([(1, 1), (2, 2), (1, 2), (1, 2)])         expected = [[1], [2]]           self.check_cycle_algorithm(G, expected, chordless=True)                  G.add_edges_from([(2, 3), (3, 4), (3, 4), (1, 3)])         expected = [[1], [2], (3, 4)]          self.check_cycle_algorithm(G, expected, chordless=True) ```  Changes made include :  1. Interpretation of self-loops to be chordless cycles, except in multigraphs with multiple loops in parallel. 2. Removing self loop nodes from being a further part of chordless cycles of length greater than 1.  Addresing issue #7867  
issue
Fixed the return type from an empty dict to an empty set#TITLE_END#Addressing issues #7909   This PR involves the suggested changes :  1. fix the typo {} -> set() 2. Adding documentation text under the Returns section about when an empty set is returned.  Additional Changes :  1. Fix test assertion `tests_minimum_st_node_cut()` by using set() instead of {} for empty set comparison.  
issue
Correcting the example given under subgraph_is_monomorphic.py#TITLE_END#This pull request resolves a documentation issue in the NetworkX isomorphism module, where the functionality of `subgraph_is_monomorphic` was incorrectly described as checking for subgraph isomorphism.  Current Documentation : The current example incorrectly implies that `subgraph_is_monomorphic` is used to check for isomorphism:  ```python Check whether a subgraph of H is isomorphic to G:          >>> isomatcher = nx.isomorphism.GraphMatcher(H, G)         >>> isomatcher.subgraph_is_monomorphic()         True ```  Expected Documentation Update : The example should describe the functionality accurately as checking for monomorphism:  ``` python Check whether a subgraph of H is monomorphic to G:          >>> isomatcher = nx.isomorphism.GraphMatcher(H, G)         >>> isomatcher.subgraph_is_monomorphic()         True ```
issue
Added a note to the contributor guideline to avoid numpy scalars as a…#TITLE_END#Added a note in `contributing.rst` under the guidelines section to avoid using numpy scalars as a return type.  ## Related Issues :  Fixes #7349 
issue
Corrected 2 documentation errors in CONTRIBUTING.rst#TITLE_END#Issue1 : Fixed the Package Not Found error in the installation of main development and runtime dependencies. Updated the document to address the issue where changelist==0.5 was not available in the current channels.  Issue2 : Added missing command for installation of pre-commit.  Addressed the command not found error while executing pre-commit install. 
comment
Hey @rossbar, @dschult is this issue available to work on? 
comment
From what I understood from the discussion, the most effective approach would be to iterate through F itself after removing all self-loops. This is what I'm planning to implement—let me know if this approach is viable.
comment
@dschult We can also modify the `DegreeView.__getitem__()` to raise an exception when checking for missing nodes.   ```python if n not in self._graph:         raise NetworkXError(f"Node {n} not found in the graph") ``` Simultaneously, we could make an additional change by raising an error in the else statement of `nbunch_iter`. If no one has taken up this issue, I'm willing to make the changes!
