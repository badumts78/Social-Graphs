issue
Remove matplotlib drawing to separate package#TITLE_END#Maybe it violates the _batteries included_ philosophy - but how about removing the nx_pylab.py code to a separate package?  Maybe it will get more attention that way or people would be inspired to write a better version?  I'd suggest keeping it at github networkx/networkx-matplotlib so then it could be _batteries included but contained in separate packaging_.  Comments? 
issue
Numpy array printing changes causing test failures#TITLE_END#Here is the info on the changes to the Numpy array printer format.  This change is showing up as errors in the Travis builds for 2.7 pip='--pre'.  https://github.com/numpy/numpy/blob/master/doc/release/1.14.0-notes.rst#many-changes-to-array-printing-disableable-with-the-new-legacy-printing-mode  The errors are in the doctests for `convert_matrix.py` and `attrmatrix.py` 
issue
Add clustering coefficient measures for directed graphs#TITLE_END#See previous PR #1051. I suggest a pure python implementation (previous used numpy/scipy). 
issue
max_weight_matching returns set of edges#TITLE_END#Change return of max_weight_matching to be set of edges instead of dict of "mates". Required changes in covering.py to adjust for this change. The set of edges now does not return duplicates (e.g. only u,v and not v,u).  Fixes #1918 
issue
Avoid keyword and attribute clash#TITLE_END#Avoid keyword/attribute clash for nodes: n edges: u,v  Fixes #1582
issue
Add threshold option to spring layout#TITLE_END#Fixes #2449
issue
The graph6 generator fails when relabeling#TITLE_END#When relabeling for consecutive nodes 0,..,n-1 use the new (relabeled) graph to test for edges.
issue
NetworkX Jython compability#TITLE_END#We are not officially supporting Jython.  But it might be nice to try to fix these small things so that it can be installed. http://stackoverflow.com/questions/30302622/installing-networkx-1-9-1-on-jython-2-7-0/31438974#31438974 
issue
str formatting of views#TITLE_END#I prefer seing  ```python >>> print(G.edges()) [(1, 2), (2, 3)]) ``` over ```python >>> print(G.edges()) EdgeView([(1, 2), (2, 3)]) ``` as the current 2.0 views show.  One goal of of ``__str__`` is to make a user-readable representation of the data and the user probably doesn't care much that this is an EdgeView.   If they do then they can use ``__repr__`` right?  Opinions?  Does that change break anything?  It seems more consistent with networkx-1. output  
issue
Fixes 2 bugs in dominance frontier code#TITLE_END#1) Should not discard source u. Fixes #2071 2) Ignore unreachable entry poitns. Fixes #2070 
issue
Generator rename#TITLE_END#Rename generator.py to community_generator.py  Fixes #2087  
issue
Remove encoded ... to plain ascii#TITLE_END#Replace encode elipses with asci ... in file wih no encoding declared. 
issue
Should multigraph keys use uuid.uuid1()?#TITLE_END#Currently we are providing sequential integer keys for multigraphs when adding an edge with an unspecified key.  Should we switch to using uuid to get unique keys?  This will solve some issues when working with more than one multigraph like #1619. 
issue
Generate non-isomorphic trees#TITLE_END#As part of the paper  http://link.springer.com/article/10.1007/s10910-013-0294-9 Michael Schutte coded the WROM algorithm to generate non-isomorphic trees. It would be a good inclusion: https://gist.github.com/hagberg/7979081 
issue
networkx-2.0#TITLE_END#We are due for a networkx-2.0 release so let's start aiming for that.    The current list of open issues and pull requests labeled with the networkx-2.0 milestone: https://github.com/networkx/networkx/milestones/networkx-2.0  Let's see if we can get these resolved in the next few weeks by adding code, fixing bugs, making technical design decisions etc.  If some are in appropriately tagged for networkx-2.0 let's defer them for https://github.com/networkx/networkx/milestones/networkx-2.1.  If there are some tagged with https://github.com/networkx/networkx/milestones/networkx-future and you want to address now please do so.  Many of the issues and pull requests are very close to finished and just need some small fixes or review.  Open issues and pull requests - https://github.com/networkx/networkx/issues?q=is%3Aopen+is%3Aissue+milestone%3Anetworkx-2.0 - https://github.com/networkx/networkx/pulls?q=is%3Aopen+is%3Apr+milestone%3Anetworkx-2.0 
issue
Need updated web front page.#TITLE_END#Now that readthedocs holds our docs and hopefully soon tutorials and examples in notebooks... A more modern web front door would be great.  Any takers.  See https://github.com/networkx/networkx-website/issues/9 
issue
Travis with Python3.6 miniconda fails#TITLE_END#Fails with a gdal conflict error. Can a conda expert help us figure out how to fix this?   ``` UnsatisfiableError: The following specifications were found to be in conflict:    - gdal -> libnetcdf 4.3.2 -> curl 7.38.0 -> openssl 1.0.1*   - gdal -> numpy 1.9* -> python 2.7*   - python 3.6*  Use "conda info <package>" to see the dependencies for each package. CondaEnvironmentNotFoundError: Could not find environment: test-environment . You can list all discoverable environments with `conda info --envs`. ```
issue
Fix test ordering issues for python3.6#TITLE_END#The new dictionary implementation in Python3.6 exposes some test issues.  In test where ordering of the dictionaries is assumed the tests may/will fail.  The types of issues most common are  - Nonunique answer (e.g. shortest path on a grid) - Need to use assert_edges_equal or assert_nodes_equal to handle ordering differences  - Test isomorphic istead of equal, e.g. we can't guarantee roundrip of networkx->scipy sparse -> networkx with the same labels with our current setup.   In order to do that we'd need to store networkx labels with the scipy sparse data.  Some test items were commented out and need inspection   - non-unique chain test  - kcomponents test depends on order
issue
Test ordering#TITLE_END#Addresses some of #2203 
issue
Add bipartite readers and writers#TITLE_END#Add bipartite edge readers and writers and biadjacency matrix reader.   See #1293. 
issue
Document integer-only numeric mixing#TITLE_END#The numeric mixing and assortativity algorithms only work for integers. They could be implemented to work for floating point numbers too.  Fixes #1981  
issue
Convert examples (or create new ones) for Jupyter/IPython notebooks#TITLE_END#Notebooks now render in github so this is a great time to create some for NetworkX http://blog.jupyter.org/2015/05/07/rendering-notebooks-on-github/  The tutorial and examples would be better as notebooks. 
issue
spring layout breaks with single node graph#TITLE_END#This corner case (drawing 1 node) breaks spring layout and shouldn't  ``` python import networkx as nx   G=nx.Graph()   G.add_node("spam")   nx.draw_networkx(G, node_size = 2000, node_color = "white", pos=nx.spring_layout(G)) ```  http://stackoverflow.com/questions/36612991/how-to-draw-graph-having-single-node-in-networkx 
issue
Fix bug in dtype-valued matrices#TITLE_END#2 bugs: 1) to_numpy_matrix didn't respect integer types (and possilbly others) since a floating value np.nan is added to mark non-edges 2) to_pandas_dataframe missing dtype and order keywords 
issue
Remove add_path, add_cycle, add_star#TITLE_END#Remove from base classes and consider putting in as functions along with add_clique? #1562  
issue
networkx-1.11 release#TITLE_END#I put networkx-1.11rc1 at pypi https://pypi.python.org/pypi/networkx/1.11rc1  If it looks OK let's release the final version ASAP. Comment here on working/not working and fixes welcome. 
issue
Remove sorting from assert_nodes_equal, assert_edges_equal#TITLE_END#Removes sorting requirement for nodes and edges in test for comparison.  This way builds dictionaries and compares the dictionaries.  Possibly not the most efficient way to do it but correct? Needs review @dschult?  Addresses #1911  
issue
graphviz_layout() selection#TITLE_END#The graphviz_layout() algorithm selection code doesn't work right. In the case below I have graphviz_layout() provided by nx_agraph and not one of the pydot packges. The pydot package loader doesn't bomb on ImportError until run time.  ``` python In [1]: import networkx as nx  In [2]: G = nx.path_graph(4)  In [3]: from networkx import graphviz_layout  In [4]: pos = graphviz_layout(G) --------------------------------------------------------------------------- ImportError                               Traceback (most recent call last) <ipython-input-4-29bbca8a9cf2> in <module>() ----> 1 pos = graphviz_layout(G)  /home/aric/.local/lib/python2.7/site-packages/networkx-2.0.dev_20150613115110-py2.7.egg/networkx/drawing/nx_pydot.pyc in graphviz_layout(G, prog, root, **kwds)     255     This is a wrapper for pydot_layout.     256     """ --> 257     return pydot_layout(G=G,prog=prog,root=root,**kwds)     258      259   /home/aric/.local/lib/python2.7/site-packages/networkx-2.0.dev_20150613115110-py2.7.egg/networkx/drawing/nx_pydot.pyc in pydot_layout(G, prog, root, **kwds)     269     >>> pos=nx.pydot_layout(G,prog='dot')     270     """ --> 271     pydot = load_pydot()     272      273     P=to_pydot(G)  /home/aric/.local/lib/python2.7/site-packages/networkx-2.0.dev_20150613115110-py2.7.egg/networkx/drawing/nx_pydot.pyc in load_pydot()      45     else:      46         msg = "pydot could not be loaded: http://code.google.com/p/pydot/" ---> 47         raise ImportError(msg)      48       49     return module  ImportError: pydot could not be loaded: http://code.google.com/p/pydot/  In [5]: from networkx import nx_agraph  In [6]: pos = nx_agraph.graphviz_layout(G) ``` 
issue
Don't use import * on nx_agraph and nx_pydot#TITLE_END#This means to use the functions in either module you will need to say e.g. networkx.drawing.nx_agraph.write_dot() or from networkx.drawing.nx_agraph import write_dot; write_dot() 
issue
Add random kernel graph generator#TITLE_END#This adds a generator of random graphs with a specified kernel.   Random graphs based on kernels are designed to produce  graphs with the number of edges scaling linearly with the number of nodes. They are especially interesting because many graph properties can be analyzed.  See http://arxiv.org/abs/math/0504589  This algorithm is a fast way to sample random instances from this model.  A simple test and example to generate a G(n,p) graph using a kernel are provided.  The method is detailed in the (currently unpublished) paper, 'Fast generation of random kernel graphs', Aric Hagberg and Nathan Lemons, 2015.  I'll update the reference with (pre)publication info when available. 
issue
iter_refactor branch review#TITLE_END#Let's make one more review of the iter_refactor branch before we merge. Look especially for documentation that didn't get updated to reflect the fact that we are returning iterators for (most) methods that formerly returned containers. 
issue
Doc fixes#TITLE_END#Fix documentation typos, formatting errors, spelling, etc. 
issue
Don't copy subgraphs in cuthill_mckee_ordering().#TITLE_END#The connected_component_subgraphs() function used in the cuthill_mckee_ordering() makes a copy.  That causes new node objects to be created which might not be desirable when returning a node ordering if the hash changes when a new object is created.   The fix is to not copy nodes when forming a subgraph of connected nodes.  This fixes a bug identified in the stackoverflow question  http://stackoverflow.com/questions/15451289/python-networkx-cannot-use-current-flow-betweenness-centrality-function/  Example that shows the problem:  ``` python import networkx as nx  g=nx.Graph() a=object() b=object() c=object() d=object() g.add_edge(a,b,{'distance': 4.0}) g.add_edge(a,c,{'distance': 1.5}) g.add_edge(b,c,{'distance': 2.2}) g.add_edge(c,d,{'distance': 2.6}) result = nx.current_flow_betweenness_centrality(g, weight='distance') ``` 
issue
fix eigenvalue example#TITLE_END#Needed several changes to work with newer naming. 
issue
Only rescale if fixed is None#TITLE_END#For the Fruchterman-Reingold layout only rescale positions on return and if only if no nodes are requested to have fixed positions.  This might cause some algorithm performance issues if the spring physics generates large or small numbers?  Addresses #1228. 
issue
fix waxman_graph domain#TITLE_END#bug in domain specification   fixes #1302  
issue
change encoding in expanders.py to utf-8#TITLE_END#need encoding for utf-8 text 
issue
Test failure on random graph (singular matrix)#TITLE_END#Mildly annoying test failure when the gnp graph used in `test_eigenvector_v_katz_random` unluckily produces a singular matrix for katz centrality: https://travis-ci.org/networkx/networkx/jobs/40577402 
issue
Bipartite reader writer#TITLE_END#Addresses #1294. 
issue
Simplify Davis Club example#TITLE_END#Keep track of ordering in davis club and show how to use one-mode projections. 
issue
network-1.9.1 release#TITLE_END#Previously discussed at #1223 Needs release notes. 
issue
Pydot fix#TITLE_END#Addresses #923  I think this is really a bug in pydot and probably should be fixed there. 
issue
Fix bug in average weighted connectivity for when using in-degree#TITLE_END#Fixes #627. Add test. 
issue
Provide non-minimized d3 javascript or consider removing #TITLE_END#These files are considered binary blobs and non-free by debian.  So they get removed in debian packaging - examples/javascript/force/d3 - doc/source/static/force/d3js  We could either put in real source code instead of minimized versions or maybe just remove them and point to the d3 github site for the source. 
issue
Closeness centrality doc updates#TITLE_END#Addresses #824 
issue
Weight keyword#TITLE_END#Add weight keyword to Katz centrality and eigenvector centrality measures to allow optional specification of no weights (weight=None) or edge attribute string (weight=attributestring)  Fixes #920 
issue
Drop official Python3.1 support#TITLE_END#But pretty much everything will work OK.  Some small issues with xml.etree are annoying to support specially for Python3.1.  Addresses #974 
issue
Cuthill-Mckee - ordering heuristic#TITLE_END#Deprecate start= option to Cuthill-Mckee algorithms and provide option for specifying heuristic.  A common (the default) heuristic is to choose a node from a "pseudo-peripheral pair".  But now you can use whatever heuristic you like such as minimum degree node (example provided).  Update docs. 
issue
Pajek utf8#TITLE_END#Workaround for shlex.split for reading non-ascii encoded data with Python2.x. 
issue
Force double precision in scipy matrix for layout#TITLE_END#Gets rid of scipy eigs/eigsh warning of double precision only 
issue
Fix tests that fail due to hash randomization#TITLE_END#Addresses #975  I couldn't get the dict_to_numpy_array() tests to fail as sin #975 (1000 tries, Python3.4 alpha) 
issue
Add docs for dfs and bfs#TITLE_END#Examples and docuemntation for dfs and bfs algorithms. Fixes #1037 
issue
Use sparse matrix solver for eigenvector centrality#TITLE_END#Addresses #969  A sparse matrix solver (SciPy/ARPACK) for eigenvector centrality. 
issue
Bug writing shapes in GEXF format#TITLE_END#Simple fix to correct handling of shape strings in gexf writer 
issue
Force float dtype in to_scipy_sparse_matrix().#TITLE_END#This fixes the reported divide by zero error. 
issue
networkx-1.9 release#TITLE_END#We are well overdue for the release of networkx-1.9.  There are many great new additions; I think we should make a release ASAP.  Here are the current open issues marked for the networkx-1.9 milestone.  Most of those look complete, or fairly simple to close: https://github.com/networkx/networkx/issues?milestone=2&state=open  Actions: - Comment/fix/defer the open issues (comment in relevant issue thread) - Add possible missed issues urgent for this release - Comment here on release notes, or other items not associated with a current issue 
issue
Fix bug in gml writer#TITLE_END#Remove unneeded extra recursive call to listify. It added an extra set of double quotes.  Pointed out in report at http://stackoverflow.com/questions/19352960/networkx-parse-gml-writing-unusable-gml-files 
issue
Add converter from dictionary of values to 1d numpy array.#TITLE_END#Modify dict_to_numpy_array to handle both dict of values and dict of dict of values. Addresses #788  
issue
787 hits#TITLE_END#For HITS, use explicit normalization to l1 norm.   Addresses #787  
issue
Sphinx fixes#TITLE_END#These fixes allow the documentation to be built with Python3 versions of sphinx and numpydoc. - Fix customroles.py for Python3  - Use development numpydoc.  Remove from NetworkX distribution in favor of installing from https://github.com/numpy/numpydoc 
issue
Selfloops counted twice#TITLE_END#Fix bug where Graph selfloops get counted twice when converting to scipy sparse matrix.  Fixes #1075 
issue
Use sparse matrix#TITLE_END#Switch linalg package to use sparse matrices.    Improve efficiency in to_scipy_sparse_matrix  Addresses #969 and #913  
issue
Remove json helper#TITLE_END#Added complexity not needed - you can just call json.dump() on a serialized graph.  Addresses #1110 
issue
Use long type for GEXF with Python2 long or Python3 int.#TITLE_END#GEXF has a "long" integer type.  Use that for Python2 longs and the standard Python3 int.  Addresses #940 
issue
Sparse6 format padding fix#TITLE_END#Addresses #955 
issue
Use "left eigenvector" (in-edges) in centrality#TITLE_END#Change both Katz centrality and eigenvector centrality for directed graphs to use "left eigenvector" centrality (in-edges). Use G.reverse() to get out-edge centrality.  Addresses #1016 
issue
Remove @require decorator#TITLE_END#Addresses #838 
issue
symlink to copy of atlas.py example#TITLE_END#Fixes #1087 
issue
Doc fixes (bad module paths)#TITLE_END#Just some fixes for broken doc paths. 
issue
Explicitly import graph atlas#TITLE_END#Fixes broken example blocking doc build - issue #1087 
issue
Directed Gnp model (fast version) has wrong start index#TITLE_END#For the directed graph version of the Gnp model the starting target has to be at v=0 not v=1 or no out-edges will ever be generated from node 0. 
issue
Add JSON format documentation#TITLE_END#Fixes #848 
issue
Documentation for components change to generator.#TITLE_END#Addresses #964 
issue
Split graph6 and sparse6, deprecate *list functions#TITLE_END#Split graph6 and sparse6 into modules.  Remove read_list and write_list functions - it is simple to write  small loop and makes for lots of duplicate documentation otherwise.    Update documentation to standards.  Addresses #1029 
issue
Use lowercase true|false in GEXF writer#TITLE_END#Fixes #912 
issue
Relabel docfix#TITLE_END#Clarify relabling in docs. Fixes #938 
issue
Multigraph operators#TITLE_END#Force input types to be both graphs or multigraphs.  Fixes #849 
issue
Windows compatibility for read LEDA test#TITLE_END#Fixes #946   Python 3 friendly. 
issue
JSON graph fix to handle unicode dict keys#TITLE_END#This fix allows unicode for attribute keys in the JSON graph format writers.  This code demonstrates the issue and should now work correctly  ``` python #  -*- coding: utf-8 -*- import json import networkx as nx from networkx.readwrite import json_graph G = nx.Graph() q = u"qualité" G.add_node(1, {q:q}) s = json_graph.node_link_data(G) output = json.dumps(s, ensure_ascii=False) data = json.loads(output) H = json_graph.node_link_graph(data) ``` 
issue
Remove normalization option to current_flow_closeness#TITLE_END#Update docs, small formatting changers  Addresses #983 
issue
Connected component subgraphs copy#TITLE_END#Add option to copy (or not) subgraph data. Rework connected components algorithms as generators.  Suggestion for #964 
issue
Stochastic zero row + pagerank#TITLE_END#Allow zero out-degree (row sum) but warn in stochastic_graph Allow (working) multigraph input to pagerank_numpy and pagerank_scipy Clarify types of graphs allowed 
issue
Max matching order#TITLE_END#Algorithm didn't allow for edge ordering u-v and v-u in undirected graphs  Addresses #1022 
issue
Doc fixes#TITLE_END#Formatting fixes to docs. Changes in examples to use new component generators API. 
issue
Force ordering in dict_to_numpy_array functions#TITLE_END#Force ordering to be that of the provided mapping dictionary or else create a mapping using the input data dictionary ordering.  Fix tests to determine ordering.  Addresses #975 
issue
Comments in configuration model about multi-edges#TITLE_END#Add comments about scaling of the number of parallel edges and self loops. Addresses #986 
issue
read_pajek() shouldn't require network name#TITLE_END#Attempt to read in a network with no name (pajek doesn't require one be included), the following bit of code in pajek.py fails:  ``` python         if l.lower().startswith("*network"):             label,name=l.split()             G.name=name ```  because there is no exception if the *network line lacks a name. 
issue
Line graph algorithm doesn't handle self-loops correctly#TITLE_END#The algorithm in line_graph() doesn't capture self-loops correctly. See the discussion and code examples at  http://stackoverflow.com/questions/18214305/correctness-of-networkx-line-digraph-construction/18244242 
issue
Deprecate create_degree_sequence.#TITLE_END#Warn on upcoming removal of create_degree_sequence() function.  Addresses #863 
issue
Add 'weight' keyword option to eigenvector and Katz centrality#TITLE_END#Both the eigenvector_centrality() and katz_centrality() functions use edge weights if 'weight' is an attribute of the edge but don't describe that in the documentation.  Also other NetworkX functions  allow the user to specify the attribute keyword when calling the function with weight= including the option for weight=None. These should be the same. 
issue
Connectivity + dominating set move#TITLE_END#Move dominating set algorithms to new package (separate from connectivity).  Allow ordering of nodes for matrix in all_pairs_node_connectivity() function.  Add tests for dominating sets and connectivity. 
issue
Nav smallworld#TITLE_END#Don't allow self loops for long-range connections. 
issue
Update release 1.8#TITLE_END#Merge this for final networkx-1.8 release 
issue
Link fixes#TITLE_END#Missing or broken links.  Make linkcheck breaks on a few more but they are OK. 
issue
Copy of subgraph() might be harmful#TITLE_END#Copying subgraphs (c.f. connected_component_subgraphs()) might be a good idea to not create surprise.  That way changing attributes in the subgraph then doesn't change them in the original graph.  But it might cause errors for example in #856 .  Are there other places in the code base with this issue? 
issue
Extra delimiter from write_adjlist()#TITLE_END#The write_adjlist() function adds a trailing delimiter to each line.  When not using space-delimited nodes this causes an extra node (label of empty space) to be added to the graph when calling read_adjlist() 
issue
Don't modify input list in _all() operators.#TITLE_END#Fixes issue #891 
issue
Pydot test fixes#TITLE_END#Catch error when pydot can't be imported because of missing pyparsing. 
issue
Threshold test#TITLE_END#Comment out failing test for threshold graphs. Needs rework for laplacian_matrix type return 
issue
Laplaican matrix functions should return matrix#TITLE_END#The Laplacian functions (unlike other linear algebra functions) return a numpy array instead of a numpy matrix.  Switch to returning a matrix so they are all consistent.   
issue
Fix core tests#TITLE_END#Adjust test graph to map nodes since the new havel hakimi algorithm generates different node ordering 
issue
Fix id specification for GEXF writer edges#TITLE_END#The GEXF writer failed to take into account user specified node ids.  Addresses bug discussed in #842  
issue
Katz Centrality (update from #767)#TITLE_END#An update from #767 with Katz centrality.  It still needs  1) Implementation of the beta parameter as dict and vector 2) More tests, especially with different alpha and beta 3) Copy-editing in the docs for clarity 
issue
Linear-time graphical tests#TITLE_END#Addresses #706 by adding the linear-time graphicality tests. 
issue
Directed Havel-Hakimi generator#TITLE_END#Directed Havel-Hakimi generator and faster undirected version.  Addresses #706 
issue
Multigraph handling for JSON formats#TITLE_END#The multigraph handling when producing formats for JSON serialization didn't account for edge keys.  The keys are now included in the edge/link data.  Fixes #827  
issue
Return relabel mapping as node attributes#TITLE_END#Addresses #820 
issue
Cycle basis loop#TITLE_END#Bug in cycle_basis() in comparing nodes for self-loop check. 
issue
Add force scaling parameter to spring layout.#TITLE_END#Add parameter k to specify optimal distance between nodes in spring layout. The default is 1/sqrt(number of nodes). Rescale layout on every step to be in [0,1] box. 
issue
Remove generation of "name" line from Pajek writer since many Pajek read...#TITLE_END#Since many Pajek readers can't process it.  Fixes #765 
issue
Add isolated nodes to tree in dfs_tree() and bfs_tree().#TITLE_END#Fixes #772 
issue
Move Python version specific decorator code to separate packages.#TITLE_END#Add switch in setup.py to only install either the Python2 or Python3 version to avoid bytecompile errors during packaging.  Addresses #759 
issue
Add a "name" attribute for each node in the force.py example graph.#TITLE_END#The force.js example uses the "name" attribute for the mouse-over value. This small fix uses the node id as the name so the mouse-over is defined. 
issue
Fallback to string type in GraphML reader if none given in <key>#TITLE_END#Addresses #784  Is there any harm in guessing the type as a string and use warnings.warn to notify the user that something is suspicious with the GraphML file? 
issue
Attempt to handle current figure access with pyplot.#TITLE_END#...even though we are not using the matplotlib API properly.  Addresses #729 
issue
Split front page of website to networkx-website.  Add gh-pages function ...#TITLE_END#...to push docs to networkx-doc github site. 
issue
Add developer guide.#TITLE_END#Add gitwash developer guide.  More info is needed in source/developer/index.rst. 
issue
Doc fixes#TITLE_END#Small doc fixes to layout and spacing of contents page and template. 
issue
Directed laplacian#TITLE_END#Updates to documentation  fixing TeX formatting and rst processing issues.  Permanent renaming of laplacian, norrmalized_laplacian, etc to have _matrix as a suffix.  E.g. laplacian() -> laplacian_matrix().  Update authors and copyright date. 
issue
Fix missing SkipTest import in utils tests#TITLE_END#nose.SkipTest import needed for skipping numpy tests 
issue
Add python3.3 to test_pr script#TITLE_END#NetworkX works with Python3.3 so add that to the test_pr.py script. 
issue
Fix errors in docs.#TITLE_END#Errors in processing special characters and one missing file ref. Docs build cleanly now with no errors. 
issue
Fix density to handle single node with self loop case.#TITLE_END#Make note that graphs with self loops can have density > 1. Addresses #795 
issue
Remove "sci" calls that break embedding in wx. Switch from pylab to pyplot#TITLE_END#Switch to using pyplot interface instead of pyplot.  Remove pylab.sci() calls that break embedding in gui canvases.  I couldn't figure out why pylab.sci() was needed and I think it is safe to remove them.   
issue
Close file in decorator tests.#TITLE_END#Tests OK with linux.  Fixes #746 
comment
Hi @AthinaSpanou - thanks for suggesting this addition.  NetworkX is primarily about data structures and algorithms for graphs and we are interested in adding new algorithms.  Are you proposing adding an algorithm that can approximate the PC-TSP solution?   
comment
See #2152
comment
There must be a more efficient way. 
comment
I believe we did it that way on purpose since in this case higher-weight (longer distance) leads to lower centrality and we thought it would be less confusing.  In some cases you might e.g. want something like distance = 1/weight. 
comment
There are some random sample helpers in networkx.utils too. 
comment
Maybe they should be in "utils" or as examples? There is also this old thread on the mailing list about choosing random edges  https://groups.google.com/forum/#!topic/networkx-discuss/qH3WynBjp3g Maybe something useful there?   
comment
Needs updating to address comments. 
comment
@jfinkels has a good idea here to use existing readers to help make writing this easier.  I looked at the formats for UCINET DL - www.analytictech.com/ucinet/documentation/usersguide.doc - https://gephi.org/users/supported-graph-formats/ucinet-dl-format/  They all are basic text files with a header and then data.  Perhaps the simplest approach is to write a function that will parse the header and dispatch based on that.    e.g. for  ``` DL n=5 format = edgelist1 labels: george, sally, jim, billy, jane data: 1 2 1 3 2 3 3 1 4 3 ```  The steps would be - Parse the lines before "data" - e.g. create label mapping made {1: george, 2:sally, ...} - Call parse_edgelist() on the rest of the file - Relabel nodes with the mapping  In that case we already have an edgelist reader.   For matrix formats we could fairly simply use either one of  - http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.loadtxt.html - http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.genfromtxt.html   and then do a little manipulation depending on the header (fill in diagonal, fill in lower part, etc).  For writing data a similar approach could be used. 
comment
This is an interesting idea.  I looked around some at the way others have implemented planar graphs.  It seems the most efficient data structure is not the dict-of-dict approach that networkx uses.  Have you considered using other data structures?  It may be simpler to do that.   
comment
I don't remember the details of where I looked.  It seemed that keeping track of nodes, edges, and faces was a good approach and I remember some discussion about doubly linked edge lists.    My general comment was more to suggest starting from the point of how you would want a planar graph class to look and work and then see how it may or may not overlap with a general graph class.  The Python dictionary hash implementation is very good so I wouldn't worry about that first. 
comment
Can you point to the definition of graph power?  The ones I see generate graphs not multigraphs. 
comment
I like the idea of including this but we need to come to a conclusion on the function design first. 
comment
I was thinking of a Python implementation something like this. (But with the correct handling of self-loops and parallel edges).  Untested...  ``` python import networkx as nx from itertools import izip_longest def pow(G,k):     H = nx.Graph()     for n in G:         p = nx.single_source_shortest_path_length(G,n,cutoff=k)         H.add_edges_from(izip_longest([n],p,fillvalue=n))     return H  if __name__=='__main__': #    G = nx.cycle_graph(5)     G = nx.grid_2d_graph(5,3)     H = pow(G,2)     print H.edges() ``` 
comment
I don't agree that this is too complicated or slow to do in Python without Scipy.  Anyone want to finish this PR? 
comment
Would it be more efficient (at least for larger graphs and longer length `k` walks)  to use the adjacency matrix representation?  Something like  `W = nx.adjacency_matrix(G)**k` with a wrapper to handle labeling and returning results. 
comment
I'm not enthusiastic about reading through #1307 on graph powers again (I don't think we came to a conclusion there).  But for the purpose of counting walks the matrix approach seems best.  It wouldn't be too hard to implement that in Python without scipy but I'd expect the CSC matrix representation in scipy to do this calculation fairly efficiently (didn't actually look at the code). 
comment
If it isn't too complicated or confusing we could keep both versions.  I didn't look carefully at the computational complexity of the algorithm you implemented but it seemed like it might be trouble for larger length walks.   
comment
There are some strategies to convert scipy sparse matrices to networkx graphs in https://github.com/networkx/networkx/blob/master/networkx/convert_matrix.py The basic approach there is to make edge triples and is a little complicated because of the different sparse matrix formats it has to to handle efficiently.  Since you can control that you only need to choose one (CSR) so that makes it simpler.   Maybe some version of   ``` python     nrows = A.shape[0]     data, indices, indptr = A.data, A.indices, A.indptr     for i in range(nrows):         for j in range(indptr[i], indptr[i+1]):             yield i, indices[j], data[j] ``` 
comment
That is pretty complicated for a simple feature and not quite what I had in mind.  I'll add more comments when I get a chance. 
comment
Needs update or closing. 
comment
On Fri, Sep 14, 2012 at 2:21 PM, Moritz Emanuel Beber notifications@github.com wrote:  > I suppose it is only sensible to put as much of the code from > algorithms/community/spectrum.py and > algorithms/community/laplacian_spectral.py into the fitting linalg modules. > Any estimate on when #741 will be merged?  It looks like #741 needs some more tests and docs.  I'll try to find time to review that in the next week or so. 
comment
Another addition could be "nodal roles" in #1060  
comment
Pull request at https://github.com/pedros/networkx/pull/1 with some suggested changes. 
comment
Maybe it is simpler and faster to not use numpy? e.g. (untested)  ``` python def hd(G, H):     count = 0     for e in G.edges_iter():         if not H.has_edge(*e):             count+=1     for e in H.edges_iter():         if not G.has_edge(*e):             count+=1     return count ``` 
comment
I didn't read the whole paper.  But in that context doesn't it make sense to just use two directed edges for a "bi-directed edge"?  On Sat, Aug 1, 2015 at 12:44 PM, Mridul Seth notifications@github.com wrote:  > As @chebee7i https://github.com/chebee7i pointed out MixedGraph are > helpful for Bayesian networks. >  > http://www.jmlr.org/papers/volume10/silva09a/silva09a.pdf >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1168#issuecomment-126944345. 
comment
Needs a shapefile expert to sort this out please.
comment
For some tests of algorithms see http://i11www.iti.uni-karlsruhe.de/extra/publications/TR-mw-aesgp-11.pdf 
comment
It looks like some variant of "Faster, better, cheaper - pick two".   Some of the algorithms might be pretty complicated to implement. 
comment
I don't know if any references on "edge load" - but there might be.  We could simplify/normalize the names by just using load_centrality and edge_load_centrality. 
comment
Yes, please make those updates. 
comment
Needs pull request to update code for intersection of graphs to allow intersection of graphs with different node sets.
comment
Hi Aleksey - Thanks for your code and comments. There are many different "preferential attachment" models one could use (and many have been explored in the literature). I think the goal for the barabasi_albert_graph code was to implement the model in this paper http://barabasi.com/f/67.pdf. Can you take a look and see if that is the case?  It looks like there are a couple of other barabasi-albert variants in the networkx code base - see https://github.com/networkx/networkx/blob/master/networkx/generators/random_graphs.py If there are other useful variants perhaps suggest adding them?  Aric   On Thu, Dec 27, 2018 at 2:20 PM Aleksey Vorona <notifications@github.com> wrote:  > barabasi_albert_graph starts with G=empty_graph(m) instead of G = > complete_graph(m). > As a result, the early new nodes are almost certain to become hubs, since > the distribution is heavily skewed in their favour. The initial m nodes > have almost zero chance to be a hub. > > To reproduce: > > hubs = [] > for i in range(100): >     G = nx.barabasi_albert_graph(1000,500) >     hubs.append(sorted(G, key=lambda n: len(G[n]), reverse=True)[0]) > count_dict={} > for hub in hubs: >     count_dict[hub]=hubs.count(hub) > print ('\nfrequency:\t',count_dict) > > # prints > # frequency:	 {500: 34, 501: 27, 502: 13, 503: 10, 504: 11, 505: 2, 506: 3} > > Quote from Wikipedia ( > https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model ): > > The network begins with an initial connected network of m_0 nodes. > > And logically it has to start from a connected network for the initial m > nodes to have a chance to be a hub. > > The effect would not be noticeable in "small world" kind of networks > (medium node degree << number of nodes) but is profound in networks with n > and m close enough to each other. > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/3281>, or mute the thread > <https://github.com/notifications/unsubscribe-auth/AALd4yMd4S2Vqxe02eGPzrQq1BEDvVDXks5u9TmMgaJpZM4ZjM5Y> > . > 
comment
Yes, I agree with your assessment.    And for the barabasi_albert_graph algorithm in networkx I'm advocating to keep it as written to match the original paper.   I'd recommend either adding an option (not the default) to fully connect the initial nodes or a separate algorithm that implements that variant.   In principle one could initialize the algorithm with any graph on m nodes.  
comment
Thanks for this PR.  I see we'll have to include the Louvain community detection for this to work.   Maybe someone will make a pull request to do that? 
comment
@bjedwards has thought the most about what kind of object we should return from a community-finding algorithm.  I don't know if there is a consensus yet.  It would be nice to be consistent but if not we can provide conversion algorithms. 
comment
Python3.4 has a stats module https://docs.python.org/3.4/library/statistics.html with mean and std. So someday we can use that for mean() and stdev(). 
comment
I don't have a problem with the numpy dependency in this module.  As @chebee7i points out there are plenty of numpy dependencies in NetworkX.  I do want to keep it protected though (soft dependency) so those without numpy can still use the non-numpy parts. 
comment
I'm not very enthusiastic about adding PEP 484 style type annotations.  Perhaps when we drop Python 2 support and the tools for type checking or code generation are more mature it would be worth a discussion. 
comment
I'd still vote no until type annotations are more mature and they provide some wider value.  But if there are lots of other developers that want this I am willing to support putting some .pyi files in a stubs directory or maybe they should go here https://github.com/python/typeshed/tree/master/third_party 
comment
I'm not keeping you from having type hints.  I'm suggesting that they be separate from the networkx code in a special directory or at the typeshed project.  This project isn't ready to drop Python3 support yet so I'd like to postpone the discussion of inline type annotations until we get there. 
comment
I think that is what I suggested you do.   
comment
Could you give an example where the performance is bad?  I could see this being an issue but it would be nice to have a real-world case to check the practical consequences of making a copy or swapping in a weighted choice without replacement algorithm.
comment
The nodes need to be chosen proportionally to degree.  That is being done by sampling uniform from the list `seq` with repeated nodes.  Alternatively networkx.utils.random_weighted sample(degree_dict,m) would probably work.  But it might be slower for all but pretty unusual cases.  There may be better algorithms.
comment
Almost,  but we want "without replacement". Interesting weighted random choice  is now in Python3.6. The current solution is probably fastest in most cases.  
comment
It's less typing to write some code and benchmark. Here is code that I think implements the same BA algorithm with a weighted choice.   https://gist.github.com/hagberg/9b81be32933551b324ed40ef6cb8c73f  Benchmarking left as an exercise.  
comment
Needs review and benchmarking.
comment
For the multiple edge bug see https://github.com/networkx/networkx/issues/2625
comment
Interesting idea.  How about just drawing the self loops with bezier curves?  I'm not sure this is the best approach for parallel edges.   
comment
It would be great if pydot/pydotplus would fix this.  Would you report it there?  I know we can fix it to round-trip correctly with networkx but as I pointed out in #2050 we've spent quite a lot of time dealing with quotes and special dot string issues already - https://github.com/networkx/networkx/issues/1872 - https://github.com/networkx/networkx/issues/1348 - https://github.com/networkx/networkx/issues/317 - https://github.com/networkx/networkx/issues/350 - https://github.com/networkx/networkx/issues/184 
comment
This does work correctly with pygraphviz.  e.g.  ``` python import networkx import networkx.drawing import tempfile  g1 = networkx.DiGraph() g1.add_node("foo", nodeattr=":FOO:") g1.add_node("bar", nodeattr=":BAR:") g1.add_edge("foo", "bar", edgeattr="FOO->BAR")  with tempfile.NamedTemporaryFile() as f:         networkx.drawing.nx_agraph.write_dot(g1, f.name)         g2 = networkx.DiGraph(networkx.drawing.nx_agraph.read_dot(f.name))  print("G1:", g1.node["foo"]) print("G1:", g1.edge["foo"]["bar"])  print("G2:", g2.node["foo"]) print("G2:", g2.edge["foo"]["bar"]) ``` 
comment
Thanks, that is great, https://github.com/carlos-jenkins/pydotplus/pull/9 
comment
I'm don't know the approaches for this - maybe telling crawlers not to index the old pages?  https://support.google.com/webmasters/answer/93710
comment
Maybe it would be better to generalize `from_numpy_matrix` to `from_adjacency_matrix` and allow either a numpy matrix or a square pandas dataframe?  The reasoning is that it would be almost the same code so this way we won't be repeating ourselves in a new function.  The current from_pandas_dataframe is essentially an edgelist with optional edge attributes so that function, while useful, isn't quite correctly named either.  Note that the [stackoverflow post](http://stackoverflow.com/a/21210849/2014591) you mention fstarts with a "biadjacency matrix" which is yet another graph data structure used for bipartite graphs.  There is a pair of functions for biadjacency data too https://networkx.github.io/documentation/latest/reference/algorithms.bipartite.html#module-networkx.algorithms.bipartite.matrix 
comment
Maybe related to this? https://github.com/pygraphviz/pygraphviz/issues/111, https://github.com/pygraphviz/pygraphviz/issues/133
comment
How long should it take?
comment
Ok. Suggestions?  
comment
Sure, I bet importing, numpy, pandas, scipy, and matplotlib takes some time.  And all of those load when you import networkx. 
comment
Closed due to inactivity.
comment
Is this still an issue?   Re-open if so and we'll see if we can troubleshoot more. 
comment
Dot guide is at https://www.graphviz.org/pdf/dotguide.pdf
comment
No update provided. 
comment
No follow up.
comment
Maybe this paper helps with algorithms? https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-110
comment
Need more info on the graph data to troubleshoot this.  
comment
It looks like node data for shapefiles can't be tuples?  
comment
Re-open if pull request available. 
comment
Please open new specific issues as documentation updates on use of weights. 
comment
Re-open if a PR is submitted. 
comment
See https://github.com/pygraphviz/pygraphviz/issues/207
comment
Addressed on stackoverflow.
comment
The max_weight_matching algorithm is for undirected graphs. 
comment
Needs more information to be able to debug any issue here. 
comment
There is a good (short) discussion on self-loop and parallel edges here https://en.wikipedia.org/wiki/Configuration_model#Algorithm  Self-loops and parallel edges are not a problem but a consequence of the algorithm.   It may be possible to swap some edges to realize a graph (without parallel edges or self loops) and with the same degree distribution.  If you would like to contribute such an algorithm please do make a PR. 
comment
Yes, this should be documented somewhere since it does depend on the implementation choices in the code.  @dschult should it be https://github.com/networkx/networkx/wiki/Design-Specification or somewhere in the doc tree?
comment
The discussion here covers this topic https://github.com/networkx/networkx/wiki/Design-Specification  It may be worth having a separate "performance" wiki page if we want to make this easier to find. 
comment
As far as I know this hasn't been implemented.   It would be fairly simple to write something basic to just output nodes and edges (I wrote such a thing in 2002). As soon as you start thinking about more interesting features like labels, attributes, subgraphs/clusters, ports, etc it becomes complex.   
comment
This would make a nice Jupyter notebook example #1581 #1754  
comment
I took a look at #2050 and my previous grumpy assessment that this is a pydot bug.   I still agree but I am open to figuring out if there is a way to make it work with networkx instead of fixing pydot.  Suggestions?  I don't see on quick inspection why the workaround is throwing an exception but that one I think we can fix.
comment
Yes, I get the graphviz node port syntax with a colon.  What I think is wrong is for pydot to strip the colon and later from the name of the node e.g. https://github.com/networkx/networkx/issues/2050#issuecomment-203707913    I've weighed in as negative on automatically adding quotes but could be convinced.  The idea of throwing an exception and suggesting adding quotes by hand might be better.  I think what you are saying is that graphviz node attributes can't have colons so the workaround as stated in the docstring fails.   I could believe that- hard to tell from the error messages.  
comment
My understanding probably comes from my pygraphviz bias which interacts with the dot language through the graphviz libcgraph interface.  In that case you wouldn't label a node with a port "colon" syntax but would instead specify the headport and tailport when creating an edge with those nodes.   This is a consistent approach that round-trips write/read.  Any name with a colon is automatically double quoted as an allowed double quoted string type.    This works reasonably with networkx/pygraphviz like this ```In [23]: G = nx.Graph()  In [24]: G.add_edge('foo','foo',headport='n',tailport='s')  In [25]: G.add_node('bar:bar') # automatically double quoted  In [26]: nx.nx_agraph.write_dot(G,'foo.dot')  In [27]: cat foo.dot graph "" { 	foo:s -- foo:n; 	"bar:bar"; }  In [28]: H = nx.nx_agraph.read_dot('foo.dot')  In [29]: H.nodes(data=True) Out[29]: NodeDataView({'foo': {}, 'bar:bar': {}})  In [30]: H.edges(data=True) Out[30]: MultiEdgeDataView([('foo', 'foo', {'headport': 'n', 'tailport': 's'})]) ```  Pydot works similarly but as we know treats names of nodes with colons as special so you get this kind of confusion ``` In [34]: G.add_edge('bar:n','bar:s')  In [35]: nx.nx_pydot.write_dot(G,'foo.dot')  In [36]: cat foo.dot graph  { foo; bar; bar; bar; foo -- foo  [headport=n, tailport=s]; bar:n -- bar:s; }  In [37]: H = nx.nx_pydot.read_dot('foo.dot')  In [38]: H.nodes(data=True) Out[38]: NodeDataView({'foo': {}, 'bar': {}, 'bar:n': {}, 'bar:s': {}})  In [39]: H.edges(data=True) Out[39]: MultiEdgeDataView([('foo', 'foo', {'headport': 'n', 'tailport': 's'}), ('bar:n', 'bar:s', {})])  ```  Maybe the simplest solution is to double quote any strings with colons before sending them to pydot? ``` In [41]: G = nx.Graph()  In [42]: G.add_node('"bar:bar"')   In [43]: nx.nx_pydot.write_dot(G,'foo.dot')  In [44]: cat foo.dot strict graph  { "bar:bar"; } ``` That way the names would be preserved and you can still use 'ports' if you want by setting keywords as above.          
comment
This shows the version (and raises an error). python -c "import pygraphviz; A = pygraphviz.AGraph(); A.layout('dot','-V')' Would be a reasonable feature to add to pygraphivz.   On Thu, Jun 14, 2018 at 1:32 PM Dan Schult <notifications@github.com> wrote:  > My installs are using graphviz 2.36. > I'll try to get an environment set up to test this. I've got multiple > versions of Graphviz, but can't easily tell which one pygraphviz is using... > > Anybody know a quick way to have pygraphviz report which version of > Graphviz it is using? > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/3003#issuecomment-397412511>, > or mute the thread > <https://github.com/notifications/unsubscribe-auth/AALd4_YisPjJGXkJBIEHyLg8GnQWiB7Nks5t8rpdgaJpZM4UfKYx> > . > 
comment
Graphviz/pygraphviz issue resolved in later versions.
comment
On Wed, Sep 12, 2012 at 11:59 AM, rcompton notifications@github.com wrote:  > When I try to read/write the example dynamic .gexf format at > http://gexf.net/data/dynamics.gexf I end up with a file that Gephi can't > read. This code will reproduce the error: >  > import networkx as nx >  > def main(): >  > G=nx.read_gexf("/home/ryan/Documents/dynamics.gexf") >  > nx.write_gexf(G,"/home/ryan/Documents/dynamics_out.gexf") >  > return >  > if name == 'main': > main()  Can you be more specific about what the problem is with the write_gexf() output? E.g. point to the standard/specification? 
comment
The gexf reader needs updating to version 1.3. 
comment
Is there a difference in the functionality between edge elements and attrvalues?  Can you suggest a fix? 
comment
Yes, this would be an interesting addition to the package.  Please update with a PR if you like.
comment
The problem is known to be difficult.  Here is an interesting CS stack exchange discussion that explains it well https://cstheory.stackexchange.com/questions/20767/partition-a-graph-into-2-connected-subgraphs
comment
You are right.  NetworkX does not add quotes in that case. If you want a double quoted string you could make one.  I'm not sure if this is a bug or a not?  ```python import networkx as nx G = nx.Graph() G.add_node('n0',label=9.0) G.node['n0']['\"Node Label\"']='A' import sys nx.drawing.nx_pydot.write_dot(G,sys.stdout)```  ``` $ python foo.py strict graph  { n0 [label="9.0", "Node Label"=A]; }``` 
comment
Yes, right.  Definitely needs a double-quoted string.  But is this a user bug (user didn't provide a double quoted string) or a bug in write_dot (not automatically adding double quotes to an ID with  whitespace)?   This might apply to other cases to - e.g. we don't automatically check if you have written a XML string and automatically add <>.
comment
Spielman said (in a lecture) that his group intends to release some code in the next 6 months or so.  If anyone on that team wants help integrating with NetworkX or with a Python wrapper we can help. 
comment
No it can't be ported. The code in Sage has an incompatible license (GPL). 
comment
Could you explain more what you are proposing/requesting?  I'm not familiar with that terminology. 
comment
I think this could be added with the community detection (linked) code. 
comment
Here is a recent paper about the problem - http://arxiv.org/abs/0912.5327 anyone want to take this on? 
comment
Great - I'm not familiar with either of those papers but they are interesting. Yes you can put the module in networkx/algorithms/ 
comment
That might be hard to test.  There are some other approximation algorithms in networkx/algorithms/approximation. Maybe look there and see what was done? 
comment
Yes, we could add that code to NetworkX.   PR welcome. 
comment
Thanks @SuperAce99  for the clear summary.  I agree with you on the data structure ( iterable object that yields objects that support set operations) and I think we should proceed that way. That handles the case of overlapping partitions easily too.  There are likely some operations that are easier to perform with an inverted representation like a dict with labeled communities.  In particular finding which group a node is in.  But it is straightforward to convert to that structure if needed. 
comment
It seems we are all in agreement on how to proceed.  So bring on the pull requests :thumbsup:  
comment
Sure, that would be great.  Let us know if we can help. 
comment
I think #1550 makes the same argument for using the RandomState interface.
comment
I advocated (too weakly) for this in #1550.  It doesn't look like anyone ever created a PR that implements the RandomState approach.
comment
I wonder if that is an error.  Should the default attributes be applied to every node or edge? I would say no.   The default value is captured in the G.graph attribute  ``` python In [1]: import networkx as nx  In [2]: G = nx.read_graphml('colored.graphml')  In [3]: G.graph Out[3]: {'edge_default': {}, 'node_default': {'color': u'yellow'}} ```  You could apply the default to your nodes like this  ``` python In [4]: default_color=G.graph['node_default']['color']  In [5]: for node,data in G.nodes(data=True):    ...:     if 'color' not in data:    ...:         data['color']=default_color    ...:           In [6]: G.node Out[6]: {'n0': {'color': u'green'}, 'n1': {'color': u'yellow'}} ``` 
comment
I don't know what drawing tool made the diagram on that page but it clearly applies default values. The matplotlib drawing package in networkx doesn't do that (among many shortcomings).    But if we apply the default values when reading the GraphML file then writing the file will create a different output than the original.  That probably isn't the desired behavior. 
comment
The problem is that the NetworkX/Matplotlib drawing solution isn't comprehensive.  I don't think we should fix that by messing around with the GraphML reader and writer.  An example is fine though. 
comment
You can set the default attributes to output as GraphML.  Need a PR with some examples here instead of code fixes. 
comment
The 'nstart' parameter is, as you say, the initialization vector for the power iteration algorithm to compute the pagerank vector.    The vector is sometimes chosen to be constant (e.g. normalized vector of all 1's), or random.   The choice will have an effect on the time to convergence since e.g. if you start very close to the answer you will need very few iterations.      I don't see any reason why we couldn't add the ability to specify the starting vector for the pagegrank_scipy algorithm.    The pagerank_numpy algorithm uses a direct eigenvalue solver (not an iterative method) so it doesn't apply there. 
comment
I like adding the rings as in your last example.
comment
I'm not familiar with the use of these data as graphs.  Which do you think is the best (or least surprising) solution? 
comment
Thanks for explaining this in detail.  You are far more expert me and probably most others here.  So let's fix it the way you think makes the most sense.
comment
Would it be possible to implement this without using numpy?  It might be much faster that way. 
comment
Apparently that code has been removed.  Maybe @peterderivaz would be interested in contributing it to NetworkX? 
comment
On Thu, Oct 18, 2012 at 12:26 PM, peterderivaz notifications@github.com wrote:  > I'd love to. I actually used networkx to check that the implementation was > sound and found a bug in the description of their algorithm (I emailed them > and they corrected it in v2 of the paper). As most of the time is spent in > computing set intersections, Python actually does an amazing job of > implementing this and in fact my Python times were faster than their C times > for some of the test cases :)  Great - we'll help get it integrated.  > The only snag is that they have politely asked me to delete the github > repository until they have had a chance to publish the paper. They have said > they will let me know when they are happy for me to put it back. Once this > happens I will also try to prepare a pull request for networkx.  OK, interesting.  The paper is definitely published at http://arxiv.org/abs/1209.5818  > As it stands their algorithm just returns the maximum size. I assume that > it would be more useful to return an actual instance of a set of nodes with > the max clique size. (Or perhaps even use yield to produce an iterator over > all max cliques?) I think these changes are quite easy to add to their > algorithm although I haven't done so yet.  I like the idea of an iterator over all max cliques. 
comment
Any bad engineering decisions here are likely my fault.   Here is how it works.  For handling dot files in NetworkX the G.graph attribute dictionary has to take into account graph, node, and edge default scope.  For example  for a dot file (from tonyballantyne.com/graphs.html) with graph, node, and edge level defaults   ``` digraph hierarchy {  		nodesep=1.0 // increases the separation between nodes 		 		node [color=Red,fontname=Courier,shape=box] //All nodes will this shape and colour 		edge [color=Blue, style=dashed] //All the lines look like this  		Headteacher->{Deputy1 Deputy2 BusinessManager} 		Deputy1->{Teacher1 Teacher2} 		BusinessManager->ITManager 		{rank=same;ITManager Teacher1 Teacher2}  // Put them on the same level }  ```  the resulting networkx graph looks like this  ``` In [1]: import networkx as nx  In [2]: G = nx.drawing.nx_pydot.read_dot('a.dot')  In [3]: G.graph Out[3]:  {'edge': {'color': 'Blue', 'style': 'dashed'},  'graph': {'nodesep': '1.0'},  'name': 'hierarchy',  'node': {'color': 'Red', 'fontname': 'Courier', 'shape': 'box'}} ```  The graph level attributes are keyed with 'graph', node keyed with 'node', and edge keyed with 'edge'.    The attribute 'name' is just the name of the dot graph (might be empty).     
comment
This is going to break in other writers too.  It might make sense to decide on a comprehensive solution.  At this point if you are not using a built-in Python type the text-based graph format writers may not always work.   The workaround is to convert the attributes to Python types. 
comment
Yes, that is an issue.  I think the (mistaken) assumption is that either all of the nodes and edges would have ids or none of them would.  In that case everything is fine.  So a simple workaround is to add ids to new nodes and edges `G.add_node(7, id=42)`, etc.   As for the proper fix, I can think of two ways: - Discover all of the existing node and edge ids before writing and store them in a set to check when generating new ids (integers I would propose in this case) - Generate random ids using e.g. the Python uuid module (call uuid.uuid1() or similar). 
comment
Any opinion on the best fix for this?  I like the uuid approach for the simplicity in implementation but it would lead to some pretty ugly  id values.  
comment
Generating uuids seems overkill.     The issue is that when reading the GEXF file NetworkX loads the edge id value as the edge attribute 'id'.  That value obviously can be changed or other edges can be added with the same id.     When writing the networkx  GEXF implementation looks for an id attribute and if it doesn't find one starts generating integers starting at 0.  This clearly will cause conflicts.    One approach would be to ignore the id attributes that GEXF uses and just generate new ones.  That breaks identity in round-trip reading/writing though.  Another approach is to loop through all of the edges before writing to discover the existing id attributes.  Then for edges without ids generate new ones with some algorithm that don't conflict with existing ids.   This should be fixed but since we haven't decided on an approach I'm moving to networkx-2.1.  
comment
Needs design decision.
comment
Yes, we'll need this in the master branch too. 
comment
Thanks (especially for the tests).
comment
I think I have to agree that a path of length zero (and a star with no leaves) should be included.  Would you like to create a PR with this fix?
comment
Fixed in #2397
comment
I remember it being intentional to not automatically import the approximation package.    In addition to the (fixable) name conflicts I argued that this might easily confuse users.  If you don't know the context of the approximation validity (which can be a little complicated) you can easily get bad answers or maybe just plan wrong answers.
comment
I'll just come out from under the bridge again and say - I think we should consider abandoning the create_using= keywords completely.  There is an older discussion somewhere I'll try to dig up. My biggest complaint are such allowed uses as passing a digraph into a generator that wants to create a graph or vice versa, etc, and any other Latin I can think of.    I don't have a simple alternative to suggest for graphs that aren't connected.  (Connected graphs we can generate edges and use graph.add_edges_from().) 
comment
I was thinking that the graph generators would generate edges (now they are edge generators?).    That is, return a generator that you can pass to G.add_edges_from() so it would be efficient and simpler code.  That doesn't work for isolated nodes though.  Of course you could return both an edge generator and a node generator... 
comment
Yes, I haven't really thought this through.  But something like this  ``` python def generate_path(n):     nodes = iter(range(n))     edges = ((v,v+1) for v in range(n-1))     return nodes,edges  def path_graph(n):     G = nx.Graph(generate_path(n)) # <- would need to modify constructor     G.name="path_graph(%d)"%n     return G ```  It could be that some algorithms require some memory of  previous edges but I couldn't think of one right now. 
comment
You'd also need a  `G.add_nodes_from(H.nodes())` there.  Should we consider G.add_graph() that takes a graph or a tuple of (nodes,edges)? 
comment
I'd like to pursue this .  It might make it simpler and clearer.  Anyone have more thoughts?  The current strawman in something like @ysitu suggests for G.update(H) or G.update_graph(H) perhaps with the ability to take a graph or a (nodes,edges) tuple.   
comment
Deferred, needs design decisions.
comment
Thanks for this.  I couldn't get your code to work for me  ``` python $ python -c "import networkx as nx; G = nx.Graph([(1,2)]); nx.draw(G)" Traceback (most recent call last):   File "<string>", line 1, in <module>   File "/mnt/hgfs/aric/Software/hagberg-networkx/networkx/drawing/nx_pylab.py", line 133, in draw     draw_networkx(G, pos=pos, ax=ax, **kwds)   File "/mnt/hgfs/aric/Software/hagberg-networkx/networkx/drawing/nx_pylab.py", line 272, in draw_networkx     node_collection = draw_networkx_nodes(G, pos, **kwds)   File "/mnt/hgfs/aric/Software/hagberg-networkx/networkx/drawing/nx_pylab.py", line 416, in draw_networkx_nodes     position=positions[ii], IndexError: index 2 is out of bounds for axis 0 with size 2 ``` 
comment
I think I understand what you have done with the scaling transforms.  Zooming in with this code makes everything uniformly larger.  But I actually prefer the way it currently works - when you zoom in the objects stay the same size and the window frame range gets smaller.  It is harder to get the arrows drawn correctly that way though.   
comment
It's definitely possible to design something that scales dynamically - we have that now.  It isn't so easy to keep the nodes and edges from overlapping or crossing but that is more a layout issue than a drawing issue.  Last time I checked in with the matplotlib developers (about a year ago) there were some missing pieces to make it easier to draw arrows (I forget the details, related to arrow collections and transforms).  You can even make curved edges, edges that connect to the nearest polygon corner, etc.   
comment
I'm not an expert in matplotlib or graph drawing.  I have learned that drawing graphs is tricky and that there are many different approaches and goals for drawings.  I doubt that any one system will satisfy everyone or even most.  That is one of the reasons I have proposed moving the matplotlib drawing code out of the networkx library.  That reason isn't stated in #1325 where there are some other thoughts and ideas but doing so would allow multiple drawing systems with different approaches for different goals.  I'd also like to see the design of the current drawing package redone to be simpler to use and to understand.    The reason I bring this up here is that this PR, though it doesn't modify the drawing API much,  substantially changes the output and might be better as a completely separate drawing function, e.g. aimed more at publication-quality small graph drawings than interactive exploration of the data. 
comment
Yes, there must be something wrong in the implementation of `strongly_connected_components`.   The recursive version of Tarjan's algorithm `strongly_connected_components_recursive` and `kosaraju_strongly_connected_components` both show (using your code) linear scaling in num_nodes + num_edges.    Where is the bug?  Thanks for the simple example that shows the issue.
comment
Re: **author** . Yes those were the days before git and github and use of this variable is optional. 
comment
Thanks for tracking this down.   I agree with you that it is reasonable to normalize by the potential number of paths.  This should be a pretty easy fix.  
comment
Yes, go for it.
comment
Looks reasonable.   Perhaps we should add a test  so we can verify that this does manage to read files with extra whitespace like  ``` graph   [   directed 0   node   [     id 3     label "a"   ]   node   [     id 8     label "b"   ]   edge   [     source 8     target 3   ] ] ``` 
comment
I don't have an opinion on which way is better.    For testing I was suggesting adding a test function to test_gml.py to cover this case.    
comment
It's a reasonable idea.  A couple thoughts - - The dependence on Numpy is all  (or mostly) through "linear algebra" approaches to graph problems so this would add a general dependency in other places. - Without making it overly complicated would it be possible to still use the built-in Python random generator?   That would be nice for e.g. pypy or other cases when Numpy is unavailable. 
comment
Yes, I missed that. Let's add an alias for now anyway. 
comment
Yes, let's resolve the merge conflicts and then it is OK. Also we need to document these changes in the 2.0 api doc. 
comment
OK - We'll get to #1693 soon. 
comment
Needs tests.
comment
Perhaps time to deprecate G.name?  
comment
How many places do we need to consider this?  Could we switch to *args processing instead? It would make the function signature less nice. 
comment
This looks very interesting.  Thanks for the contribution.   It makes sense to put it in `network.algorithm.cycles` as @jfinkels suggests.  It looks like the test failure is due to some travis issue and not this code - @chebee7i it is suggesting we update from the "legacy infrastructure" http://docs.travis-ci.com/user/migrating-from-legacy/?utm_source=legacy-notice&utm_medium=banner&utm_campaign=legacy-upgrade 
comment
Worth revisiting now with networkx-2.0 subgraphs.  @dschult 
comment
I agree with @dschult here.  It makes sense to remove the connected_component_subgraphs methods to make the user explicitly decide on copy/no copy.  We can add subgraphs as an example.  
comment
Can you advise on the JIT JSON format for directed graphs and/or provide a fix?
comment
See #2753
comment
Thanks, that is a good start.  Can you figure out how to get the arrows to scale correctly with node size?  For circular nodes this shouldn't be too complicated.  For other shapes this will be harder since you probably want to find the closest polygon vertex and route the arrow there.    ``` G = nx.Graph([(1,2)]) nx.draw(G,node_size=10) nx.draw(G,node_size=1000) ````  ![figure_1](https://user-images.githubusercontent.com/187875/32992533-0961d596-cd09-11e7-86db-25bdd7ecb445.png) ![fig2](https://user-images.githubusercontent.com/187875/32992536-0f8a994e-cd09-11e7-8dbe-59f3dc85be51.png)    
comment
That looks good.  I agree the general marker case is a little tricky.  One thing that I would expect users to want to do is to adjust the arrow sizes.  At minimum maybe setting the mutation_scale?    
comment
Also see #2163 for arrow colors fixes
comment
And #1199 for alpha-values setting
comment
This is a really great contribution. I suggest calling it good for this PR with maybe the addition of making sure our desire for arrow collections to enable this more efficiently is noted over at the matplotlib site.
comment
As you pointed out those errors look like they come from a change in output text format for numpy arrays and matrices.  Your PR is good - thanks! 
comment
Yeah, we've been there before (2010) (adding kwds) https://github.com/networkx/networkx/commit/0a8d31dfe027f8ea0edd24026f458ed9d9161b3f#diff-6acdfd281af96e48405fe5f51cad122f (then removing) https://github.com/networkx/networkx/commit/3843ffe3b428344ae2f173217f9ba3057ae581b4#diff-6acdfd281af96e48405fe5f51cad122f  If there is a particular keyword/s we might add to help you I think that is the safest approach. 
comment
Yes, *kwds should be removed in `draw_networkx_nodes` too. Let's add `edge_color` as a keyword.   Since the `draw_networkx_nodes` function returns the node collection you can set the edge color that way too ```python In [1]: import networkx as nx  In [2]: G = nx.Graph()  In [3]: G.add_edge(1,2)  In [4]: pos = nx.spring_layout(G)  In [5]: n = nx.draw_networkx_nodes(G,pos)  In [6]: n.set_edgecolor('k') ```
comment
How about putting this in `networkx/generators/mycielski.py` as two functions `mycielski_graph(n)` and `mycielskian(graph, iterations=1)`? I'm suggesting that because while it can be viewed as a graph operator, the implementation in networkx will need to return a new graph with different (and new) labels.  Also it is essentially a way of generating higher chromatic number triangle-free graphs.  `mycielski_graph(n)` would just call `mycielskian(singleton_graph, iterations=n)`  where singleton_graph is just the graph with one node labeled with the integer '0'. 
comment
I would prefer the mycielskian not be in unary.py since it is fairly specialized.  It doesn't operate (correctly anyway) on graphs with triangles and requires relabeling of nodes and addition of extra nodes.   
comment
NetworkX is so old it should have a Usenet newsgroup. 
comment
It's a known issue that has come up in other parts of networkx #1582. So far we have only made a few changes to address the name clashes.  There is really no great solution since you either have to pick "obscure" keyword argument names or live with potential clashes.  
comment
I'd like to include this if you are willing to make a pull request. 
comment
Yes :-)  Both would be great to include.  On Sat, Jan 18, 2014 at 9:32 AM, chebee7i notifications@github.com wrote:  > @hagberg https://github.com/hagberg are you referring to the currently > unimplemented triangulation code or the code that appears in this gist: > https://gist.github.com/chebee7i/7509453 ? >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/1012#issuecomment-32685889 > . 
comment
Closed due to inactivity.
comment
We are close to a 2.0 beta release (later this week) so if you update before then we can include it in the beta.  Else we'll put it in when it is ready and mark it for 2.1.
comment
Great, we'll finish up when you get back to your computer.
comment
Thanks Andrey for your patches.  We're still adjusting to the workflow after switching to Github (and don't have it documented yet).   
comment
I'd like to add code for shortest paths with restrictions, but I don't like the idea of putting node/edge filtering code into many different algorithms.   It will make those algorithms harder to use, test, and understand.    What is the best alternative to that?  Clearly you can modify the graph before calling the existing functions.  Are there clean and efficient ways to engineer a subclass or context class to do this? 
comment
I'm not concerned that this might not work with "frozen graphs" - we don't guarantee that you can "freeze" a graph and use it with any particular algorithm.  Can you help us figure out how to implement a context class approach?   Thanks for your help on this - I have scheduled it for the networkx-1.8 release (soon) since I think it is a great feature and would like to include it as soon as we can. 
comment
Ah, I see.  Here is a prototype version that allows for filtering (hiding?) nodes and edges.  https://gist.github.com/4566525 It won't work perfectly for multigraphs but I think it can be modified for that. Is this the right way to go? 
comment
Hmm.  This probably isn't a good idea - but how about monkey patching with a context manager? Much faster - e.g. (partial implementation) https://gist.github.com/4591198 
comment
I'd prefer to keep those classes simple.  (In fact I'd consider removing some methods).    If we made a new class (or classes) I'd suggest creating hide/unhide methods. 
comment
Let's move forward then with the monkey-patch approach to implement the restricted shortest-path algorithms.   I'd suggest we keep the monkey-patch context manager thing localized to just these algorithms until we get more experience with the pros and cons of this kind of hackery. 
comment
I rescheduled this for networkx-1.9 release since I'd like to move forward now with the overdue networkx-1.8.  If anyone wants to create a PR ASAP we can still consider it for networkx-1.8. 
comment
Thanks,  that looks like a very good start. We can't include GPL licensed code in NetworkX.  If you would be willing to license it with BSD or other BSD compatible license that would work.   
comment
Looks like the code is here: https://github.com/kakila/cycle_space Needs a PR attached to this issue. 
comment
Let's return a set of edges for `max_weight_matching` to make them the same.
comment
We could at least fix the (narrow) issue posed here with the suggested ```python def to_agraph(N, node_attr=None):     ...     # add nodes     for n,nodedata in N.nodes(data=True):         A.add_node(n,**{k: v for k,v in nodedata.values() if node_attr is None or k in node_attr})    ```
comment
Closed due to inactivity.
comment
Is this still needed? 
comment
Suggest leaving this as is unless a good reason to change is brought.
comment
That code is patches on hacks and really does need a cleaner overall approach (and proper arrow drawing).  But maybe another patch like you suggest is the best approach at this point. 
comment
We can add this feature to #2760 which is making "proper" arrows.
comment
OK.  Needs a PR. 
comment
Yes, I see that.  Moved.
comment
I looked at the algorithm as coded.  The reason why you cant set `tau2=1` is that it is an exponent of a powerlaw as in x^-tau2 so you the value 1 doesn't make sense.  The paper does say it uses the value 1 in some experiments (\beta=1 in the paper) it must be using a cutoff or some non-powerlaw approximation.  I suggest we leave this as it stands unless someone can come by and re-open this with a detailed description of the model.
comment
The calling argument file= is wrong as you say and should be replaced with filename=  I don't think we want to read the file into memory before handing off to pygraphviz (AGraph) if we don't need to.  The typical case is reading from a file into a graph data structure and it is more efficient to not read the data into memory first.   
comment
Sure that could be done.  But note that when pygraphviz loads a string (the `from_string` method) a temporary file is written and then read back in using the graphivz c library.  So you proposal would end up reading the file data twice (and writing it once).  This is a limitation of pygraphviz that could be fixed.  
comment
This is really a pygraphviz issue.  If pygraphviz accepted file-like objects than this could be implemented.
comment
Yep, that looks like the right fix.
comment
PR is #2736 
comment
It's true that the graph6/sparse6 format it technically bytes.  They are only allowed in the range of printable ascii characters though so I don't mind using text (strings).    The reading and writing of text-based data could use some engineering update.  I'm not sure if this is the best place to start though.  It is useful to be able to generate strings of data without having to learn or remember how to use StringIO so I support the idea of keeping a pure text based generator. 
comment
I agree it should be bytes.  Doesn't matter to me if it is the same or different PR.  A long time ago (#643) I was proposing restructuring the read/write system.  I don't remember exactly why I was suggesting to do it that way - is that a good idea?  Your naming idea make me think of that old issue. 
comment
That sounds like a good plan.  It would be easy to implement a spelling change of e.g. to_graph6_string as graph6.to_string if we later decided that was better. 
comment
The intent of all_simple_paths is to produce paths without cycles. The function doesn't check if the source = target which can produce cycles in that special case (that probably is a bug).  Is that the issue you are asking about?
comment
This is a simple fix - if the user asks for source == target just return an empty list.
comment
Yep, that is a bug.  Thanks for the report.   This is a simple fix.
comment
It's tedious to review because 1) All of the pep-8 changes create diff noise 2) The tests are also changed so everything needs to be inspected more carefully to make sure the tests are still OK 
comment
It most certainly is pythonic to use dict(). It was the only way to do it before Python 2.7. So let's not get so hung up on "code cleanup." 
comment
That would be a great addition the the networkx/matplotlib drawing package.  It is technically possible but so far there have been no contributions that solve the general arrow drawing problem.  
comment
See #2760 
comment
See #782.
comment
This is a good start.  Could it be rewritten to allow 2d or 3d layouts?  
comment
In #2514. 
comment
Closed due to inactivity.
comment
Not sure - but my understanding that this is a hyperopt issue.
comment
Included with other PR
comment
Closed due to inactivity.
comment
This is really just syntax and can wait for future release. 
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Can anyone suggest an algorithm to implement? 
comment
Closed due to inactivity.
comment
As far as I can tell Matplotlib (which is doing the drawing) doesn't provide a way for collections such as lines or shapes to have varying alpha values.  You can draw and set individual nodes and edges as a work-around. 
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
I'd like to see those algorithms get implemented.  It would be a good start to @jfinkels program to make efficient isomorphism checkers 
comment
See #1811
comment
There is discussion over at #1878 that should be here. 
comment
Copyright and authorship are different things.  So they could be the same or not. Since we haven't been very strict about these things I wouldn't assume anything regarding either. For example there are certainly some places where my name is on the copyright but I didn't write the code.    I suggest something like If there is an `__author__`  variable change it to an author comment. If there is no author then there is no author other than we can determine from commits. If there is an author with no copyright we might consider asking the author to be more clear about copyright intent. 
comment
Closed due to inactivity.
comment
Now that @jarrodmillman has shown us the light in how to generate multi-format output with the Sphinx tooling I think we should do that (basically the status quo).    I'm not opposed to having other notebooks (separate repository) but I can see the significant advantages of keeping everything in sync using this approach.  
comment
I think we agreed on keeping the examples as is.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Self loops are definitely included in the current flow betweenness centrality calculation.  Whether they are included _correctly_ or not is a good question.  You can easily remove self-loops with `G.remove_edges_from(G.selfloop_edges())` where `G` is your networkx graph object. 
comment
Closed due to inactivity.
comment
Is there something not available with sklearn that you are thinking of?  Else I suggest we use the ML functionality in sklearn.
comment
We could make sklearn an optional dependency (if you want to use this feature). 
comment
Seems like a bug somewhere in the vicinity of where you suggest.   Another report which could be the same issue http://stackoverflow.com/questions/43382204/networkx-cant-calculate-algebraic-connectivity/43383130#43383130
comment
See #2681
comment
See #2712 
comment
Needs test.
comment
I vote for removing the max_weight code in favor of letting the user scale the weights.
comment
We'll need more details to find where this issue is.  A small example would be helpful.
comment
The `attr_dict` argument is no longer used.  Use keywords for networkx-2.0 instead `graph.add_edge(1,2,key=0,time=5)`  ```python In [13]: import networkx     ...: graph = networkx.MultiDiGraph()     ...: graph.add_nodes_from(range(1,2))     ...: graph.add_edge(1,2,key=0,time=5)     ...: print(networkx.shortest_path_length(graph, 1, 2, weight="time"))     ...:  5 ```    
comment
It's correct for the definition of subgraph isomorphism that NetworkX uses in this case.   My understanding is that the terminology is fairly confusing - the NetworkX VF2 algorithm is for node-induced subgraphs so in your case the node-induced subgraph on nodes [1,2,3] will always contain the edge (1,3) for H.  There is discussion of this in the networkx documentation at https://networkx.github.io/documentation/stable/reference/algorithms/isomorphism.vf2.html#subgraph-isomorphism  There is some further discussion on the terminology differences in SO at https://stackoverflow.com/questions/459799/whats-the-difference-between-subgraph-isomorphism-and-subgraph-monomorphism
comment
It's not so simple to do this nicely.  You are welcome to work on an implementation.  The long-term plans are to rewrite the matplotlib graph drawing in a separate module that could include self-loop drawing and many other features like parallel edge drawing, interactive adjustment of layouts, etc but so far nobody has taken that on. 
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Looks nifty.  Why not make a PR so we can play with it? 
comment
Feature in network-2.0
comment
I don't mind leaving many of these algorithms as is.  There are surely some though that could benefit from becoming iterators (e.g. traversal algorithms).   If we want to change the type of return on algorithms I suggest we do it one at a time so we can keep track of errors/changes.   Some algorithms might require significant changes to make them iterators. For others it won't be worth doing at all since as @dshult mentions we already have to build a list or other data structure.     
comment
Closed due to inactivity.
comment
It would be nice to have some other ideas for logos.  They don't have to be literal (graphs) or based on the text in "networkx".    e.g. https://github.com/logos 
comment
The X just stands for X.  Like Planet X. 
comment
I was hoping somebody might come up with something a little less literal.  If we want to use this logo I suggest separating the NX symbol part from the text and use transparent background (no background).  Then create two versions of the logo - 1) just the symbol and 2) the symbol with the letters "networkx" following it on the right.  Just like the Python logo... 
comment
Hi @lamprini-koutsokera  - NetworkX is primarily about data structures and algorithms for graphs.  Are you proposing implementing an algorithm to for solutions to the CTP? 
comment
I see that algorithm mentioned here http://erikdemaine.org/papers/CanadianTraveler_ICALP2014/paper.pdf along with a greedy algorithm.   (It's an interesting paper too).  If you want to implement that algorithm we will consider it for inclusion in networkx.  Take a look at some examples of existing algorithms to understand how to format things and ask here for help. 
comment
See #2153
comment
I think `edge_dfs` was written to specifically handle multigraphs and also to avoid overhead of reversing the graph if you needed that.    For your purposes could you use bfs_edges?  ``` python In [10]: G = nx.DiGraph([(0, 1), (0, 2), (1, 3), (2, 3)])  In [11]: list(nx.bfs_edges(G,0)) Out[11]: [(0, 1), (0, 2), (1, 3)] ```  You probably don't want that (2,3) edge if you need a tree. 
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Graph visualization is hard.  Especially in that there are many different objectives when making a drawing  (good labels, specific layout, large number of nodes or edges, etc).   It's would be pretty difficult  to make a single tool that can satisfy even a large subset of those.     Since NetworkX is primarily for graph data structures and algorithms I have been generally advocating against including special purpose graph drawing or even graph drawing in general.  The current matplotlib drawing was mostly written 10 years ago (!) and is obviously a hack. I've been avoiding improving it since I'd prefer it to be a separate (compatible) package.   I'd prefer separate packages for other drawing tools as well.    I do think there is a place in NetworkX for algorithms that position nodes (and maybe edges) for use in drawing algorithms.  The current set of those (mostly in layout.py) is pretty slim.  And as @rainwoodman points out they could certainly be improved and would potentially useful to any graph drawing package such as an svg tool. 
comment
Any progress on a separate package to do layout and SVG output? 
comment
Closed due to inactivity.
comment
PR in #2052 
comment
It's fairly straightforward to find the stationary distribution of a Markov chain using e.g. Numpy to get the eigenvector:  ``` python import networkx as nx import numpy as np #A = np.array([[1.0, 1.0, 1.0], #              [1.0, 2.0, 1.0], #              [1.0, 2.0, 3.0]])  edges = [(0, 0, {'weight': 1.0}),          (0, 1, {'weight': 1.0}),          (0, 2, {'weight': 1.0}),          (1, 0, {'weight': 1.0}),          (1, 1, {'weight': 2.0}),          (1, 2, {'weight': 1.0}),          (2, 0, {'weight': 1.0}),          (2, 1, {'weight': 2.0}),          (2, 2, {'weight': 3.0})] G = nx.DiGraph(edges) P = nx.to_numpy_matrix(nx.stochastic_graph(G)) # transition matrix eigenvalues,eigenvectors = np.linalg.eig(P.T) one = np.argmax(eigenvalues) # index of eigenvalue=1 pi = eigenvectors[:,one]/eigenvectors[:,one].sum() # normalize print(pi) ``` 
comment
Yes, the eigensolver in numpy is not scalable for large problems.  But we could use the ARPACK wrapper from scipy or even the power method for many cases (like we do to compute PageRank, etc).    Are there other (non linear algebra) approaches to these problems that you are thinking of? Maybe I am not understanding what you are proposing to build? 
comment
Closed due to inactivity.
comment
Needs 2 PRs.  One for the general problem and one for DAGs.
comment
I don't see why that would obviously be faster (at least with Python3) in cases where we aren't generating a list of items to pass to dict().  It's less characters to type. 
comment
Wow, nice.  That is better.  Thanks for posting that. 
comment
Bumped the PR #1700 which needs conflict merge.
comment
I don't recall exactly why we did it that way.  Perhaps it was because we were thinking of eventually making a more sophisticated edge object (other than tuples) and didn't want to make a more complicated interface until we worked that out. 
comment
I didn't find very many places where this occurs, function.py, graphmatrix.py, binary.py - so maybe this isn't an important thing to fix.  It looks like it would be a bad idea (or at least require some more code) to make the three-tuple for graphs return (u,v,0) in binary.py 
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
METIS is truly awesome software.  There are some Python wrappers already available but I haven't tried them.  Is the idea to connect to one of those?   If by 'integrating' you mean to include the C source or other non-Python wrappers then I think it would be better as a separate package. 
comment
What is the advantage of "optional components" vs a completely separate and compatible package? 
comment
Closed due to inactivity.
comment
Maybe not so simple, but efficient.   @article{Epp-SJC-98,   title = {{Finding the $k$ shortest paths}},   author = {David Eppstein},   journal = {SIAM J. Computing},   publisher = {SIAM},   volume = {28},   number = {2},   pages = {652--673},   year = {1998},   url = {http://dx.doi.org/10.1137/S0097539795290477},   review = {MR-99h:05073}}  http://www.ics.uci.edu/~eppstein/pubs/Epp-SJC-98.pdf 
comment
Added in `shortest_simple_paths`.
comment
I'm not sure I like the silent "failure" that this fix would generate.  Say you think you have a node label 1 when you really have instead the string '1'.  You set labels ={1:'a'} but it never shows up....and there is no error.    I'd prefer a better interface where e.g. you can apply labels by setting them explicitly as node attributes. 
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Here are a few comments - it's pretty easy to get "off in the weeds" with this discussion (we've done that before..).  But we always agree that we should do something... - I agree with @dschult's comments above on technical issues.  We have tried some matrix implementations before which I can link to here if that helps with the discussion.  Our previous discussions often lead to the desire to use the (de-facto) NetworkX API, or something similar, as an organizing interface.   Then algorithms could (mostly?) be written generically and benefit from alternate "backends" (data-store, currently dictionary-of-dictionaries) such as  a numpy array or scipy sparse matrix, a graph database, SQL, custom C graph code, etc.  depending on performance (see below) needs.    - I hope I didn't discourage @jakevdp from contributing code.  I like what he and others have done in SciPy and I consider NetworkX to be part of the SciPy community.  Better interaction/interoperability with scipy and other tools would be a big plus for me.  It's true we don't have any Cython code in NetworkX but we have experimented with algorithms with good success.   - Performance means different things to different people.    I know of at least three that come up: speed, memory, and human effort.  I think many of us like Python because it makes us feel like super-human programmers and NetworkX has obviously tried harder to optimize personal productivity over other things.  We have been pleasantly surprised with the good speed performance we get from just using plain-old Python dictionaries.   
comment
To address @bjedwards comment about scipy and numpy (which apparently I just deleted?, sorry, good content, wrong button)...  There already is a large `numpy` and `scipy` dependency in NetworkX.  But right now they are not required.  
comment
Here is the scipy sparse NetworkX graph class.   https://github.com/hagberg/networkx/tree/scipy-sparse It looks like it has been a couple years since I thought about that so no guarantees any of it works (or ever worked correctly).   
comment
Closed due to inactivity.
comment
Closed due to inactivity,
comment
If I remember right the good algorithms are tricky.  Here is a Python interface to John Boyer's planarity package: https://github.com/hagberg/planarity/ 
comment
Looks like the boost planarity tester is calling the Boyer-Myrvold code like https://github.com/hagberg/planarity/ does. 
comment
Functionality available in https://github.com/hagberg/planarity/ 
comment
I'm not sure this is a such a good idea.  Maybe I haven't through through all of the implications but it feels like we are suggesting a more database-like approach to an API and not one that is, say, familiar to a graph theorist or network scientist.  At some point it just comes down to how we want the user to be able to interact with the data structure.   
comment
I remember that discussion.  I think my biggest concern is keeping the interface simple and focussed on on graph theory/network science.    So it doesn't really matter if it is a method or a function if in the end we write algorithms using that interface and it becomes "less graph-like" and more "database-like". 
comment
Closed due to inactivity.
comment
It might not be to hard to add the vector_\* types to the gml reader.    The reader code is already a little ugly though (partially from adding support for another GraphML extension used by yEd). 
comment
I was looking in the decode_data_elements() methods.  That is where the data gets parsed. 
comment
Hi Tiago (@count0), thanks for checking in here.  Your approach to the GraphML types is reasonable and we could try to support that.  Handling the float types might not be that hard.  I think in Python you can just use the hex() method  ``` In [1]: a=42.0  In [2]: a.hex() Out[2]: '0x1.5000000000000p+5'  In [3]: a.fromhex(a.hex()) Out[3]: 42.0 ``` 
comment
See PR #845  
comment
Closed due to inactivity.
comment
I agree that it is annoying to handle special cases for Graph/MultiGraph differences.   A general solution would be to make a custom edge object but we have rejected that in the past since the performance is then up to 10 times slower for many algorithms.  Is there a specific use case that you have that might help us understand if this is fixable in our current code base? 
comment
On Thu, Feb 28, 2013 at 10:39 AM, jamiefolson notifications@github.comwrote:  > I was thinking of scenarios like betweeenness centrality's shortest paths. > The main shortest path implementation allows for multigraphs, but requires > explicitly checking for multigraphs inside the main loop. >  > That is a good example.  In that case I don't think there is a standard > definition - so it could make sense to count the weights differently (sum, > min, max) for multiple edges between two nodes.  It might be simplest to > require that the input be a graph.  I quite liked the idea of adding an optional argument to neighbors to  > return (neighbor, edgedata) pairs. I noticied there hadn't been any > comments on that thread in some time. Was that idea rejected? >  > It was put on hold.  I didn't go back and read our discussion but I thought > it was basically two points. > 1) We already essentially have that functionality with edges(neighbor, > data=True) > 2) It is a lot slower and thus decreases performance substantially in inner > loops like Dijkstra's algorithm >  >  — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/852#issuecomment-14245948 > .  ##   Aric Hagberg Los Alamos National Laboratory math.lanl.gov/~hagberg 
comment
I think the performance issue was just the basic fact that using a neighbors() method (or any method) is slower than accessing the internal dictionary structure directly.   
comment
Closed due to inactivity.
comment
Yes, I think this is still waiting for some code.  If you want to contribute that would be great. There are many algorithms in NetworkX that use can weights (e..g shortest paths, pagerank, betweenness, minimum spanning tree, etc). But I'm sure there are more that could be added. 
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.
comment
Closed due to inactivity.,
comment
See #764
comment
Closed due to inactivity.
comment
Closed due to inactivity
comment
See #1536 
comment
I think the code was removed in this commit https://github.com/networkx/networkx/commit/43150a06b0b5a7a7fb6ade7e060e883f882643b0 I'm not enthusiastic about adding a class to do this.  But it could make a decent example of subclassing the Graph class to add additional functionality. 
comment
True, but you could do that without a special class too.   Finding components isn't the expensive part of that algorithm. 
comment
Closed due to inactivity
comment
See #2757 
comment
See #2757 
comment
Closed due to inactivity.
comment
See #1523
comment
Closed due to inactivity.
comment
Implemented in networkx-2.0
comment
See #1737
comment
Closed due to inactivity.
comment
See #951
comment
Closed due to inactivity.
comment
Closing this issue due to inactivity.  Open new issues for clustering algorithms.
comment
Yeah, we could do that.  I know we have discussed this before somewhere - if I remember right the primary concern is that for larger graphs the isomorphism algorithm might have very long run times.   In that case it is not really appropriate for an equality test - or at least might be surprising to many users.
comment
I suggest we leave it `__eq__` unassigned because of complexity of the algorithm and the implementation.   We'd probably have to consider the graph, node, and edge data associated with the graphs too.
comment
`simple_cycles` is a generator so you can stop the iteration after a certain number of cycles.  One way to do it is with islice  ```python In [1]: import networkx as nx  In [2]: G = nx.DiGraph([(0, 0), (0, 1), (0, 2), (1, 2), (2, 0), (2, 1), (2, 2)])    ...:   In [3]: list(nx.simple_cycles(G)) Out[3]: [[0, 2], [0, 1, 2], [0], [1, 2], [2]]  In [4]: from itertools import islice  In [5]: list(islice(nx.simple_cycles(G),3)) Out[5]: [[0, 2], [0, 1, 2], [0]] ``` 
comment
There are currently no open issues to add this.  Would you be interested in contributing the algorithm?  
comment
If you are able to create a draft implementation we can help you integrate it into networkx. Take a look at https://networkx.github.io/documentation/stable/developer/index.html for information on contributing code to the package. 
comment
The ordering of edges in for NetworkX Graphs is arbitrary.  The edge has no direction so either (u,v) or (v,u) represents the same edge.  If you want ordered edges then the correct NetworkX graph class is DiGraph (or MultiDiGraph if you want parallel edges).  
comment
Thanks for the report.   The pygraphviz/agraph interface uses the graphviz libraries directly (with python wrappers) and pydot is a pure-python implementation of the dot format data and output.  Fortunately those graphs are both isomorphic.  Unfortunately the text is different.    It may be possible to adjust this in the pydot package - you might consider asking there.  
comment
NetworkX is a pure-python library.  It does take advantage of other libraries for algorithms (though not for data structures) for calculations such as dense and sparse matrix algebra.    There is no "C interface" to the NetworkX library but the other way around - interfaces to existing C libraries which are typically provided by other Python packages.
comment
Use  `from networkx.drawing.nx_agraph import read_dot` if you have `pygraphviz` installed or  `from networkx.drawing.nx_pydot import read_dot` if you have  `pydot` installed.
comment
duplicate #1556
comment
It's likely just following the spec here http://www.fim.uni-passau.de/fileadmin/files/lehrstuhl/brandenburg/projekte/gml/gml-technical-report.pdf which says ``Key ::= [ a-z A-Z ] [ a-z A-Z 0-9 ]`` 
comment
This timing statement isn't running the complete task `timeit.timeit('m = float(max(d.get("weight", 1.0) for u,v,d in edges))', setup=setup, number = 5)`  In that case `edges` in an iterator that gets exhausted during the first loop of the test.  For the next four loops it is empty.   Those loops will run faster but give the wrong (no) result.  
comment
When we moved to RTD (from hosting on github) the idea was that it would be simpler engineering than building and pushing docs to github.   If there is a CI approach that is reasonable to maintain that seems like a good path forward.  
comment
This may be hard to address in NetworkX.  Is there a version of the decorator module that works with IronPython? 
comment
That seems like a reasonable idea.  
comment
Do you want just the edge data?  Then `G.get_edge_data(1,2)` does that.  Note also https://github.com/networkx/networkx/issues/2630 on `G.edge`.
comment
Maybe I'm not understanding your exact use case.  But you definitely need to calculate the correct shortest path lengths (and those could change if removing edges.  e.g. like your example slightly modified)  ```python import networkx as nx graph = nx.cycle_graph(5) paths= dict(nx.all_pairs_dijkstra_path_length(graph)) graph.remove_edge(3,4) for component in nx.connected_component_subgraphs(graph):     print(nx.eccentricity(component, sp=paths))     print(nx.eccentricity(component)) #{0: 2, 1: 2, 2: 2, 3: 2, 4: 2} #{0: 3, 1: 2, 2: 3, 3: 4, 4: 4} ```  The eccentricity code is very simple so if you have a way to quickly update the shortest paths in your graph perhaps you could just update the eccentricities that change?
comment
So you have a special use case (a tree) where the path lengths don't change.    Here is a way (untested) you could compute eccentricity on trees without recomputing the paths.   ```python import networkx as nx graph = nx.path_graph(5) paths= dict(nx.all_pairs_dijkstra_path_length(graph)) graph.remove_edge(3,4) for component in nx.connected_component_subgraphs(graph):     print(nx.eccentricity(component))  # compute sp every time     print({u: max(d for v,d in paths[u].items() if v in component) for u in component}) ```
comment
See  https://github.com/pygraphviz/pygraphviz/issues/133 for follow-up.
comment
It looks like that the documentation is unclear about this in other places too.   
comment
I like the idea of explicit separate functions. 
comment
I agree with the changes.  I do think it is wise to consider keeping (maybe removing and re-adding later) the pypy and ipy testing.  Those implementations found a lot of output ordering issues with our tests since the dictionary implementations are better.  I think they could have even uncovered a bug or two in some algorithms. 
comment
It's certainly possibly to check `max(nx.adjacency_spectrum(G))` to estimate appropriate values for alpha.   I think we must not be doing that since it adds extra computation - maybe roughly doubles the computation time.   A middle ground would be to put some information in the docs on how to compute the largest eigenvalue. 
comment
Needs a PR to add documentation on how to compute the required eigenvalue.
comment
Can you expand on the change that makes this break?
comment
I'd say that is unintended.  The DiGraph.subgraph function intentionally uses a shortcut in assigning the successors and predecessors which can permute the order of predecessors (as you have found).   This implementation of subgraph is slower but should preserve the order.  ``` python     def subgraph(self, nbunch):         """Return the subgraph induced on nodes in nbunch.          The induced subgraph of the graph contains the nodes in nbunch         and the edges between those nodes.          Parameters         ----------         nbunch : list, iterable             A container of nodes which will be iterated through once.          Returns         -------         G : Graph             A subgraph of the graph with the same edge attributes.          Notes         -----         The graph, edge or node attributes just point to the original graph.         So changes to the node or edge structure will not be reflected in         the original graph while changes to the attributes will.          To create a subgraph with its own copy of the edge/node attributes use:         nx.Graph(G.subgraph(nbunch))          If edge attributes are containers, a deep copy can be obtained using:         G.subgraph(nbunch).copy()          For an inplace reduction of a graph to a subgraph you can remove nodes:         G.remove_nodes_from([ n in G if n not in set(nbunch)])          Examples         --------         >>> G = nx.Graph()   # or DiGraph, MultiGraph, MultiDiGraph, etc         >>> G.add_path([0,1,2,3])         >>> H = G.subgraph([0,1,2])         >>> list(H.edges())         [(0, 1), (1, 2)]         """         bunch = self.nbunch_iter(nbunch)         # create new graph and copy subgraph into it         H = self.__class__()         # copy node and attribute dictionaries         for n in bunch:             H.node[n]=self.node[n]         # namespace shortcuts for speed         H_succ=H.succ         H_pred=H.pred         self_succ=self.succ         self_pred=self.pred         # add nodes         for n in H:             H_succ[n]=H.adjlist_dict_factory()             H_pred[n]=H.adjlist_dict_factory()         # add successors         for u in H_succ:             Hnbrs=H_succ[u]             for v,datadict in self_succ[u].items():                 if v in H_succ:                     Hnbrs[v]=datadict         # add predecessors         for u in H_pred:             Hnbrs=H_pred[u]             for v,datadict in self_pred[u].items():                 if v in H_pred:                     Hnbrs[v]=datadict         H.graph=self.graph         return H ``` 
comment
Comments on adding this as a fix or a more efficient solution? 
comment
Try adding it and see if it works for you.  We'll figure out how to proceed with a fix with the code and/or docs. 
comment
If you want to help you could check out the behavior of `copy`, `to_undirected` and `reversed`.  Those might be out of scope for your interest but we'll need to figure that out before we decide on how to fix this. 
comment
Take a look at our earlier discussion and give your opinion on what we should do https://github.com/networkx/networkx/issues/2126   On Fri, Mar 17, 2017 at 6:37 AM Ulf Aslak <notifications@github.com> wrote:  > I concur this appears to be a pretty serious bug. The normalization term > should be inverted or divided onto closeness_centrality[n]. > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/2391#issuecomment-287343078>, > or mute the thread > <https://github.com/notifications/unsubscribe-auth/AALd45cyppB-boQ-pwsFPEFcb9tHmjEQks5rmn5sgaJpZM4Mgblk> > . > 
comment
I'm going to close this.  Please add discussions to the longer thread at #2126. Let's try to get consensus on what the best (or least confusing) approach is.
comment
I'm pretty sure I'm guilty here https://github.com/networkx/networkx/commit/7df946596899468c0b114adf3fb581d830b7c051. I'm not sure what the right answer is.      
comment
Some earlier discussion  #824 https://groups.google.com/forum/?hl=en#!topic/networkx-discuss/ZfCQDrZcvYI #241 
comment
Yes, "normalized" does seem a little bit of a stretch for what this does.  Got a better name? 
comment
Or we could just write another function with a different name. 
comment
See #2391 for more confusion over normalization.
comment
This works correctly with Python3.  A Python2 workaround is to use a raw string, e.g. ```python G.add_edge(1, 3, description=r"Sample edge 1 \n with line break") ```
comment
We could special case the sort function (which will slow things down) or filter or raise an exception as @chebee7i suggests.  In general I don't like a lot of input checking in the algorithms.  What about filtering them out since those edges can't participate in any spanning tree?   
comment
Looks good - doesn't this feature belong as a (more general) subgraph view?  That is, a subgraph where you specify the edges and/or nodes you want to view.  Depending on the application it could be easier to specify inclusion or exclusion.
comment
I'd like to keep some way to attach a string label or identification to a graph e.g. to implement https://github.com/networkx/networkx/issues/1373  It could be that Graph.graph would work too. 
comment
I'm for not setting (against setting?) the defaults in the generators.   
comment
Yep.
comment
This turned out to be epic.  Thanks for seeing it through. 
comment
I restarted the test and it passed.  Not sure what happened there but definitely not your fault. 
comment
Just to clarify - I didn't suggest ignoring all doctests.  I meant that in some cases is is difficult to write clear doctests that can be tested with arbitrary ordering.  In that case we might skip the test or not write a doctest. 
comment
Yes, good catch. 
comment
Do any of the functions in https://github.com/networkx/networkx/blob/master/networkx/testing/utils.py help? (assert_nodes_equal)
comment
I'm not sure why it wasn't at the root level anyway...so yes, this is a good start.  Please do update this to reflect current practices, improve clarity, etc. 
comment
I'm not sure I know what LogGraph is.  Perhaps it was referring to https://github.com/networkx/networkx/blob/master/examples/subclass/printgraph.py ?  I think it is fine to remove the ordered graph examples as you suggest.
comment
The sphinx-gallery looks great.  +1 from me and thanks a lot for taking all of this on.  It is much appreciated.  I have to admit I agree with @jarrodmillman that examples that are stand-alone are easier (for me) to  handle with testing, integration, etc.   I assumed everyone else had just started using notebooks while I've not been paying attention.   For sure I would use Jupyter for everything if it ran inside Emacs.    So let's do both - examples in the source tree that autogenerate a gallery (and notebooks) and also a separate repository for notebooks.  In the notebook repository it will be easier to write more expository examples with references and figures and formatting, etc.
comment
Yes, merge ASAP.
comment
I'd prefer to completely remove the degree sequence generators.   Except maybe discrete_sequence which is a little nontrivial and perhaps useful.  Would that break things in many places?    The tests in kcutsets and kcomponents that use create_degree_sequence aren't very compelling.  They could be removed (or replaced) with something else. 
comment
No champion for this.  Closing for now. 
comment
See #874  
comment
This attached code might suitable but needs some tests and clarification for definitions with directed graphs. 
comment
This issue needs a PR with the proposed code - several versions are attached here. 
comment
Can't this be done in a simpler way?  Something like the following shouldn't be too inefficient  ``` python import networkx as nx  def compress_paths(G):     def remove_edge(G):         for (n,d) in  G.degree_iter():             if d == 2:                 p = G.predecessors(n)[0]                 s = G.successors(n)[0]                 G.add_edge(p,s)                 G.remove_node(n)                 return True         return False      while remove_edge(G):         pass     return G  if __name__=='__main__':     G=nx.DiGraph()     G.add_path([1,2,3,4])     G=compress_paths(G)     print "edges",G.edges() ``` 
comment
On Sun, May 27, 2012 at 1:48 PM, pengyu reply@reply.github.com wrote:  > I'm sure there is a way better than what I have already implemented. But the example you used should returns a single node `1;2;3;4`. I think that the first priority is to get the logic correct, then we can always improved the efficiency. >  > Would you please take a look at the examples that I made at https://github.com/pengyu/networkx/tree/master/tests/algorithms/compress_path so that we can be sure we are on the same page at the logic?  I guess the original problem statement wasn't well-defined.  I think I understand now that you indend to only process directed graphs and if the in-degree of the beginning of a path is zero, or the out-degree of the end of the path is zero those nodes should be removed/combined.  At any rate I think the code can be much simpler (one function).  Also could you point to some uses of this type of algorithm (e.g. in published literature)? 
comment
No consensus on problem being solved or solution. 
comment
Thanks for suggesting this.    I usually argue against including simple one-line functions.  It's not that they aren't useful it's that it makes one more function to keep track of (document, test, etc) for not much gain.   A middle ground is to add some examples that show how to write the one-liners.  For the transitive reduction function I'd like to see a pure python implementation (not using scipy).  That might be a lot more efficient.  Here are some ideas http://stackoverflow.com/questions/17078696/im-trying-to-perform-the-transitive-reduction-of-directed-graph-in-python http://www.boost.org/doc/libs/1_55_0/libs/graph/doc/transitive_closure.html 
comment
Hi - thanks for this suggestion.  Your proposal is good - go ahead and add the algorithm.  You might want to just create a new file since degree_seq.py is getting kind of large and random_graphs.py is even bigger.  Maybe joint_degree_seq.py?   
comment
The best way is to make a pull request to the github repository.  That way we can easily make comments and suggestions and help you get the code integrated smoothly into networkx.    There is some info here on how to get started https://networkx.readthedocs.org/en/stable/developer/index.html Documentation and testing are an important part of a successful contribution so consider writing that while you develop the code.  
comment
Interesting suggestion.  Similarly I was wondering if we should return something for `remove_edge()`. That would maybe address @ysitu's concerns with #1349  
comment
Yes, I think it should be a separate function. 
comment
This is excellent. But if we are sure that this isn't the right engineering solution I would advocate to include this instead in "examples" or in some other way that indicates we won't support this code in the future. 
comment
I'm not sure if that name switch is so clear since the Graph/DiGraph classes uses dicts too. Other suggestions? I'm still pretty neutral on adding this to the supported code.   
comment
Unless there are some suggestions on a fix we should close this. 
comment
How is vertex cover defined for directed graphs?  
comment
I'd still like a pointer to the definition so we can reference this correctly in the code. 
comment
Fixed in #2039 
comment
There is another version of this algorithm in  networkx/algorithms/shortest_paths/weighted.py. Does that one have better performance?   In principle we should only have one implementation.  I can't remember right now why there are two. 
comment
Probably - I haven't had time yet to review it and figure out why we have multiple versions. 
comment
How about specifying any non-standard (GraphML) types through an input parameter?  I know @count0 mentioned that as the least favorite option but it doesn't require changes to any internals of  NetworkX to handle typed properties, and could allow other arbitrary types to be added by a user.  Maybe the input could be a dictionary that provides a mapping from attribute name to a function that returns a type name and the typed data? 
comment
Thanks for chiming in here @chebee7i.  I think it makes a lot of sense in this case to figure out what the big goal is here.  Maybe some sort of direct connection between NetworkX and graph-tool is a good way to go?    And it is a good point that extending the NetworkX graphml code really might be more complicated and less clear than just writing a specific "graph-tool graphml" reader/writer.   
comment
I can see the limitations of the various graph file formats.   There is no arguing that exchanging rich graph data (not just topological structure but complex attributes) is important.  At this point I personally don't have a strong desire to design and implement yet another graph file format (YAGFF pronouced "ugh").  But I won't discourage others from doing so.  At some point it would make sense to look at the graph databases which do handle a lot of complex data.  The Gephi project team might have some good advice on this topic since they have tried to do something very similar with the GEXF format.  
comment
Needs new PR for graph-tool friendly/extended graphml reader/writer. 
comment
It looks like https://github.com/networkx/networkx/blob/master/networkx/algorithms/centrality/subgraph_alg.py#L242 need to be changed to say `expm(A.A)`
comment
Fixed in #2372 
comment
That seems fine to me.  
comment
Yes, there was discussion about pydot vs pydotplus and that was the result (pydot).  tldr here https://github.com/networkx/networkx/issues/2235 
comment
Looks like some bad hygiene there.   Those should be folded in. 
comment
Yep.  Delete.
comment
Is your suggestion a standard definition of maximal independent set for directed graphs?  It makes sense that it would be but I didn't find a definition after a little searching.  The alternative would be to return an error when the function is called with a directed graph. 
comment
Needs PR to raise exception if called with directed graph.
comment
I suggest we close (until we have a dags class or something where we can easily check for dags).  Consider putting https://github.com/networkx/networkx/pull/1118#issuecomment-41171119 into topological sort.
comment
I agree that we should make a release ASAP.    We made quite a few changes with the base graph class functions since 1.11.   I'd like to label the next release 2.0 to indicate that.    I think the short-term options are either to  1) drop some of the 2.0-tagged issues and milestones and expedite the 2.0 release 2) make a networkx-1.99 release as a 2.0 pre-release  Other ideas or comments?
comment
I agree with @dschult.  I propose we target an new networkx-2.0 release date for June.    Let's reprioritize the open issues and pull requests for 2.0 to make this doable.
comment
I don't think that helps much.  There have been many changes since 1.11 and which few do you pick and why? 
comment
So far, after more then 10 years, no logo! 
comment
Will revisit if #1411 happens.
comment
OK, whew.  248 files.  Thanks for taking this on.   Fixing documentation is hard to get to the top of anyone's list but it is important.  I'll help you get this finished.  In general this seems like it is a good idea.  There was matplotlib plot directive  when we started and sphinx was just a baby.  So some technical editing and updating is certainly in order.  And there are also some easy opportunities to make the documentation structure and layout easier for people to navigate. I see you are also working to improve that too which is great.     I would advocate to organize the documentation (eventually) as follows.  We may not want to take this all on right away but it might be good to make sure we know where we are headed. - Landing page: networkx.github.io - Landing page with pointers to everything similar to current page.  Perhaps the "Overview" info in the reference guide could go here? (Could also be updated for more modern style...) - Reference: readthedocs.github.io    (in some organizational structure we like)   - Detailed API reference, Release Log, API changes     - License, Installing, Credits, Citing, Glossary - Tutorial: I think ideally this is a Jupyter notebook hosted at Github - Examples/Gallery: Jupyter notebooks at Github - we started here https://github.com/networkx/notebooks - Developer info: https://github.com/networkx/networkx/wiki - Whatever we want to keep from what we are calling "Testing" and "Working with NetworkX source code" 
comment
On Fri, Apr 1, 2016 at 11:23 PM jfinkels notifications@github.com wrote:  >> Landing page: networkx.github.io - Landing page with pointers to >>everything similar to current page. Perhaps the "Overview" info in the >> reference guide could go here? (Could also be updated for more modern >> style...) >  > Okay, the source for the "landing page" seems to be stored in the > https://github.com/networkx/networkx-website repository, and the > generated HTML is stored in the > https://github.com/networkx/networkx.github.com repository it seems. > Since this is fairly simple and static, why not just have a single > repository at https://github.com/networkx/networkx.github.com with just > the HTML and CSS?  Yes.   We can just use one repository for that with master and source branches.  We might consider using networkx.github.io since that is the current way github is using for site naming.  >> Reference: readthedocs.github.io (in some organizational structure we >> like) >> - Detailed API reference, Release Log, API changes >> - License, Installing, Credits, Citing, Glossary >  > Makes sense. >  >> Tutorial: I think ideally this is a Jupyter notebook hosted at Github >  > So this would be in a repository like https://github.com/networkx/tutorial > ?    Sure, that seems clear.   >> Examples/Gallery: Jupyter notebooks at Github - we started here >> https://github.com/networkx/notebooks >  > Makes sense. >  >> Developer info: https://github.com/networkx/networkx/wiki - Whatever we >> want to keep from what we are calling "Testing" and "Working with NetworkX >> source code" >  > Good, I think the landing page should be able to direct people to the > right place. Just have to make sure that most links for NetworkX (the PyPI > page, the Github source repository, etc.) point to https://networkx.github.io. >  > Should I modify this pull request so that it handles only the "Reference" > part, which is the only part that will be built on readthedocs? In other > words, should I remove the overview, the tutorial, the examples, the > gallery, and the developer info? I can of course leave some explanatory > text directing the reader to the correct other website when appropriate.  Yes, that would be great. 
comment
I'd suggest we use some simple Jekyll templating of some kind and let github make the static html when we push changes.  It's a personal goal of mine to never again edit html.   
comment
Yes, the notebook examples are generally a kind of tutorial anyway.
comment
 Are there test hooks for automatically testing jupyter notebooks?   Someone must have worked that out by now.    I suggested a separate repository to reduce the amount of code burden in the library repository.  Of course that adds some complexity in versioning and testing a notebook repository as you suggest.  That could possibly be done by different people than the library developers (though practically may not be).
comment
Hi @MridulS - good idea.  I won't be in Austin this year.  I'm willing to help frame up some sprint items.  
comment
See #2054.  Let's use notebooks for examples #1754 and we can avoid this issue. 
comment
FYI http://mybinder.org/ 
comment
This is great.  I think we are pretty close to this being usable and to replace the examples and tutorial in the reference documentation.  That is what we are doing at #2054.   The next steps are to finish #2054, make the notebooks nicer, and create a new landing page  at networkx.github.io (some kind of single page, easy-to-update) that points to all of our infrastructure - pypi, github, readthedocs, notebooks, stackoverflow, ... 
comment
I'd say that is a bug.  A workaround is to use   ``` python x = unicode(json.dumps(a)) ```  for python2 (it works correctly with Python3).  The issue comes up because of the 'unicode_escape' in https://github.com/networkx/networkx/blame/master/networkx/utils/misc.py#L104  @chebee7i can you comment on why that is there and what the best solution is here? 
comment
Not sure if this requires a fix or not - @chebee7i do you have an opinion? 
comment
I suppose the right thing to do is raise an error when attempting to write data that isn't GML spec. 
comment
It's fine with me to allow some options.  But the downside is that the function may get complicated to maintain and use (see some of the other output format code for examples of complexity).  If we check all of the data for validity the performance might be impacted too.   
comment
Looks like GML bools are 0,1 so we should fix this bug to parse and generate those correctly. 
comment
I'd suggest to only to write valid GML.  But I don't mind accepting somewhat out-of-spec data on input - in this case underscores.  Of course that means you could potentially read a (broken) GML file with NetworkX that you can't output with the GML writer. 
comment
Having a remove_underscores converter would be OK.    It's too bad the spec doesn't include underscores.  But if we don't write GML spec files then the next Github issue will be "NetworkX GML files can't be read by X" 
comment
There is a bibtex link here http://networkx.readthedocs.io/en/latest/reference/citing.html ``` @inproceedings{hagberg-2008-exploring, author = {Aric A. Hagberg and Daniel A. Schult and Pieter J. Swart}, title = {Exploring network structure, dynamics, and function using {NetworkX}}, year = {2008}, month = Aug, urlpdf = {http://math.lanl.gov/~hagberg/Papers/hagberg-2008-exploring.pdf}, booktitle = {Proceedings of the 7th Python in Science Conference (SciPy2008)}, editors = {G\"{a}el Varoquaux, Travis Vaught, and Jarrod Millman}, address = {Pasadena, CA USA}, pages = {11--15} } ```
comment
It seems the implementation isn't quite complete for using source and target together.
comment
It might be better to point to the specification http://www.fim.uni-passau.de/fileadmin/files/lehrstuhl/brandenburg/projekte/gml/gml-technical-report.pdf since there are also other restrictions on what keys and values can be. 
comment
I don't mind making suggestions.  But I don't like being overly prescriptive ("should", "must").   Any document we create with that info will be inconsistent in some ways with the current code base.    Do you think this is holding up contributions and workflow? 
comment
Those are good arguments.  Let's do it.  @jg-you  would you like to create a draft? 
comment
It looks like the code has changed significantly since your previous experience.   I didn't look into the details yet. A workaround for this error is to add "multigraph 1" to the header of the file to reflect that it has multiple edges ``` graph [   directed 1   multigraph 1   node [     id 1 ... ``` The previous version of read_gml figured out whether the input file was a graph or multigraph by inspecting all of the edges.
comment
At minimum this needs some documentation.
comment
I might be not following the details here - but the goal was to put the reference documentation at readthedocs and have the landing page be at networkx.github.io.   More discussion at #2054 
comment
I guess it is just the links on that page that that are wrong.  The PDF is here https://media.readthedocs.org/pdf/networkx/stable/networkx.pdf   
comment
We should also make formal or deprecate the de-facto methods that use direct access to the data structure now, e.g. what G, G[u], G[u][v], should get.   
comment
Good discussion.   Should I turn on the wiki so we can build a specification there? 
comment
Needs update to merge without conflicts. 
comment
Ack, didn't get this merged in time.  More merge conflicts. 
comment
See #2332.  No need to map on to indices.
comment
I agree, that isn't going to work.  I guess we are not testing this and we should.   
comment
Anyone interested in finishing this PR? 
comment
The build gets killed for some reason.  Any idea?
comment
+1
comment
See #2282.
comment
I see what you are saying.  You are right - this is different than that earlier issue and it is a bug. I think the fix should be in digraph.reverse() ```python #            H.node=deepcopy(self.node)             for n in self.node:                 H.node[n]=deepcopy(self.node[n]) ```  Thanks for your careful look at this. 
comment
Your mapping will work but there are cases where you can't do it in place.   e.g. ```python import networkx as nx                                                            G = nx.Graph()                                                                   nx.add_path(G,[1,2,3,4,5])                                                       mapping= {3:4,4:3}                                                               G = nx.relabel_nodes(G, mapping, copy=False) ```                                                                               
comment
Yes, the duplicate can be removed - either one. 
comment
That is confusing.  Let's just remove the "shortest path is one example" sentence.
comment
This produces plenty of errors though:  ``` $ python -c "import networkx; print(list(networkx.Graph().degree(0)))" ```
comment
If we make some labels I'm willing to attach some to issues.  Also I will try to find some more beginner-friendly projects.  I suggested a few GSoC projects at #1341 that might suitable for newcomers. 
comment
If you are using the latest version of networkx (probably the development version here at github) you can iterate over the single source shortest path  lengths and not have to build all of the path lengths in memory.  e.g. ```python In [1]: import networkx as nx  In [2]: g = nx.path_graph(4)  In [3]: for p in nx.all_pairs_shortest_path_length(g):    ...:     print(p)    ...:      (0, {0: 0, 1: 1, 2: 2, 3: 3}) (1, {0: 1, 1: 0, 2: 1, 3: 2}) (2, {0: 2, 1: 1, 2: 0, 3: 1}) (3, {0: 3, 1: 2, 2: 1, 3: 0})```  
comment
Yes, it is a generator.  So if you don't need to store all of the path lengths this is a more memory efficient way to compute them.  If you do need to store them there might be 2 x 10^9 values in your case (assuming a connected graph). 
comment
Not sure.  I probably had a good reason to do that at some point 😄  
comment
Significant changes in dictionaries in 3.6.  This is a decent discussion with pointers http://stackoverflow.com/questions/39980323/dictionaries-are-ordered-in-python-3-6
comment
That looks like a bug to me.  Any idea where the issue is in the code?
comment
Some of those functions, like `dfs_edges` return Python generators.  This is often the most efficient way to iterate over the edges.  f you want to expand the result to a list you can call `list(dfs_edges)`.  The type of object returned is documented in the "Returns" section of the function.  Produce is maybe not the best choice of verb for what this does.  We could alternatively simply use "generate" since the function returns a "generator". 
comment
The docs look ok to me.  And there are examples.  Do they make sense to you?  If so perhaps we can close this.  On Wednesday, March 2, 2016, Michael E. Rose notifications@github.com wrote:  > This is about algorithms/traversal/depth_first_search.py > https://github.com/networkx/networkx/blob/master/networkx/algorithms/traversal/depth_first_search.py. > I has changed a little since Jun 2014, but not the docs. >  > I think it's easy to expand on dfs_edges by showing how a graph could be > rebuild using the edges or by showing how one could get nodes from the > result. >  > However I don't understand the proposal for dfs_tree: Why should the > function work on a list of nodes (as is the case for DiGraph.nodes() or > DiGraph.node_iter())? >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1195#issuecomment-191454823. 
comment
I don't remember discussing it. It looks like it produces the same output as numpydoc?  In that case either would be fine.  If it is different we should evaluate how it does on our docstrings.  Nice that it supports "Yields". 
comment
What you see here is the full discussion and nothing further has yet been proposed or implemented. 
comment
Fixed in #2397
comment
Looks good - here are a couple of general comments. - Since we don't require that nodes are sortable maybe we have add labels or create a labeled copy? - It would be good to fail gracefully for graphs that aren't trees including directed graphs  
comment
I don't see any problem with forcing people to relabel the graph 1 to n.   But we better check that on input or in the algorithm. 
comment
We primarily use `gnp_random_graph` and `gnm_random_graph` to refer to these graphs without ambiguity or potential attribution confusion.   As for the use of the name "Erdős–Rényi" to refer to gnp it is pretty common in the community literature.  As you point out it is probably another example of Stigler's law.   We do cite both 1959 papers in the documentation.
comment
There is still no implementation for drawing arrows correctly with matplotlib and networkx.  As you have seen this is tricky to get right.  The best workaround is to export your results and draw them with a dedicated graph drawing package like Graphviz.
comment
I think the current behavior reflects the Python principle that "errors should never pass silently".  In many cases I'd expect that passing a dictionary with keys (nodes) that aren't in the graph is an error by the user.  In this case it is pretty straightforward to filter the input dictionary to only contain nodes in the graph.
comment
I was thinking of PEP-20 https://www.python.org/dev/peps/pep-0020/.  That actually mentions both that "errors should never pass silently"  and "unless explicitly silenced" as in the dict example you give.     I am not dogmatic about these kind of things and generally like to follow the common practice or whatever is "least surprising".     If we decide we would like to include a method that sets attributes and ignores errors we might as you suggest call it something else - like `update_node_attributes`.  
comment
I think it is OK.  We are importing the function name not the module.   We could import the module but then we'd need to refer to the function name too e.g ```python In [1]: from networkx.algorithms.flow import boykovkolmogorov  In [2]: boykovkolmogorov.boykov_kolmogorov ``` 
comment
Nope, only for integer numbers. 
comment
The `G[n]` syntax shows the internal edge dictionary.  Since you have no edges it is empty. ```python In [1]: import networkx as nx  In [2]: G = nx.Graph()  In [3]: %paste >>> G.add_node(1, time='5pm') >>> G.add_nodes_from([3], time='2pm') >>> G.node[1]  ## -- End pasted text -- Out[3]: {'time': '5pm'}  In [4]: G.edge Out[4]: {1: {}, 3: {}}  In [5]: G.edge[1] Out[5]: {}  In [6]: G[1] Out[6]: {}  In [7]: G.node[1] Out[7]: {'time': '5pm'} ```
comment
There is (probably!) only one GML spec.  The latest version of networkx can read files with or without newlines.  That was handled in this fix #1962.
comment
For weighted graphs the default is to use Dijkstra's algorithm to compute shortest paths.   Through the advanced interface https://networkx.readthedocs.io/en/stable/reference/algorithms.shortest_paths.html you can specify other algorithms and functions for shortest paths.  
comment
I think we should just make it match the D3 v4 approach.  Integer indices were just there to make this work with D3.
comment
Is this OK now @MridulS?
comment
A fix would be appreciated.  Historical context for this kind of bad behavior: http://users.csc.calpoly.edu/~jdalbey/SWE/Papers/att_collapse.html
comment
Looks like the doctests are failing because   ``` Collecting scikits.sparse   Downloading scikits.sparse-0.2.zip (221kB)     100% |████████████████████████████████| 225kB 4.2MB/s      Complete output from command python setup.py egg_info:     Traceback (most recent call last):       File "<string>", line 1, in <module>       File "/tmp/pip-build-w8kvc_fh/scikits.sparse/setup.py", line 38, in <module>         from Cython.Distutils import build_ext     ModuleNotFoundError: No module named 'Cython' ```
comment
Maybe a conda expert can help us?    I suggest we merge this ASAP.
comment
I merged this even though it fails the conda install 3.6 tests. We'll either figure that out ASAP or disable the tests.
comment
Yes, but that is what OSMnx was designed to do.  It uses matplotlib and networkx.
comment
Aren't the source and target indices ordered, 0,1,... in the node list (the first array above)? 
comment
I thought the list order would be preserved.    This JSON format for graphs is not one NetworkX invented.  It is/was used in d3.js.  See also #2332 
comment
Let's fix this in #2332.
comment
Try using `nx.weakly_connected_component_subgraphs()` since your graph is directed. 
comment
The functions might be a little hard to understand by the names - they should perhaps be maximal_matching and maximum_matching.  I think you are getting the correct answers for both algorithms though - set([('c', 'b'), ('a', 'e')]) is a maximal matching - you can't add any other edge and still have a matching.
comment
The mathematical definition excludes self loops.  I'm not familiar with the Haskell implementation you are using.  Perhaps the authors had a good reason to include self loops?
comment
This looks good and could use some tests.  I see you have modified what @bjedwards to use NumPy eigensolvers instead of the power method. That is probably a good move for small graphs but for larger graphs we'll want sparse matrix representations. I think that is where we were headed with that code.  Ben do you recall what our plans were?  The bigger picture is that we will want to provide sparse matrix versions for all of the functions in networkx/linalg - but that is a bigger project and should be a new issue. 
comment
Let's put your version in (with some tests) and then open another issue to create a sparse matrix version. 
comment
On Sun, Jul 22, 2012 at 4:06 PM, Ben Edwards reply@reply.github.com wrote:  > I think the plan was to provide a power method version, a numpy version, and a sparse matrix version, then import based on what is available. >  > I think part of the consensus was if you are doing spectral graph theory, you like have numpy or scipy installed. So we could potentially make these modules require numpy and not provide a power method version. >  > Moreover, I don't think numpy is as big of a dependency hassle as it used to be, so I actually wouldn't be opposed to just making it a requirement for networkx, but that is a horse of a different color.  I don't mind part of networkx depending on numpy and scipy (as it does now).  For the spectral graph theory part of networkx it is required. One thing we might consider doing is not importing the linear algebra tools automatically, i.e. require from networkx import linalg (or whatever name).  That could make the dependency handling simpler.  One idea that occurred to me is to make all of the linalg functions return scipy sparse matrices and then just call .todense() to get a full matrix.  What are the performance implications of that?  Anyway - I don't want to get too far off track.  I think with some tests and docs Alejandro's pull request is good. 
comment
**NetworkX: Test results for pull request #741 ([aweinstein 'directed_laplacian' branch](https://github.com/aweinstein/networkx/tree/directed_laplacian))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit ede03b2 into NetworkX master ef11bff) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=59) Ran 1532 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1723 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1702 tests (libraries not available: ogr, pygraphviz, pydot) - python3.3: :eight_spoked_asterisk: OK (SKIP=37) Ran 1596 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) 
comment
I can't seem to figure out how to merge this correctly (which is likely my inexperience with git). @aweinstein can you look at it?  I'd like to include this code for the networkx-1.8 release. 
comment
 I think if you can rebase your changes on the latest master then it would work. 
comment
Looks good, thanks!  I'll review it and do the merge if it all is OK. 
comment
**NetworkX: Test results for pull request #741 ([aweinstein 'directed_laplacian' branch](https://github.com/aweinstein/networkx/tree/directed_laplacian))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit 4f6b5ae into NetworkX master fd11407) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=60) Ran 1533 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1724 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1703 tests (libraries not available: ogr, pygraphviz, pydot) - python3.3: :eight_spoked_asterisk: OK (SKIP=37) Ran 1597 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) 
comment
I added some updates in #811 to address some formatting and naming issues. 
comment
Thanks for the report.  It looks like a bug to me (I think in _is_connected_by_alternating_path). @jfinkels may be able to comment if that is the issue.
comment
Looks good.  Please do make a PR.
comment
When I run your code (modifying nodes_iter -> nodes and edges_iter-> edges in the latest API) I get the result you expect.  ``` python $ ipython Python 3.4.3 (default, Oct 14 2015, 20:28:29)  Type "copyright", "credits" or "license" for more information.  IPython 1.2.1 -- An enhanced Interactive Python. ?         -> Introduction and overview of IPython's features. %quickref -> Quick reference. help      -> Python's own help system. object?   -> Details about 'object', use 'object??' for extra details.  In [1]: %paste from networkx import Graph, DiGraph, transitive_closure from networkx.algorithms.bipartite import hopcroft_karp_matching, to_vertex_cover  # Build the DAG G = DiGraph() G.add_edge("A", "C") G.add_edge("A", "B") G.add_edge("C", "E") G.add_edge("C", "D") G.add_edge("E", "G") G.add_edge("E", "F") G.add_edge("G", "I") G.add_edge("G", "H")  tc = transitive_closure(G) btc = Graph()  # Create a bipartite graph based on the transitive closure of G for v in tc.nodes():     btc.add_node((0, v))     btc.add_node((1, v))  for u, v in tc.edges():     btc.add_edge((0, u), (1, v))  matching = hopcroft_karp_matching(btc) vertex_cover = to_vertex_cover(btc, matching) independent_set = set(G) - {v for _, v in vertex_cover} print(independent_set) ## -- End pasted text -- {'H', 'I', 'B', 'D', 'F'} ``` 
comment
I just looked at this again - aren't both answers valid? 
comment
Indeed, I see.  Let me try your suggested experiment. 
comment
Interesting (bad).  I get the incorrect result with Python2 and correct result with Python3.  So there must be a bug somewhere.  Where? Maybe in the matching algorithm, @jfinkels?  ``` python $ ipython2 Python 2.7.6 (default, Jun 22 2015, 17:58:13)  Type "copyright", "credits" or "license" for more information.  IPython 1.2.1 -- An enhanced Interactive Python. ?         -> Introduction and overview of IPython's features. %quickref -> Quick reference. help      -> Python's own help system. object?   -> Details about 'object', use 'object??' for extra details.  In [1]: %paste from networkx import Graph, DiGraph, transitive_closure from networkx.algorithms.bipartite import hopcroft_karp_matching, to_vertex_cover  # Build the DAG G = DiGraph() G.add_edge("A", "C") G.add_edge("A", "B") G.add_edge("C", "E") G.add_edge("C", "D") G.add_edge("E", "G") G.add_edge("E", "F") G.add_edge("G", "I") G.add_edge("G", "H")  tc = transitive_closure(G) btc = Graph()  # Create a bipartite graph based on the transitive closure of G for v in tc.nodes():     btc.add_node((0, v))     btc.add_node((1, v))  for u, v in tc.edges():     btc.add_edge((0, u), (1, v))  matching = hopcroft_karp_matching(btc) vertex_cover = to_vertex_cover(btc, matching) independent_set = set(G) - {v for _, v in vertex_cover} print(independent_set) ## -- End pasted text -- set(['A', 'B', 'D', 'F', 'I', 'H'])  In [2]: matching Out[2]:  {(0, 'A'): (1, 'C'),  (0, 'C'): (1, 'E'),  (0, 'E'): (1, 'G'),  (0, 'G'): (1, 'I'),  (1, 'C'): (0, 'A'),  (1, 'E'): (0, 'C'),  (1, 'G'): (0, 'E'),  (1, 'I'): (0, 'G')}  In [3]: vertex_cover Out[3]: {(0, 'E'), (0, 'G'), (1, 'C'), (1, 'E')}  In [4]: import networkx  In [5]: networkx.__version__ Out[5]: '1.11' ```  ``` python $ ipython3 Python 3.4.3 (default, Oct 14 2015, 20:28:29)  Type "copyright", "credits" or "license" for more information.  IPython 1.2.1 -- An enhanced Interactive Python. ?         -> Introduction and overview of IPython's features. %quickref -> Quick reference. help      -> Python's own help system. object?   -> Details about 'object', use 'object??' for extra details.  In [1]: %paste from networkx import Graph, DiGraph, transitive_closure from networkx.algorithms.bipartite import hopcroft_karp_matching, to_vertex_cover  # Build the DAG G = DiGraph() G.add_edge("A", "C") G.add_edge("A", "B") G.add_edge("C", "E") G.add_edge("C", "D") G.add_edge("E", "G") G.add_edge("E", "F") G.add_edge("G", "I") G.add_edge("G", "H")  tc = transitive_closure(G) btc = Graph()  # Create a bipartite graph based on the transitive closure of G for v in tc.nodes():     btc.add_node((0, v))     btc.add_node((1, v))  for u, v in tc.edges():     btc.add_edge((0, u), (1, v))  matching = hopcroft_karp_matching(btc) vertex_cover = to_vertex_cover(btc, matching) independent_set = set(G) - {v for _, v in vertex_cover} print(independent_set) ## -- End pasted text -- {'F', 'B', 'I', 'D', 'H'}  In [2]: matching Out[2]:  {(0, 'A'): (1, 'C'),  (0, 'C'): (1, 'D'),  (0, 'E'): (1, 'F'),  (0, 'G'): (1, 'H'),  (1, 'C'): (0, 'A'),  (1, 'D'): (0, 'C'),  (1, 'F'): (0, 'E'),  (1, 'H'): (0, 'G')}  In [3]: vertex_cover Out[3]: {(0, 'A'), (0, 'C'), (0, 'E'), (0, 'G')}  In [4]: import networkx  In [5]: networkx.__version__ Out[5]: '1.11' ``` 
comment
I still get the same (bad) answer with the latest development version and Python2. 
comment
I believe the bug is https://github.com/networkx/networkx/blob/master/networkx/algorithms/bipartite/matching.py#L415 with the call to bipartite.sets().  This partitioning by coloring isn't unique in your case and the bipartite sets it returns are not the ones you started with.   This breaks with both python2 and python3.  We were just lucky with Python3 that we got the correct partition. 
comment
The Python2 vs Python3 issue was just a distraction.   Python3 has added some hash randomization so the ordering of nodes produced (given the same input data) can vary.  We were just getting lucky that `bipartite.sets` was sometimes giving us the correct result.  If you adjust the line I mentioned above to (for your specific case to get the nodes correctly partitioned)   ``` python     L = set([n for n in G if n[0]==0])     R = set([n for n in G if n[0]==1])    #    L, R = bipartite_sets(G) ```  I believe it always works correctly.  The general fix is to specify the two node sets explicitly somehow - I haven't looked at this code design to see the best way to do that.  We sometimes use the node attribute part=0 part=1 to do that since there isn't a "BipartiteGraph" object in networkx. 
comment
I'm not sure that is the right place to fix this.  Perhaps in the `bipartite.sets` function or else in this case maybe in the `to_vertex_cover` function. 
comment
Version info is on the networkx.github.io front page.  http://networkx.github.io/
comment
Thanks for the report.  This works correctly for me with the master branch of networkx in the github repository.   Could you try that version?  ```python In [7]: G = nx.from_pandas_dataframe(df, source='O', target='D',    ...:                              edge_attr=['St','Co','Mi'],    ...:                              create_using=nx.MultiDiGraph())  In [8]: list(G.edges(data=True)) Out[8]:  [('X1', 'X4', {'Co': 'zA', 'Mi': 0, 'St': 'X1'}),  ('X1', 'X4', {'Co': 'zB', 'Mi': 54, 'St': 'X2'}),  ('X1', 'X4', {'Co': 'zB', 'Mi': 49, 'St': 'X3'}),  ('X1', 'X4', {'Co': 'zB', 'Mi': 44, 'St': 'X4'}),  ('Y1', 'Y3', {'Co': 'zC', 'Mi': 0, 'St': 'Y1'}),  ('Y1', 'Y3', {'Co': 'zC', 'Mi': 34, 'St': 'Y2'}),  ('Y1', 'Y3', {'Co': 'zC', 'Mi': 29, 'St': 'X2'}),  ('Y1', 'Y3', {'Co': 'zC', 'Mi': 24, 'St': 'Y3'}),  ('Z1', 'Z3', {'Co': 'zD', 'Mi': 0, 'St': 'Z1'}),  ('Z1', 'Z3', {'Co': 'zD', 'Mi': 14, 'St': 'X3'}),  ('Z1', 'Z3', {'Co': 'zE', 'Mi': 9, 'St': 'Z2'}),  ('Z1', 'Z3', {'Co': 'zE', 'Mi': 4, 'St': 'Z3'})] ```  
comment
In this case, since you have a digraph, the method `number_of_edges` counts the directed edges between the source ('node1') and target ('node2').   That is consistent with using `number_of_edges` on the whole graph (edges are only counted once).   The documentation should say (and doesn't) that it counts directed edges.    
comment
The network.convert_node_labels_to_integers() function might help.   On Thu, Feb 9, 2017 at 7:09 AM edouardklein <notifications@github.com> wrote:  > I can't reproduce the bug with a nice minimal working example, so instead > I'll strip the offending graph of all that data that I can not share, and > see where I can go from there. It may take a while. > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/2363#issuecomment-278650864>, > or mute the thread > <https://github.com/notifications/unsubscribe-auth/AALd46aEzD7-0w1sp-8R37PthWhAaQa3ks5rax4bgaJpZM4L8Gqf> > . > 
comment
Thanks for the bump.  We have been working on this in #2319.  I think all of the test are passing now except for the doctests.  We'll need to figure out what to do with those and then merge.
comment
This seems to work for me with the 2.0 development version.  Perhaps you could try that?   ```python In [1]: %paste import networkx as nx G=nx.path_graph(4)  fixed_positions = { 1: (0.5, 0.5) } fixed_nodes = fixed_positions.keys()  pos=nx.spring_layout(G, pos = fixed_positions, fixed = fixed_nodes)  print(pos)  ## -- End pasted text -- {0: array([ 0.3142939 ,  0.74262896]), 1: array([ 0.5,  0.5]), 2: array([ 0.70349586,  0.23389249]), 3: array([ 0.88815749, -0.00774765])} ``` 
comment
Need to fix conflicts and coordinate with #1733  
comment
Needs merge conflict update. 
comment
I like catching confusing errors and we should do that.   I don't like so much the idea of checking input data for validity.  In this case the errors aren't that subtle or confusing to me.  If you get a negative page rank your input data is probably suspect and if you put zeros in the personalization parameter you'll get an explicit ZeroDivisionError.    Was there something particularly difficult to understand with the results you got that we might clarify with a clearer message? 
comment
I think the user should check for valid input.  It's just going to be too hard to check for all combinations of strange input.  
comment
I'm pretty sure that is intentional.  But I can't remember the reason just now. 
comment
Fixed in #2215
comment
Hmm, that's ugly.  I don't know how to fix that properly.  @chebee7i do you have any idea? 
comment
It's also broken on Python3 or I would have already suggested to just use Python3 😄  
comment
Thanks for the clear explanation.   I was confused over encoding and decoding and thought it was also wrong for Python3.   I suggest we leave it this way since there is a reasonable workaround for Python2. 
comment
Thanks, I get the issue with `edge_boundary` now. We should definitely fix that. 
comment
See #2203 #2207 #2209 #2305 
comment
Thanks for checking in on this.  If we still have places where tests rely on ordering let's get them fixed.  The tests do pass for pypy and for later versions of Python using randomized hashing so hopefully there aren't that many fixes required. 
comment
It's not surprising to me that we have passing tests.  We designed them that way.  The randomized hashing gave us a good reason to fixof the tests that depended on ordering of nodes or edges.  Maybe we missed a few and so far have had no reports for some reason.     I'm not sure what is happening with your Jython tests.  If you want to explore that further go ahead.  As pointed out in #1671 we are not supporting Jython though making NetworkX work with Jython would be a plus.  The doctests are primarily for showing some use cases so we don't need such thorough testing there.  In some cases it doesn't make sense to write a complicated doctest that handles the arbitrary ordering. 
comment
Those fixes generally look good (using assert_nodes_equal and assert_edges_equal).  We could include them now.   Some I don't understand why they would fail with Jython.  e.g. replacing  ``` python -        assert_equal(sorted(d for n, d in G.in_degree()),[0, 0, 0, 0, 1, 2, 2]) +        assert_nodes_equal((d for n, d in G.in_degree()),[0, 0, 0, 0, 1, 2, 2]) ```  Why should the first line fail?  All values are numeric.  They should sort correctly regardless of the Python implementation. 
comment
Oh, I see.  Would you be willing to make a PR with those test changes? I'd like to incorporate those.  Also if you would add here the Jython test output that might inspire someone to look into that further. 
comment
This was very helpful.   I think we should follow up and fix these issues.  With the changes from #2207 and #2209 the tests all pass with Jython in networkx/classes/test and networkx/test.  There are still many failing tests in other parts which as you pointed out might simple ordering of edges and nodes.  Some are clearly more complicated and might be algorithms with multiple alternate solutions.   It's certainly possible this could expose some bugs which is why I advocate for making the tests work with Jython. 
comment
See #2207 and #2209.
comment
It looks like we should be using `networkx.testing.utils.assert_edges_equal()` here. 
comment
See #2203 and friends.
comment
I don't think this is a bug.  If two of your node objects have the same `__repr__` (and I'm guessing the same `__str__`) then any string-based output from networkx will generate the same node string for both.  If you want distinct nodes then they should have distinct `__repr__` or at least `__str__`.  As you have discovered you can have the same `label` (or other) attribute for multiple nodes if you want to draw them with the same name, color, shape, etc. 
comment
Raising an exception here doesn't mean that it is a software error it means that there is a problem with the output of the algorithm.  In this case the error is trying to compute the shortest path between disconnected nodes.   We have other exceptions like that where we try to catch input errors that cause the algorithms to produce results that are reasonable by wrong. Those are hard to spot.  I'd prefer raising the exception to be explicit about the disconnected nodes and lack of a path.  In that case the user has to decide how to handle the situation - setting to inf if that is appropriate.  There is less chance of a silently ignored error when dividing by inf or some similar situation.   
comment
Harmonic centrality, sum(1/distance),  and closeness centrality, 1/sum(distance) are different.  Can you be more specific about what you mean by memory leak?   The code for harmonic centrality is pretty simple:  ``` python from __future__ import division import networkx as nx def harmonic_centrality(G, distance=None):     if G.is_directed():         G = G.reverse()     sp = nx.shortest_path_length(G, weight=distance)     return {n: sum(1 / d if d > 0 else 0 for d in dd.values()) for n, dd in sp} ``` 
comment
Oh, I think I see the issue.  The closeness centrality code compute single source shortest path in a loop over every source which takes much less memory than the harmonic centrality implementation that uses all-pairs shortest path.  I think the latter could be easily fixed to be more memory efficient. 
comment
Yes. That should be very similar in runtime and use much less memory.  On Fri, Sep 9, 2016 at 12:11 AM jfinkels notifications@github.com wrote:  > @hagberg https://github.com/hagberg Do you want the single-source > shortest path version in NetworkX? >  > — > You are receiving this because you were mentioned. >  > Reply to this email directly, view it on GitHub > https://github.com/networkx/networkx/issues/2238#issuecomment-245828084, > or mute the thread > https://github.com/notifications/unsubscribe-auth/AALd47bXzhrdNLBUqRkCtjgymkIohS58ks5qoPiogaJpZM4Jwjcu > . 
comment
Fixed in #2247.
comment
Another approach would be to add extra nodes on the parallel edges to transform to a graph with unique edges, e.g. A->B, A->B transforms to  A->x->B, A->y->B 
comment
We don't have any progress bar technology set up.  As @dschult points out it is difficult to estimate run time for some algorithms.
comment
Fixed by #2333. Open a new issue for further enhancements.
comment
Release date has been updated to March 15 2017 https://github.com/networkx/networkx/milestone/3 
comment
Closed by #2344
comment
I don't think we need to protect the master branch but we could. Given then general way we work on this project i don't think it would change much.  I added a project page for networkx-2.0 https://github.com/networkx/networkx/projects/2
comment
I doubt there will much practical benefit from this though:  ``` python In [16]: G = nx.gnc_graph(1000)  In [17]: %timeit nx.dag_longest_path(G) 100 loops, best of 3: 11.9 ms per loop  In [18]: %timeit nx.dag_longest_path_length(G) 100 loops, best of 3: 11.7 ms per loop ``` 
comment
I think the most likely problem is in the `__hash__` or `__eq__` functions of the custom object you are using for nodes (`DiscreteFactor`) .  Those look pretty complicated and might not be working they way you expect.  We had a longer discussion about using custom objects here https://github.com/networkx/networkx/issues/2282 of you are interested in the topic.
comment
I'm not sure why the documentation is unclear.  It should be "at least one non-zero value" as you suggest.  I'm less excited  about adding another keyword parameter for a default value.  In your use case the default is 0.  How about just allowing a dictionary with only some of the node keys and the rest get set to zero inside the pagerank functions?  If a user requires a different default from 0 then the full dictionary would need to be specified.  
comment
It's reasonably easy to do that using dict.fromkeys()  ```python In [1]: import networkx as nx  In [2]: G = nx.path_graph(10)  In [3]: d = dict.fromkeys(G,0.5)  In [4]: d Out[4]:  {0: 0.5,  1: 0.5,  2: 0.5,  3: 0.5,  4: 0.5,  5: 0.5,  6: 0.5,  7: 0.5,  8: 0.5,  9: 0.5}  In [5]: d.update({0:1,2:1})  In [6]: d Out[6]: {0: 1, 1: 0.5, 2: 1, 3: 0.5, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.5} ```
comment
It looks like it is using al of your memory.  The most likely solutions are to find a more efficient memory implementation or a machine with more memory.  Unfortunately the current flow centrality algorithms require a linear equation solve which uses a lot of memory.   You might also try the "cg" solver option to current_flow_betweenness_centrality which uses less memory.
comment
On my test of your code the full value of d is ```python In [2]: d Out[2]:  {'directed': False,  'graph': {},  'links': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}],  'multigraph': False,  'nodes': [{'name': 3}, {'name': 4}, {'name': 5}, {'name': 6}, {'name': 7}]} ```  The nodes are ordered in the list.  So your edge 3-4 translates to 0-1 etc.   
comment
It looks like your graph.yaml.zip example has 20,000 edges and only 19967 nodes.  That is very sparse.   The fact that your nodes are strings is not relevant because a label information is discarded with these formats.   If you want to use a format like graph6 I suggest using sparse6 which will easily be able to handle large sparse graphs quickly and without so much memory.  ``` python In [21]: G = nx.gnm_random_graph(100000,200000)  In [22]: %timeit len(nx.generate_sparse6(G)) 1 loops, best of 3: 1.96 s per loop ``` 
comment
Sure, there would be some benefit from that.  If you have an implementation or plan please share it.
comment
See #2316
comment
NetworkX is using Python dictionaries to store the node and edge information.   The expectation is that the objects used as nodes are hashable to make this possible.  You got that satisfied by providing a hash function.  But the way you set it up two objects that aren't the same can have the same hash.  It looks like that collision causes ambiguity in which objects get stored/replaced in the dictionaries.  I'm not sure if this is really a NetworkX bug.    Is there a reason you can't use a better (unique) hash function? 
comment
Yes, @ProgVal put it more clearly than what I wrote (indeed there can be hash collisions). 
comment
Thanks for the update on this.  I think I see the situation now.  NetworkX assumes that objects compare equal are the same object.  There is some inconsistency when existing (compare equal) nodes and edges are added again to the graph data structure (i.e. the internal dictionary of dictionaries).  This is probably fixable with some performance loss by either deleting the existing objects and inserting the new ones or ignoring the new objects completely (but somehow we'd want to update the values of some of the existing dictionary keys).  I can't tell if this is a subtle and awful bug lurking in the depths or more of a "don't do that" advisory when designing custom objects to use with Python dictionaries.  The issue is the few lines in add_edge https://github.com/networkx/networkx/blob/master/networkx/classes/digraph.py#L557 (and likely other places where we modify the adj/pred/succ dictionary structure). 
comment
Indeed - so a vote for not a bug and "don't do that"? - i.e. always put equals semantics on custom objects that match the expectation of the object user...  
comment
I bumped https://travis-ci.org/networkx/networkx/jobs/170435540  if that is the one. Failed because of a pypy recursion limit error and is now passing. 
comment
Questions are better on the mailing list. https://groups.google.com/forum/#!forum/networkx-discuss 
comment
That would make sense but it is not how it currently works.   You need to set the "graph" attribute of the graph attribute (ugh) like this  ``` python In [1]: import networkx as nx  In [2]: G = nx.DiGraph()  In [3]: G.graph['graph']={'rankdir':'LR'}  In [4]: import sys  In [5]: nx.drawing.nx_agraph.write_dot(G, sys.stdout) strict digraph  {     graph [rankdir=LR]; } ``` 
comment
Needs merge conflict resolution, then good to go. 
comment
I agree with @chebee7i's approach here.  That is the simplest interface and least code too. 
comment
Yes, please do update the pull request.    It was a good approach to copy the closeness centrality interface.   I likely wrote that so I have now apparently changed my mind on the function interface :innocent: . 
comment
The way we convert directed multigraphs to multigraphs is problematic.  As you discovered the result is dependent on order that the graph is explored.  I can't think of a good fix for this other than removing this option. 
comment
I'm not sure I understand what is going on there. It could be a bug.  On Sun, Jul 31, 2016 at 3:40 PM Valentin Lorentz notifications@github.com wrote:  > Ok. > But why does the directed graph have a simple dict in one case, and a dict > of dict in the other one? >  > — > You are receiving this because you commented. >  > Reply to this email directly, view it on GitHub > https://github.com/networkx/networkx/issues/2210#issuecomment-236458353, > or mute the thread > https://github.com/notifications/unsubscribe-auth/AALd49LVwOFkmxUkCr12MqDckriUjyAgks5qbRZDgaJpZM4JZHXC > . 
comment
This looks good too me.  If @dschult agrees, let's merge it. 
comment
Yes, revert those.  The communicability_alg.py module also needs this fix IIRC. 
comment
Could you post more details on the error and networkx version including a small non-working example.  Some of this code has changed (including the names) in networkx and it might be helpful to update to a newer version.   See #1887 #1950 #1958 
comment
As far as I can tell that is safe (sending an array).  This works with older versions of numpy and scipy and I'm guessing this is a scipy bug. 
comment
I'm not too worried about the 63K, fine to compress.  The ordering of the graphs should be fixed though. 
comment
I suppose it makes most sense to be like in the book.  Here are graph 55 and 56 https://books.google.com/books?id=QOPuAAAAMAAJ&focus=searchwithinvolume&q=g55 
comment
I might be able to find a book.  Maybe @pieterswart could tell us about the graph ordering - he wrote the atlas module. 
comment
@jfinkels  I agree with your proposal 
comment
I'm going to suggest not including this simple function.  It is pretty straightforward (one-line) to test if the graph is complete and the 75 lines we need to include, document, and test this are a lot of overhead. 
comment
This is a good start.  We could fix the numpy import error that Travis is hitting using pypy but it might be better  (simpler and faster) to just make it pure Python.  
comment
Very nice. Thanks! 
comment
I agree the error message isn't clear.   And we should include a biadjacency matrix reader.  Here is a version I have used https://gist.github.com/hagberg/456523d36af61109c9bd#file-gistfile1-py There is already a biadjacency matrix generator in the networkx.algorithms.bipartite package. 
comment
I just meant we already have a function that can create a biadjacency matrix from a networkx graph. So we should have, as you suggest, a function that does the reverse. 
comment
Sure we should add that.  I also have some bipartite edge readers and writers we could add.  I added issue #1294.  This issue still needs the error message fix. 
comment
Please don't change the copyright dates without reason. 
comment
This kind of pull request just generates work for reviewers with very little benefit so I closed it. 
comment
It wasn't broken before.  Your code tries calling the nonexistent sort method on an iterator. 
comment
My code is already more beautiful than any automatic machine can write 😄  
comment
Please don't submit PEP-8 changes only.  We are improving our PEP-8ness when we update or add code. 
comment
We decided to make `G.edges()` for DiGraphs be the same as `G.out_edges()`. It was confusing to users (and developers) to return both in- and out-edges for `G.edges()` and it was causing bugs in some algorithms.  It may not be ideal but some of our decisions are a compromise between Python programmers and graph theorists to reach a middle ground in naming and notation. 
comment
So far we have been trying to avoid custom classes for performance reasons.  I'm not sure I understand what you are proposing to add here.  We definitely allow users to mess up the internal data structures in any way they like and are trusting that if they do that they know what they are doing. 
comment
If you want specific handling of edge attributes for mulitgraphs you'll need to write your own code to transform your data input.    If you post an example of your input and desired output we can help you. 
comment
The imports for write_dot and graphviz_layout need to be adjusted for networkx-1.11.  ``` python import networkx as nx from networkx.drawing.nx_agraph import graphviz_layout,write_dot # pygraphviz #from networkx.drawing.nx_pydot import graphviz_layout,write_dot # pydotplus import matplotlib.pyplot as plt G = nx.DiGraph()  G.add_node("ROOT")  for i in xrange(5):     G.add_node("Child_%i" % i)     G.add_node("Grandchild_%i" % i)     G.add_node("Greatgrandchild_%i" % i)      G.add_edge("ROOT", "Child_%i" % i)     G.add_edge("Child_%i" % i, "Grandchild_%i" % i)     G.add_edge("Grandchild_%i" % i, "Greatgrandchild_%i" % i)  # write dot file to use with graphviz # run "dot -Tpng test.dot >test.png" write_dot(G,'test.dot')  # same layout using matplotlib with no labels plt.title("draw_networkx") pos=graphviz_layout(G,prog='dot') nx.draw(G,pos,with_labels=False,arrows=False) plt.savefig('nx_test.png') ``` 
comment
Oh, I see, yes.  It is broken in networkx-1.11.  We removed `draw_graphviz` completely now in the development version.   
comment
Use `Digraph.edges`.  See http://networkx.readthedocs.io/en/latest/reference/release_2.0.html#api-changes for more details. 
comment
This looks really interesting, thanks!  We'll be glad to include that. The test are great.  We'll need to include a little more documentation on how to use the function including the reference to your paper.  ps.  The build test is failing simply because of formatting issues in the **init**() functions. Python doctests expects the line following any >>> line to be the result of runing that code.  So adding a blank line after   ``` python >>> G2 = nx.Graph(nx.path_graph(4, create_using=nx.Graph())) ```  will fix that. 
comment
I like the idea of using datetime objects.  That seems the most flexible without getting into the weeds with timezones, etc.  The user can figure all of that out and parse into a datetime object that makes sense for the specific input data.   
comment
@chebee7i is this ready to merge? 
comment
Let's straighten out the names and algorithms in #1239 first. I prefer not to have the modules be too big but we can adjust the documentation so it is clearer. 
comment
Continue at #1239  
comment
What do we need to do on #1239? 
comment
Now that #1239 and #1958 are done is there anything left to do here? 
comment
I just ran this and got -1.054905350542667e-18 for the minimum of the eigenector.  That is effectively floating point zero on my machine.   
comment
The eigenvector that goes with your largest eigenvalue (which is about 398077) is not the zero vector.  It does have some entries that are close to zero.  Try plotting it to see, use `plot(sorted(n_eigenvectors.values())`.  Note also that the result from the `eigenvector_centrality_numpy` algorithm is normalized with the "l2-norm" - the sum of the squares of the values is 1. 
comment
Probably it ran out of memory.  There may be some possibilities for more efficient memory implementations of `simple_cycles` if you don't have a machine with more memory.  For example there is a copy of the graph made at the beginning of the function. 
comment
We do provide a slightly awkward way to get the edge keys for a pair of edges  ``` In [1]: import networkx as nx  In [2]: G = nx.MultiGraph()  In [3]: G.add_edge('a','b', color='red')  In [4]: G.add_edge('a','b', color='blue')  In [5]: G.get_edge_data('a','b') Out[5]: {0: {'color': 'red'}, 1: {'color': 'blue'}}  In [6]: list(G.get_edge_data('a','b')) Out[6]: [0, 1] ``` 
comment
I think @jfinkels idea is the simplest approach that doesn't break things too much.  So let's do that. 
comment
You could use itertools.groupby to remove repeated elements in your list.  ``` python In [1]: l = [1,2,2,3,2]  In [2]: import itertools  In [3]: [i for i,_ in itertools.groupby(l)] Out[3]: [1, 2, 3, 2] ``` 
comment
That is the fastest way if you have a container of edge tuples.   It takes less than 200ms on my setup.  ``` python In [1]: import networkx as nx  In [2]: G = nx.gnm_random_graph(22000,60000)  In [3]: edges = list(G.edges())  In [4]: %timeit G = nx.DiGraph(edges) 10 loops, best of 3: 165 ms per loop ``` 
comment
You are right, that is confusing.  Thanks for pointing this out.   
comment
Needs merge conflict resolution. 
comment
Are you compressing the data?  That would help some.  ``` python import networkx as nx G = nx.fast_gnp_random_graph(10000,0.01) print G.number_of_nodes(),G.number_of_edges()  import gzip import bz2 import cPickle as pickle with open('foo.pkl', 'w') as f:     pickle.dump(G, f)  with bz2.BZ2File('foo.pkl.bz2', 'w') as f:     pickle.dump(G, f)  with gzip.GzipFile('foo.pkl.gz', 'w') as f:     pickle.dump(G, f) ```  ``` bash [aric]$ ls -lh foo.pkl* -rw-r--r-- 1 aric aric  16M Apr 20 17:51 foo.pkl -rw-r--r-- 1 aric aric 4.1M Apr 20 17:51 foo.pkl.bz2 -rw-r--r-- 1 aric aric 5.4M Apr 20 17:51 foo.pkl.gz ``` 
comment
The empty dicts still take 160 bytes each or something like that. Aric 
comment
I agree this is somewhat surprising behavior.  I suggest that we do not mutate the user input attribute dictionary.  I'd prefer to not make the `add_node` and `add_edge` methods more complicated to so let's see if we come up with a good  solution.  @dschult what do you think? 
comment
Fixed in #2107 #2132 
comment
Thanks. We have updated this in our development version but the docs have not been rebuilt yet.  You can get the zip file at  https://readthedocs.org/projects/networkx/downloads/htmlzip/stable/ 
comment
When I run this version of your code it produces the same result for G,H,J.  Could you try that?  ``` python import networkx as nx  def compute_betti(G):     #Compute cycle basis     cb = nx.cycle_basis(G)      #Compute Betti number of graph     betti = 0     for cycle in cb:         if len(cycle)>3:             betti += 1      #Return (betti number, graph size, cyclomatic number)     return betti, len(list(G.nodes())), len(cb)   #G = nx.read_adjlist(some_file) #original graph G = nx.gnp_random_graph(100,0.4) H = G.copy() #deep copy J = nx.Graph(G) #shallow copy print(compute_betti(G)) print(compute_betti(H)) print(compute_betti(J)) ``` 
comment
Be careful here.  You'll likely get different results if you e.g. reorder your input file.   So I don't think OrderedGraph fixes this for you.   The cycle basis output is often misunderstood. @dshult probably has a standard form for answering questions about it.    It is not computing all cycles of a certain length. 
comment
Thanks, we are addressing this in #1990.  the pdf is at https://media.readthedocs.org/pdf/networkx/latest/networkx.pdf 
comment
That is odd.  There were some changes in the scalings at some point.  Would you double check the versions by running  ``` sh $ python -c "import networkx; print(networkx.__version__)" ``` 
comment
Thanks for the pull request.  This code is part of the core of networkx so it may take us a little while to figure out how we want to proceed (and why).   I'd especially like comments from @dschult.  What is the utility of having attr_dict unless we actually want side effects like we are trying to prevent here? 
comment
How about eliminating the attr_dict optional keyword completely?  That would make the methods simpler.    Also #1583 is still open and maybe relevant if we consider change the function arguments. 
comment
That doesn't seem so bad.  The situation with multigraphs and directed graphs should be very similar.  It might be a little harder with `add_edges` and `add_edges_from` since my feeling is the attr_dict pattern is used a lot more in those cases.  I might be wrong.  About the function naming (plural/singular) and *args approach.  I agree that there may be some opportunities to make the interface simpler.  It might be a little to out-of-scope for this release but maybe there is a clean way to handle backwards compatibility if we decide we want to proceed. 
comment
This looks like a good start.  Let's first focus on getting the `add_node`, `add_nodes_from`, `add_edge`, and `add_edges_from` functions the way we want them (completely removing the attr_dict argument).   
comment
I was looking at convert.py a little.  It breaks the tests in a place where a slightly more ugly fix is needed - e.g.  ``` python             for u,nbrs in d.items():                 for v,data in nbrs.items():                     if (u,v) not in seen: #                        G.add_edge(u,v,**data) # can't do this with integer keys                         G.add_edge(u,v,key=0)                         G[u][v][0].update((k,v) for k,v in data.items())                     seen.add((v,u))         else: ``` 
comment
Note that my code above is a hack forcing 0 as the key for every multigraph edge.  Since the for that section input isn't a multigraph this would be ok since there are not supposed to be duplicate edges in the output.  The convert.py code is a bit tricky to handle the multiple graph classes we are currently using. 
comment
The "return edge key" issue is indeed unresolved and a little tricky to change.  If we can proceed here without opening that up that would be good.   
comment
I don't think we should return anything from `add_edge`.  I know that means some engineering work-around for this case but that fix is better to postpone since it opens up a lot of other design tweaks. 
comment
Ok, thanks!  I added the "needs review" tag and we should take a close look at the changes since this touches 31 files including the core graph classes. 
comment
I looked through all of the changes and they seem fine to me.    The multigraph key implementation we are using definitely needs more thought and redesign.  The current code is OK but that will come back to bite us some other day... 
comment
There are two other things I thought of  1) We could make a note about this in https://github.com/networkx/networkx/blob/master/doc/source/reference/release_2.0.rst 2) I didn't look closely but it should be documented in the `*_from()` methods that if an attribute is specified with keywords it is applied before any node-specific attribute in the container of nodes or edges.   
comment
This is intentional and was changed between networkx-1.10 and networkx-1.11.   See for example the networkx-1.11 docs.   http://networkx.readthedocs.io/en/networkx-1.11/reference/generated/networkx.drawing.nx_agraph.graphviz_layout.html#networkx.drawing.nx_agraph.graphviz_layout  Our previous clever use of imports was causing problems and confusion so we decided to require explicit imports to be clear on which version of the graphviz interface is selected. 
comment
#2093   Apparently there is some trickery to make the redirect work with github pages but it is beyond my technical ability.  I hope the site gets reindexed by Google soon.    The documentation is at https://networkx.readthedocs.org/  All of the old documentation is at https://networkx.github.io/documentation/ 
comment
The v1.10 documentation is at  http://networkx.readthedocs.org/en/networkx-1.10/ All of the reference documentation will be found there going forward.  Please excuse Google for the time it takes to update the search engine results. 
comment
Yes, that is the expected, possibly annoying, behavior of the way keys work with the current Mulitgraph class.  If you specify an existing edge in add_edge (including using the key) it will be replaced. The only safe way is to check if the edge exists first.   We have considered changing this and the design needs some thought, #1654. 
comment
This is a bug that is fixed in the current development version (2.0) of networkx.   
comment
Version 2.0 isn't released yet. Use pip install https://github.com/networkx/networkx/zipball/master 
comment
Fixed in https://github.com/networkx/networkx-website/commit/2523f5e25fcacbc2204db73ea8d7cb964d81dbd7 
comment
This needs some review and decision on if/how to complete. 
comment
Needs updated docstring and merge. 
comment
I think the newline is valid whitespace.  If that is true then the parser needs to be adjusted to allow whitespace including newline after a key. 
comment
#1985 is a separate issue.   
comment
This can be updated now with #1690. 
comment
Thanks for making those formatting changes.  It looks good (though I didn't check the formatted doc output).  One more small suggestion would be to rename the function to `joint_degree_graph` instead of `joint_degree_model`.  There are some cases where functions that return graphs aren't named that way (e.g. `configuration_model`) but I prefer the `graph` name approach. 
comment
Any comment on this approach vs http://arxiv.org/abs/1503.06725? 
comment
HI @mgjoka, thanks for that very helpful comment.   It would be a good project for someone to implement  that method too - but is a separate issue.  Thanks for your contribution here. 
comment
Is this a definition issue?  Is a node a dominator if itself? 
comment
Thanks. Let's fix this bug then. 
comment
If someone can help me with the definition/algorithm of dominance frontier I will fix this. 
comment
That helps a little.  Though I'll probably have to read http://www.hipersoft.rice.edu/grads/publications/dom14.pdf to get it right. 
comment
Sure I can remove the discard(u).  But I got stuck when I looked at   ``` g = nx.DiGraph() g.add_edges_from([('a','b'),('b','c'),('b','a')]) df = dominance_frontiers(g, 'a') ```  Should df['a']=['a']? or [] that the code produces. 
comment
I updated according to what @RazorBladed suggested. I am still not sure I understand what happens with loops.  There are two examples in the tests that show this implementation is not producing the stated result.  See eg https://github.com/hagberg/networkx/commit/321364aeabec891ac9d3023fbc07cf618b6fd265#diff-095a9a6207b37916664a95b2f3bdf9b5R229 
comment
Sorry, that comment wasn't very helpful.  The tests pass the way they are written. But in the example I linked above at http://ecee.colorado.edu/~waite/Darmstadt/motion.html there is a 4 in node 4's dominance frontier which our algorithm doesn't produce.  It could be related or the same as the question I asked above in the a->b->a graph. 
comment
``` python print nx.immediate_dominators(g,'entry') {'exit': 'entry', '1': 'entry', '3': '2', '2': '1', '5': '4', '4': '3', '6': '5', 'entry': 'entry'} ``` 
comment
Good suggestions.  I wondered why we weren't using sets there.  Updated #2092 and all looks good. 
comment
Can you be more specific on the fix?  I'm not an expert in this algorithm. 
comment
Does this need fixing for v1.11? 
comment
Does #1221 address this? 
comment
Can we fix this or should we close this for now? 
comment
You are right - that function only works for discrete integer values.  It wouldn't be that hard I think to generalize that to the continuous case if someone would like to do that.  In the mean time the docs should be improved as you suggest. 
comment
Either way is fine with me.   
comment
OK - that makes sense.  Would you like to add that with a pull request? 
comment
Needs PR. 
comment
It's good, and needs one small fix in the tests. 
comment
We don't import them into the networkx namespace other than at networkx.utils.  We could not import them at all by default if that helps. 
comment
If you look around the utils package there is a pretty diverse collection of code and algorithms.  I'd say most of them are not very closely related to graph theory or network science which is likely why they ended up there.  I don't see any reason to be prescriptive for what belongs there or why.  In general I would argue it is good to keep the amount of code there somewhat limited so we don't end up with a package of utility functions copied from stackoverflow that  also happens to have graph algorithms. 
comment
I think we are agreeing to disagree here.  So let's leave it at the status quo until we find a good reason we agree on to make a change. 
comment
I wouldn't say this is a bug.  It is a feature request to add a weighted option to the modularity matrix. That seems like a good idea and should be pretty straightforward to implement.  It needs  1) Modified code in modularitymatrix.py to add a weight=None optional keyword 2) Tests for the weighted case added to test_modularity.py 3) Updated documentation including reference to the definition. 
comment
Yes, please do implement the changes.  
comment
Looks good with a nice simple fix. Thanks. 
comment
Fixes #2072 
comment
In networkx-1.11 and later you need to say `from networkx.drawing.nx_agraph import write_dot` or `from networkx.drawing.nx_pydot import write_dot`  then   ``` python write_dot(G,"grid.dot") ```  An example is at   https://networkx.readthedocs.org/en/stable/examples/pygraphviz/write_dotfile.html 
comment
Thanks for pointing out the documentation issue. We have moved our documentation home from github to readthedocs but haven't clearly marked the older versions.  I'll fix that. 
comment
Added redirects to networkx documentation to point "latest" and "development" docs to networkx.readthedocs.org.  That won't fix all of the old links and some will make 404s but it's the best we can do using simple techniques at networkx.github.io. 
comment
Yes, we need to either remove those (I recommend) or point them to the correct PDFs e.g. https://media.readthedocs.org/pdf/networkx/stable/networkx.pdf 
comment
But let's accept the 0 and 1 (and strings of 0 and 1) anyway.  
comment
Well, that might be True.  But we could be more liberal in what we accept.  After all:  ``` python In [1]: True==1 Out[1]: True  In [2]: False==0 Out[2]: True ``` 
comment
I am happy with this.  Merge? 
comment
Yes I suggest we allow reading 0 and 1 as booleans.   
comment
This just needs a merge conflict resolution. 
comment
I haven't looked at the paper but I guess this is for undirected and connected graphs only? 
comment
No problem ignoring the weights.  But this will potentially give confusing answers for directed graphs and raise an exception for disconnected graphs. 
comment
Looks OK.  
comment
Yes, we should definitely do something about this.  I suggest we all start using Python 3. 😃  
comment
It looks like this needs further documentation and testing.  There is no ref to the algorithm source. 
comment
Incomplete/garbled PR.  Please correct or open new PR. 
comment
networkx/algorithms/hybrid.py 
comment
This function just does  ``` python     pos = nx.drawing.graphviz_layout(G, prog)     draw(G, pos, **kwargs) ```  Can we just avoid the complexity of all of the possible graphviz installations and simply remove it? 
comment
``` python In [1]: import networkx as nx  In [2]: G = nx.cycle_graph(4)  In [3]: nx.is_simple_path(G,0,2) Out[3]: False  In [4]: nx.is_simple_path(G,[0,2]) Out[4]: True ``` 
comment
There are definitely some issues with the girvan_newman() algorithm. 1) Your case is triggering a bug where self-loops are not considered - Self loops need to be removed, e.g.   ``` python     g = G.copy().to_undirected()     g.remove_edges_from(g.selfloop_edges()) ```  2) Your original graph has betweenness centralities all equal which triggers a bug where all of the edges are removed in the first pass (one community).  The _remove_edge code needs to be something like  ``` python     number_components = nx.number_connected_components(G)     while nx.number_connected_components(G) <= number_components:         betweenness = nx.edge_betweenness_centrality(G, weight=weight)         edge = max(betweenness, key=(lambda k:betweenness[k]))         G.remove_edge(*edge) ```  3) The tests are hard to understand and possibly not robust to alternate edge orderings that may occur and are still correct.  
comment
The girvan newman function has some bugs #1799  
comment
Needs coordination with other G-N pull requests and issues e.g. #1799  
comment
Looks good.  Maybe a couple new lines in the doc files would be good too. 
comment
No - you already got it in the rst file.  I just missed it. 
comment
We could do that. But perhaps the problem is better addressed in pydot (pydotplus)?    ``` python In [7]: import pydotplus  In [8]: P = pydotplus.Graph()  In [9]: P.add_node(pydotplus.Node('foo:bar'))  In [10]: print(P.to_string()) digraph G { foo; } ```  Note that this works with pygraphviz  `````` python import networkx as nx G=nx.Graph() G.add_node("foo:bar") print(nx.nx_pydot.to_pydot(G).to_string()) print(nx.nx_agraph.to_agraph(G).to_string())```  ``````  strict graph "" { foo; }  strict graph  {     "foo:bar"; }  ``` ``` 
comment
I still think this is a pydot/pydotplus bug.  Can't you do a workaround by adding quotes in your node names?  ``` python In [14]: G = nx.Graph()  In [15]: G.add_node('"foo:bar"')  In [16]: print nx.nx_pydot.to_pydot(G).to_string() strict graph "" { "foo:bar"; } ```  Also you likely have a broken pygraphviz installation.  It should be faster than pydot - for me writing a 750 nodes 7000 edges dot file from networkx is 3-4 times faster with pygraphviz.  ``` python In [1]: import networkx as nx  In [2]: G = nx.gnm_random_graph(750,7000)  In [3]: %timeit -n 1 -r 2 nx.nx_pydot.write_dot(G,'foo.dot') 1 loops, best of 2: 2.33 s per loop  In [4]: %timeit -n 1 -r 2 nx.nx_agraph.write_dot(G,'foo.dot') 1 loops, best of 2: 671 ms per loop  In [5]: import pygraphviz  In [6]: pygraphviz.__version__ Out[6]: '1.4.dev'  In [7]: import pydotplus.version  In [8]: pydotplus.version.__version__ Out[8]: u'2.0.2' ``` 
comment
``` python In [3]: import networkx as nx  In [4]:  G = nx.gnm_random_graph(750,7000)      In [5]: %timeit -n 1 -r 2 nx.nx_pydot.write_dot(G,'foo.dot')    1 loops, best of 2: 1.28 s per loop  In [6]: %timeit -n 1 -r 2 nx.nx_agraph.write_dot(G,'foo.dot')     1 loops, best of 2: 548 ms per loop  In [7]: import pygraphviz       In [8]:  pygraphviz.__version__              Out[8]: '1.3.1'  In [9]:  import pydotplus.version            In [10]:  pydotplus.version.__version__ Out[10]: u'2.0.2' ``` 
comment
I'm not enthusiastic about putting in hacks to fix bugs in other packages.  So I'm going to keep advocating for not "fixing" this and encouraging the pydot maintainers to do so instead.    I've spent quite a lot of time dealing with quotes and special dot string issues: - https://github.com/networkx/networkx/issues/1872 - https://github.com/networkx/networkx/issues/1348 - https://github.com/networkx/networkx/issues/317 - https://github.com/networkx/networkx/issues/350 - https://github.com/networkx/networkx/issues/184  I don't want to break any of that.     I hope you can make pygraphviz work.  It can handle more complicated files than pydot and in the end might be worth the time to get installed correctly.  E.g. try pydot on https://raw.githubusercontent.com/ellson/graphviz/master/rtest/windows/tree.gv 
comment
There is probably some issue with your pygraphviz install.  Does it run the tests? `python -c "import pygraphviz; pygraphviz.test()"`  ``` $ time python -c "import networkx as nx; G=nx.nx_agraph.read_dot('J1fn.txt');nx.nx_agraph.write_dot(G,'foo.dot')"  real    0m2.087s user    0m1.390s sys 0m0.592s ``` 
comment
Also, there is another option for you.  Adjust this line  https://github.com/networkx/networkx/blob/master/networkx/drawing/nx_pydot.py#L202 in your own copy of the nx_pydot module. 
comment
Also I just noticed that running that J1fn.txt file through the graphviz dot layout program causes it to never finish (or at least takes a very long time).  It does process correctly with other layouts (neato, sfdp,...) That is  `dot -Txdot J1fN.txt >foo.dot` isn't working well - that is a graphviz issue. 
comment
You can distribute your own copy of nx_pydot.py with the rest of your software.   
comment
In my tests I have been using graphivz-2.36.0 or graphviz-2.38.0 on a Linux system (Ubuntu). Both work for me.  But also OSX  ``` $ time python -c "import networkx as nx; G=nx.nx_agraph.read_dot('J1fn.txt');nx.nx_agraph.write_dot(G,'foo.dot')"  real    0m1.030s user    0m0.961s sys     0m0.061s aric:tmp aric$ uname -a Darwin aric.local 14.5.0 Darwin Kernel Version 14.5.0: Mon Jan 11 18:48:35 PST 2016; root:xnu-2782.50.2~1/RELEASE_X86_64 x86_64 $ dot -V dot - graphviz version 2.38.0 (20140413.2041) ``` 
comment
We won't fix this as part of networkx.  The `write_dot` function either uses the pygraphviz or pydot package to output the graph data in the dot language.  Those are maintained separately.   I'd suggest contacting those projects. 
comment
This seems OK to me.  It touches a few files so would someone else please review and verify? 
comment
How is this different from G = nx.Graph(subedges)? 
comment
Nice addition.  It would be nice to add an example (notebook I guess now) showing how to use the laplacian smallest nonzero eigenvalue to create the initial partition. 
comment
Needs conflict resolution. 
comment
Is it faster to use heapq like this instead of sorting? 
comment
There are also a lot of other changes besides the underlines - adding automodule directives and switching from authofunction to autoclass.   Are those from errors or changing the format/function? 
comment
I think there was a reason why we didn't want to use autoclass.  But I don't remember exactly why right now.  Maybe the autoclass format didn't present the info in the way we wanted so we used a more explicit approach of specifying each method. 
comment
How do I set up local doc build for this that matches readthedocs tools?  Then I could test this locally. It's some version of sphinx and tools right? 
comment
I agree with @chebee7i on this.   I don't think changing a bunch of old working code just to follow a new standard is a compelling argument.    For new code, or when modifying old code to fix bugs or add/change functionality, following PEP-8 is good (though not slavishly please).  Separate commits for style changes are helpful to be able to easily review substantive v cosmetic changes.  According to PEP-8 Guido's insight was (essentially) that readability counts.  As a mathematician I like reading thinks like  ``` python for u in G ```  or  ``` python (u,v) = e ```  but maybe I am in the minority.    
comment
Style does matter but overall I suggest we spend our time working on other things in the code base. E.g. the documentation can be improved in many many places.  So if someone updated the documentation in a package, and, in a related commit to those changes, made some PEP-8 (especially whitespace) changes I would review and approve that ASAP.   
comment
@MridulS it would be very helpful if you pointed out the confusing parts for beginners (separate issue or issues from this PR).  
comment
Maybe my comments here are misunderstood.  There is plenty of work to be done  - and sincere thanks for everyone's help - but fixing whitespace and formatting isn't even close to the top 100 list of things that could be done to improve this package.  So carry on people.  Over and out.  
comment
I wrote above "For new code, or when modifying old code to fix bugs or add/change functionality, following PEP-8 is good (though not slavishly please). Separate commits for style changes are helpful to be able to easily review substantive v cosmetic changes." 
comment
But don't we want to distribute the tests - so users can say  python -c "import networkx; networkx.test()" to make sure the installation is OK? 
comment
Maybe nobody else but me does,  but I use   ``` python -c "import networkx; networkx.test()" ```  and sometimes ask others to do that to verify that the installation is correct and that the expected extra packages are installed.  So I like having the tests distributed with the code.   I read the blog post and arguments about moving the tests but it doesn't convince me this is the right thing to do now. 
comment
No strong practical arguments for moving tests out of source tree at this point. 
comment
It's helpful to the packagers and maintainers to explicitly indicate dependencies so I don't mind being more detailed here in how those dependencies come about.   i.e. 3 files is OK with me. 
comment
Definitely looks wrong.  Please make a PR to fix. 
comment
Any speedup using multiprocessing will depend on the algorithm.  You don't say what kind or size of graph you are trying to generate it isn't possible to determine if your code can be improved. 
comment
Lets go for it, it won't get easier... 
comment
I prefer explicit math mode as @jfinkels suggests.  It'll require some work to update. 
comment
No - we need to handle that with our tests, not Travis CI.  On Tue, Feb 23, 2016 at 9:17 AM, Michael E. Rose notifications@github.com wrote:  > I meant, the bug is that Travis CI complains about that. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/2000#issuecomment-187769593. 
comment
It's not unpredictability in the Travis testing causing your test failures.  It is the unpredictable output ordering of Python dictionaries using Python 3.  We are obligated to write our tests to handle that.  I suggest to start using Python 3 when you can.   
comment
Fixes #1949  
comment
Apparently no interest this year. 
comment
You can use neworkx to write gexf files.  You don't need to install pygexf too. 
comment
This is not a networkx issue.  It is a pygraphviz/graphviz issue.  (I can't reproduce it though). 
comment
If I remember correctly how this format works this is a correct output.   The 'nodes' key in the output data tells you that node 0 (the first element of the list) is called 'k' and node 1 is called 'l'.  Then you have the links key which says source 0 is connected to target 1.   
comment
There are many json graph formats.  This one is based on d3's style.  You can configure the name of the attributes.  e.g.  ``` python In [11]: json_graph.node_link_data(G,{'id':'name','source':'source','target':'target'}) Out[11]:  {'directed': False,  'graph': {},  'links': [{'source': 0, 'target': 1}],  'multigraph': False,  'nodes': [{'name': 'k'}, {'name': 'l'}]} ```  Maybe that should be the default? 
comment
I'm not sure why we are using `id` instead of `name` as the default.  If the latter is more common I support changing that.  G.graph is not always empty and allows us to round-trip all of the data we store with networkx graphs. It can be ignored by any program that doesn't want it.  ``` python In [5]: G.graph['title']='my title'  In [6]: json_graph.node_link_data(G,{'id':'name','source':'source','target':'target'}) Out[6]:  {'directed': False,  'graph': [('title', 'my title')],  'links': [{'source': 0, 'target': 1}],  'multigraph': False,  'nodes': [{'name': 'k'}, {'name': 'l'}]} ``` 
comment
Yeah, we can certainly do that.  The great part about graph data formats is that there are so many standard ones.   
comment
Needs PR. 
comment
Contains fix + extraneous commit.  Please fix. 
comment
It's not the first time I've gotten such a compliment but usually it is for my research papers not code :smile:. I think I wrote that since I figured people might not run e.g. chrome with -disable-web-security etc. in order to see the results.  But we could suggest that if it is more relevant and understandable.  It still doesn't work with lynx though. 
comment
That could work.  But it adds another dependency (flask). 
comment
Yes, we should fix that starting tag.  I don't have any strong opinions about meta tags.   It might not be a bad idea to note that it was networkx that created the broken gexf file :smiley:  
comment
This change was part of https://github.com/networkx/networkx/pull/1577 
comment
It's 2 problems. 1) The link should be updated to point to networkx.github.io 2) The redirected at networkx.lanl.gov is broken (I'll fix that) 
comment
Install the decorator package $ pip install decorator  https://pypi.python.org/pypi/decorator 
comment
is_path is maybe interesting enough move somewhere better than "utils". 
comment
Yes, that sounds good. 
comment
Questions are best for the mailing list https://groups.google.com/forum/#!forum/networkx-discuss 
comment
Perhaps it was temporary?  This link works for me http://cs.anu.edu.au/~bdm/data/formats.txt  (redirects to http://users.cecs.anu.edu.au/~bdm/data/formats.txt) 
comment
The history is at #493 and #596. Franck Kalala and I wrote those functions.  I'd be glad to fix them to correctly reflect the definitions and naming conventions.  Could you point us to the papers with the definitions? 
comment
Thanks for picking this back up.  We do need to rename things here - obviously it is confusing to me and probably others too.  Here are some questions for you  1) Can subgraph centrality and communicability centrality be combined together in one algorithm with a switch (open v closed walk)?  2) How about renaming things to open_walk_  and closed_walk instead of the current names?  I haven't looked (recently anyway) at the papers you listed so maybe I am misunderstanding things here. 
comment
Well, it's all clear now :confused:. I think we need a pull request with a proposal to endorse. 
comment
I was hoping you would figure that out for me.  I might not get to reading all of those papers and unwinding it myself for a while. 
comment
II probably just didn't quite understand here.  I'm working on cleaning up many old issues.  I'd like to get these resolved and thanks a lot for your help. 
comment
OK - those changes are good. Could you make a pull request? 
comment
I like `cyclic` and returning a pair (s1,s1) for a single element iterator. 
comment
Can we get by with not using unordered data structures in the doctests or just skip this doctests? I think there is a middle ground between forcing ordered datastructures and not testing any docs. (And I agree with Tim Peters that this is a doctest feature not a doctest bug). 
comment
I took a quick look at that paper again. I can't see for sure without some closer reading - but double check the `inc(bin[du])` there too. 
comment
It could be the right thing to do is to consider the undirected version of the graph unless you want to do something like http://www.graphdegeneracy.org/dcores_ICDM_2011.pdf 
comment
I prefer node sets instead of node lists from a mathematical standpoint.  I don't see any practical reason not to switch `connected_components` to return a generator of sets of nodes. 
comment
@jtorrents are you satisfied with this?  It seems ready to merge. 
comment
The last time I tried this approach (FancyArrow) it was pretty slow for graphs with more than 10s of nodes.  But maybe the performance has improved in matplotlib?  Also it would be nice to get the arrows pointing at the node patches instead of the center of the node patches.  That is tricky, I know.  Also #1325. 
comment
Partial solution.  See #1325  
comment
This looks like a great approach.    A small thing: would it be possible to split the PEP-8 adjustments into a different commit or PR?  I'm looking at this and the diff noise from the PEP-8 changes is obscuring the signal. 
comment
Well, we should look at all the PEP-8 changes anyway to double-check nothing strange happened with autopep8.  I don't expect that but a lot of modules in networkx use these :-) 
comment
I don't think we need to wait for #1168.  Can we include this now? 
comment
Still waiting to hear other opinions/comments about including this. 
comment
#1152, #1917 
comment
The tests are still failing.  Fixing those is necessary to do the merge.  I think the rest needs a careful review and likely some adjustments.   I'd also like to see scipy sparse matrices used. That is what we are now using (or intend to change) in the  rest of the NetworkX package. 
comment
The scipy import needs protection from when the module isn't found (scipy is not required). See e.g. pagerank_alg.py for how to do that. 
comment
Well, the future is now!  So might as well get it implemented with scipy.sparse matrices so we don't have to go and fix it later.    
comment
See #1538  
comment
The way I had in mind to use the context manager was to not modify any of the shortest path functions and instead do e.g.  ``` python with hidden_nodes(G, nodes=exclude) as H:     path = nx.shortest_path(H,source,target) ```  It seems the basic concept can work in this specific case.  The hidden edges function should be separate I think. 
comment
I suggest we implement a "view" concept instead of this approach #1896  
comment
Superceded by #1555 
comment
Sure that is good.   
comment
For the fourth combination isn't it just e.g. for a graph G:  `H = nx.Graph(G.edges()); H.add_nodes_from(G)` 
comment
I generally prefer  We could put your recipe for new datadicts in the copy() docstring and only provide G.copy(deepdata=True/False) in the method.  Did we already discuss the option of making two methods?  `G.copy()`, `G.deepcopy()` 
comment
I agree with you @dschult.  Let's implement it that way. 
comment
Does #1957 close this too? 
comment
It looks like there is a name issue with the imports of communicability and communicability_exp.  Note that the name of the module and functions should be different to avoid name collisions.  This is why the module was called communicability_alg.py before.  If you haven't already it is a good idea to install python-nose and run the tests on your code (and the whole package) to see that everything is working as expected.  Not sure what your operating system is but 'pip install nose', then run `nosetests -v networkx` in the source treet. 
comment
Looks like there are still some issues.  Note that your creation and import of a subgraph.py module collides with an existing module name and causes some errors.  I suggest changing that. 
comment
If you follow the details link in the travis-ci report above you can see the errors. 
comment
@goulu would you make these small fixes so we can merge? 
comment
Small thing but you could bump the copyright year and comment out the __author__strings. 
comment
I guess the issue is that the title of the example should be "degree rank" or something similar? 
comment
It still looks a little confusing when you look for versions since there is networkx-1.10 and v1.11 there. 
comment
Unfortunately this still doesn't completely fix the problem.    If pygraphviz is installed and pydotplus is not the tests will still fail.  The functions `write_dot`, `read_dot`, and `graphviz_layout` from nx_pydot will be imported into the namespace and replace the (functioning) ones from nx_agraph since the nx_pydot import comes last.    There are a few ways this could be fixed.   My favorite at the moment is to not import any of these packages by default and force the user to make an explicit import to use them. 
comment
No problem with the testing.  Whether users can figure it out is a good question.  Your idea is fine with me.  The only principle I am working under is to keep it simple - no fancy import tricks which is what got us here in the first place. 
comment
Should we do this for networkx-1.11 too? 
comment
Interesting.  I don't know what the correct definition is.  But we should be consistent. Maybe don't allow self loops?  If we document that and give an example of how to remove self loops then both cases should be covered. 
comment
Anyone have comments on how to resolve this?   
comment
Would someone be willing to implement this? 
comment
Should we just call this 1.11?  It may be around for a while since the 2.0 changes are significant. And we may want to add a few more backports or fixes here too. 
comment
Need to add to reference documentation. 
comment
I like that suggestion; it makes sense to be consistent.  Anyone want to do the implementation? 
comment
Fixed in #1782  
comment
Yes, we chose not to import those algorithms with the main package.  There are some others too. This is certainly up for discussion.    Some of the reasons not to import everything are - namespace gets too large to easily inspect,  speed of `import networkx`, and memory usage. 
comment
I like the idea of not importing the approximation package automatically.  Perhaps we can suggest in the docs using `from networkx.algorithms import approximation`?  
comment
:dog2:  
comment
Don't we want to use Slack like all of the cool kids? 
comment
I think the Slack folks will be amused to hear it called "overkill." 
comment
Are we done here? 
comment
I usually advocate to remove the create_using() idiom here - #1393 etc. But I don't have consensus on a replacement plan yet. 
comment
continue at #1393 
comment
There are quite a few other algorithms for ranking nodes - for example there is HITS, betweenness centrality, closeness centrality, etc.   Is there a publication with the method you are suggesting we implement? 
comment
We discussed that briefly before in #693 with no final implementation or decision on including those algorithms.  I think the basic issue is that we couldn't understand how they would be used. 
comment
Could you explain why you think that should be added? 
comment
An easy workaround is to just use deepcopy as a function:  ``` python In [1]: import copy  In [2]: import networkx as nx  In [3]: G = nx.path_graph(3)  In [4]: memo = {}  In [5]: H = copy.deepcopy(G, memo) ``` 
comment
I see your point but I am not in favor of sprinkling a "memo" keyword around the NetworkX graph api and algorithms.  It is an interesting feature of the Python copy function but  - for most users it might confuse, or at least complicate, understanding how to use the code - aligning labeled graphs is a simple lookup problem if you keep a unique id for the nodes - it might be hard to implement if an alternative (non-Python) backend was built  (ps - if we implement the generator proposal for connected_component_subgraphs() then your later use case can also be handled by calling copy explicitly, https://github.com/networkx/networkx/pull/973 ) 
comment
I'm still not enthusiastic about adding the memo keyword.  I do see your point and use case but from my perspective it takes a fairly strong argument to add a Python-specific and potentially confusing argument to some very basic functions.    In some cases it would be pretty simple to subclass e.g. the Graph class and replace the copy method.  In other cases keeping a node id-table would work (thought  maybe a little tricky as you point out). 
comment
No further updates.  Closing. 
comment
Dropped 3.2 suport now. 
comment
Let's keep up this kind of thinking - open new issues for specific items. 
comment
We have considered this before.  What does DiGraph.add_clique() do? If I remember right I think last time I advocated for removing add_path, add_star, etc at the end of the discussion. 
comment
See #1883. 
comment
Let's use Jupyter notebooks for this #1754  
comment
Lets move this to #1705  
comment
You can add me to the mentor list. 
comment
Here are some other ideas for projects that are not fully scoped out.  Comments? 1. Add algorithms for directed triangles.  Clustering #1331, Triadic census #191, ... 2. Add community finding algorithms.  A previous GSoC project made some progress on this #764.  There have been other contributions since #951, #1092 (Louvain) and #1265 (Girvan-Newman), #617 (label propagation) 3.  Rework the matplotlib drawing package as a separate package from networkx #1325, with a more sensible interface, proper arrows #1248, #1357, #418.   4.  Implement multilevel graph layout algorithms (force-directed, eigenvector) 5. Create IPython notebook based tutorials and examples  6. Incorporate or interface to http://www.graphclasses.org/smallgraphs.html 7. Implement more shortest path algorithms (or add features) #1120.  k-shortest paths #793, restricted paths #480, etc 8. Add network metrics for weighted networks #730  9. Add more algorithms and support for bipartite graphs #1294, #1322  10. Better XML format readers with SAX #448  11.  Create new web site with design and logo.  Improve documentation web site #1221  
comment
Yes, that would be fine @ysitu.  I don't think there are many places where we are using numpy.random() though.   And we do allow seeding the python random generators in many cases (though probably not all). 
comment
We are part of PSF. https://wiki.python.org/moin/SummerOfCode/2015 
comment
I'd rather have proper Matplotlib arrow drawing than a logo. But still see #11 above in my list. 
comment
Yes, you can discuss ideas here. 
comment
Thanks everyone for a great GSoC. 
comment
We'll need some more help with this to figure out if and where there is a bug.   Could you contact the Gephi developers with your file and errors loading into Gephi and see what the problem is?  If NetworkX is writing a non-compliant file we can fix that.   
comment
Any update on this?  There is another issue related to gexf formatting in #927  
comment
Anyone make any progress on finding out where the error is? 
comment
I think this is a pydot bug (works correctly with pygraphviz).  ``` python In [1]: import pydot  In [2]: n = pydot.Node('foo"bar')  In [3]: P = pydot.Dot()  In [4]: P.add_node(n)  In [5]: print(P.to_string()) digraph G { foo"bar; }   In [6]: P.create_dot() Warning: /tmp/tmp6RkNEi: syntax error in line 2 near ''  Out[6]: ''  In [7]: pydot.needs_quotes('foo"bar') Out[7]: False  In [8]: pydot.needs_quotes('foo\nbar') Out[8]: True ``` 
comment
I'm going to close this since there isn't a simple networkx workaround that I can see.   This does work correctly with pygraphviz so I suggest using that if you can. 
comment
I don't think there are many places where we are using that kind of nonsense.  Maybe there are more than I remember (didn't check now) and we should try to eradicate them.  They are likely in the code where we are trying to interface to an external library or tricky standard. 
comment
I am not enthusiastic about adding that dependency for a few cases.  I'd like to _remove_ the ugly code not add to it. 
comment
I'm not enthusiastic about six.  As you can see it more or less replaces one hack with another. Worse, if we depend on it then some overachievers will find lots more places to use it to try to "optimize" code.  I'd prefer we leave the hacks and focus on writing good Python3 code. 
comment
Yes, I have fought off using iteritems() and friends  with six several times before.  We should be thinking Python3.   The places where #1709 uses `six` are not compelling to me. - we don't need flatten() in layout.py - I don't understand the nx_pydot() need to test for strings (and it is a simple hack) - the graphml.py change is no simpler or clearer - the gml.py reader and writer are so amazing already - it would be great to simplify there including not using lib2to3. 
comment
If you ignore the gml.py code (which is already very complicated) it doesn't simplify things. I'm fine with making these modules Python3 only.  On Sat, Aug 8, 2015 at 1:16 PM, Neil notifications@github.com wrote:  > I have to disagree that the changes in #1709 > https://github.com/networkx/networkx/pull/1709 are "no simpler or > clearer". At the very least they are 30 lines fewer. >  > I agree with you that we should "be thinking in Python3", which means that > things like basestring should not appear in the code. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1702#issuecomment-129037570. 
comment
No, you misunderstood.  I like Python2 and fine with me if it lives forever.  But we can write Python3 code so why not? 
comment
Can we agree we disagree and close this?  And not use six :-)  
comment
I mean agree to close :smiley:  
comment
I agree that using a module level attribute isn't a great idea.  But  **We will absolutely not remove author information.**  Remember this software project is more than 10 years old.  It started long before there was `git` or `github`.  In fact I think the first version control system was `cvs`.   We have tried to retain the authorship history through the years with version control but it is possible some is lost.  Also there are examples I remember where I added authors to the list because they made substantial contributions using channels other than version control such as email. 
comment
How about we agree to update the author variables to comments when we modify the file for some reason? 
comment
Is this now resolved? (with v1.11) 
comment
Fine with me to stop supporting 3.2.   
comment
Fixes #1867 
comment
Closed for now.  Will resolve itself when sphinx issue is fixed.   
comment
I suggest we release 1.11 with these and any other small annoying bugs including doc fixes. But let's try to do it soon. 
comment
Fixing the doc builds would be great.  The #1398 GraphML decision is not quite resolved. 
comment
Yes, we should make a release ASAP.  Are there any loose ends to take care of before we do that?  On Thu, Dec 24, 2015 at 8:21 AM, AgostinoSturaro notifications@github.com wrote:  > Now that release 1.10 is included in Anaconda, I have to tell people to > manually patch against this issue #1760 > https://github.com/networkx/networkx/pull/1760 > Is there any news on 1.10.1? > Thanks. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1759#issuecomment-167125526. 
comment
See #1882  
comment
You are not actually generating JSON there.  You are making "node_link_data" out of a networkx graph.  That data may be serializable with the Python json encoder.  e.g.  ``` python d = json_graph.node_link_data(G)                                                 print json.dumps(d) ```  but in your case it can't serialize your custom object and you get  ``` TypeError: Foo|2 is not JSON serializable ```  A basic serializer would look like this:  ``` python import json                                                                      class MyEncoder(json.JSONEncoder):                                                   def default(self, obj):                                                              if hasattr(obj, '__json__'):                                                         return obj.__json__()                                                        return json.JSONEncoder.default(self, obj)                               print json.dumps(d, cls=MyEncoder)                                               ``` 
comment
You are probably looking for the `copy()` method of the graph classes. http://networkx.readthedocs.org/en/latest/reference/generated/networkx.Graph.copy.html 
comment
Oh, I see what you want to do.  I don't think there is a function to do that.  You could call the copy() method and remove the edges but that isn't so efficient.    In looking at the create_empty_copy() function I'm not sure why we aren't adding a way to copy the data so you proposal make sense.  Also I'm not sure why we have the option to add the nodes.  Do we really need that? 
comment
Yes, I agree with you.    We might consider removing the with_nodes= argument too unless we can remember or figure out why that is useful.   
comment
It's not clear from the Sphinx project pages when this fix will be released.  But it seems like a lot of work (and error prone) to create all of those lists.  So I suggest we build the pages ourselves and push them to networkx.github.io for now.   
comment
There were moved to algorithms.bipartite.generators https://github.com/networkx/networkx/tree/master/networkx/algorithms/bipartite  https://github.com/networkx/networkx/pull/1390 
comment
This could get tricky if the types are not so easily abstracted.  For example if someone has an attribute with  '1', 1, True ,and "True" as values, etc.  Maybe we shouldn't attempt to do something fancy here? 
comment
It might be more clear if we use different error messages based on whether the graph is directed or not (use the is_directed() method). 
comment
How about both?  i.e. "Invalid degree sequence: odd sum". 
comment
I'm pretty sure BibTeX is supposed to handle that case correctly and it is a typical example :https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#Authors 
comment
I use the IEEEtran bib style and it works fine.  In fact there are even examples for IEEEtran.bst style that don't have commas. e.g.  ``` an article in a journal Note the use of the IEEE_J_EDL string, defined in the IEEEabrv.bib file, for the journal name. IEEEtran.bst defines the BibTeX standard three letter month codes per IEEE style. From the June 2002 issue of "IEEE Transactions on Electron Devices", page 996, reference #16. @article{IEEEexample:article_typical,   author        = "S. Zhang and C. Zhu and J. K. O. Sin and P. K. T. Mok",   title         = "A Novel Ultrathin Elevated Channel Low-temperature                     Poly-{Si} {TFT}",   journal       = IEEE_J_EDL,   volume        = "20",   month         = nov,   year          = "1999",   pages         = "569-571" } ``` 
comment
Sure, I see the point but these changes require review and attention by developers so they aren't free.  It looks like more than half of the last 20 pull requests are of this type (not bug fixes or feature additions). 
comment
I don't understand what is happening there.  It looks like a lot of errors - is there something global wrong with the config (e.g. numpy docstyle) or is it just that there are many small fixes needed? 
comment
#1597 fixes the issues that were breaking the doc builds.   I have been building the docs nightly at around midnight US Mountain time.  But it's too hard to stay up that late. If we can use RTD to build them automatically that would be great.  Note that there are also pdf versions.  I'm not sure if anyone looks at those though. 
comment
The documentation was correctly built with my automated scripts and appears at http://networkx.github.io/documentation/development/  But this isn't resolved.  It would be better to have the documentation built with a system that is not dependent only on me (not noticing it is broken, machine down, etc). 
comment
OK - I added the readthedocs hook. 
comment
Maybe the problem is that the parser doesn't recognize tabs as whitespace separators? 
comment
Oh, right.  That isn't part of the spec.  We'd need to add something like              r'[A-Za-z][0-9A-Za-z_]*\s+',  # keys to the parser to accept that. 
comment
The performance should be better though in all cases dense than trees right? 
comment
I'm not a user of this feature but it looks fine to merge. 
comment
This is probably a bug.  It's at least unexpected from the documentation.  For example  ``` python In [1]: import networkx as nx  In [2]: G = nx.Graph()  In [3]: H = nx.Graph()  In [4]: G.add_node(1,color='red')  In [5]: G.add_node(2,color='blue')  In [6]: H.add_node(1,color='green')  In [7]: H.add_node(2)  In [8]: R = nx.compose(G,H)  In [9]: R.node Out[9]: {1: {'color': 'green'}, 2: {}} ```  is surprising since the attribute of node 2 in graph G is replaced by the "no-attribute' state of graph H.  Bug or feature?  Fixes? 
comment
I'm not in favor of adding checks unless things really are confusing.   In this case it really doesn't make sense to use None as a node and we advocate against that. 
comment
LANL is sponsoring SciPy 2015 and I am planning on being there.  I'm not sure which dates yet. I am happy to meet up with NetworkX developers and users there.  If there is enough interest perhaps we can have a formal gathering or sprint. 
comment
I remember the same discuss @chebee7i.  I agree with the approach of fixing the _iter functions and doing light work on some warts. And call that 2.0. It could be difficult (and probably out-of-scope for a GSoC project) to redesign the entire API.  There will certainly be many opinions on what a "clean design" interface looks like.   
comment
I hope we won't use "viewkeys()" and friends.  They don't exist in Python3. 
comment
I am -1 on using future and six to put lots of Python2/3 cruft in the code. Python 3 is here people. And it works great. 
comment
Should we make a release before 2.0?  There have been many changes and additions since the last release.  These could be used now without having to worry (much) about API changes that will be in 2.0.   And we pushed the date of 2.0 now until September. 
comment
I'm not trying to generate a long sidetrack here but...  How about exposing the adj dictionary as Graph.adjacency()?  Then (with Python3 anyway) you get iterable views with Graph.adjacency().items(), etc. 
comment
Yes, it doesn't expose the internal data structure but instead provides an interface to it. But I was thinking it would be exactly the same thing.  Which brings up a question - on which PR/Issue should I raise it or maybe we already have? - is the plan to return actual iterators for methods or, like the one proposed above, is an iterable view OK too? 
comment
Yes, G.adj and G.adjacency() would be the same then for this implementation.  But perhaps somebody wanted to use a different "backend", then this would facilitate that without having to make adj be a dict-like object (which of course is also do-able). 
comment
The question is whether we are specifying that we return an iterator or a no-copy iteratable for the graph classes. Python 3 gives us iterable views that allow low memory traversal of e.g. dicts.  But do we get much with that here?  e.g. re-use.    One reason we wanted to use iterators was to reduce the memory of building a potentially large list or other data structure in cases where it wasn't needed (G.edges()). 
comment
No there is no version-agnostic way to do it as far as I know.  But adj.items() works for both Python2 and Python3 with the benefit of being a view in Python3.     And I don't care about improving Python2 performance...  I'm too busy improving my FORTRAN77 code. :smiley: 
comment
We're all using Python3 right? 
comment
There are (at least) two  issues here that are getting conflated 1. What should G.adjacency() return? 2. Should we consider returning (Python3) iterable views instead of iterators in some cases (e.g. G.node.items() or iter(G.node.items())  I'm thinking 1. Yes 2. No 
comment
I was just being snarky and checking to see if anyone was reading.  Of course you all are.  I don't know if I have a proposal for 1).  But yes that is what I was thinking for 2).  It seems simpler to explicitly return iterators (with a **next**) even though we could get away with iterable data structures such as lists or views.  Maybe we should think of use cases for adjacency() to try to discover what it really would be good for? Apparently we weren't using it much? I can see the value of something like G.adj.items() to look at all edges but also G.adj.values() if, say, you wanted to build an adjacency matrix. 
comment
I'm the one who suggested it.  So "yea" from me.  There will clearly be places where we need to use etc. list(Graph.nodes()) instead of Graph.nodes() if it is an iterator.  On Wed, Jun 11, 2014 at 7:52 PM, chebee7i notifications@github.com wrote:  > Yeah, I can see the value in doing it earlier. Do we have a third "yay" to > start implementing this? Here is where we find out how good of test > coverage we have. :) >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/572#issuecomment-45821813. 
comment
You are right.  The power method the way we have implemented it won't converge for adjacency matrices with a pair of extremal eigenvalues with opposite signs.  The odd-node path graphs have that property.  Maybe the solution is to use power iteration on A^2?  @dschult? 
comment
Yes, that is the "shifted power method".  I think it won't work in general for this case unless you know what shift to use.   
comment
Great.   It seems that as long as we shift the eigenvalues to the right (A + alpha I, where alpha>0) this should be safe and will compute the eigenvalue/eigenvector pair we want.   
comment
If the cutoff is small the single_source implementation might be faster too. 
comment
Absolutely not.   
comment
I guess this returns the number of elements in a _finite_ iterator. I'm not sure why we want to add these type of functions if they are e.g. only used in one place.  That is, I am not enthusiastic about building up a little library of aliases to one-liners. 
comment
I suggest we wait until we need such functions. It's not clear to me that this makes things simpler for beginners.   
comment
I'm not in favor of adding this as a method.  The "freeze" concept is not a real way to protect the data structure from modification.  It may be useful to some users but I don't like the implication that the graph or data is fixed if you use this. 
comment
@chebee7i suggested it 7 years ago https://networkx.lanl.gov/trac/ticket/176 
comment
I like this implementation. 
comment
Yes, fine.  There are documentation format issues that are breaking the automatic nightly builds #1571. It would be nice to clean those before we release 1.10. 
comment
I don't have any strong opinions other than to keep it simple.  In the past the pace of development was slow enough that we could make some RC versions on the master branch and not have a lot of extra additions in between.  Maybe we need to make a networkx-1.10 branch and put tags on that for RCs? 
comment
Shall we make make an rc2 and ask people to test?  Maybe we could get @santrotosi to take a look to see if we need to fix anything for Debian packaging?  He has been very helpful and I'd like to make it as easy as possible for him 
comment
It looks like we are close.  How about we make an rc2 and ask people to test?  There are often some small issues that can be easily resolved regarding packaging or documentation.   
comment
OK everyone - please test.  I added a Python wheel there too. https://github.com/networkx/networkx/releases/tag/networkx-1.10rc2 It might be good to fix the release.py file so it reflects 1.10rc2 instead of 2.0dev (I did that in the wheel). 
comment
I pushed the rc2 version to pypi https://pypi.python.org/pypi/networkx/ Would someone send a note to the networkx-discuss mailing list and encourage people to try it out? 
comment
No comments from the community.  I suggest we tag and release. 
comment
I still think the addons namespacery isn't clearly a good idea.  Can anyone provide some examples of projects that are doing this happily and what value it adds for the extra complexity? 
comment
OK - that is a different approach that doesn't use namespaces.   It uses this trickery instead https://github.com/mitsuhiko/flask/blob/master/flask/ext/__init__.py 
comment
I'm ready to release this if someone wants to make a tag.  No issues reported with rc2. 
comment
#1660 is merged.  Anything else?  I'll make a the 1.10 release later today. 
comment
I think it is just waiting for me.  I'll do this ASAP. 
comment
I just made the release. 
comment
I'm not sure what the best practice is here.  But I like the idea of the tutorial be a notebook only.  That way it can be downloaded by new users to get started.  One approach would be to put it in it's own repository and remove it completely from the source code repository.  Opinions? 
comment
Here are some possible advantages: - not tied to any release schedule, continuous updating and evolving - separate issues and  pull requests from the source repository - simpler and more obvious naming so easier to find and navigate - slightly smaller source code repository for networkx   I am hoping we can collect many notebooks with examples.  Are there disadvantages? 
comment
I'm not so enthusiastic about this idea.  Using peek() might mean less characters (more readable?) but also means those that read it have to look up somewhere else what the function does.   
comment
Actually what I like about next(iter(foo)) is that you can look up the terms up in a Python book. peek() would require first looking into the NetworkX code and then figuring out the for/break idiom. Do we really need this to be fast? 
comment
Link didn't work -   ``` python def arbitrary_item(S):     """     Select an arbitrary item from set or sequence S.     Avoids bugs caused by directly calling iter(S).next() and     mysteriously terminating loops in callers' code when S is empty.     """     try:         return next(iter(S))     except StopIteration:         raise IndexError("No items to select.") ```  I like this approach - good name.  Bonus corner case error handling. 
comment
isinstance(x, collections.Iterator)? 
comment
Looks fine to me. 
comment
It's not that hard.  But nobody has created a robust implementation. Similar to the arrow drawing... Graphviz works well to draw self loops.  On Mon, Jul 20, 2015 at 11:36 AM, Sanket Dasgupta notifications@github.com wrote:  > I think it is not implemented yet, same reason on why we parallel edges > can't be drawn. Its probably very hard to implement these in matplotlib. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1672#issuecomment-122963215. 
comment
Why does this matter with Travis?  There is no persistent cache between tests right? 
comment
It might be worth straightening out the cyclic imports.   Most of the rest are style. 
comment
Shouldn't we rebase this branch to the latest changes in master and fix any conflicts? 
comment
I'd guess the original intent was for nbunch to restrict searching to a subset of nodes reachable from those in nbunch.  Maybe @dschult could remember.    Do I understand that correctly that your use case is to try to force an ordering by specifying an initial ordering for all of the nodes?  Two small notes if we want to do this 1) out_edges() isn't a great name since it is really out_neighbors() (nodes not edges) 2) it would be good to adjust the recursive version accordingly 
comment
I was wondering about the purpose above too.  I was guessing that the reason was to try to impose some  order - e.g. if you had two disconnected paths and wanted the nodes of one to appear before the other. 
comment
I don't know the literature on lexicographical dfs but maybe we are trying to do something here that is more sophisticated than a standard topological sort?    Maybe I'm advocating for a new function and to keep topological sort simpler (for the majority of the use cases). 
comment
Those are two separate issues.    Some things are hard to test thoroughly with hash randomization turned on.  That was a new development for us (note that this software package was started in 2002) and we have figured out reasonable ways to test most things.  I haven't thought about this instance carefully but we could e.g. , for small examples anyway, catalog all correct topological sorts and make sure the result is one of those. 
comment
There are alternative implementations of some algorithms in NetworkX such as the recursive topological sort.  While you may not want to use it it is there for reference and learning.  
comment
Learning is not the only goal.   There are others like readability, usability, efficiency, etc.   Obviously they can't be all optimized.    In this case I was wondering whether it would be better to have a different function from topological_sort() that did what you want.  I haven't looked carefully but maybe there already are known algorithms for partial orderings with topological sorts which we could implement.  The toolgoical_sort() code was written long before dfs_postorder_nodes() so that is why this algorithm appears twice.   
comment
I haven't thought about his much -  - People do read networkx code for reference and learning.  We have solved a few homework assignments for sure.   - For the topological sort I haven't looked into the reference literature on algorithms.  There is something called "lexicographical first topological sort" (or something like that) which may be interesting. - There are two (or more) approaches here which are slightly different   1) Topological sort with a specified partial order on input   2) Topological sort with a key that creates an ordering of the nodes 
comment
The input I was thinking of was nbunch.  Your original idea to provide a partial ordering on all of the nodes.   Maybe it ends up being almost the same algorithm in the end?    It's arguable whether we should have two different topological sort algorithms if they are the same complexity.  Though someone might show that one of them performs better on a particular problem, etc.    But if we really are implementing "lexicographical first topological sort" then why not create a separate algorithm for that?     
comment
I like the alternative version.  It is simple and doesn't need the "reverse" option.  I think we can use that in place of the original and slower version.   
comment
@MisterSheik also add yourself to the authors list if you want. And update the copyright year on the file. 
comment
I'm around some of the week.  If you can't find me try aric.hagberg@gmail.com. 
comment
`dict(G.degree())` 
comment
That seems like a good place to start for further discussion anyway. 
comment
Yes, go ahead. 
comment
Single value.  Didn't we discuss a good way to do this somewhere else?  Lots of interconnected PRs and issues here.   
comment
I'm not enthusiastic about two functions (degree(), degrees()). 
comment
Let's not get too hung up in timing creation of iterators.  That doesn't seem so interesting right? How about using list(G.degree_*) to produce the data?  That is the use case. 
comment
Wait, `nbunch in self` is OK right? It'll check to see if a specified list or iterator is a node which should never work since nodes must be hashable.  We could get in trouble with nbunch=None though since we discourage but allow None as a node. 
comment
It's OK no?  ``` python In [17]: G = nx.path_graph(4)  In [18]: G.nodes() Out[18]: [0, 1, 2, 3]  In [19]: [1,2] in G Out[19]: False ``` 
comment
Yes I remember #1381.  I was advocating documentation (don't do that!) instead of checks. Which I still like.   I was worried there that we'd need to put checks in many places to be consistent. 
comment
In my opinion arbitrary breaking of ties is fine (but others might disagree).  The existing tests can be easily rewritten to account for that. 
comment
Unfortunately that is a bug the original code.  So we should fix it. For example if you mixed strings and integers as nodes they are not comparable and will cause an error.  ``` python In [14]: from operator import itemgetter  In [15]: min([[1,'2'],[1,2]],key=itemgetter(0)) Out[15]: [1, '2']  In [16]: min([[1,'2'],[1,2]]) --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) <ipython-input-16-cf094c25ae37> in <module>() ----> 1 min([[1,'2'],[1,2]])  TypeError: unorderable types: int() < str() ``` 
comment
Ok fair point about fixing bugs vs adding features.    As for testing the situation with ties you could just check to make sure the result is one of the multiple valid solutions (hopefully there are not too many to enumerate easily). 
comment
I will try to get those implemented ASAP. 
comment
Done. 
comment
Closing due to no update. 
comment
Something is probably not working correctly on the machine I am using to build that documentation. I'll take a look. 
comment
There are a bunch of errors in the build.  No time to trouble shoot right now. This is a good reason to host the complete documentation at readthedocs. Is there anything keeping us from doing that? 
comment
Some of those old versions are at networkx.github.io. And the rest are at http://networkx.lanl.gov/archive/ (those are probably only if historical interest now). I should be able to keep that working as long as the networkx.lanl.gov machine is running. 
comment
I'm not going to take the time to debug the failing automatic builds I am doing since readthedocs is working well. 
comment
Seems good for now.  But does anyone know why these are taking so long?  Do the tests take that long on all setups or is travis not working right with pypy?  (no time to try right now). 
comment
It might be better to wait and give us a chance to try it to work out any issues.   
comment
Looks fine. It seems strange to me to put this in a package 'tree'.  I would assume algorithms for trees go there, not necessarily algorithms that make trees.  But not really important in the end. 
comment
The Pandas release notes are really wonderful.  We should aspire to that. 
comment
It's fine to leave them all in the `tree` package.   
comment
Looks like the numpy import failed with pypy.  It isn't so useful to test the performance of external packages anyway so maybe we should remove the numpy timing?  My understanding is that you want to capture the performance of the base classes with these tests. 
comment
I haven't been following pypy very closely but they might not have a numpy implementation that provides what we need.  Certainly no scipy package. 
comment
I'm relying on @chebee7i  and @ysitu to help with the travis config.  It is fairly comprehensive/complicated now. 
comment
Is this ready to merge? 
comment
Fine with me to add to 1.10. 
comment
We are using the convention that a self loop adds 2 to the degree. 
comment
@mancuso can you help @bbphd with this issue? 
comment
@jtorrents - with the set consolidation part could you use connected components or union find? 
comment
I'm for adding "largest_connected_component".  As for the naming I prefer nx.connected_components() return subgraphs not nodes. (That isn't backward compatible but it is more consistent with graph terminology, right?).  To get nodes you could simply say, e.g. nx.largest_connected_component(G).nodes() 
comment
The timing tests are quite as bad if you use `copy=False` for `connected_component_subgraphs`. But still more than 2 times slower than `connected_components` 
comment
After thinking about this I agree with @ysitu that this is a one-liner that should be an example (it's very useful).  I'd like to see many more examples liek this as IPython notebooks.... 
comment
That looks like a bug.  Suggestions for a fix? 
comment
I can't think of a better way right now.  I think the corner case is that the mapping function ( a dictionary) has exactly the same keys as values.  So that is pretty simple to check - something like list(mapping.keys()) == list(mapping.values()) ? 
comment
Yes, that looks good.  I think it would also be good to add the extra functions you suggest. 
comment
I posted some code that gives the longest path in a dag at Stackoverflow: http://stackoverflow.com/questions/17985202/networkx-efficiently-find-absolute-longest-path-in-digraph  Would someone like to integrate this and finish up this PR? 
comment
Algorithmically gnp_random_graph should scale as O(n^2) where n is the number of nodes. And fast_gnp_random_graph should scale as O(p n^2) - the number of edges. But it looks like practically adding edges to the graph costs something so gnp_random_graph also has some dependency on the number of edges. 
comment
It might be simpler and more useful to make a small function that converts a weighted graph (ints) to a multigraph.    But on the other hand it looks like the from_numpy_matrix code isn't that complicated. How about making the parallel edges the default if the type is int and create_using=MultiGraph()?  That is, just doing that automatically instead of using a keyword. 
comment
I agree that my suggestion isn't backwards compatible but it might be less surprising to return a graph with parallel edges when requesting a MultiGraph type.    There would be a simple workaraound to get a MultiGraph with single weighted edges - just use the defaults to create a weighted graph G and then call M = nx.MultiGraph(G).  That seems like it might be a less common use case than wanting the graph G or a MultiGraph with parallel edges. 
comment
Yes, bummer.  Can you advise on how to write the warning about breaking backwards compatibility? 
comment
I don't have any strong preference - @jfinkels? Adding the keyword means adjusting the code a little I guess. Whichever is the simplest to understand or the most explicit is probably the right choice. 
comment
Looks good.  Thanks. 
comment
For efficiency I'm not sure we want to use that approach.   We tried to optimize somewhat the conversion of matrices to networkx graph dictionaries (I'm sure it can be improved). Making dense copies is going to eat up a lot of time and memory.   I don't remember exactly but for Graph objects we might have decided that adding the edge again (it is ignored) in the symmetric case was the fastest and lowest memory use way.  But for MultiGraphs that won't work.  I don't have a good suggestion no how to do that without a copy or modifying the original matrix. 
comment
I'm still not very enthusiastic about the `triu` function since it makes a copy.  What about special casing the undirected multigraph path (e.g. for sparse matrices) with  ``` python     if G.is_multigraph() and not G.is_directed():         triples = ((u,v,d) for u,v,d in triples if u<=v)     G.add_weighted_edges_from(triples, weight=edge_attribute) ```  and similar for `from_numpy_matrix()`? 
comment
Looks good with one question above.  
comment
I think this is ready to merge.  Anyone else want to take a look?  We'll need to add a note in the release that mentions this change in case anyone is relying on this (incorrect) double-edge effect that we are fixing. 
comment
This needs a manual merge.  @jfinkels could you fix it? 
comment
Yes, you are right.  Those tests are not related to your code but instead in the timing tests.  @dschult can you look at this?  Do we need to run these tests regularly or are they primarily for evaluation performance  when suggesting changes? 
comment
@jfinkels I took at look at the code and I see it follows the wikipedia article as you mention.  I was looking at the Eppstein implementation http://www.ics.uci.edu/~eppstein/PADS/BipartiteMatching.py which is quite different.  Do you have comments on the differences?   
comment
This is fine with me to merge. 
comment
Yes, let's not use create_using.  See #1393 for discussion on reasons.  I like the use of accumulate.  We also have  networkx.utils.cumulative_sum() which will do the same thing without adding more code.  Maybe cumulative_sum() uses should instead be replaced with accumulate? 
comment
How about putting the accumulate import ugliness in networkx.utils.misc.accumulate and we'll using that function until eventually we deprecate Python 2?    
comment
The two auxiliary functions `_reweight_graph_johnson` and `_add_node_johnson` are fairly small.  Perhaps everything can just go in one function?  How about generating an unique node identifier (see generate_unique_node()) instead of using the fixed name `new_node`?  There is always some possibility that name is already being used in the graph.  Instead of making a copy of the graph would it be better to just augment the original graph with the new weights?  And then either remove them or possibly leave them - are they useful to the user? 
comment
Yes, @theosotr that is what I meant - add the new weights in a new edge attribute.  We could allow the user to specify that attribute name with a keyword.  If the keyword is None then we pick a unique weight attribute name to use and delete it when finished.   
comment
Don't worry about the coverage fails.   I don't understand why it fluctuates so much. 
comment
Let's just call it `power`. 
comment
Checking the degree of nodes is probably pretty fast too.  
comment
The algorithmic efficiency depends on the density of the graph.  The new proposal is O(n^2) for n nodes regardless of the density.  Worst case for a complete graph you have to check all n^2/2 possible edges even though there are none to be output. 
comment
Yes, I see your points. But how about something like this (the undirected case)?  ``` python def non_edges(graph):                                                                                                      nodes = set(graph)                                                                                                      while nodes:                                                                                                                u = nodes.pop()                                                                                                         for v in nodes - set(graph[u]):                                                                                               yield (u, v)  ``` 
comment
Yes, please do update the PR.   In my opinion the correct workflow is the one that gets code into the repository fastest and I don't want to be the bottleneck.  On Thu, Apr 23, 2015 at 6:20 AM, rafguns notifications@github.com wrote:  > @hagberg https://github.com/hagberg @jg-you https://github.com/jg-you > Yes, your proposal is definitely better. Should I update the pull request > with your code? I'm not sure what the correct workflow is. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/pull/1469#issuecomment-95567239. 
comment
I'd be willing to drop it officially now for networkx-1.9 - though currently everything works for Python2.6. 
comment
Can we make some improvements so the algorithm is useable (e.g. less copying)? 
comment
Yes, will defer this for now. 
comment
Defer until #1331  
comment
I'm sure we can fix the merge issue.  More generally though I would prefer that we implement a directed version of clustering without using numpy or scipy.  The code might not be simpler but I don't think it would be much more complicated.  And if we decide to keep the version using linear algebra it should use sparse matrices.  
comment
Close and replace with issue #1331.  At least a new PR is needed here and I suggest a pure python implementation. 
comment
This is interesting and appears to be the right way to go.    Some things I would like to discuss -  - What is the best "syntactical sugar" to achieve this?  Inherit from base class with the custom storage types?  A class factory?  I don't really like adding more arguments to e.g. the Graph constructor. - I'd guess we don't need the full Python dictionary interface for an alternate data storage type.  What is the minimum required?  Example? 
comment
I was thinking of a class-based approach something like  ``` python class Backend(object):     graph = dict     edge = dict     node = dict     adjacency = dict  class Graph(Backend):     def __init__(self, data=None, **attr):         self.graph = Backend.graph()          self.node = Backend.node()             self.adj = Backend.adjacency() ```  etc, but done properly - this is just a strawman  Then use a plugin system to allow alternate backend data storage to be specified at run-time.  Then you could set the backend before you called networkx.Graph() if others were available and you didn't want the default.   Something like the answer here might work http://stackoverflow.com/questions/3659773/python-plugin-system-howto/ 
comment
The way NetworkX is currently designed graph, edge, and node data are dictionaries.  I could see replacing those with e.g. a simple on-disk key-value database with a Python dictionary-like interface (as @dschult describes above using the MutableMapping class).   But maybe the adjacency structure of some graph data-store would not be as easily or naturally factored into a dictionary-of-dictionaries structure? Say, a Numpy 2d array (though I suppose you could make an array of arrays).  Or some graph database.  How would that work? 
comment
I'd like to do it without any extra arguments to Graph.**init**().  It would be great if you could set the backend storage dynamically before you instantiate a Graph() object with a default setting of python dictionaries like we have now.  Perhaps the default could also be set through a config file if that is more convenient.  I hope we could do that without to much abstraction (e.g. metaclasses) but maybe that is the best way. I haven't looked into it - perhaps some experts could comment? 
comment
In my view "backends" are alternate data storage for graph data, for example a database of any kind.  At least that is the use case I am thinking of.    The use case of making a graph class function differently (return nodes in a particular order, assign default attributes, etc) seems different and might break existing algorithms in obvious or subtle ways.  So I would say it is better to call those something else even if a "backend plugin" mechanism is used to create them. 
comment
We aren't ruling out a metaclass solution.  But so far nobody has come up with a clean and straightforward approach (if that is even possible...) 
comment
This seems to work for me. https://gist.github.com/hagberg/6876588#file-sample1-gml  ``` python import networkx as nx import urllib2 import sys  data = urllib2.urlopen('https://gist.github.com/hagberg/6876588/raw/2f24e7a7e4486e6b74db6b852dd25ece0b2b0ed4/sample1.gml') G = nx.read_gml(data) for n in G:     print G.node[n] nx.write_gml(G,sys.stdout) ```  ``` python {'id': 2} {'model': u'host', 'bytesInAsClient': 0, 'bytesOutAsServer': 0, 'ip': u'10.0.0.1', 'start_time': u'2013-09-08T13:24:28Z', 'label': 164966, 'coid': u'11111', 'bytesOutAsClient': 0, 'DNS': u'string.some.domain.net.', 'flowsAsServer': 0, 'id': 13595, 'cidr': u'10.0.0.0/23', 'flowsAsClient': 0, 'Class': u'Device', 'bytesInAsServer': 0} graph [   node [     id 2     label 2   ]   node [     id 13595     label 164966     model "host"     bytesInAsClient 0     bytesOutAsServer 0     ip "10.0.0.1"     start_time "2013-09-08T13:24:28Z"     coid "11111"     bytesOutAsClient 0     DNS "string.some.domain.net."     flowsAsServer 0     cidr "10.0.0.0/23"     flowsAsClient 0     Class "Device"     bytesInAsServer 0   ] ] ``` 
comment
I fixed a GML writing bug in #981 - I don't think it changes this issue though. If you post a non-working file we can probably find the problem. 
comment
This works for me.  I'll close if if no further reports. 
comment
I'll close this now.  If we can get a bug report where we can reproduce the error, just reopen it. 
comment
I think the correct spec is here.  http://www.fim.uni-passau.de/index.php?id=17297&L=1 
comment
OK - I see.  Please do suggest a fix and more code or tests are very welcome. We've discussed improving the parser before (probably eliminate pyparsing for better performance) but so far nobody has taken that up. 
comment
Thanks for suggesting the algorithm and paper. Does this  find all maximal cliques?  How is it related to the existing Bron-Kerbosch algorithm? 
comment
To get this included we'll need some tests.  Also - I don't think we don't require that nodes be sortable (or comparable), only hashable. So if there needs to be an ordering it should be made explicitly. 
comment
You can put tests in networkx/algorithms/tests/test_clique.py 
comment
If we don't mid loosing the integer keys I suggest we review this.  Are there implications of using a reference instead of a copy for the `graph` dictionary? 
comment
Thanks @chebee7i, I agree with you.  Let's go ahead and include this. 
comment
Is this ready to merge? 
comment
Looks good.  I'll review it and make some suggestions. 
comment
Yes, please update the documentation regarding graph/multigraph returns.    As for the memory usage vs disk usage - we more commonly get issues with people trying to read large graphs and struggling to fit them into memory.  So keeping the memory footprint reasonably small is a good goal.  How many passes are needed to parse and load the data?  Can it be done in one pass? 
comment
It's intentional.  The output is a weighted graph:  ``` python In [9]: print "from_numpy_matrix edges: ", list(g1.edges(data=True)) from_numpy_matrix edges:  [(0, 1, {'weight': 2}), (1, 0, {'weight': 1})] ```  It definitely could be more clear in the documentation.  You can generate a multigraph from that weighted graph by adding more edges for every edge with weight greater than 1. 
comment
It's also slightly confusing that you can pass create_using=nx.MultiGraph. That implies you may be able to produce a graph with parallel edges. 
comment
I guess this is another implication of hash randomization.  So turning that off would do it too. (Yes, I am suggesting that again...) 
comment
This should be resolved now that we added #1314.  ``` python import networkx as nx from collections import OrderedDict class OrderedDiGraph(nx.DiGraph):     node_dict_factory = OrderedDict     adjlist_dict_factory = OrderedDict edges = [(7,8),(7,11),(5,11),(3,8),(3,10),(11,2),(11,9),(8,9)] O = OrderedDiGraph(edges) print(nx.topological_sort(O)) ``` 
comment
That looks like the correct fix based on the pyparsing release notes at http://pyparsing.wikispaces.com/News 
comment
I guess the question is whether to instead just advise the user that converting from "coo" format is fastest...  What is the memory overhead of converting all matrix formats to coo first? 
comment
It's probably not a factor of 20 more for a NetworkX graph that holds weights vs a scipy sparse matrix. But I would believe 5-10.    I was thinking that maybe the conversion from csr and csc to coo is very fast and doesn't use memory so would be a good choice to do that in those cases. 
comment
I think the careful way to do an iterator zip is to use Python 3 :-)  I'd like to avoid python2/3 line noise needed just to make Python2 code more efficient.    
comment
We use the coo multiple values feature in to_scipy_sparse_matrix() 
comment
That looks good.  I like the idea of generating tuples and feeding them to add_edges_from().  I don't think we need to use range here.  It isn't that much overhead and how about let's all use Python3?  Same with izip and iteritems - we'll have to go back and clean up that cruft someday if we put it in now. 
comment
Well we certainly can't control what version of Python everyone uses (or even if scientific programmers use Python).  But we can control what we put in NetworkX.  I'm not advocating to drop support for Python2 - but I don't like the idea of putting in a lot of cruft to make NetworkX somewhat more efficient for Python2 users.  I'll be surprised if in 5 years people are still mostly using Python2. 
comment
Is adding ipy to the suite going to make our Travis test times grow a lot? 
comment
#1322  
comment
I think the create_using argument is dangerous and we should consider not using. For example - creating a directed graph output from this code produces something but probably not what you want. 
comment
The checks for multigraph and directed are good.    For connected graphs I would prefer to generate the edges and let the user just call the constructor with the edge generator.   But that wouldn't work in general.   
comment
Yes, let's merge this if @ysitu is ready. 
comment
It's better to have only one place where the defaults are mentioned so there aren't contradictions like this.   This fix is fine - but I am with @chebee7i  that we can do this as we go (unless someone wants to volunteer to fix them all). 
comment
I don't think it helps much either.  If this becomes a Python standard then we can do it. 
comment
Hi @jakevdp - All you need to do is check the networkx code base to see that we have no concerns about 'perfect being the enemy of good' :-)  If you want to advocate for this widely - let's go for it. 
comment
Correct. We can't include code from SAGE unless the author agrees to contribute it to this project with the BSD license. 
comment
You could ask the authors for permission directly https://sage.informatik.uni-goettingen.de/src/combinat/posets/hasse_diagram.py 
comment
OK with me. 
comment
The PR is fine.  We need to figure out how to fix the coverage/coveralls sensitivity or turn it off. 
comment
Relabeling the sets is easy so let's do that.  The more common name is "Complete Bipartite Graph".   It might make sense to move generators/bipartite.py into the bipartite pakage and refactor those names instead.  Let's open a new issue if we want to go forward with that. 
comment
I think this is good and that we should include it. But ordered graphs SHOULD NOT be used for doctests or unittests. I think relying on ordering, though practical, is a bad implementation idea for mathematical objects that are by definition unordered.  That is we might create algorithms that pass our order-dependent tests but that fail otherwise. 
comment
I'm a little ambivalent about the doctests. But say a user pastes a doctest example into a Python session.  It won't necessarily then give the same output.  So that seems bad to me. (Separate issue but maybe doctests should be very simple and more complicated examples as separate code or notebooks which could be more easily verified as working correctly). 
comment
I was just looking at that but won't get back to it until later.  Let's merge this. Move discussion to #1351 
comment
There is layout._rescale_layout() 
comment
Thanks for uploading this.  I see what you mean about the global edge key. I suggest we merge. 
comment
It looks like this is just failing on the numpy import with pypy.  But I only see numpy used for numpy.inf. Could it be replaced with float('inf') and drop the numpy dependency? 
comment
Seems like a positive improvement.   Hopefully this is forward compatible with the use of a custom graph edge object (I don't see why not). 
comment
This is great. I suggest we merge this ASAP.   
comment
Or just range() when appropriate. 
comment
Exactly.  Fewer dependencies and cruft are good.   And, use Python 3. 
comment
There were a few small changes to that function between 1.8.1 and 1.9 that could have affected the ordering.   But the output of components is not guaranteed to be in any specific order.  If you want a specific ordering you'll have to design that yourself.  Could you point to where this contradicts the documentation?  Perhaps we need to fix that. 
comment
Yes, you are right, that documentation is no longer correct and should be fixed.  In order to produce a list like that you can use  ``` python sorted(nx.kosaraju_strongly_connected_components(G), key = len, reverse=True) ``` 
comment
Let's leave it open and close it later when we get the doc pull request that fixes it. Thanks for your help. Please let us know if you find any other errors like this. 
comment
If the problem you have is that the arrows are thicker lines in the links, then we don't have a solution. Drawing arrows correctly with Matplotlib has been hard to implement. I'd suggest using Graphviz. You can make suitable dot format file with nx.write_dot(). 
comment
The drawing of nodes is done by matplotlib's scatter() function.   So the size is the radius of the nodes. 
comment
I guess what must be happening is that two or three nodes are disconnected from the rest of the graph. This isn't the case all of the time since the gnp_random_graph() generator produces random graphs with varying structure.  There is not really a great fix for this.  You might consider using pygraphviz or pydot to do the position layout (e.g. pos = nx.graphviz_layout(G)). 
comment
If you can show an example of incorrect behavior we can look into it.  On Tue, Mar 24, 2015 at 3:12 PM, Yongren notifications@github.com wrote:  > Thanks for your response. I also want to let you know is that the layout > algorithm works fine in networkx 1.7, so I guess there should be unintended > changes in 1.1.9. >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1409#issuecomment-85692651. 
comment
The example is non-deterministic so I might not be seeing what you are seeing.  And I don't see the attached plots.  
comment
This is the same as weighted degree right? `G.degree(node, weight='weight')` 
comment
This is the expected behavior with graphviz  - see the escString documentation at http://www.graphviz.org/doc/info/attrs.html  So you will need to write "A\A" for a label in order to render what you want when drawing with graphviz. 
comment
I'm ambivalent about trying to adjust this behavior.  I can see that it is annoying to have a \ swallowed by Graphviz because it wasn't escaped by another \, or quotes disappearing.  But I suspect that fixing some of these issues might be tricky (or break something else).  If you want to propose a fix in to_agraph() and to_pydot() that addresses the Graphviz escString transformations - go for it. 
comment
With a multgraph you can't set `graph.edge[source][target][attr]` directly.  There is an additional 'key' level so it is `graph.edge[source][target][key][attr]`.  But we don't recommend setting data attributes this way.  A better way is to use   ``` python graph.add_edge(source,target,attr_dict=jsonGraph['edge']['source][target]) ```  (I am assuming your jsonGraph graph is a dictionary). 
comment
It doesn't produce an error. But it corrupts the internal data structure which then produces the error you get in generate_pajek(). 
comment
Do we really need coveralls to run on PRs? How about turning it off? 
comment
I didn't see that screen to adjust the coverage threshold reporting. @chebee7i where do I find that? 
comment
GML is 7-bit ASCII format.  So it isn't an appropriate format to store non-ASCII characters. https://www.fim.uni-passau.de/fileadmin/files/lehrstuhl/brandenburg/projekte/gml/gml-technical-report.pdf 
comment
Maybe this is the one you are looking for?https://networkx.github.io/documentation/latest/reference/generated/networkx.generators.degree_seq.expected_degree_graph.html#networkx.generators.degree_seq.expected_degree_graph Didn't read the paper or longer discussion to see if that is true. 
comment
For the expected degree graph you could use the following. Note that powerlaw_sequence() returns floating point numbers which are then used as weights in the expected_degree_graph() model (this is the model from the Chung and Lu paper you mention).  ``` python In [1]: import networkx as nx In [2]: s = nx.utils.powerlaw_sequence(100,2.5) In [3]: G = nx.expected_degree_graph(s, selfloops=False) ``` 
comment
You can throw away the isolated nodes.  But there is no guarantee you'll get a connected network after that.  In general this isn't such an easy problem.   
comment
Well, it might be time to rethink your problem.   If you are trying to reproduce the results in the papers you mention you might try writing to the authors and asking them for more details if there aren't enough in the papers.    There is no bug or code issue here so I suggest the discussion move to the mailing list or SO. 
comment
Google says "co-affilation graph" has 1 hit and "coaffilation graph" has 0 hits. So I'm not seeing how that terminology would help general understanding.  We could add some documentation to explain that the projection algorithms are making one-mode graphs from two-mode (bipartite, affiliation) graphs.  Do you think that would help? 
comment
The GraphML spec says http://graphml.graphdrawing.org/primer/graphml-primer.html#AttributesDefinition The type of the GraphML-Attribute can be either boolean, int, long, float, double, or string. These types are defined like the corresponding types in the Java(TM)-Programming language.  I'm not a Java(TM) Programmer so someone might chime in with info on booleans.  At any rate I don't see why we shouldn't accept 1 and 0 as booleans in this case. 
comment
Your mailing list question is fine.  No need to open an issue here. 
comment
That might be the most obvious way if the data are numbers representing weights.  But when the data are categorical (e.g. colors) it could be a lot more complicated.    If it is confusing that the edge data has an arbitrary selection I'd prefer just not allowing this.  
comment
But then you'd have to decide what to do with the numbers.  You want to sum them but I want to take the harmonic mean or find the greatest common divisor.  It's cleaner just to let a user decide how to combine attributes.  
comment
Adding an optional function would work.  The question would be whether it is simpler for a user to learn how to write such a function to pass into this method or instead to just loop over the edges directly and insert the desired data into a new graph.  I think doing like the example @MridulS pointed to above is simplest. 
comment
Right, it gets complicated pretty fast.  So I am advocating that the user explicitly write code that takes a MultiGraph, manipulates the edges, and returns a Graph.  It may be simpler to do that than to learn how to write a dict of functions.  Aric 
comment
Yes, examples.  I proposed creating notebooks and examples as part of the docs for GSoC. This is exactly the kind of thing that would be appropriate. 
comment
@Midnighter  it is not easy to do it well.  It requires quite a lot of technical skill, creativity, and writing ability. 
comment
I still don't think it is a good idea to try to create a complicated functional interface to something that might be very simple - (e.g. add weights).  It's probably mostly a philosophical approach. But practically imagine that I wrote 20 functions like convert_to_graph (such as get_neighbors_with_data() etc).  Now the problem for a user is to search the catalog of those functions to find one that does what they want and learn to use it.  I am arguing that it is better to show a user how to write a few lines of Python code instead.   
comment
I have never argued against documentation and have personally written quite a lot.  So go for it.   
comment
Also  ``` python In [1]: import networkx as nx  In [2]: G = nx.MultiGraph()  In [3]: G.add_edge(1,2)  In [4]: G.add_edge(1,2)  In [5]: G.add_edge(3,4)  In [6]: G.remove_edges_from(G.edges([1,2]))  In [7]: G.edges() Out[7]: [(3, 4)] ``` 
comment
I agree that this shouldn't be part of remove_edge.  But why is my suggested `remove_edges_from(G.edges())` solution not OK?  Too clunky or not obvious enough?  As a new method maybe `remove_all_edges()`?  We have a 'remove all nodes' (and edges) already with Graph.clear(). 
comment
I'm also ok with `remove_all_edges(u=None, v=None)` or something like that. 
comment
Well there is  `````` python In [1]: set.pop? Type:        method_descriptor String form: <method 'pop' of 'set' objects> Namespace:   Python builtin Docstring: Remove and return an arbitrary set element. Raises KeyError if the set is empty.```  so this type of thing isn't completely unprecedented.  As long as it is documented I feel it is OK.  `````` 
comment
The only other language I know other than Python is FORTRAN :-) So maybe being weird is OK? 
comment
I don't understand the fear of randomness or arbitrariness here.    It seems like there are 3 cases for a multigraph M with multiple edges between u and v 1) want to remove edge and don't care which one - `MultiGraph.remove_edge(u,v)` 2) want to remove edge and have specific key - `MultiGraph.remove_edge(u,v,key=key)` 3) want to remove all edges - `MultiGraph.remove_edges_from(M.edges(u,v))`  Is that too confusing? 
comment
I suggest we close this and not adjust the behavior of `remove_edge()`.   
comment
Those are interesting questions but maybe they shouldn't all be addressed/discussed in one pull request. 
comment
I am typically not in favor of adding more keywords to the core methods. In this case I'm already not very enthusiastic about adding `neighbors(data=True)` .  I feel it could be confusing which data is returned.  It isn't unreasonable to assume that since `neighbors` are nodes that you would return the data with those nodes (not the edge data).  Can we really not simply use `edges(data=True)`? It's a one-liner to get the edge data and process it any way you want.  ``` python [(v,d) for u,v,d in G.edges(1,data=True)] ```  And just to throw another wrench into the works :smiley:  Shouldn't `neighbors` take an `nbunch`?   
comment
Different note = different issue, please. 
comment
Voting again for no `neighbors(data=)` keyword.  Instead use `edges(data=True)`. 
comment
I guess the reason for `neighbors` to take a container  is that other methods do (edges, degree, etc). Though it isn't very hard to implement that as a one-liner either. 
comment
I don't have any clear philosophy on when to add a method or keywords.  I like simple. And I generally prefer adding a method than making a more complicated single method with a keyword.  I would also say that it makes sense to have methods that "agree" with general graph terminology.  So predecessors, successors, edges, degree, neighbors are good.    For the case of `neighbors()` I don't like that the method would get significantly more complicated (I'd advocate for another method like @chebee7i  suggests) and that 'neighbor data' is potentially confusing  - why doesn't it return the data for the neighbor nodes (instead of the edges)?   
comment
Maybe @chebee7i's idea is the genius compromise here.  Keep the graph methods simpler but make a module that provides some basic one-liner like functions.  We are already doing that for nx.get_node_attributes() and others.  I don't remember anyone suggesting adding those as methods to the graph container classes but I might have missed it. 
comment
I suggest we close this PR as 'declined'.  Adding functions as suggested by @chebee7i  or @dschult should be a separate issue.  
comment
Thanks for the suggestion.  Unfortunately it's hard to get the arrows right. Your approach isn't quite enough.  E.g. with this code   ``` python In [1]: import networkx as nx  In [2]: G = nx.DiGraph()  In [3]: G.add_path([1,2,3])  In [4]: nx.draw(G) ```  you get an interesting but not so pretty drawing ![foo](https://cloud.githubusercontent.com/assets/187875/6315313/c05ac0e6-b9bb-11e4-94de-1e04fac5a621.png) 
comment
Unfortunately, not a general solution for arrow drawing. 
comment
Counting graphs with 5 nodes:  ``` python In [1]: from networkx.generators.atlas import graph_atlas_g  In [2]: len([g for g in graph_atlas_g() if len(g)==5]) Out[2]: 34 ```  I don't see a benefit from adding non-isomorphic graphs here. 
comment
It's called http://reference.wolfram.com/language/ref/GraphData.html now. And implementing that is some amount of work. Take a look at http://graphclasses.org.  I think SAGE includes some of that data. 
comment
If there is a specific proposal let's open a new issue. 
comment
Nice.  Less time building, more time testing. 
comment
Good to see you again.  Yes, we had two projects.  I was the mentor for @bjedwards on the community detection project.   We had a a great summer and made good progress - some code made it into the codebase and there is more we could glean from the project.   Overall the experience for me was great.  I would be interested in supporting this again for NetworkX.  2011 was our first attempt and we learned a lot about how to scope projects, communicate code, etc.  Now that we are using github the discussions and integration of code would be better facilitated. 
comment
So we have about a month to solicit mentors and ideas for projects.  I think that is doable. @ysitu are you willing to coordinate this effort?   
comment
OK - let's go for it.  @bjedwards could you drop a note to Teri Oda and find out how to proceed? 
comment
This is a http://github.com/pygraphviz/pygraphviz/issues 
comment
Better topic for the mailing list.  
comment
Also PyGraphviz works with Python3 and provides the same (or pretty similar anyway) functionality.  Windows users often have difficulty installing it though. 
comment
Good question.  Maybe it isn't working properly on Windows with Python3.  I have no access to Windows or developer tools so I can't make wheels.  In the past others have done so. 
comment
FYI - you can install from github and bitbucket with pip https://pip.pypa.io/en/latest/reference/pip_install.html#vcs-support 
comment
I see what the goal is.  I don't have any strong preference of changing cutoff vs removing from travis testing. 
comment
Addressed in #1313 
comment
Could you provide a small example showing how it breaks and include the error?  You shouldn't be triggering line 849 with that label (cb.is_string_like(label) is True). 
comment
It will work if you change your Node class to inherit from unicode (or str for Python 3)  ``` python class Node(unicode): ```  or add the method  ``` python     def __add__(self,other):         return self.name+other ``` 
comment
It's true we could be more clear about custom node objects.  In this case you can't use the matplotlib drawing without implementing a string-like (that passes our is_string_like() test) object.  But nothing in the graph class itself requires that.  I suspect almost all of the networkx code will work correctly with your custom Node object.  We could even remove that is_string_like() test in favor of something better.  Python 3 solves this problem easily but we still need Python 2 support (unicode v str). 
comment
I didn't check carefully all the places is_string_like() is used and it appears to be in the intput/output and drawing functions.  It might be possible to replace them with something less strict.  But is it necessary? 
comment
I'm going to defer action on this since there is an easy work-around and #1325 
comment
There is some discussion on multigraph search in issue #1193.  Nobody proposed implementing Dijkstra's algorithm though.  For your case maybe this would work?  ``` python for edge in zip(path,path[1:]):                                                      data = MG.get_edge_data(*edge)                                                   print edge,min(data.items(),key=lambda x : x[1]['duration']) ``` 
comment
That graphviz layout ("neato") is the Kamada-Kawai spring model.  The spring_layout in networkx is the Fructerman-Reingold model which is similar to the "fdp" graphviz model.  Try  ``` python nx.draw_graphviz(T,prog='fdp') ```  for comparison.  The Kamada-Kawai layout appears to work much better for balanced trees. 
comment
Some experimenting shows that this drawing appears spread out most of the time.   You can increase the number of iterations to the spring layout which might produce better drawings:  ``` python pos = nx.spring_layout(T,iterations=200) nx.draw(T,pos) ``` 
comment
``` python import networkx as nx                                                                                                         G = nx.complete_graph(4)                                                                                                      personalization = {0: 1, 1: 1, 2: 4, 3: 4}                                                                                    p = nx.pagerank(G, alpha=0.0, personalization=personalization)                                                                # answer = {0: 0.1, 1: 0.1, 2: 0.4, 3: 0.4}                                                                                   print p                                                                                                                       p = nx.pagerank(G, alpha=0.85, personalization=personalization)                                                               print p   ``` 
comment
NetworkX uses either pydot or pygraphviz to read dot format graph files.  Pygraphviz is likely faster. The timing on my machine isn't as dramatic as yours.  The dot files have a factor of 2 or 3 slower performance.  ``` python In [1]: import networkx as nx  In [2]: rg = nx.fast_gnp_random_graph(100, 0.25, directed=True)  In [3]: nx.write_graphml(rg, "rg.graphml")  In [4]: nx.write_dot(rg, "rg.dot")  In [5]: %timeit foo = nx.read_graphml("rg.graphml") 10 loops, best of 3: 21.4 ms per loop  In [6]: %timeit foo = nx.read_dot("rg.dot") 10 loops, best of 3: 78.1 ms per loop  In [7]: %timeit nx.write_graphml(rg, "rg.graphml") 10 loops, best of 3: 43.2 ms per loop  In [8]: %timeit nx.write_dot(rg, "rg.dot") 10 loops, best of 3: 94.5 ms per loop ```  Note that `nx.read_dot()` is a wrapper around pygraphviz   ``` python In [19]: nx.read_dot?? Type:        function String form: <function read_dot at 0x2d5b758> ... Definition:  nx.read_dot(path) Source: def read_dot(path):     """Return a NetworkX graph from a dot file on path.      Parameters     ----------     path : file or string        File name or file handle to read.     """     try:         import pygraphviz     except ImportError:         raise ImportError('read_dot() requires pygraphviz ',                           'http://networkx.lanl.gov/pygraphviz ',                           '(not available for Python3)')     A=pygraphviz.AGraph(file=path)     return from_agraph(A) ``` 
comment
Maybe you are using pydot instead of pygraphviz?  ``` python In [6]: %timeit foo=nx.drawing.nx_agraph.read_dot("rg.dot") 10 loops, best of 3: 78.4 ms per loop  In [7]: %timeit foo = nx.drawing.nx_pydot.read_dot("rg.dot") 1 loops, best of 3: 2.7 s per loop ``` 
comment
Hi @paulgirard  - the read_gexf() function has a different interface than the write. Try  ``` python g = networkx.read_gexf("test.gexf") ```  and it should work correctly. Please let us know of any bugs or issues you find. 
comment
There is also an 'iteritems" in there and maybe some other Python2isms. I suggest we write some proper tests for that module or else consider removing it. 
comment
Here is an nice example that uses read_shp(), http://ipython-books.github.io/featured-03.html 
comment
Yes, but it is an indication that there are users and making it Python3 compatible would be good.  I wonder if the osgeo library this uses is Python3 ready?  On Tue, Jul 29, 2014 at 8:32 PM, argriffing notifications@github.com wrote:  > From that link: "At the time of this writing, NetworkX's support of > Shapefile doesn't seem to be compatible with Python 3.x." >  > — > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1227#issuecomment-50566825. 
comment
Maybe @bwreilly (original author) can help us update this and figure out how to test it?  
comment
Ah yes, there are tests.  I completely missed that.  Updating to Python3 should be pretty simple (xrange->range, iteritems()->items() etc).  I'll take a look and see if there are other issues. 
comment
I tried fixing those two issues (xrange,iteritems) and the tests fail - maybe the file isn't getting loaded correctly? So any help you can give would be great. 
comment
Yes, that is the correct package (python-gdal, python3-gdal).   
comment
This needs someone familar with the GDAL bindings to take a look (at minimum the `f.geometry()` issue). 
comment
That hash() differences are indeed the issue here.    At this point networkx can make no guarantee, even with hash randomization turned off, that the data output will be exactly the same between different architectures or operating systems.    It's arguable whether this is a bug or not.  NetworkX is a library for graphs (networks) which are mathematical objects based on sets with no implied ordering.   That is, the output you are seeing is correct from a mathematical standpoint of comparing input and output of graphs.  But it certainly could be very useful to produce consistent or stable ordering in some cases, e.g. based on order of node/edge creation, or file input.    We have discussed this occasionally and even recently #980, #1181, #1244. I believe the current opinion is that the best approach would be to (re)architect the graph data structures to allow other underlying structures to be used instead of Python dictionaries.    So far (I think) there is not a complete proposal or prototype implementation. 
comment
Just to be clear we "allow" this now in the sense that we don't prevent anyone from designing an output format that has a consistent ordering on all platforms.  If your nodes and attributes are sortable I suspect you could do this pretty simply (maybe just modifying one of the output functions).   
comment
Yes, it is a pygraphviz issue.  For Python 3 maybe we need to decode the bytes?  e.g.  `warnings.warn(b"".join(errors).decode(), RuntimeWarning)` 
comment
I suppose there are infinite categories of labels...if we want to use labels to help us with the workflow that is great.  But let's stick to something standard-ish and document it so everyone can understand what the point of them is.    I would like to attach a milestone to everything that is either complete or we have scheduled to complete.  Issues without milestones should find a champion (assignee) or we should consider dropping them. 
comment
I deleted the checkmark labels. 
comment
How about turning off the hash randomization?  ``` $ PYTHONHASHSEED=random python3.4 -c 'print(hash("a"))' -1769101628 $ PYTHONHASHSEED=random python3.4 -c 'print(hash("a"))' -661212464 $ PYTHONHASHSEED=0 python3.4 -c 'print(hash("a"))' -845962679 $ PYTHONHASHSEED=0 python3.4 -c 'print(hash("a"))' -845962679 ``` 
comment
Example please where you need hash randomization and determinstic output.  Otherwise it seems like a pretty special case? 
comment
Yes, I can understand the surprise and frustration that output varies from run to run.  But I still think the solution is to disable hash randomization (or use the same randomization) if you want more deterministic behavior (as used to be in Python2).   I can't evaluate the security risk in disabling the randomization.  It seems to be basically a denial of service attack so if you are using NetworkX in an uncontrolled situation with malicious users it could make sense to randomize.  In general graphs are sets of nodes and edges with no implied order. We should certainly document this fact as it relates to input and output of data.   
comment
Sure we could do that.  We need to be careful that the error produced when nodes are not sortable makes sense to the user.  I don't think many people use nodes that can't be sorted but we allow it. 
comment
I still feel that the best solution here is to turn of the hash randomization.  I haven't yet been convinced that this isn't a valid solution that will work for @josch.    If we want to support ordering of nodes and edges in output files that networkx generates (I'm not in favor myself)  I suggest we allow direct specification of a node list and edge list instead of sorting. 
comment
Yes, attributes too, ugh.  Now I'm really -1 on adding this as a feature. 
comment
We have previous discussed ideas along the line @ysitu suggested in #980 (and earlier).  I am for designing a way to use alternate 'backends' to store graph and attribute data such as an SQL database, OrderedDict, sparse matrix, etc.  So far we have not converged on a engineering solution to do that.  NetworkX 2.0 is an opportunity to adjust the basic API/interface some if necessary to make this happen.  It seems do-able but there are issues and trade-offs that should be discussed in a different issue.  That approach may solve @harlowja's issue in other ways (e.g. OrderedDict as a backend) than turning off the hash randomization.  So I suggest we not implement a half-way approach here to provide ordered storage or reading and writing of nodes, edges, and attributes.   
comment
Also you need to say  ``` python print list(clique.find_cliques(g)) # output # [[1, 0], [1, 2], [3, 0], [3, 2], [3, 4], [5, 4], [5, 6], [6, 7], [7, 4]] ``` 
comment
Some of this has been completed.  Open a new issue or PR for adding more approximation algorithms. 
comment
Thanks @cournape.  It looks like there is a small Python3 issue https://travis-ci.org/networkx/networkx/builds/10754681  I think we have been using StringIO in some places to do these kind of tests too. 
comment
Try `pip -v install networkx` and look at the output for which versions are examined and chosen. 
comment
Does this fix and #1025 fix the installation issue with Python 3 pip? Should we make another release (1.9.1 I guess) to fix this?  It's pretty annoying for Python 3 users. 
comment
It works correctly for me now too.  I vote for fixing #1203 and then making another release. 
comment
I think this was enough to cover the networkx-1.9.1 milestone.  @ysitu and @chebee7i should we make a release here? 
comment
I consider the 1.9.1 marked issues/prs as bug/performance fixes.  We can make a tag wherever we want - so let's pick a place and make a tag/release with the fixes included. 
comment
I think we should tag networkx-1.9 after the commits for this PR.   I'll move the networkx-1.9.1 milestone labels off of the later issues and PRs. 
comment
@ysitu can you take a look and put the networkx-1.9.1 tag in a reasonable place? 
comment
Great, I opened #1237 for the networkx-1.9.1 release discussion. 
comment
Yes, this looks like it is what we need - https://www.mpia-hd.mpg.de/~robitaille/wheelhouse/ https://github.com/scipy/scipy/blob/master/.travis.yml 
comment
We might have added some long-running tests in some recent pull requests.  I haven't  been looking at that. 
comment
I don't think it is our problem.  I don't all of the packages set up but you can see the test times on my laptop are about even between different versions (at least not wildly out of whack).  ``` bash $ for test in nosetests-3.4 nosetests-3.3 nosetests-3.2 nosetests-2.7; do echo $test; $test networkx; done nosetests-3.4 ---------------------------------------------------------------------- Ran 1981 tests in 74.927s  OK (SKIP=11) nosetests-3.3 ---------------------------------------------------------------------- Ran 1789 tests in 64.514s  OK (SKIP=60) nosetests-3.2 ---------------------------------------------------------------------- Ran 1986 tests in 67.620s  OK (SKIP=9) nosetests-2.7 ---------------------------------------------------------------------- Ran 2004 tests in 74.801s  OK (SKIP=5) ``` 
comment
Yes, I maintain pygrapvhiz for python2.x.  No plans to support python3.x. https://github.com/networkx/networkx/issues/758 https://github.com/cylc/cylc/issues/479 
comment
Thanks, I updated it. 
comment
I'm surprised to see the wide variation in the performance - but I don't know the details of these algorithms well.    For the ARPACK `eigs` it might be a good idea to set the `ncv` parameter explicitly.  That is the number of vectors used in the Kyrlov subspace and by default gets set to 2k+1 https://github.com/scipy/scipy/blob/v0.13.0/scipy/sparse/linalg/eigen/arpack/arpack.py#L331 - so 5 in this case.  It's possible setting that larger will improve the performance in some cases. 
comment
I finally had a chance to look at this.  I am ambivalent about adding this code.   If you want to help maintain this in NetworkX then let's go for it.  On the positive side it is obviously a great idea and good code and should be available for people to use.  On the negative side it requires os-specific stuff with some trickiness around files that I prefer not to include since it is a pain to maintain and debug.  Also this would mean that the PyGraphviz/NetworkX interface will be different so they won't be interchangeable (they are supposed to be more-or-less that way now).  In general I am leaning toward moving the drawing parts of NetworkX to separate packages.  So that may eventually be the recommendation for this code too.   
comment
Moving the drawing code probably won't happen before the end of the year.   
comment
This isn't really a networkx issue but right now there is no PyGrpaphviz github site.  I don't have any plans to update PyGraphviz to Python3.  If it was as simple as fixying some Python2->Python3 syntax I would have already done it.  But the text/unicode change in Python3 requires some serious  reworking for PyGraphviz.  I think a better approach would be to make a simpler interface with Cython instead of SWIG that would work with both Python2 and Python3.    I'll try to update the PyGraphviz web page to indicate the current status more clearly. 
comment
I'm going to close this.  It's a pygraphviz issue and there are no plans to port to Python3. 
comment
Pygraphviz has been ported to Python3 https://github.com/pygraphviz/pygraphviz/pull/19 
comment
Thanks for the report.  This is also an issue with `networkx.write_graphml`. 
comment
The documentation is clear to me. 
comment
I'd like to see these as (3?) separate pull requests.  Sorry for the trouble but it is much easier to comment and keep track that way. 
comment
I didn't realize the dependencies. But maybe the shortest path function you need is specialized enough that it should just be local to the code where you need it? 
comment
The test_pr.py script isn't working for me - maybe I'm missing something?  ``` [aric@ll tools (tools)]$ python test_pr.py 742 Traceback (most recent call last):   File "test_pr.py", line 340, in <module>     test_pr(args.number, post_results=args.publish)   File "test_pr.py", line 312, in test_pr     testrun = TestRun(num)   File "test_pr.py", line 77, in __init__     self.pr = gh_api.get_pull_request(gh_project, pr_num)   File "/home/aric/Software/hagberg-networkx/tools/gh_api.py", line 95, in get_pull_request     return json.loads(response.text, object_hook=Obj) AttributeError: 'Response' object has no attribute 'text' ``` 
comment
OK - I figured it out.  My Ubuntu installation had an old/different version of the "requests" package. I installed the latest one from here and it works: http://docs.python-requests.org/en/latest/index.html  But I don't have a PYTHONPATH set and it breaks looking for that:  ``` Traceback (most recent call last):   File "test_pr.py", line 340, in <module>     test_pr(args.number, post_results=args.publish)   File "test_pr.py", line 316, in test_pr     testrun.run()   File "test_pr.py", line 231, in run     passed, log = run_tests(venv)   File "test_pr.py", line 267, in run_tests     orig_pythonpath = os.environ["PYTHONPATH"]   File "/usr/lib/python2.7/UserDict.py", line 23, in __getitem__     raise KeyError(key) KeyError: 'PYTHONPATH' ```  These are small things but we should update the script to work (or break gracefully) in these cases.  I have a stock Ubuntu 12.04 install. 
comment
On Sun, Aug 12, 2012 at 2:11 PM, Jordi Torrents notifications@github.comwrote:  > I added the PYTHONPATH bug when trying to support pypy in Ubuntu. The > commit above fixes it. But it is still not clear to me who is to blame for > not adding the correct dist-packages path when creating a pypy > virtualenv. I'll look at that, but will not have a lot of time the next few > days. Maybe an option is to remove pypy support for now, and add it when we > have figured it out.  Yes, maybe we should take the pypy part out if it adds that complexity of relying on PYTHONPATH.  > I also added a check for requests version and if our requirements are not > met the script exits with an informative message. >  > Great. 
comment
Are you using unicode in Python? It should work something like this:  ``` python # -*- coding: utf-8 -*- import sys import networkx as nx G = nx.Graph() G.add_node(u'福') print G.nodes() nx.write_adjlist(G,sys.stdout) ```  ``` bash $ python foo.py  [u'\u798f'] #foo.py # GMT Sat Dec  7 14:54:39 2013 #  福 ``` 
comment
Yes, the GML specification only allows ASCII text encoding so we use latin-1 as required. 
comment
Looks fine, thanks.   
comment
I'm not sure if we're ready for more complexity in the workflow.  I'd prefer faster merging of simpler PRs.    @ysitue our development has sometimes been pretty slow (many of us have busy day jobs) and you are pushing our pace up.  So that is a great thing, and thanks.  We'll try to keep up.  Do you want to be added as a developer so you can fix things faster? 
comment
I vote for dropping the backwards compatibility in this case in favor of clear and simpler interfaces.. 
comment
I think this PR should be merged before it grows so large that it eats all of the other PRs... 
comment
It would be great if @dschult takes a look at this.  In #604 he experimented with other improvements to Johnson's algorithm too.  As I pointed out earlier there it seemed that there might be some inefficiencies (or incorrect implementation) in how the strongly connected components were computed - maybe your code fixes that? 
comment
That is a very interesting table. Thanks for this!  As for refactoring - I can't quite tell what your error is. Can you post the whole message?  
comment
I prefer to test docstrings.  It's pretty frustrating to paste in an example and it doesn't work.   Testing helps avoid that.   But here is another suggestion.  For longer examples on how to use a function or module we can create  examples in networkx/examples and make a link. 
comment
**NetworkX: Test results for pull request #767 ([ComplexSystemTelecomSudParis 'master' branch](https://github.com/ComplexSystemTelecomSudParis/networkx/tree/master))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit 3df12ea into NetworkX master 3710816) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=64) Ran 1570 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1771 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1742 tests (libraries not available: ogr, pygraphviz, pydot) - python3.3: :eight_spoked_asterisk: OK (SKIP=37) Ran 1636 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) 
comment
PR #819 is the correct one here, please ignore the earlier (now closed) PR. 
comment
Thanks for looking into this.  You are right that the alpha channel is optional.  So I suggest we include that fix.    Does the file read with non-ascii characters work using read_gexf(node_type=unicode) or maybe even nx.read_gexf(node_type=None)?  Maybe None should be the default? 
comment
On Fri, Oct 5, 2012 at 8:47 AM, Ramiro Gómez notifications@github.com wrote:  > Using read_gexf(node_type=unicode) and nx.read_gexf(node_type=None) both > works, thanks for you're help. >  > I agree None would be a better default for readgexf as it is also the > default for GEXFReader __init_ method.  I don't see any other drawbacks to setting the default to node_type=None. Would you make that change (also the docstring mentioning the default)? Then I think we can merge the pull request. Aric 
comment
Yes, definitely interesting.  And A+ for writing more code for tests than the algorithms :-). Let us know how we can help. 
comment
1) Yes, if the algorithm doesn't apply for directed graphs it is a good idea to warn the user 2) How about using a generator approach and yielding (u,v,p) or (u,v,{probability:p|) or... ? That way you don't have to explicitly build a dictionary (more efficient) and the edges can be easily passed to other functions like G.add_edges_from().   No need to repeat the edge v-u for u-v either. 3) We'll probably need to see how the code works first 
comment
You are correct.  The JSON formats are using the "id" key to identify the nodes.  We could either document that properly or maybe make it optional (or allow the name to be specified explicitly). 
comment
How about  ``` python In [1]: from networkx import *  In [2]: G=DiGraph([('n_d', 'n_e', {'label': u'an(v)'})])  In [3]: nx.adjacency_matrix(G) Out[3]:  <2x2 sparse matrix of type '<type 'numpy.int64'>'     with 1 stored elements in Compressed Sparse Row format>  In [4]: nx.adjacency_matrix(G).todense() Out[4]:  matrix([[0, 1],         [0, 0]])  In [5]: nx.attr_matrix(G) Out[5]:  (matrix([[ 0.,  1.],         [ 0.,  0.]]), ['n_d', 'n_e'])  In [6]: nx.attr_matrix(G,edge_attr='label',dtype='S8') Out[6]:  (matrix([['', 'an(v)'],         ['', '']],         dtype='|S8'),  ['n_d', 'n_e'])  In [7]: nx.attr_matrix(G,edge_attr='label',dtype='U8') Out[7]:  (matrix([[u'', u'an(v)'],         [u'', u'']],         dtype='<U8'),  ['n_d', 'n_e'])  ``` 
comment
These will needs tests before they could be included. 
comment
I'm -1 on including these in the algorithms/ section of NetworkX.  I don't think they are widely used enough or well-studied enough to go in the core library. Perhaps they could be included in examples/ ? 
comment
The test are currently all working for me with Python3.3.  We can open a new issue if needed. 
comment
It would be good to fix that.  Is there a way to make heapq only sort on the first element of the tuple?  Or some other idea? 
comment
Yes, I just read through so SO responses on that question.  Is there any reason we shouldn't just use your BinaryHeap implementation? 
comment
@ysitu - I see, that is a simple and good solution.  There might be some other places we are using heapq where this could crop up.  Our testing doesn't often mix node types so we can easily miss this kind of error.   
comment
Right in Python 3.x you can't sort heterogeneous data. 
comment
I guess one issue could be if we want Dijkstra's algorithm to return the same (deterministic) results for cases where there are multiple shortest paths.  Without ordering the nodes in the heap this isn't guaranteed.   
comment
Yes, we could use "id".  So we would then use a three-tuple of (weight,id,node)? 
comment
I guess, as usual, None is a special case:  ``` python >>> (1, 1, None) < (1, 1, None) False >>> (1,1,"1") < (1,1,1) Traceback (most recent call last):   File "<stdin>", line 1, in <module> TypeError: unorderable types: str() < int() ``` 
comment
So it seems that the simple (weight,id,node) tuple will do the job? 
comment
@ysitu I'm not sure what the language spec says.   Here is a high-level description https://docs.python.org/3.0/whatsnew/3.0.html#ordering-comparisons 
comment
We could do that but doesn't the three-tuple idea work here?   
comment
In this case it is just used for the heapq to (secondary) sort the nodes for stable (reproducible) algorithm performance. 
comment
Yes certainly @ysitu.  I was thinking of the case where you start the interpreter and load a graph then run a shortest path algorithm more than once.  It would likely be surprising if the results were different.   We haven't had many questions about hash randomization yet - but as soon as people start using newer Python versions I'm sure we will get users surprised that run-to-run output is different. 
comment
@ysitu I like that solution the best.  We can simply use `itertools.count()` 
comment
I still like the three-tuple `(weight, count, node)` idea the best.  And you are right that betweenness.py and mst.py need some adjustment too. 
comment
Looks good.  Thanks for fixing the astar algorithm too. 
comment
Thanks for checking back.  I was looking at this and wondered how it was related to the articulation_points() algorithm   networkx/algorithms/components/biconnected.py Maybe some of the code can be shared? 
comment
OK - thanks! I'll look at the tests. 
comment
Could you tell us more about the algorithm?  Is it more efficient than computing the shortest path from each node in serial?  
comment
Looks like self loops are getting handled incorrectly (doubled). Thanks for reporting this. 
comment
PR #1078 fixes this issue by using the convention that self loops should only be counted once. 
comment
Thanks, let's merge and give it a try.  The dev doc build server is off the air right now but I'll update it soon. 
comment
Yes, we will get rid of the orange.  Numpy/Scipy have designed a nicer looking blue style and I'd like to switch to that.  In the meantime you can download either PDF (black and white) or the html in a zip file from http://networkx.github.io/documentation/latest/ Edit the style sheet _static/sphinxdoc.css to change the link color... 
comment
I updated the development documentation so you can see this live.  If we stay with this theme for the docs the main website should be redesigned to match.  Note that the header navigation got dropped with the RTD theme so this would be a time to revisit the website layout too. 
comment
Wow, that is really fantastic. Thank you for implementing this and making such a beautiful pull request. I think it is ready to merge.  @loicseguin do you have any comments? 
comment
No worries - I'll reopen it.  Or you can too.  Thanks for helping fix all of the little details (tests). 
comment
I'll merge this.  Thank you for this great contribution @ysitu.  I hope you will consider making other contributions in the future. 
comment
Good idea.  Code looks fine to merge.   
comment
It is somewhat faster to do it this way- instead of calling random.choice use random.random() to choose a list of random nodes (with replacement).  Also using G[n] instead of G.neighbors(n) is also faster.  ``` python def average_clustering(G, trials=1000):     n = len(G)     nodes = G.nodes()     triangles = 0     for i in [int(random.random() * n) for i in range(trials)]:         nbrs = G[nodes[i]]         if len(nbrs) < 2:             continue         u, v = random.sample(nbrs, 2)         if u in G[v]:             triangles += 1     return triangles / float(trials) ``` 
comment
Oh, I meant random.choice().  It was called in a loop and it is faster to produce a list of random node indices.  I edited the comment to be more clear. 
comment
There must be a ref for this somewhere?    This is an old (19th century!) problem  - maybe the one from the Wikipedia article has more info:  Fleischner, Herbert (1991), "X.1 Algorithms for Eulerian Trails", Eulerian Graphs and Related Topics: Part 1, Volume 2, Annals of Discrete Mathematics 50, Elsevier, pp. X.1–13, ISBN 978-0-444-89110-5. 
comment
Thanks, that is great!   I suggested some small fixes at https://github.com/gavento/networkx/pull/1 for you to consider. 
comment
All the tests pass now so we can merge this.   More documentation is always better... Thanks again for contributing this. 
comment
Actually the thing to do is to leave this open and just push more changes to your networkx:dispersion branch. Then we can have the full discussion here instead of opening another pull request. 
comment
Looks fine.  A link in the algorithms.centrality.rst docs file would be good. 
comment
OK with me.   
comment
Can you comment more on the use of the special heaps? I"m not familiar with this algorithm at all and was wondering why the Python heapq couldn't be used.   It must be the "get" function?  Is the queue performance the  bottleneck? 
comment
@ysitu did you test the performance with the Python heapq?  I wonder if for many cases it is the fastest even if the storage complexity is bad?    If we care about storage we might consider not copying the graph if the user doesn't need it anymore.  Is the HeapDict code usable for this? https://pypi.python.org/pypi/HeapDict 
comment
OK thanks @ysitu for clarifying that.  I appreciate your work in adding those auxiliary data structures. I think we are done here unless anyone else has comments. 
comment
Thanks for fixing this.  Note that we also automatically test pull requests with Travis CI now so these scripts are mostly just for convenience. 
comment
You could certainly draw flowcharts with NetworkX and some custom Matplotlib code. But it probably isn't the best tool for that. 
comment
This is probably not a NetworkX bug. Assigning to your graph G in this way will corrupt the edge data structure if y == x:     G["s"]["demand"] = -x     G["t"]["demand"] = x  You probably don't mean the edge between 's' and 'demand' but that is what this assigns. Use G.node['s']['demand'] to modify node attributes. 
comment
That example works OK for me.  The layout isn't great but it's not all pos=0,0 ![path](https://f.cloud.github.com/assets/187875/2023098/98cde928-8856-11e3-9836-02a98001ec27.png) 
comment
The algorithm runs a DFS from the node you specify and outputs dictionaries of predecessors and successors.    When you start with node 'a' the DFS is A->B->C->D - you get the successors by following the head of the arrow and predecessors by following the tail.    When you start with node 'd' the DFS is empty since there are no nodes reachable from 'd'. 
comment
Good idea to add an example. It might make sense to have predecessors return a list to match successors - it's not a list because there can be only one predecessor in a DFS. 
comment
For a Graph or DiGraph type when you add the same edge again with a different weight then the weight is updated.  ``` python In [1]: import networkx as nx  In [2]: G = nx.Graph()  In [3]: G.add_edge(1,2,weight=7)  In [4]: G[1][2] Out[4]: {'weight': 7}  In [5]: G.add_edge(1,2,weight=99)  In [6]: G[1][2] Out[6]: {'weight': 99} ``` 
comment
Could you give a reference with the correct definition for directed graphs? I'd like to get it fixed and documented correctly. 
comment
I suppose Katz didn't mention directed graphs in the original paper? The algorithm already cites the Newman book (which I don't own, sorry Mark I'll buy a copy...)  I also noticed that for eigenvector centrality we use "right eigenvector centrality" by default (out-edges) and suggest using G.reverse() to get the in-edges centrality.  So is that the wrong convention there too? 
comment
I think the common way to write the adjacency matrix is i->j is an entry in the matrix at A_{ij}.  At least that is the convention that NetworkX is using.  With that definition the in-edge centralities are "left eigenvectors" and the out-edge centralities are "right eigenvectors"  It seems from the literature that we should be using the in-edge versions.  Right now pagerank is that way but the eigenvector centrality and Katz centrality are using out-edges.   So I propose we change those both to default to the  in-edge (left) centralities.  If the opposite case is less interesting (out-edges) we could just leave the interface as it is and suggest using G.reverse() on the graph (a cheap operation) for that. 
comment
We could provide an option shallow=True to do that.  In fact I think the default copy used to be a shallow copy.  But I'd prefer not to do this since it can be a little hard to understand the difference between shallow and deep copies if you are not a Python expert.  When we discussed this before (can't find the discussion now) the result was that deep copying was the least surprising to casual users.  And you can always copy.copy(G) to make a shallow copy if you really know you want that.  On the other hand it might be useful in many cases to get a copy of the graph with no edge, node, or graph data.  That is what you proposed to do in the simple_cycles function in #1017 and I think it is the right idea there. 
comment
OK - I'll close this and we address the current issue in #1017. 
comment
Can we instead just use the label_attribute keyword, like this?  ``` python In [1]: import networkx as nx  In [2]: G = nx.Graph()  In [3]: G.add_path(['a','b','c'])  In [4]: H = nx.convert_node_labels_to_integers(G,label_attribute='labels')  In [5]: nx.get_node_attributes(H,'labels') Out[5]: {0: 'a', 1: 'c', 2: 'b'} ```  I'd prefer that to returning different objects depending on keywords. 
comment
Yes, that is a pydot issue.  If you can use pygraphviz instead of pydot the memory usage should be lower. 
comment
As far as I remember the whole point of the make_str function was because we just wanted to return str(x) unless x was already str or unicode - then leave x alone.  That is, we put that code in there to allow either str or unicode types to be used as NetworkX nodes.  Of course if you want to output to other data streams you have to think about what you are doing (and hopefully choose unicode).     So I'm not sure we want to demand that all strings be unicode in Python2.     This nonsense all goes away next year when we all are using Python3. 
comment
Thanks for looking at that.  If we are only using make_str when we output data to files or strings meant for other programs then it does make sense to me to convert to unicode (then encode properly for writing/sending). So I am for this change.  And everybody let's all use Python3 from now on OK? 
comment
I don't want to revisit all of those graph format writers to fix Python2 warts.  Some of these issues are certainly bugs but they seem like corner cases (using a non-encodable delimiter).  We are all using Python3 next week anyway.... 
comment
For python3 can't we just return str(x)? (no ugly type checking) 
comment
And could we just use a try/except instead of checking for python version?  ``` python try:     #python 2     if isinstance(x, unicode):    .... except NameError:    # python 3    return str(x) ``` 
comment
Can't you still push more to the same branch?  On Tue, Oct 22, 2013 at 6:15 PM, chebee7i notifications@github.com wrote:  > I merged too soon. Hah. >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/990#issuecomment-26868708 > . 
comment
The default data type returned from to_scipy_sparse_matrix() is 'i' (and not one of 'dDfF' as required by eigs). You can fix that with, e.g.  ``` python A = nx.to_scipy_sparse_matrix(G, format='csc', dtype=float) ```  This default is being generated inside scipy.sparse.coo_matrix.  Do we want to explicitly set it in networkx.to_scipy_sparse_matrix? 
comment
I think that is a feature.    If you take a look at dfs_labeled_edges (no implementation for bfs) I think what you are looking for is "reverse" edges.  Those are not part of the tree that is formed by a BFS (or DFS). 
comment
Let's fix the test and open other issues for deciding on how we handle random numbers more generally. 
comment
These docs come from https://github.com/matthew-brett/gitwash so you might want to notify there too.  It doesn't look like they have been udpated. 
comment
No submodule.  The plan was to just update as mentioned in the gitwash docs  
comment
Which versions of networkx and numpy are you using? I get a different result:  ``` python In [10]: %paste import networkx as nx g = nx.DiGraph() g.add_node(0) g.add_node(1) g.add_weighted_edges_from([(0,1,1)]) nx.floyd_warshall_numpy(g)  ## -- End pasted text -- Out[10]:  matrix([[  0.,   1.],         [ inf,   0.]])  In [11]: nx.__version__ Out[11]: '1.8.dev_20130108070258'  In [12]: import numpy  In [13]: numpy.__version__ Out[13]: '1.9.0.dev-78e29a3' ``` 
comment
Thanks for bringing this up.  I think you summarized the situation well.  It's not really "creep" in the sense that we have switched to using SciPy sparse matrices for algorithms where we use linear algebra - see #1021.  There are still places where NumPy matrices are used but those should be in one of three categories 1) Not converted yet to use scipy.sparse - should open an issue to update 2) An alternate implementation, e.g. pagerank - possibly a target for removal 3) Intentionally, e.g. to create a numpy matrix with non-numeric entries (to_numpy_matrix)  NumPy and SciPy are not currently hard dependencies for NetworkX.  You can import and use NetworkX without either of them but with some loss of functionality.  We do this with lots of checks to see if numpy and scipy are importable - and that adds a lot of code noise.  But I'd prefer to keep the dependency as it is (optional, adds features).  Maybe it would be easier to keep track of and simplify maintenance of the numpy/scipy algorithms if we segregated them into a package, or several packages, that would not be automatically imported with `import networkx`.  I regularly advocate for pure-Python algorithms and I think that is a good starting point for adding new code into NetworkX.   In some cases using NumPy or SciPy makes writing the algorithm much easier (other cases not so much).   Generally I'd prefer not to have more than one version of every algorithm in NetworkX unless there are compelling reasons.  Adding a `sparse=False` code path where we switch between numpy and scipy falls in that category - I'm not enthusiastic about writing/testing/maintaining two versions of every algorithm that uses a numpy or scipy matrix.  As for installation issues with SciPy I think the scene has evolved pretty spectacularly with the development of the all-inclusive scientific Python environments such as Enthought's Canopy  https://www.enthought.com/products/canopy/ and Continuum's Anaconda  https://store.continuum.io/cshop/anaconda/.   Both of those include  SciPy and NumPy.  So 'hard to install' isn't such a valid complaint anymore.  It's also straightforward to quickly set up a virtual machine with Linux (lots of VM options) and apt-get everything you need.   
comment
Note that the issue on the table here isn't whether to stick NumPy into the core of NetworkX (though something like that is over at #1076).   We are already very committed (more than 10 years) of a pure-Python set of data structures and (most) algorithms.  So don't worry.    The basic story is 1) Linear algebra manipulations are sometimes very useful for graph problems. 2) NumPy is the de facto linear algebra system for Python. 3) NetworkX uses NumPy (and SciPy) for linear algebra manipulations.  I appreciate your views about difficulty with installing these packages but worrying too much about how to compile something, or if it fits on disk is a 1990's flashback of problem for most people.   There are plenty of options for pre-built code (some I mentioned).  As for 'interfaces', I'd be glad to do the engineering so that others can use the API with different "backend" data storage options.   
comment
$ sudo apt-get install python-scipy 
comment
Looks fine to me.  Thanks for adding the tests.  Ready to merge? 
comment
Thanks for writing the new tests.  I'll merge them into the pull request and get them included. 
comment
I merged #1070 including the extra tests which should fix this. 
comment
Maybe to_numpy_matrix() should be marking missing edges with a numpy.nan instead of zero? What implications does that have for the rest of the algorithms? 
comment
Actually I don't think we want to use np.nan for zeros.  The convention is zeros. But we could add an option in to_numpy_matrix() to specify what goes in for missing data. Something like to_numpy_matrix(,missing=0.0) 
comment
That is a good idea @chebee7i  Does using attr_matrix like that fix this issue? 
comment
Before you merge I'd like to review.   I think adjusting to_numpy_matrix() is the right approach. 
comment
If that suggested change is OK, let's merge this. 
comment
I like the idea of making the keyword a callable.  We have discussed that before and it even implemented something like that in relabel().    I'm not so excited about the numpy.ma module.  It also has performance issues and corner cases that I'm not enthusiastic about finding right now.    Also we have discussed a few times before the idea of just using scipy sparse matrices everywhere we can and the todense() method if we need a numpy matrix.  After all  ``` python $ python -c "import this" |grep Sparse Sparse is better than dense. ```  So given that, I think I'd like to try to not fix too many things at once with this PR and just solve the original problem (e.g. solution before the last commits).  When we decided to go forward with sparse matrix code (I hope soon), the approach will be different to NaNs and I'd like to try to put in a callable option for the matrix entries like you suggest.  We have a some code already plus a few PR's hanging around too.  Maybe we should open a "sparse is better than dense" PR? 
comment
I updated the development docs.  I can script that so they get updated regularly (daily). @chebee7i  if you want to provide them at rtd that is fine with me too. 
comment
Updated automatically every day now.   
comment
I can certainly remove the pages there - they are from before the move to github.io.  But I thought it was best practice to not break the Google and other old references. 
comment
It's old docs.  I left them there (an also left old Trac site dev info) instead of putting them down the memory hole.  I can adjust the server however we like as long as it isn't too complicated. I like @chebee7i's suggestion of blocking the crawlers from indexing the old docs. 
comment
This should always return real values anyway so the complex type was not appropriate. 
comment
My feeling is that we should make an effort to keep backward compatibility.  But if there is a good reason to break it for clarity, efficiency, consistency, etc.  then we should discuss it and find a consensus.  For example, having a little flexibility to redesign interfaces will surely help in the long run to make them clearer and also keep us from getting stuck on the initial design phase (a good example of stuckness is in #1090). 
comment
I meant my comments on backward compatibility to be a little more liberal especially related to newer code contributions.  I don't think we should worry too much about changing interfaces that are obviously not what we want, confusing, or have some other issue.    
comment
Looks like it is missing.  And should be added. The code is kind of 'hidden' over in the utils/ directory... 
comment
Added in PR #1113. 
comment
Thanks for the clear description of this issue.  I think the problem is in selecting the starting node for the algorithm (as chosen by networkx.utils.find_pseudo_peripheral_node_pair().)  To get the best ordering the start should be 0 but in this case it gives 5.  It could be that using the psudo-peripheral node pair method just doesn't give the optimal start. 
comment
I think the typical heuristic is to start with a "psudo-peripheral node" and not just the smallest degree.  Note that nodes 3,6 are in a disconnected component in the graph G above so if you process those first you'll still have to choose node 0 as a start eventually since the graph is processed in by component. 
comment
Updated to allow specification of heuristic.  See PR #1113. 
comment
@nonhermition does this change make sense to you?   
comment
Maybe we should just remove that code (in serialize.py).  Is it really adding much value? 
comment
Yes, looks good. 
comment
This is a good idea.  In fact we could have sparse matrix versions of other functions too e.g. adjacency_matrix() and laplacian_matrix().  There are some design choices for the names and arguments for those functions and I don't think we have come to a decision on how to do it.  We could do as you suggest and add a keyword argument whether you want a sparse or dense matrix format.  Or we could use two different functions like we do in convert.py for generating adjacency matrices.   Once you have a sparse matrix S you can easily get a dense matrix by calling D = S.todense() so that is another option (only have sparse matrix formats).  So before we incorporate this code I'd like to come to a consensus on how to proceed with sparse matrices.   We can comment here or maybe the mailing list too. 
comment
I'm +1 on only having sparse matrix formats and letting users use todense() to get a dense numpy matrix.     It might be worth looking at how that impacts performance of some algorithms.  For small graphs the sparse matrices might be slower to generate and convert. 
comment
Thanks for pointing these out.  These tests that rely on hash ordering are sometimes hard to find.  I thought I had them all fixed in #727 but clearly there are still some lurking. 
comment
Any more of these to fix? I couldn't get the numpy tests to break as shown by @Arfrever above. 
comment
Thanks for following up.  I fixed those tests now too in PR #1006. 
comment
There are a few issues here -  1) The tests should pass with Python3 using hash randomization. So we should fix those.  I can't reproduce these errors with my setup.  Is there a way to change the randomization seed or something that will help me do that?  2) Nodes don't need to be sortable. So we should assume that in general.  For a test we control we can use sortable nodes.  3) In general for graphs no order is implied.  We provide specification of node ordering in some functions - that might be useful in some cases where users are trying to produce known deterministic output and don't want to turn off hash randomization.   An example is nx.adjacency_matrix(). 
comment
@jtorrents solution looks good.  I can't get the tests to fail for me (before the fix) either. 
comment
Thanks @Arfrever, that is very helpful.  I configured my Python 3.4 setup the same way and can now reproduce these errors.    I merged @jtorrents pull request but let's leave this open and see if we can find and fix the rest of the tests that have this issue. 
comment
After PR #1086 was merged I'm not finding any more of these testing failures due to hash randomization. Anyone else? 
comment
This looks like a good idea to me and a clean interface.  I don't think we need to benchmark. 
comment
Can we just use to_scipy_sparse_matrix() instead of smat?  There is discussion in #913 about using sparse matrices for all of the linear algebra operations in NetworkX. 
comment
Using the sparse eigensolver is probably the right approach for large enough problems and I like the approach. Could you post the exception so we can figure out how to fix the to_scipy_sparse_matrix() function?  
comment
Can this be updated to use to_scipy_sparse_matrix() ? 
comment
They are the format that the D3.js package uses.   And you are right there should be some documentation. 
comment
That is true.  There are a few different formats of JSON serialization hat are used in the d3.js examples and you can, as @mbostoc points out, parse CSV data or whatever you want.    The three formats that you can generate with NetworkX are  - node-link like in the d3.js example http://bl.ocks.org/mbostock/4062045 - tree like in the d3.js example http://bl.ocks.org/mbostock/4063550 - adjacency like in the d3.js example http://bost.ocks.org/mike/miserables/ 
comment
I took out those sci calls last August: https://github.com/networkx/networkx/commit/e5c530c0ba90910429b193bcfaa268d164e93b73 So an updated networkx package should do it. 
comment
Yes, that is definitely a bug.  Thanks for the report.   This is a tricky algorithm - we have had some discussions (about performance) related to this algorithm implementation before https://github.com/networkx/networkx/pull/874 
comment
How soon until it gets fixed is hard to tell.  I've looked at this algorithm before (and read the paper) and wasn't convinced the implementation was completely correct.  If anyone wants to take this on I think it is a good challenge. 
comment
I don't remember this history of why both algorithms are there.  The first implementation was in this ticket https://networkx.lanl.gov/trac/ticket/394. Maybe @dschult can comment - he worked on this and alternate algorithms at some point. 
comment
The (recent) history is here - https://github.com/networkx/networkx/commit/d38de1f4df8c50917fb2b82ea15e3f816d59e942 
comment
Basic support in this issue means "can import networkx in Jython".  If someone wants to suggest more changes or improvements for Jython support please open another issue. 
comment
Thanks for the report. Does PR #1023 fix this for you? 
comment
We could change the code to allow a True/False option for copying:  ``` python def connected_component_subgraphs(G, copy=True):     cc = nx.connected_components(G)     graph_list = []     for c in cc:         if copy:             graph_list.append(G.subgraph(c).copy())         else:             graph_list.append(G.subgraph(c))     return graph_list ```  I believe the reason we added the copy to that function was to due to an issue with attribute handling.  But I don't remember all of the details - it looks like the relevant discussion is here https://networkx.lanl.gov/trac/ticket/588 
comment
#973 and #1009 resolve this issue.  I'm going to leave this open since we need to add a note about API changes to the docs before it is complete.  
comment
Reopen with a PR if someone wants to add this an an example. 
comment
Though I am generally not enthusiastic about magic-looking syntax or trickery I like the decorators.  Is the performance hit really that bad?  If I remember correctly we implemented the decorators for at least these reasons - Boilerplate checking of types and imports at the beginning of functions is tedious and ugly.   - Standardizing the error messages and import statements (e.g. import numpy as np ) makes it easier to understand the code and any errors  For @requires() I agree that it could be better to just raise the ImportError.  For regular Python users it is pretty clear that that means.  For people just starting or occasional users having a little more help can be useful (e.g. where to look for what you are missing).   For @not_implemented_for() we were solving a specific problem that we allowed users to pass in whatever type of graph/multigraph/digraph they wanted into functions that in the best case actually broke but in the worst case give wrong or misleading answers.  There are still quite a few functions where this is still the case.  There are three obvious solutions to that problem (are there more?)  1) Check input types and only allow the ones that make sense (hence the @not_implemented_for() decorator approach) 2) Make all functions methods of the graph classes with only those that work correctly attached appropriately 3) Some other namespace hackery to indicate which functions work with which types  We are currently doing 1)  We decided against 2) a long time ago with the goal to keep the basic graph type classes simple and small (in fact I recently suggested that I would be willing to remove some methods) I think the proposal above is to do something like 3).  I'm  interested to see how that might look but suspicious that it might be more confusing for users and potentially harder to debug. 
comment
At this point I suggest we drop the @require decorator but keep the @not_implemented_for. The latter helps us simplify the boilerplate code for checking input graph types until we agree on a better solution.  So let's make a PR to remove @require. 
comment
I'm going to close this even though it is not fully resolved.  If there is still a strong desire to remove the @not_implemented decorator in favor of a different approach let's open another issue and reference this one. 
comment
Do we really need to support Python 3.1 here?   If so maybe we can find a way to fix this without an explicit version check. 
comment
You are right.  We aren't currently testing against Python 3.1  (as is obvious from this issue).  So we should decide to either properly support and test against Python 3.1 or edit that file... 
comment
It looks like there was some temporary confusion over what the xml.etree tostring() should return?  I don't think I understand well what it is supposed to do.    As for future 2/3 compatibility - at some point we won't support Python2.x at all.  I don't think that is very soon but it will happen some day.  Python 3 has been here for 5 years. I don't think we have any explicit version checks right now (except in setup.py) but we should eliminate them if we do.  I'm not too enthusiastic about using the "six" module to do it though. 
comment
I think we should drop Python3.1 support.  Python3.4 is almost here. 
comment
I don't think they are backwards compatible.  If I remember right there are two incompatible verisions 1.1draft and 1.2draft which the code both reads and writes (at least most features). 
comment
Yes, the "personalization" parameter will possibly change the results.   Do you have a bug to report? 
comment
OK - please detail the bug and we'll try to fix it. A short example demonstrating the problem would be great. 
comment
This is a follow up to both this issue and #978.  Could you help me understand this issue in more mathematical terms?  From the article referenced in the NetworkX function, page 12 A. Langville and C. Meyer, "A survey of eigenvector methods of web information retrieval." http://meyer.math.ncsu.edu/Meyer/PS_Files/Survey.pdf you can read about the definition of the "Google Matrix" and "personalization vector".  ![googlematrix](https://f.cloud.github.com/assets/187875/1309372/7e5a3764-31de-11e3-88cb-2ae8f5905b0c.png)  For your example, NetworkX produces the following (alpha=0.85 by default) which shows that the personalization vector does affect the matrix in (I think) the correct way and produces different rank scores.  ``` python In [1]: import networkx as nx  In [2]: g = nx.DiGraph()  In [3]: g.add_edge(1, 2)  In [4]: nx.google_matrix(g,personalization={1: 1, 2: 1}) Out[4]:  matrix([[ 0.075,  0.925],         [ 0.5  ,  0.5  ]])  In [5]: nx.google_matrix(g) Out[5]:  matrix([[ 0.075,  0.925],         [ 0.5  ,  0.5  ]])  In [6]: nx.google_matrix(g,personalization={1: 0, 2: 1}) Out[6]:  matrix([[ 0.   ,  1.   ],         [ 0.425,  0.575]])  In [7]: import numpy.linalg  In [8]: numpy.linalg.eig(nx.google_matrix(g,personalization={1: 1, 2: 1})) Out[8]:  (array([-0.425,  1.   ]),  matrix([[-0.87970651, -0.70710678],         [ 0.47551703, -0.70710678]]))  In [9]: numpy.linalg.eig(nx.google_matrix(g,personalization={1: 0, 2: 1})) Out[9]:  (array([-0.425,  1.   ]),  matrix([[-0.92033092, -0.70710678],         [ 0.39114064, -0.70710678]])) ```  Is there something wrong with this data?   
comment
It is true what you say if you set alpha=0 when it is all personalization but then the matrix is reducible  ``` python In [7]: nx.google_matrix(g, alpha=0.0, personalization={1: 0, 2: 1}) Out[7]:  matrix([[ 0.,  1.],         [ 0.,  1.]]) ```  I don't see why you should get that result otherwise.    I believe that NetworkX implements correctly the PageRank score as described in the paper referenced above (Langville & Meyer, esp page 12, I didn't reproduce the whole thing inline).   I have looked at the original Page et al paper and though that description is a little confusing I think Langville and Meyer got it right. 
comment
How about we allow the user to also specify the vector for the "dangling nodes" (nodes with no out-links)? There are many ways to make the out-link matrix stochastic. Choosing to add a row with 1/n entries as in the PageRank definition is one way.  Choosing an arbitrary vector w>0 would work too.     I think that would allow you to adjust the matrix the way you want by setting both vectors.  But we should set the default to be the standard PageRank way and add a warning about arbitrary setting of these vectors. 
comment
Yes, please do send a pull request.  Yes - a warning in the docstring would be good where the new parameter (not sure what it should be called) is defined.  A few sentences in the notes section with pointers to the references (already there) would be helpful too. 
comment
Please do fix PEP-8 issues.  It's nice if they are in a separate commit from the other modifications so it is easier to figure out the substantive changes.  Aric  On Mon, Oct 21, 2013 at 3:42 PM, Brandon Liu notifications@github.comwrote:  > Will do -- might be a few days since I have some other stuff on my plate > for now. One other note -- the offending file right now doesn't follow PEP8 > style guidelines. Is there any particular reason, or would you mind if I > did some reformatting? Mainly things like placing spaces around operators > and some other non-Pythonic things. e.g., if x is None should be changed > to if not x. >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/960#issuecomment-26759610 > . 
comment
Oops I was looking at the related #960 and commented over there.  Maybe this are both the same question? 
comment
I suggested some changes with a PR to your branch at https://github.com/thenovices/networkx/pull/1  The tests still fail - but now because the dangling nodes examples don't converge.  Maybe the example is not a good one?   
comment
I think we are done here.  Other updates? 
comment
Is the normalization wrong or the documentation confusing?  That is, what should the correct answer be? 
comment
If I remember correctly (didn't check carefully now) there are different conventions for normalizing centralities. I used the definition suggested in Brandes and Fleischer http://www.inf.uni-konstanz.de/algo/publications/bf-cmbcf-05 
comment
I think that code correctly implements algorithm2 in the Brandes and Fleischer paper for normalized=False.  I don't see in that paper where there is mention of how to further normalize those values.  Maybe there is an agreed upon standard we can match?    
comment
Maybe the normalization is just backwards?  i.e. normalized=True should be normalized=False?  You are right that we don't need the duplicate line.  The two try/except clauses about the imports could be combined but this way there is a message that says which package is missing.  I think the general consensus is to remove these kind of try/except clauses for imports in NetworkX since the standard exceptions are clear. 
comment
@sbromberger I just suggested a PR that removes the normalization option completely.  Does this make sense and look OK?  I think it is less confusing since there doesn't seem to be (that I can find) any standard normalization definition. 
comment
I fixed the test.  I don't mind breaking the backwards compatibility here (removing the normalization= keyword). I don't think many people use this and it will be pretty obvious to those that do. 
comment
That looks great.   Here are some comments.  I only really care about the max coverage number which is with Python2.7 right now (because of missing python3 modules like PyGraphvz etc).  So I would be satisfied with only running the coverage report for one version (Python2.7).  Though it would be nice to have TravisCI do Python3.3 testing of everything including numpy etc., but I think we can do without that untill they upgrade the test environment to include those packages.  I am now set up to test regularly with Python3.3 and Python3.4. 
comment
Yes, let's just submit the coverage for python2.7.  I don't really care if it takes a while to report.  The coverage data is interesting but doesn't provide much value other than a rough guess at how good we are doing at testing (e.g. doesn't reflect the quality if the actual tests at all). 
comment
We need an example of a file to with ImageNodes to test for reading and writing. 
comment
No other interest in implementing this after about a year. 
comment
Thanks - I proposed an update of the comments based on your note. 
comment
That seems like a reasonable suggestion.  I thought we forbid using a tuple for a reason but I couldn't come up with that right now or from looking through the code revisions.  It may be a corner case because we using 2- and 3-tuples for edges.   Or e.g. this behavior   ``` python In [36]: list((0,1),) Out[36]: [0, 1] ```  @dschult do you have comments or ideas on this? 
comment
OK, thanks @dschult for the details - I think this is safe to merge. 
comment
I think this is actually a PyGraphviz bug but I haven't tracked it down yet. If you can use pydot you can avoid this issue for now: networkx.drawing.nx_pydot.read_dot() 
comment
@bminano does this fix your memory leak issue? 
comment
No consensus on how or if to proceed with this. 
comment
I think we fixed this in #772. That fix is included in networkx-1.8. 
comment
How about we just change bfs_tree() (and dfs_tree()) to   ``` def bfs_tree(G, source):     """Return directed tree of breadth-first-search from source."""     G = nx.DiGraph()     G.add_node(source)     G.add_edges_from(bfs_edges(G,source))     return G ``` 
comment
Yes for the DFS algorithms that is true (the bfs_edges() implementation doesn't work the same way). The simplest solution for dfs_tree() might be  ``` def dfs_tree(G, source):                                                             T = nx.DiGraph()                                                                 if source is None:                                                                   T.add_nodes_from(G)                                                          else                                                                                 T.add_node(source)                                                           T.add_edges_from(dfs_edges(G,source))                                            return T  ```  In this case of source=None this might not return a tree (a forest instead) 
comment
Thanks for the report.  I squashed what I think is the bug.  See PR #956. 
comment
This should be addressed in #812  
comment
nxpd looks good.  I'm going to close this.  If there is a further update of the Pydot3 package we can revisit. 
comment
I agree this is dubious approach to creating a valid degree sequence.   Maybe we should just remove the function completely? 
comment
I don't mind breaking this part of NetworkX.  These functions are really not part of "complex networks" or graph theory as much as statistics and sampling.  There might even be other packages now that provide some better solutions.     So I'm currently for removing create_degree_sequence() 
comment
Yes, that would work but is also not very mathematically sound.  I'm still voting for removing the function. 
comment
That fix might work.  But you could also in your case call the function with   ``` nx.difference (G,nx.MultiGraph(T)) ```  to have the same effect.  I wonder if it might be safer instead to forbid comparisons between graphs and multigraphs.  That way there wouldn't be ambiguity in the case, e.g. when the multigraph G has two edges between a pair of nodes and the graph H only one.  I could see an argument for the difference() to be either 1 edge or 0 edges.  If we force the user to "cast" one of the graphs to the same type then it would be the users choice. 
comment
They keys used for MultiGraph() edges differentiate the edges.  So they can't be the same (if you try to add another edge with the same key   ``` python In [1]: import networkx as nx  In [2]: G = nx.MultiGraph()  In [3]: G.add_edge(1,2,key='a')  In [4]: G.edges(keys=True) Out[4]: [(1, 2, 'a')]  In [5]: G.add_edge(1,2,key='a')  In [6]: G.edges(keys=True) Out[6]: [(1, 2, 'a')]  In [7]: G.add_edge(1,2,key='b')  In [8]: G.edges(keys=True) Out[8]: [(1, 2, 'a'), (1, 2, 'b')] ```  I'm not sure we thought very carefully how the binary operators work with graphs and multigraphs when we wrote them.  When I look at it now it seems there could be some ambiguity (as above) in some cases when mixing graphs and multigraphs.  I was suggesting maybe we resolve that by disallowing the mixed case.  But I'm open to other suggestions too. 
comment
I'm thinking of something like this:  ``` python In [1]: import networkx as nx  In [2]: G = nx.MultiGraph()  In [3]: G.add_edge(1,2,key='a')  In [4]: G.add_edge(1,2,key='b')  In [5]: H = nx.Graph()  In [6]: H.add_edge(1,2) ```  These two results might be confusing:  ``` In [8]: nx.difference(H,G).edges() Out[8]: [] ```  (and if we use your suggested fix)  ``` In [9]: nx.difference(G,nx.MultiGraph(H)).edges() Out[9]: [(1, 2), (1, 2)] ```  You could argue that it is the right answer but I think it could be simpler (for sure in the code) and safer to not allow mixed type graphs. 
comment
Given all of the logic/errors/warnings that would need to be put in the code I'm still in favor of raising an error if the two graphs don't match types.  That way the user will have to make a decision about what it means to compare edges of graphs and multigraphs with edge keys.  But I think it is better that the user do that than for us try to guess or catch all of the possible cases. 
comment
It is definitely missing (for Python 2.x anyway).  It looks like there is a GEXF "long" type http://gexf.net/1.2draft/gexf-12draft-primer.pdf‎ Shouldn't it use that instead? 
comment
It seems like I just overlooked the GEXF "long" type when that code was developed.  So we can just add it - there is no other part of the gexf.py code that needs changing.  The Python3 integer type might better be mapped to the GEXF "long" type too. 
comment
@gdbassett can you take a look at this small PR and see if it works for you? 
comment
I don't understand the problem being addressed here.  Can you clarify? 
comment
Good idea.  We'll update the documentation.   
comment
Jordi - the testing script looks like it is working well!  Do you want to open at pull request for it so we can comment on it?  Maybe it could go in a tools/ directory? 
comment
It's on the list for the upcoming release - I just haven't quite got to reviewing it yet. Will do so ASAP so we can merge. 
comment
**NetworkX: Test results for pull request #742 ([jtorrents 'connectivity' branch](https://github.com/jtorrents/networkx/tree/connectivity))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit 0c9aeb2 into NetworkX master 16cd1e5) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=60) Ran 1570 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1760 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1739 tests (libraries not available: ogr, pygraphviz, pydot) - python3.3: :eight_spoked_asterisk: OK (SKIP=37) Ran 1633 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) 
comment
**NetworkX: Test results for pull request #771 ([jtorrents 'bipartite-clustering' branch](https://github.com/jtorrents/networkx/tree/bipartite-clustering))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit 3b59b4d into NetworkX master be829d3) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=60) Ran 1537 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1727 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1706 tests (libraries not available: ogr, pygraphviz, pydot) - python3.3: :eight_spoked_asterisk: OK (SKIP=37) Ran 1600 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) 
comment
I think it is fine the way it is. Let's add the functions to doc/source/reference/algorithms.bipartite.rst 
comment
That probably isn't the correct fix.  The file should have lower case "true" and "false" according to the GEXF spec.  So it is the writer not the reader that needs to be fixed. 
comment
Looks good, just need to fix the failing test. 
comment
I like the idea of storing a graph attribute as you propose. 
comment
Yes, it is a silly mistake (fixed already)  https://github.com/networkx/networkx/pull/918 where those files needed to be added to MANIFEST.in  The tests should pass with those files included https://travis-ci.org/networkx/networkx  Would you prefer a 1.8.1 release to address this issue for packaging? 
comment
https://pypi.python.org/pypi/networkx/1.8.1  Let us know if there are any other packaging issues that need to be addressed and we'll fix them right away. 
comment
Can you describe more what the problem is?  It seems like you are suggesting that the order should be reversed (reverse=True).  That would make the largest weight edge first in the edges list. 
comment
Thanks for reporting this.  We have already fixed that error.  That documentation is for version networkx-1.5 and we are currently at networkx-1.8.  The latest documentation is at http://networkx.github.io/documentation/latest/ 
comment
Thanks for looking up the old ticket.  I think you are right that the problem was that the code might never finish when rewiring highly connected graphs.  That is because the algorithm uses a simple rejection strategy to rewire edges so that there are no self loops or parallel edges.    The Watts-Strogatz paper looks at sparse graphs where roughly 1<< log(n) << k << n So limiting k to n/2 is reasonable for that purpose. 
comment
Thanks for the report.  That should be fixed to leave the original list intact. 
comment
Maybe you can install Enthought Canopy (a meta-package of many useful python tools)?  On Fri, Jul 19, 2013 at 6:00 PM, mh602653 notifications@github.com wrote:  > thanks for your quick response. I did "easy_install matplotlib" and I got > an error that I require NumPy. I installed NumPy using the file available > here: >  > http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy >  > (My python version is 2.7) >  > Then I again started: "easy_install matplotlib" >  > Now I am getting an error saying: "the directory is not empty c:\path..\ > local\temp\easy_install-x0ngnn\matplotlib-1.2.1\lib\mpl_example\user_interfaces" >  > Any idea about this one? >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/904#issuecomment-21284268 > . 
comment
This is not a bug/feature/issue.  Please use the networkx-discuss mailing list for general help with installing NetworkX.  https://groups.google.com/forum/#!forum/networkx-discuss 
comment
Could you post the input data so we can reproduce the issue? 
comment
Ok - Can you provide an example that we can run to show the problem? Aric  On Thu, Jun 6, 2013 at 5:15 PM, roguebee notifications@github.com wrote:  > The data I used is from the Ordinance Survey UK, which is free data you > can download from here > https://www.ordnancesurvey.co.uk/opendatadownload/products.html >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/879#issuecomment-19052211 > . 
comment
I need something more specific and self-contained.  For example, I don't know what C:\streetview  is ... 
comment
Right - it's more likely I will find time to see if I can fix the problem if I don't have to chase down data and formats I'm not familiar with. So if you can make an example I can run that would be great.  On Mon, Jun 10, 2013 at 12:03 PM, roguebee notifications@github.com wrote:  > streetview is the name of the directory I stored the shapefile data in. > You can download the data from > https://www.ordnancesurvey.co.uk/opendatadownload/products.html for free > under Meridian 2. >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/879#issuecomment-19215821 > . 
comment
OK, maybe someone else can help with this issue?  On Mon, Jun 10, 2013 at 12:48 PM, roguebee notifications@github.com wrote:  > Its pretty easy to download, I've given an example program and I don't > think I'd be allowed to distribute the data myself, so not much more I can > really do. >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/879#issuecomment-19218378 > . 
comment
Could you provide a small example that we can use to reproduce your issue? 
comment
#899 should fix this 
comment
Thanks for suggesting this code.  I'm not sure we should add it to the Graph (and other) classes though.  It  specifies a particular choice of a list as a container and I could make an argument for sets or other containers for other use cases.   And you could imaging wanting to remove items from a list in edge data too.   In general I'd prefer not to make the add_edge() API any more complicated.  You can manipulate edge data already by accessing the G[u][v] dictionary directly which covers all of the uses cases.  Or it is pretty simple to just make a new subclass of Graph() that does exactly what you want. 
comment
Can we close this? 
comment
It gives the correct answer (as far as I know), e.g.  ``` In [1]: import networkx as nx In [2]: G = nx.Graph([(0,0)]) In [3]: print list(nx.all_simple_paths(G,0,0)) [] ```  So maybe it is OK? 
comment
It's supposed to be here http://networkx.github.io/documentation/development/developer/index.html but is not complete yet. 
comment
This is a PyGraphviz question.  Currently there is no github site for that project.  Discuss at pygraphviz-discuss google group. 
comment
You can get an array using the A property or the getA() method of a Numpy matrix:  In [1]: import networkx as nx  In [2]: L = nx.laplacian_matrix(nx.path_graph(4))  In [3]: L.A Out[3]: array([[ 1., -1.,  0.,  0.],        [-1.,  2., -1.,  0.],        [ 0., -1.,  2., -1.],        [ 0.,  0., -1.,  1.]])  In [4]: L.getA() Out[4]: array([[ 1., -1.,  0.,  0.],        [-1.,  2., -1.,  0.],        [ 0., -1.,  2., -1.],        [ 0.,  0., -1.,  1.]])  Aric  On Wed, Jun 26, 2013 at 5:07 PM, Ryan notifications@github.com wrote:  > I've been trying to look on the internet why this decision was made. I > have not been able to find any explanation. The reason I ask is because I > have a piece of code that I would like to use arrays. I cannot find any way > to tell networkx to return a numpy array instead of a numpy matrix. Is it > possible to have networkx functions like adjacency_matrix return a numpy > array? >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/895 > . 
comment
I guess with graphs you typically want a 2d array and to manipulate it like a matrix.  I hope we are consistent and always return matrices.   It can see good arguments on both sides (for returning arrays vs matrices).  On Wed, Jun 26, 2013 at 5:17 PM, Ryan notifications@github.com wrote:  > That is really cool. Didn't know that was possible. That answers one > question. Any comment on why NetworkX uses matrices by default? >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/895#issuecomment-20087060 > . 
comment
I agree with @calmofthestorm in this case - it makes sense to return False in is_directed_acyclic_graph() when the input is undirected. So I'd like to merge this pull request.  The reason the exception gets raised is that this function calls topological_sort() which checks the input to make sure it is directed.  I suspect that prevents strange answers if someone tries to perform a topological sort on an undirected graph().    It would be worth reviewing the other functions in dag.py for this type of issue.  We could potentially use the @not_implmented_for('undirected') in some cases. 
comment
The latest version (development) does use node attributes instead of a graph attribute. Is the documentation for that clear? 
comment
The current development version has  ``` def convert_node_labels_to_integers(G, first_label=0, ordering="default", label_attribute=None): ```  where label_attribute if specified is the name of the node attribute for the old labels. Does that make sense? 
comment
Looks fine to me. 
comment
We could add an error check if this message is too confusing. 
comment
Yes, this fixes a potential problem if the user specifies a 'weight' attribute. 
comment
It's already called "degree_iter()"  ```  In [1]: import networkx as nx   In [2]: G=nx.DiGraph()   In [3]: G.add_edge(1,2)   In [4]: G.deg  G.degree       G.degree_iter     In [4]: list(G.degree_iter())  Out[4]: [(1, 1), (2, 1)] ``` 
comment
Something like this should be pretty efficient:  ```   import networkx as nx   from itertools import izip # zip in Python3   G = nx.DiGraph([(1,2),(3,4)])   print list(izip(G.in_degree_iter(),G.out_degree_iter())) ``` 
comment
Yes I suppose that is possible but I think unlikely.  At least you can check that easily. 
comment
Probably.  But I'm not sure about "hash randomization" (Python3.3), e.g. http://stackoverflow.com/questions/14956313/dictionary-ordering-non-deterministic-in-python3 
comment
On Sat, Mar 16, 2013 at 7:26 AM, Dan Schult notifications@github.com wrote:  > Wow-- hash randomization sounds like a fairly subtle yet important > change. Makes me wonder if we rely on dict order anywhere in > networkx. I don't think so, but not sure. It doesn't sound easy to > test for either.  We do rely on the ordering sometimes in tests (especially doctests). I think I have (earlier) fixed them all but I might have missed some. 
comment
I'm glad you got it to work.  It doesn't seem like a bug in the cycle_basis() function though. 
comment
The GML id must be an integer.  So the GML writer in NetworkX assigns an (arbitrary) integer when the file is written.  But the original NetworkX node name is captured in the label attribute as you can see from your example.  For the GEXF writer node ids can be strings so the default is to set the node id to the string representation of the NetworkX node name.   You can change the id by specifying a node attribute called 'id'.  The upshot of this is that the ids will not be the same in the GML format file as the GEXF format file unless you specifically set them.. 
comment
Actually it looks like there is a bug in the GEXF reader if you explicitly set the id  ``` In [8]: G = nx.Graph()  In [9]: G.add_edge('a','b')  In [10]: G.node['b']['id']=20  In [11]: G.node['a']['id']=10  In [12]: nx.write_gml(G,sys.stdout)  #OK graph [   node [ id 10 label "a"   ]   node [ id 20 label "b"   ]   edge [ source 10 target 20   ] ]  In [13]: nx.write_gexf(G,sys.stdout) # edge has wrong source/target <?xml version="1.0" encoding="utf-8"?><gexf version="1.1" xmlns="http://www.gexf.net/1.1draft" xmlns:viz="http://www.gexf.net/1.1draft/viz" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2001/XMLSchema-instance">   <graph defaultedgetype="undirected" mode="static"> <nodes>   <node id="10" label="a" />   <node id="20" label="b" /> </nodes> <edges>   <edge id="0" source="a" target="b" /> </edges>   </graph> </gexf> ``` 
comment
Everything is internally consistent in the file with the labels and ids (with the exception of the bug I pointed out above).  Unless you explicitly set the ids the GML writer will generate them for you. Here is your example from stackoverflow http://stackoverflow.com/questions/14779305/extracting-data-from-a-gml-file  ``` python In [1]: import networkx as nx  In [2]: G = nx.read_gml('t.gml')  In [3]: G.node Out[3]:  {0: {'id': 0, 'label': u'24'},  1: {'id': 1, 'label': u'25'},  2: {'id': 2, 'label': u'26'},  3: {'id': 3, 'label': u'27'},  4: {'id': 4, 'label': u'20'},  5: {'id': 5, 'label': u'21'}}  In [4]: G.node[0]['label'] Out[4]: u'24'  In [5]: G = nx.read_gml('t.gml',relabel=True)  In [6]: G.node Out[6]:  {u'20': {'id': 4, 'label': u'20'},  u'21': {'id': 5, 'label': u'21'},  u'24': {'id': 0, 'label': u'24'},  u'25': {'id': 1, 'label': u'25'},  u'26': {'id': 2, 'label': u'26'},  u'27': {'id': 3, 'label': u'27'}}  In [7]: cat t.gml graph [   node [     id 0     label "24"   ]   node [     id 1     label "25"   ]   node [     id 2     label "26"   ]   node [     id 3     label "27"   ]   node [     id 4     label "20"   ]   node [     id 5     label "21"   ] ] ``` 
comment
Your attribute strings have single-quoted double-quoted strings. If you take off the outer set of quotes it should only produce one pair of (double) quotes on output. 
comment
If you don't care about which one the edges you keep  you can use the Graph constructor:  In [1]: import networkx as nx  In [2]: M = nx.MultiGraph([(1,2),(1,2)])  In [3]: G = nx.Graph(M)  In [4]: G.edges() Out[4]: [(1, 2)] 
comment
On Thu, Feb 28, 2013 at 1:40 PM, jamiefolson notifications@github.comwrote:  > It seems like in most cases, that wouldn't be desirable. For most "weight" > semantics you'd want to either sum weights or take the max or min. It'd be > great to convert to Graph/DiGraph with some kind of "reduce" function that > combines all edges into one. >  > For sure.  And if your graph isn't weighted but has data like colors maybe > you would want to add them in RGB space, or if the edge data are files > maybe you want the diff or cat of them.... > So it's hard to make a general solution.  But if it is useful we could provide a generic multigraph->graph function that requires an input of the function to combine multiple edges. 
comment
If we want to design a new function let's open a new issue. 
comment
If you think it is a bug please try to isolate the problem and post your example here so we can reproduce it. Without that we can't help you much. 
comment
You can ask questions on the NetworkX google group: https://groups.google.com/forum/?fromgroups#!forum/networkx-discuss  I'm going to close this as not a bug. 
comment
This works for me (with matplotlib from the devel sources at github).  So we'll need more information to figure out what the problem is. Can you verify that your Matplotlib installation works properly (e..g run the matplotlib tests, other examples)? 
comment
I'm going to close this as a Matplotlib issue unless there is further information that it is caused by NetworkX code. 
comment
I would prefer that it actually does what the documentation says (set a node attribute with the old label).  In that case it might be better to offer a keyword to specify the node attribute name for the old label (default None) instead of using the keyword "discard_old_labels". 
comment
I added PR #821   with a proposed solution. 
comment
Also could use a test or two.  On Tuesday, February 5, 2013, cianci wrote:  > Ha. Thanks for catching that. Fixed now. >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/840#issuecomment-13146435.  ##   Aric Hagberg Los Alamos National Laboratory math.lanl.gov/~hagberg 
comment
I didn't try it.  We try to have tests for all functions.  A simple test in test_graphml.py would be good.  On Tuesday, February 5, 2013, cianci wrote:  > Did it fail on you? >  > Or is there something specific you would like to see? (Beyond the example > in the docstring?) >  > I apologize if I'm unaware of a particular convention that you all > habitually use... >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/840#issuecomment-13148114.  ##   Aric Hagberg Los Alamos National Laboratory math.lanl.gov/~hagberg 
comment
**NetworkX: Test results for pull request #840 ([cianci 'parse_graphml' branch](https://github.com/cianci/networkx/tree/parse_graphml))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit 3bf9030 into NetworkX master f0f88a2) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=64) Ran 1593 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1797 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1776 tests (libraries not available: ogr, pygraphviz, pydot) - python3.3: :eight_spoked_asterisk: OK (SKIP=64) Ran 1593 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) 
comment
**NetworkX: Test results for pull request #837 ([bjedwards 'master' branch](https://github.com/bjedwards/networkx/tree/master))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit 260332b into NetworkX master 8d75c8e) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=64) Ran 1592 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1796 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1775 tests (libraries not available: ogr, pygraphviz, pydot) - python3.3: :eight_spoked_asterisk: OK (SKIP=64) Ran 1592 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) 
comment
If the error message is OK we can just use the @not_implemented_for('graph') decorator. 
comment
Yes, correct, networkx-1.0. You can ask questions on the networkx-discuss mailing list. 
comment
It was added before networkx-1.0. 
comment
It should be set to zero for both directed and undirected graphs.  Either by adding an extra line as suggested (but before the if undirected condition) or else by initializing all of the d[u][u]=0 for every u in G.  Also we should add tests for this case... 
comment
Thanks - fixed it. https://github.com/networkx/networkx-website/commit/1f595cfbb7e0931a746d4636d32fe176c0a68e8e 
comment
Sorry, it's fine.  I mistakenly reopened this... Closing now. 
comment
If NetworkX is using a non-standard normalization I'd like to fix it.  There is also the issue of what to do when the graph isn't connected - in the current code closeness centrality is computed in each connected component.  Other comments? Suggestions? 
comment
We should include a formula with reference(s) to the definition we are using. 
comment
Fixed with #825 and #829  
comment
I'm closing this as complete with PR #770. 
comment
That line is apparently allowed by the "specification"  - see http://vlado.fmf.uni-lj.si/pub/networks/pajek/svganim/1.10.7.1/PajekToSvgAnim.pdf but clearly not necessary for functionality.  So I think we can remove it without any harm. 
comment
Can you provide the GraphML input file that causes the error?  It looks like the NetworkX GraphML reader might not handle one of the key types in that file (at least that is what the error message suggests).  A small file that reproduces the error would help figure out what the problem is. 
comment
I think the problem is that the MaltegoEntity <key> is missing the attr.type. Here is your file where I have added attr.type="string" in two places and that works with NetworkX. https://gist.github.com/3969750  If I understand the specification correctly (and I might not...) the attr.type is required . 
comment
We could do something like PR #785  which reworks the error handing of key/attributes to guess a string type if none is given.  Untested (needs test). 
comment
Maybe it doesn't matter here?  For the iterative algorithm to converge I think either will work and for the final result the difference is just a uniform scaling.    Is there a standard way that this is implemented? 
comment
I like @joelmiller's idea of normalizing the vectors in the algorithm to the max value.  I suggest in addition to that we add a normalize=True|False (True default) that normalizes to the sum of squares as in the Kleinberg paper.   Of course you can normalize however you like after you get the data. 
comment
**NetworkX: Test results for pull request #814 ([AndrewWalker 'master' branch](https://github.com/AndrewWalker/networkx/tree/master))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit 7115a01 into NetworkX master 4facec2) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=60) Ran 1539 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1729 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1708 tests (libraries not available: ogr, pygraphviz, pydot) - python3.3: :eight_spoked_asterisk: OK (SKIP=37) Ran 1602 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) 
comment
This was already done.  See #718 for issue of adding function to produce edges in all simple paths. 
comment
That is more than just renaming in the bfs module - but using collections.deque is a good idea and likely faster for larger problems. 
comment
These are now "one-liner" functions.  Maybe they are not so obvious at first but we could document them in the DAG module. 
comment
Yes, I see your point and I am not opposed to adding these "one-liner" functions.  But they do come at some cost of maintenance for the future - any individual function isn't such a big deal but they can add up and it is hard to remove things.   
comment
It looks like this PR for some reason can't be automatically merged.  If that can be fixed I'll merge it. 
comment
I guess so.  Not sure exactly what is happening with the current PR. 
comment
Need to add new functions to doc/source/reference/algorithms.dag.rst 
comment
Lets add a test that shows the correct normalized Laplacian as an example.   See https://github.com/networkx/networkx/blob/master/networkx/linalg/tests/test_laplaican.py 
comment
**NetworkX: Test results for pull request #738 ([aweinstein 'fix_norm_laplacian' branch](https://github.com/aweinstein/networkx/tree/fix_norm_laplacian))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit 62cd89c into NetworkX master e3231e8) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=59) Ran 1533 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1723 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1702 tests (libraries not available: ogr, pygraphviz, pydot) 
comment
**NetworkX: Test results for pull request #770 ([jtorrents 'generic-projection' branch](https://github.com/jtorrents/networkx/tree/generic-projection))** :eight_spoked_asterisk: This pull request can be merged cleanly (commit b681c3d into NetworkX master 2c9926b) Platform: linux2 - python2.6: :eight_spoked_asterisk: OK (SKIP=59) Ran 1533 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: :eight_spoked_asterisk: OK (SKIP=None) Ran 1723 tests - python3.2: :eight_spoked_asterisk: OK (SKIP=6) Ran 1702 tests (libraries not available: ogr, pygraphviz, pydot) 
comment
Thanks for pointing that out.   We need to handle self-loops specially (how?) or make that zero. At minimum we need to document how it works for self loops. 
comment
PR #801 fixes the single node with self loop case.  Self loops are counted as edges in the density function so the total density can be higher than one.   
comment
No code. 
comment
No interest in this.  NetworkX now does work with pypy. 
comment
No code provided. 
comment
Not sure what the use case is for this. 
comment
I'm getting an error with the tests - I can figure out how to fix it.  # aric@ll:~/Software/networkx/networkx-shapefile$ nosetests-2.7 networkx/readwrite/tests/test_shp.py  ## ERROR: test_shp.TestShp.test_attributeexport  Traceback (most recent call last):   File "/home/aric/.local/lib/python2.7/site-packages/nose-1.1.3.dev-py2.7.egg/nose/case.py", line 197, in runTest     self.test(*self.arg)   File "/home/aric/Software/networkx/networkx-shapefile/networkx/readwrite/tests/test_shp.py", line 113, in test_attributeexport     testattributes(edges, G)   File "/home/aric/Software/networkx/networkx-shapefile/networkx/readwrite/tests/test_shp.py", line 102, in testattributes     coords = feature.GetGeometryRef().GetPoints()   File "/usr/lib/python2.7/dist-packages/osgeo/ogr.py", line 2927, in <lambda>     **getattr** = lambda self, name: _swig_getattr(self, Geometry, name)   File "/usr/lib/python2.7/dist-packages/osgeo/ogr.py", line 55, in _swig_getattr     raise AttributeError(name) AttributeError: GetPoints  ---  Ran 4 tests in 0.039s  FAILED (errors=1) 
comment
On Sat, May 19, 2012 at 2:55 PM, Ben Reilly reply@reply.github.com wrote:  > What version of the GDAL are you using? It looks like the addition of GetPoints was [fairly recent](http://trac.osgeo.org/gdal/wiki/Release/1.9.0-News) (version 1.9.0). The pypi package was only updated to 1.9.0 on 2012-02-28.  OK - I am using 1.7.0 which is what ships with the latest ubuntu 12.04. I tried to compile the newer version 1.9.0 but haven't managed to figure out how to get it working. The setup.py file is broken, but even after fixing (maybe I did that wrong) that I'm still getting errors from gcc, e.g.  extensions/gdal_wrap.cpp:2999:67: error: ‘VSILFILE’ has not been declared  Aric  Aric 
comment
Can we make this work with both old and new versions of GDAL?  At least not break with older versions? 
comment
This PR might not resolve all the figure canvas issues mentioned in the thread.  But it should solve the one that is causing the most obvious problem. 
comment
Good idea.  I'll review this and we should add a couple of tests too. 
comment
OK that makes sense.  What is the best technical way to do this?  Some kind of hack in setup.py i guess? 
comment
I hope #779 does this correctly.  The solution is just to move the version-specific code into a subpackage and add a switch in setup.py to only include the appropriate version. 
comment
Might be good as examples. 
comment
These would be best as examples.  Open a new issue/PR if we should consider that. 
comment
On Mon, Oct 22, 2012 at 3:07 PM, Enrico Giampieri notifications@github.com wrote:  > Well, there are two solution: >  > implement the same function under DiGraph (but it would duplicate the > code) >  > make the insertion simmetric even in Graph, but would double the time of > execution. to make this would be sufficient to use the permutation function > from the itertools, as it doesn't insert the duplicate: >  > from itertools import permutations > list(permutations('ABC', 2)) > [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')] >  > a third way (a little more invasive) would be to add a flag to each class > reminding if the link are simmetric or not, and deciding which one to use on > that base  If we want to implement an add_clique() method for directed graphs we should use a standard definition if there is one. 
comment
This should really be a separate issue or a discussion on networkx-devel.  On Mon, Oct 22, 2012 at 4:09 PM, Enrico Giampieri notifications@github.com wrote:  > Speaking of code duplication, it is my impression or several methods has > clearly duplicated code? >  > remove_edges_from and remove_edge > add_edges_from and add_edge > remove_nodes_from and remove_node > add_nodes_from and add_node >  > is this intentional or I can go on and refactor those functions? (the ones > that works on collection expressed in terms of the elementary ones)  It's intentional.  It helps resolve the ambiguity of adding or removing nodes that might look like a container (e.g. a tuple or a Graph) and makes adding many nodes much faster.  Same for edges. 
comment
On Fri, Oct 26, 2012 at 12:24 PM, Enrico Giampieri notifications@github.com wrote:  > Ok, maybe i will visit the networkx-devel, thank you for the direction. >  > About the algorithm, what do you mean with standard definition? as far as I know the definition of a full connected group of nodes should be enough, doen't it?  What I mean is a "textbook standard" definition for a clique in a directed graph.  e.g. for a directed graph there is a notion of strongly and weakly connected so what you wrote above is already ambiguous.  For undirected graphs it is clear - all node pairs are connected by an edge. 
comment
On Fri, Oct 26, 2012 at 1:04 PM, Enrico Giampieri notifications@github.com wrote:  > So it may be better to move it as an external function to the Graph class, specifying the ambiguity in the docstring?  The options seem to be 1) Don't add anything  - it's a one-liner: G.add_edges_from(itertools.combinations(nodes,2) 2) Add the add_clique() method to the Graph class but override it in the DiGraph class with something... 3) Add a new function that only works on Graphs or MultiGraphs  Anyone else have comments? 
comment
Since this is a "one-liner" I think we don't need to include it:  G.add_edges_from(itertools.combinations(nodes,2) 
comment
No specific issue identified.   
comment
On Mon, Nov 5, 2012 at 12:10 AM, Vishal Goklani notifications@github.com wrote:  > I ran this code exactly: > http://networkx.lanl.gov/_static/examples/random_geometric_graph.py >  > and it fails: >  > Traceback (most recent call last): > File "test.py", line 22, in > nx.draw_networkx_edges(G,pos,nodelist=[ncenter],alpha=0.4) > File > "/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/networkx-1.7-py2.7.egg/networkx/drawing/nx_pylab.py", > line 546, in draw_networkx_edges > ax.add_collection(edge_collection) > File > "/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/matplotlib/axes.py", > line 1427, in add_collection > self.update_datalim(collection.get_datalim(self.transData)) > File > "/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/matplotlib/collections.py", > line 167, in get_datalim > offsets.shape = (-1, 2) # Make it Nx2 > AttributeError: incompatible shape for a non-contiguous array >  > I am using matplotlib version 1.1 > networkx version 1.7 (direct from github) > and numpy version 1.8  Maybe this is the same issue as https://github.com/numpy/numpy/issues/2700? (which is apparently a bad interaction between Numpy and Matplotlib when using the development NumPy version). 
comment
From the error traceback it appears to be in issue in either the pyparsing or pydot packages.  Can you confirm that you have those installed and working correctly?  I wasn't aware that Pydot worked with Python3. 
comment
OK - it looks like there is some issue with bytes vs strings.  I can't figure out the right way to call pydot. This is what I don't understand. If I can make this work I can fix your issue - maybe you can ask on the pydot list?  ``` import pydot P = pydot.Dot() D = P.create_dot().decode('utf-8') print(D) Q=pydot.graph_from_dot_data(D) ```  produces   ``` $python3.2 foo1.py digraph G {     node [label="\N"];     graph [bb="0,0,0,0"]; }  Traceback (most recent call last):   File "foo1.py", line 5, in <module>     Q=pydot.graph_from_dot_data(D)   File "/nh/nest/u/aric/.local/lib/python3.2/site-packages/pydot-1.0.15-py3.2.egg/pydot.py", line 211, in graph_from_dot_data     return dot_parser.parse_dot_data(data)   File "/nh/nest/u/aric/.local/lib/python3.2/site-packages/pydot-1.0.15-py3.2.egg/dot_parser.py", line 472, in parse_dot_data     if data.startswith( codecs.BOM_UTF8 ): TypeError: startswith first arg must be str or a tuple of str, not bytes ``` 
comment
On Mon, Nov 5, 2012 at 1:33 PM, wdanilo notifications@github.com wrote:  > I dont think it will help, the last commit of pydot was a year ago ... anyway, there it is: http://code.google.com/p/pydot/issues/detail?id=77&thanks=77&ts=1352147494 >  > Could you please tell me if there are other tools that can I use with Python and NetworkX and vizualize graphs in graphviz? (I prefer grphviz over matplotlib, because it can visualize multigraphs with circular dependencies in proper way and additional the results are not so random as with matplotlib (everytime the generated graph looks the same))  Well, both pydot and pygraphviz work with Python2.6 or later (just not Python3). If all you need to do is write a simple dot file you can probably generate the text yourself pretty easily. A basic dot file generator was proposed recently - there should be an issue or pull request about that. Aric 
comment
The code I mentioned above is here https://github.com/networkx/networkx/issues/757  That looks like a reasonable pydot fix - so I hope the pydot developers will include it. 
comment
If you want to make a pull request with code that will help us get this included faster.  On Mon, Nov 5, 2012 at 4:27 PM, wdanilo notifications@github.com wrote:  > Thank you! That code works good! I've modified it a little bit (see > below). It is now working with Python2.7 AND Python3 and sorry for the > style it is written (somebody think that two returns in short function is > ugly or "%" formatting of strings is not the most beautifull way of > formatting - I think the same way), but this code have to be as fast as > possible and this style is the fastest possible (just of curious if you > want to test, from all methods "["+str(x)+"]" AND "[{0}]".format(x) AND > "[%s]"%x, the last is the fastest. (I appologize for this small oftopic ;) >  > [code] > def convert_dot2(graph): > out = '' > def get_attribs(data): > if data: return ' [%s];\n'%(', '.join(['%s=%s'%(key, value) for key, value > in data.items()])) > else: return ';\n' >  > out += 'strict digraph "%s" {\n'%graph.name > for node, attrs in graph.nodes(data = True): >     out += node >     out += get_attribs(attrs) > for u, v, attrs in graph.edges(data = True): >     out += '%s->%s'%(u, v) >     out += get_attribs(attrs) > out += '}\n' > return out >  > [/code] >  > — > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/issues/790#issuecomment-10092938.  ##   Aric Hagberg Los Alamos National Laboratory math.lanl.gov/~hagberg 
comment
Still a good idea.  But let's open a new issue or PR or reference #757 for this. 
comment
The GML spec says that the file should be "7 bit ASCII" http://www.fim.uni-passau.de/en/fim/faculty/chairs/theoretische-informatik/projects.html 
comment
Yes, it looks like the fix isn't in the networkx-1.7 release (July 2012) but will be in the upcoming verison (networkx-1.8).  You can use the developer version from github in the meantime. 
comment
No bug. Open a new issue or PR for adding eulerian_path(). 
comment
Great. Thanks!   
comment
On Thu, Sep 20, 2012 at 6:05 AM, condector notifications@github.com wrote:  > wschlauch... no problem... work either.. but I use a DiGraph... and only > import networkx 1.7 (installed via easy_install) and this error occur. Your > solutions is more elegant.. but need to insert into networkx package on next > version. > It looks like that import statement has already been included into the > latest code: > https://github.com/networkx/networkx/blob/master/networkx/classes/function.py 
comment
Great, thanks!  Good idea to use append() rather than insert().  And I think the original algorithm was written before set() existed in Python...  If this passes our tests we can merge it ASAP. 
comment
How about just making operators.unary.reverse() call the graph method reverse()?   (We do that with subgraph).  The reverse operation can  be pretty "low level" depending on the implementation and I think it is wise not to be swapping around the graph class data outside of the class methods.   For the additions to the shortest path (target only case) I'm fairly ambivalent.  It is simple code but makes the documentation for usage much more complicated.   
comment
Looks good.  Maybe add a line in the docs for this new function at https://raw.github.com/networkx/networkx/master/doc/source/reference/algorithms.operators.rst 
comment
Thanks for looking at it @dschult.  I merged the PR yesterday. 
comment
There are two versions of the decorator code _dectorator.py (Python2.x) and _decorator3.py (Python3.x) with a dispatch from __init__.py in that directory.  So _decorator.py should never get run by Python3.   I guess the virtualenv setup is calling that somehow? 
comment
On Sun, Aug 12, 2012 at 11:47 AM, Thomas Kluyver notifications@github.comwrote:  > Thanks - one of the reasons I haven't spun it out into a separate project > is that I worry it would rapidly gain complexity to handle the requirements > of different projects. >  > I'm thinking of insisting projects use a standard way to run tests, > something like: we will install your project in a virtualenv, then call > "import Foo; Foo.test()". We already have IPython.test(), although test_pr > doesn't use it at present. Then all the configuration it needs for a > project should be: >  > [ipython] > github_repo = ipython/ipython > package_name = IPython >  > I see you're already doing something similar in your variant of the script, > but specifying arguments to .test(). Would it be acceptable for .test() to > be called with the default arguments if test_pr became a generic thing?   Sure that would be fine. Aric 
comment
Your code looks reasonable and seems to be decoding 'utf8' correctly to unicode.  Try printing the edges (print DG.edges()) and see if they look like what you expect.  If that looks correct and the drawing is still wrong can you post a small example that we can run? 
comment
Still not a complete example (missing files pos.txt, neg.txt). Can you just isolate it to some simple case? 
comment
That looks like a Matplotlib font configuration issue.  You might want to ask on the Matplotlib mailing list.  You'll probably need to add something like  ``` python # -*- coding: utf-8 -*- import networkx as nx import matplotlib as mpl mpl.rcParams['font.sans-serif'] = ['SimHei'] ``` 
comment
Could you also add a simple test? 
comment
We never got around to including this.  And the d3 interface has changed too. The social_graph.py part would be go to add as an example - does it still work correctly? 
comment
Thanks. Patched in http://networkx.lanl.gov/hg/networkx/rev/24dc63984b6c 
comment
Thanks for pointing this out and suggesting a fix.  I added a slightly different (maybe better performance) fix at https://networkx.lanl.gov/trac/ticket/602. Does that fix the problem correctly? 
