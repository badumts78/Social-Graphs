issue
PageRank algo convergence condition#TITLE_END#This issue was raised in #1465 six years ago but incorrectly dismissed. The tolerance should not be multiplied by the number of vertices since the sum of vertex scores is `1` irrespective of the graph's vertex count.  For large graphs of around one million vertices the current implementation will likely terminate after a single iteration.  ### Current Behavior For the example below, the convergence error for the current PageRank implementation is `err = 0.322`.  ### Expected Behavior The convergence error should be on the order of the tolerance, which by default is `1e-06`. By dividing `tol` by `N` we undo the multiplication by `N` in PageRank and generate a convergence error (L1-norm) of `err_accurate = 2.23e-07`.  Both of the `pagerank` and `pagerank_scipy` methods erroneously multiply `tol` by `N`.  ### Steps to Reproduce ``` import inspect import networkx as nx  # https://snap.stanford.edu/data/com-Youtube.html G = nx.read_edgelist('com-youtube.ungraph.txt') N = G.number_of_edges()  tol = inspect.signature(nx.pagerank).parameters['tol'].default pr_low_tol = nx.pagerank(G, alpha=0.9, tol=10**-12/N, max_iter=1000)  # will compare "err < N * tol" pr = nx.pagerank(G, alpha=0.9) err = sum(abs(pr[i]-pr_low_tol[i]) for i in pr_low_tol.keys())  # will compare "err < N * tol / N" == "err < tol" pr_accurate = nx.pagerank(G, alpha=0.9, tol=tol/N) err_accurate = sum(abs(pr_accurate[i]-pr_low_tol[i]) for i in pr_low_tol.keys())  tol, err, err_accurate ```  ### Environment Python version: 3.9.2 NetworkX version: 2.5
