issue
Refactor components API#TITLE_END#Following from the discussion on #1404, it would be good to remove the `[bi]connected_component_subgraphs` functions and make `[bi]connected_components` yield subgraphs (now they yield sets of nodes).  We decided to not make this change right now, on the one hand, in order to not overwhelm users with API changes (2.0 release will come with many changes), and on the other hand, because right now generating subgraphs (with `copy=True` which is the default) is much slower than generating sets of nodes.  We expect that the speed of generating subgraphs will improve in the short term if we extend #1164 approach to subgraph generation. Once we do that we can revisit this to see if the speed penalty for subgraph generation is a showstoper for this API change. 
issue
Add flow based node and edge disjoint paths.#TITLE_END#This is a flow based implementation of edge and node disjoint paths. We compute the maximum flow between source and target on an auxiliary directed network. This auxiliary network is the same used in the flow based edge and node connectivity algorithms. See `networkx.algorithms.connectivity.utils` for details. The saturated edges in the residual network after running the maximum flow algorithm correspond to edge or node disjoint paths between source and target in the original network.  This function handles both directed and undirected graphs, and can use all flow algorithms from NetworkX flow package.  Related to the discussion in #2053 
issue
Fix fruchterman reingold bug and add more tests to layouts.#TITLE_END#Incorrect generator expression syntax when evaluating the maximum of the input pos dictionary. This bug only triggered when a pos dict was passed as argument to fruchterman_reingold_layout. This commit adds tests for this case.  Also added more tests for layouts, which are currently quite sparse. When doing so I also found and fixed a bug in circular_layout corner case (graph with only one node) that was using the old API (expecting G.nodes() to be a list). 
issue
 Adds structural hole measures#TITLE_END#This PR supersedes #1535, #429, and #192.  This commit fixes the implementation of constraint and effective size in PR #1535. That implementation was using `nx.all_neighbors` to iterate over all predecessors and successors of nodes. The problem was that if a neighbor is both a predecessor and a successor is appears twice in `nx.all_neighbors` and thus it was double counted.  Regarding constraint, PR #1535 was following the formula of constraint posted at a very visible web page [1], which is wrong as it says that the local constraint has to be calculated only on neighbors that are both predecessors and successors of the focal node. Burt's paper [2] at page 362 states that "I measure the constraint on each manager with respect to the immediate network of discussion partners, composed of anyone the manager cited as a discussion partner and anyone who cited the manager.", which means that it should be computed for all neighbors that are either successors or predecessors.  Regarding effective size, I implemented the original formula proposed by Burt in [3], which was different that the one used in #1535. For the case of unweighted and undirected graphs I implemented the simplified formula proposed by Borgatti [4] which is faster.  After these changes the tests posted in #192 now pass. These test were checked using UCINET, I rechecked them using IGraph and also computed them by hand to make sure that they were right as I don't have access to UCINET and effective size is not implemented in IGraph. I also added more tests to cover all cases applicable (weighted, unweighted, directed, and undirected).  I changed the API of the functions to compute the measures on all nodes of the graph and accept an arbitrary edge weight name to compute the weighted versions, following the API of other centrality measures in NetworkX.  After profiling these measures, I sped-up the running time by reimplementing the inner function `mutual_weight` as a succession of `try` and `except` instead of the original generator expression, which is ~40% faster. This inner function is called millions of times for not so large graphs (thousands of nodes) and thus the speed up is significant. On the other hand, I also removed the decorator `not_implemented_for` of this function, which according to profiling accounted for 50% of the total running time of the measures.  [1] http://www.centiserver.org/?q1=centrality&q2=Burts_constraint [2] Burt, R. (2004). "Structural holes and good ideas". American Journal of Sociology (110): 349â€“399. [3] Burt, Ronald S. (1995). Structural Holes: The Social Structure of Competition. Cambridge: Harvard University Press. [4] Borgatti, Stephen (1997). "Structural Holes: Unpacking Burt's Redundancy Measures". Connections. INSNA.
issue
Raise an Exception for disconnected Graphs in bipartite.sets#TITLE_END#This pull request fixes #2127  When a disconnected graph is passed to `bipartite.sets` more than one coloring solution is possible. This led to surprising results and hard to diagnose bugs (see #2127) in algorithms that used `bipartite.sets`, such as bipartite matching and friends. These algorithms now have a new optional parameter that allows the user to specify the nodes in one of the two bipartite sets, and thus allows to use disconnected graphs in these algorithms without ambiguity.  I've added a new exception `AmbiguousSolution` in order to be more explicit when we raise it. It might have more use cases beyond `bipartite.sets`.  The changes in this pull request are backwards compatible for connected graphs as the new parameter is optional. For disconnected graphs is not backward compatible, but since the assignation of nodes to a bipartite set depends on iteration order, that code was already subtly broken as it would yield different results on different python versions.
issue
Remove obsolete testing tools.#TITLE_END#This functionality is covered by Travis CI. Fixes #2283 
issue
Add Gomory-Hu tree representation of undirected graphs.#TITLE_END#Thos PR addresses #2421.  A Gomory-Hu tree of an undirected graph with capacities is a weighted tree that represents the minimum s-t cuts for all s-t pairs in the graph.  It only requires `n-1` minimum cut computations instead of the obvious `n(n-1)/2`. The tree represents all s-t cuts as the minimum cut value among any pair of nodes is the minimum edge weight in the shortest path between the two nodes in the Gomory-Hu tree.  The Gomory-Hu tree also has the property that removing the edge with the minimum weight in the shortest path between any two nodes leaves two connected components that form a partition of the nodes in G that defines the minimum s-t cut.
issue
Add Dinitz' algorithm for maximum flow problems.#TITLE_END#This implementation is based on Cherkassky's approach (see section 4 of this very nice paper [1]). That is, do not explicitly construct the layered network, just compute the layer number or rank of each node in the BFS tree up to distance = dist(s, t). Then in the DFS, just ignore both edges between nodes of incompatible ranks and saturated edges.  I've implemented this algorithm with hopes of speed up flow based connectivity algorithms. Turns out that Edmonds Karp algorithm clearly outperforms this one. Although this one outperforms clearly Preflow-push and Short-augmenting-paths for very sparse networks with skewed degree distributions. The paper suggest other improvements not implemented here.  [1] Dinitz' Algorithm: The Original Version and Even's Version.     http://www.cs.bgu.ac.il/~dinitz/Papers/Dinitz_alg.pdf 
issue
Workaround for gdal python3.6 at travis and more doctests fixes#TITLE_END#This PR fixes #2415 (well not really, but at least it doesn't kill the build for python 3.6 with optional dependencies).  This workaround still does not install gdal for python 3.6 but it doesn't kill the build, and gdal installation works for all other python versions. It seems that the problem is that conda for python 3.6 installs the latest version of gdal (2.1.3) which is not compatible with the package libgdal-dev provided by Travis version of Ubuntu.  Also more doctests fixes for python 3.6 at subgraph centrality. 
issue
Improve bipartite documentation.#TITLE_END#This PR addresses #2400  Improve bipartite documentation by clarifying that the use of a node attribute named `bipartite` with values 0 and 1 in order to distinguish bipartite node sets is only a convention which is not enforced in the source code. Also state explicitly that bipartite functions do not check that the input graph is actually bipartite nor than the container with a bipartite node set contains all nodes from that bipartite node set.  Added links form all relevant function docstring to the module level documentation where all this is explained.  Also added links to some custom exceptions to the generated documentation that were missing.
issue
Fix all tests for 3.6#TITLE_END#This PR supersedes #2319   Fixed all remaining doctests ordering issues for python 3.6 without using the `+SKIP` directive. I think that most docstrings examples are as clear as before, maybe with the exception of `networkx.readwrite.adjlist`. 
issue
Check alternating paths using iterative DFS in to_vertex_cover.#TITLE_END#This PR fixes #2384.  Reimplement iteratively the inner function `_alternative_dfs` in `_is_connected_by_alternating_path`. This function checks if nodes in the graph are linked to target nodes by alternating paths, that is, paths that alternate between matched and unmatched edges.  This PR also delegates to `_connected_by_alternating_paths` computing the matched and unmatched edges because these are the same for a given maximum matching and we do not need to recompute them each time that we check if a node is linked to targets by an alternating path.  I also changed the implementation of the computation of matched and unmatched edges in order to avoid assuming orderable nodes.  Added a test with the example given in #2384 and another test with unorderable nodes.
issue
Refactor Dinitz' algorithm implementation.#TITLE_END#Reimplemented the DFS iteratively instead of recursively. This speeds up the running time considerably, specially for problems in which the paths between source and sink are long, such as big and sparse networks (eg social networks). For dense networks, in which the paths are typically short, the running time is very simmilar to the recursive implementation.  The speed up in the running time makes now possible to use Dinitz' algorithm for testing the large graphs in `test_maxflow_large_graph.py`. Thus we are now using all flow algorithms in all flow tests. 
issue
Add Boykov Kolmogorov algorithm for maximum flow problems.#TITLE_END#This algorithm has worse case complexity `O(n^2m|C|)` where |C| is the cost of the minimum cut [1], but it is faster than other algorithms with theoretically better worse case complexities for some problems, such as vision and image processing, and maybe others.  For the case of connectivity problems in networks with higly skewed degree distributions (eg social networks) this algorithm doesn't run faster than Edmonds-Karp but after reading about it I thought that it was really cool and decided to implement it.  This is still WIP as the marking heuristic described in [2] is not yet implemented. According to the authors it speeds up the running time considerably.  [1] Boykov, Y., & Kolmogorov, V. (2004). An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision. http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf [2] Vladimir Kolmogorov. Graph-based Algorithms for Multi-camera Reconstruction Problem. PhD thesis, Cornell University, CS Department, 2003. pp. 109-114. https://pub.ist.ac.at/~vnk/papers/thesis.pdf 
issue
Improve Notes section on simplex and friends docs.#TITLE_END#Add a Notes section to functions that use network_simplex warning users of the potential problems of using float edge weights. Also mention the workaround of multiplying weights by a convenient constant factor.  Fixes #2076 
issue
Fix minimum_st_edge_cut documentation.#TITLE_END#A left over in the docstring from the last refactor made somewhat not clear that the functions in cuts.py do not consider weights and compute cuts in terms only of number of nodes or edges. Also added a reference to `minimum_cut` for the users interested in weighted edge cuts.  This PR fixes #1810  
issue
Fix all_node_cuts corner cases: cycle and complete graphs.#TITLE_END#This PR fixes #1875 . For cycle graphs and complete graphs all_node_cuts was giving incorrect or incomplete results. The most tricky case is cycle graphs because handling them as a part of the implementation of Kanevsky's algorithm requires checks inside tight loops that would penalize the common case.  Instead in this PR cycle and complete graphs are handled separately. It is cheap to test if the input graph is any of both, and the solution of finding all node cuts is a lot easier and faster than in the general case.  Also added tests for both cycle and complete graphs. 
issue
Add shortest_simple_paths function.#TITLE_END#This function is based on Yen's algorithm for finding k shortest paths. It generates all simple paths between source and target starting from the shortest one. See #762 and #793 for prior work on this and discussion.  This implementation is based on Andrey's work on #762. It generalizes his approach there to the weighted case, as in Greg's implementation posted on #793.  Regarding the discussion on #762 about how to do the filtering of nodes and edges efficiently, this PR adds private modifications of the functions `bidirectional_shortest_path` and `bidirectional_dijkstra` (that accept containers with nodes and edges to exclude during the SP search) for its exclusive use.  This still needs work, especially on documentation and tests. I'm pushing it so we can have the discussion with the code available.  One thing to discuss is whether or not to include an one liner function to get the k shortest paths. Could be something like this:  ``` python from itertools import islice def k_shortest_paths(G, source, target, k, weight=None):         return list(islice(nx.shortest_simple_paths(G, source, target, weight=weight), k)) ```  I think that, given that `all_simple_paths` is way faster in computing all paths, the principal use of `shortest_simple_paths` will be to get the k best/shortest paths. So it will be a more user friendly interface if we add it. However, we could also add this function as example in the `shortest_simple_paths` docstring. I'm ok either way. Thoughts? 
issue
Remove a test that only fails sporadically on Travis for k_components approximation.#TITLE_END#See #1637 for debugging efforts. So far we cannot reproduce it locally. 
issue
Release 1.10#TITLE_END#Cherry pick the two commits that remove tests based on random graphs for k-components approximation to the v1.10 branch. Those tests were failing sporadically on Travis and we were not able to reproduce them outside Travis. Since this algorithm is an approximation, and the removed tests were designed for the exact implementation at `connectivity.kcomponents.py`, it is better to just remove them for the approximation version of the algorithm.  Is there any other change that we need to add to the v1.10 branch before releasing it? 
issue
Remove some tests for k_components approximation.#TITLE_END#This algorithm is based on a node connectivity approximation that only guarantees a lower bound for node connectivity. Even though some tests only fail sporadically in Travis, and we cannot reproduce them, it's better to just remove the problematic tests based on random graphs. Those tests were designed for the exact implementation of the algorithm in `connectivity/kcomponents.py`. 
issue
Refactor connectivity package#TITLE_END#Use the new algorithms and the new interfaces provided by the flow package. I think that the code is quite clearer by using the new interface, and the new algorithms and interface provide a significant speed up in running time. For some problems, this code is one order of magnitude faster (see #1102 for bechmarks). So far the code is still backwards compatible, but we should discuss some possible non backward compatible improvements. I have some doubts about the implementation and documentation that I'd like to discuss. 1. In this first version I've added the `flow_func` parameter to all connectivity functions. It is great for tests to be able to use several flow algorithms. Also, the two algorithms that make sense in this scenario (`edmonds_karp` and `shortest_augmenting_path`) perform better in different scenarios: the former is faster in very sparse power-law like degree distribution networks, and the latter for denser networks with the edges more evenly distributed among nodes. Thus it makes sense to allow users to pick an algorithm. The implementation takes care of using the optimal parameters (`cutoff` for both and `two_phase` for SAP) for this two algorithms. So far I've set the default flow function to `edmonds_karp` because is faster in a wide set of contexts. I'll prepare more detailed benchmarks between these two algorithms. 2. Much of the increment of speed comes from reusing the data structures that we use for the underlying maximum flow computations (residual network) and local node|edge connectivity (auxiliary digraph). I think we should explain this to users and show them how they can also do that in their code. I've tried to do that in the docstings of `local_node_connectivity` and `local_edge_connectivity`. Do you think that we should document how to reuse the data structures? 3. The connectivity algorithms rely on two data structures (`residual` and `auxiliary`) that are conceptually different, but could be merged in an unique (more complex) data structure. This does not simplify the code a lot (we get rid of a parameter in function calls and a line of code each time we initialize the data structures) and I think it makes more difficult to understand what the code is actually doing. So I'd prefer to keep them separated but I'm open to merging them. 4. This changes are, so far are backwards compatible, but I think that we can improve the interface: an user that is only interested in computing node|edge cuts|connectivity will have enough using `node_connectivity`, `edge_connectivity`, `minimum_node_cut` and `minimum_edge_cut`. These functions support computing that for the whole graph and also for two nodes. So I think that these are the functions that we should import to the base NetworkX namespace. The other functions (`local_*` and `minumum_st_*_cuts`) are more specialized and should be used by users interested in building connectivity algorithms themselves. These functions accept all parameters of the the flow interface, and some more on their own, in order to reuse of data structures and achieve a significant speed up. So I think that we could keep these functions only to the connectivity package and require users that want to use them to import them explicitly from the connectivity package. 5. I was tempted to remove some connectivity functions, such as `all_pairs_node_connectivity_matrix` because it seems easy to write the few lines of code required for computing it. However I'm a bit hesitant because if you implement them without reusing the data structures, they will be a lot slower than the version that we provide here. This could be solved maybe with better documentation. 
issue
Refactor connected components#TITLE_END#Use a plain BFS to compute connected components instead of using the single source shortest path function. The latter also does a BFS but keeps track of the distances. This is only slightly faster (around 5% in my tests), but I think it's conceptually better. As @ysitu said, we have a tendency to abuse shortest paths functions for reachability.  This came out while working on #1414 and trying to speed up the set consolidation function, that uses connected_components.  Also, following @jfinkels example (great work!), I've cleaned up the `biconnected.py` docstrings and code, and improved the examples showing how to use `max` to get the largest [bi]connected component.  Another thing that I'm not sure if we should change now is that there is an inconsistency between `connected_components` and `biconnected_components`, the former yield lists of nodes and the latter sets of nodes. I prefer yielding sets, and I think that if we change it we'll not break user code, but I thought it was better to discuss it before making the change. Thoughts? 
issue
Fix all_pairs_node_connectivity results dict. Fixes #1406.#TITLE_END#See explanation at #1406 . Also added tests. 
issue
Remove flow_funcs from the base namespace#TITLE_END#This PR is for finishing up the details of the refactoring of the flow and connectivity packages. Following the discussion at #1162 , I've removed the functions that implement flow algorithms from the base namespace and also renamed files in the flow package so modules and functions have different names; we obey the informal module-name-must-be-unique rule. Updated tests and docstrings. The later need a bit more work to clearly explain that flow functions have to be explicitly imported from the flow package.  Also adapted the connectivity package to the changes in the flow package. 
issue
all_node_cuts wrap up#TITLE_END#I merged #1391 in a bit of a rush to be able to push #1414. This PR is for finishing up the last details. So far it makes `all_node_cuts` use the recently exposed `nx.antichains` function from #1413, and adds `all_node_cuts` to `api_2.0.rst`.  I also want to investigate a bit more a failed test for non repeated cuts in the pypy Travis CI (which is allowed to fail):  https://travis-ci.org/networkx/networkx/jobs/55252872#L2829  I think that it does not have to do with hash randomization, as most of the other failures there. However I cannot reproduce it in my machine (pypy 2.2.1 linux 64bit). So I added a bit of debug to that test to try to see what is going on. Also we saw a strange frailure only one time in a `pypy3` environment in another test: `test_kcutsets.test_configuration_model`. I cannot find the link of the failure right now. So far, it did not happen again. 
issue
NetworkX summit and/or sprints at Scipy 2015?#TITLE_END#It would be great if we could repeat the NetworkX summit and sprints at Scipy that we did in 2011. I also attended 2012 when we did not do a summit but had very productive sprints. Among other things, we performed a not very graceful, but successful, migration from trac to Github.  With all the changes to the new API for 2.0 and related issues, I think that high bandwidth meeting would help a lot with the discussion and implementation. If we manage to gather enough of us in the same room for a couple of days we can do a lot of work and have a lot of fun. I'd love to catch up with @hagberg @dschult @chebee7i @bjedwards @loicseguin @drewconway and to meet in person @ysitu @Midnighter and many others.  The conference is in early July but I mention this now because I just realized that the deadline to apply for financial aid finishes tomorrow! You can apply here (I just did):  http://scipy2015.scipy.org/ehome/115969/259279/?&  What do you guys think? Would you be able to make it to Austin early July? 
issue
meeting at scipy conference this week#TITLE_END#This week is the Scipy conference at Austin. Maybe we can plan for an informal meeting of developers and users at some point. Perhaps people that was not able to attend the conference can join remotely. Who is at the conference? I just landed there yesterday late night. We can use this issue to coordinate things, even if it's only to grab some drinks and chat. 
issue
Documentation not generated for some modules#TITLE_END#It seems that in the online documentation there are some modules for which documentation is not generated. See for instance:  http://networkx.github.io/documentation/development/reference/algorithms.connectivity.html http://networkx.github.io/documentation/development/reference/algorithms.approximation.html  I added these modules recently, so far I didn't find the same problem in any other module. I cannot reproduce it locally, in my machine the html builds fine. Any idea on where the problem might be?  This is relevant for #1590  
issue
Add k-(node)-components algorithm.#TITLE_END#Add Moody and White algorithm for for identifying k-components in a graph, which is based on Kanevsky's algorithm for finding all minimum-size node cut-sets (implemented in `all_node_cuts` function #1391). This algorithm consists in keep cutting a graph until we obtain either a complete or a trivial graph.  `all_node_cuts` is doing most of the work here, the only tricky implementation thing is the generation of partitions. The authors say that ``Nodes in a cutset belong to both sides of the induced cut.'', but an induced cut can split a graph in more than two sides. Thus, after a cut, we have to merge the node sets that (after adding the nodes in the cut set) share at least k nodes.  I found out that this is very related to set consolidation. I've adapted a nice function from  http://rosettacode.org/wiki/Set_consolidation . However it's not clear the license of this implementation. I'll try to figure out. 
issue
Add fast approximation for k-(node)-components.#TITLE_END#This is an approximation approach to compute the k-component structure of a graph (#1414 is the exact, and slow, version of this). I wrote a paper to explore this approach in detail, see http://arxiv.org/abs/1503.04476  The logic of the approximation algorithm for computing the `k`-component structure is based on repeatedly applying simple and fast algorithms for `k`-cores and biconnected components in order to narrow down the number of pairs of nodes over which we have to compute White and Newman's approximation algorithm for finding node independent paths (implemented at #1405). More formally, this algorithm is based on Whitney's theorem, which states an inclusion relation among node connectivity, edge connectivity, and minimum degree for any graph G. This theorem implies that every `k`-component is nested inside a `k`-edge-component, which in turn, is contained in a `k`-core. Thus, this algorithm computes node independent paths among pairs of nodes in each biconnected part of each `k`-core, and repeats this procedure for each `k` from 3 to the maximal core number of a node in the input graph.  Because, in practice, many nodes of the core of level `k` inside a bicomponent actually are part of a component of level k, the auxiliary graph needed for the algorithm it's likely to be very dense. Thus, we use a complement graph data structure to save memory (`_AntiGraph` see #599 for an earlier dicussion about this). AntiGraph only stores information of the edges that are _not_ present in the actual auxiliary graph. However, when applying algorithms to this complement graph data structure, it behaves as if it were the dense version. So it can be used directly in several NetworkX algorithms, this version is only tested for the algorithms needed here: `connected_components`, `k-cores`, and `biconnected_components`. I used a `ThinGraph` for building the `AntiGraph` and the memory consumption is quite small compared with a regular `Graph`.  I think `AntiGraph` is not useful enough to be a top level class in NetworkX, but it could be a good example of a `Graph` subclass. I'll add to the examples folder.  This PR supersedes and closes #580. 
issue
add functions that return the largest (bi)connected component#TITLE_END#Since `[bi]connected_components` and friends do not yield components sorted by their order, we could add functions to get the largest connected component. It's a quite common operation to build a graph somehow and only use the giant connected component for some analysis. Now it requires:  ``` python G = build_graph_somehow() Gc = max(list(nx.connected_component_subgraphs(G)), key=len) # It could be something like Gc = nx.largest_connected_component(G) ```  A dedicate function, with another for `biconnected_components`, will enhance readability and reduce the verbosity of the code for a common operation.   However, it might be confusing that a function named `largest_connected_component` returns a graph, given that `connected_components` yields sets of nodes, if you want graphs, you have to use `connected_component_subgraphs`. I'd rather not have `largest_connected_component` and `largest_connected_component_sugraph`, but I think that the inconsistency with the rest of the components API would be worse than having a couple more functions. Thoughts?  Also, I think that this is not a complicated addition (once we decide if we add them and how), so it would be a good oportunity to contribute to NetworkX for the first time. 
issue
remove legacy ford_fulkerson maximum flow in 2.0#TITLE_END#With the new algorithms and interfaces of the flow package, the legacy `ford_fulkerson` implementation should be removed in 2.0 (or 1.9 + 1) because it implements the same algorithm than `edmonds_karp`, does not follow the new conventions regarding residual networks and flow dictionaries, and it is way slower than new implementations (especially when computing maximum flow among several pairs of nodes on the same graph). 
issue
Add all minimum-size k node cutsets algorithm.#TITLE_END#This PR implements Kanevsky's algorithm for all minimum-size separating node sets in an undirected graph as a generator of node sets.  The most tricky thing about the implementation is that for reusing the residual network and the auxiliary graph for node connectivity (which is key for speed) we have to keep removing and adding edges to them instead of regenerating both each time we have to modify the input graph (the algorithm prescrives to add an edge each time we find a node cutset to avoid finding it again).  This PR uses almost as is a function from SAGE (see `compute_antichains`), maybe we should ask permission to it's author to include it given that SAGE is GPL and we are BSD? 
issue
Move bipartite generators to the bipartite package#TITLE_END#Following the discussion on #1379 , moving the bipartite generators to the bipartite package wil allow for shorter function names (eg `bipartite_havel_hakimi_graph` to `bipartite.havel_hakimi_graph`). We can also remove most of the generators from the main namespace (maybe all but `complete_bipartite_graph`) and ask the user to explicitly import the bipartite package to use them:  ``` from networkx.algorithms import bipartite as bp G = bp.configuration_model(aseq, bseq) ``` 
issue
Refactor complete_bipartite_graph. Fixes #1375.#TITLE_END#This commit reimplements `complete_bipartite_graph` using a generator expression instead of two nested for loops. Also adds the `bipartite` node attribute (that was missing, as per #1375).  I've also moved the `complete_bipartite_graph` function from `classic.py` to `bipartite.py`. I think it's better to have all bipartite generators together.  The function `complete_bipartite_graph` is used by other functions, such as `wheel_graph`, which was abusing the interface by passing -1 as the second argument of `complete_bipartite_graph` (for `wheel_graph(0)`) and hoping to get an empty graph with zero nodes. This is also fixed. 
issue
Expose `transitive_clousure` and `antichains` in the public API#TITLE_END#Working on #1391 we found that it would be useful to expose `transitive_clousure` and `antichains` (now two helpers for the `all_node_cuts` function) in the public API. We can move them to `algorithms/dag.py` and add proper documentation and tests.  The function `antichains` was contributed by Peter Jipsen and Franco Saliola, from the SAGE project, they also send me a reference for their implementation: Free Lattices, by R. Freese, J. Jezek and J. B. Nation, AMS, Vol 42, 1995, p. 226. Any suggestion for references for the `triadic_clousure` function? The implementation used in #1391 is based on David Eppstein's [PADS/partial_order.py](http://www.ics.uci.edu/~eppstein/PADS/PartialOrder.py) but there is no reference there.   We still do not have labels for difficulty, but I think this one is easy. If someone wants to take this I can review the changes. 
issue
bug in all_pairs_node_connectivity#TITLE_END#For building the dictionary to store the results I was using:  ``` python all_pairs = dict.fromkeys(nbunch, dict()) ```  Which is using refrences to the same dict for each node. The tests did not catch this (ouch!), I found out while working on #1405. I'll send a PR fixing it, by using:   ``` python all_pairs = {n: {} for n in nbunch} ```  I'll also add tests. 
issue
Add fast approximation for node connectivity.#TITLE_END#This PR adds White and Newman's fast approximation algorithm for finding node independent paths between two nodes. It finds them by computing their shortest path using BFS, marking the nodes of the path found as used and then searching other shortest paths excluding the nodes marked as used until no more paths exist. It is not exact because a shortest path could use nodes that, if the path were longer, may belong to two different node independent paths. Thus it only guarantees an strict lower bound on node connectivity.  This implementation uses a modified version of `bidirectional_shortest_path` that accepts the extra parameter `exclude`, which is a container for nodes already used in other paths that should be ignored. 
issue
Add mapping dict as graph attribute in condensation.#TITLE_END#I've found myself several times needing a mapping of original nodes to condensation nodes (ie strongly connected components). It is easy to compute that myself (especially since `condensation` function accepts precomputed SCC as a parameter). But then the function itself recalculates the mapping for building the condensation graph. Adding the mapping as graph attribute is simple, backwards compatible, and avoids redundant computations.  I've also updated `condensation` docstrings and added a test. 
issue
algerbraic_connecitivity tests are not skipped for scipy < 0.12#TITLE_END#It seems that the low level functions in `scipy.linalg.blas` used in `algebraic_connectivity` were added at scipy 0.12:  http://docs.scipy.org/doc/scipy/reference/linalg.blas.html  At work I'm using Debian stable which ships with `scipy` 0.10, the tests for `algebraic_connectivity` throw errors in this context (eg `AttributeError: 'module' object has no attribute 'algebraic_connectivity'`). I guess that scipy is detected, and thus test are not skipped, but functions are not imported in the namespace because `from scipy.linalg.blas import (dasum, daxpy, ddot)` fails. 
issue
Adapt ipython tools for testing pull requests#TITLE_END#@ipython has a very nice set of tools for their github based development workflow. See:  https://github.com/ipython/ipython/tree/master/tools  As @fperez suggested at SciPy 2012, it is easy and useful to adapt some of them for NetworkX development. This pull request adapts the test_pr.py script and friends for NetworkX. It automatically tests pull requests (using virtualenv and downloading the code directly from github) and optionally posts the results as a comment in the pull request page (including a link to a gist with the log for failed tests). It is a very nice and useful tool. For the origunal ipython pr_test.py see:  https://github.com/ipython/ipython/blob/master/tools/test_pr.py  I did some changes on how the tests are run inside each virualenv. IPython uses a script for running tests which is installed at the 'bin' directory of each virtialenv. But I think it is easier for NetworkX tests to simply use check_output:  ``` python py = os.path.join(basedir, venv, 'bin', 'python') cmd = [py, '-c', 'import networkx as nx; nx.test(verbosity=2,doctest=True)'] log = check_output(cmd, stderr=STDOUT).decode('utf-8') ```  I also added PyPy support (last commit). But I'm not sure if it will work in something different than a Debian based system. PyPy and virtualenv play nice together and no significative change in the code is required, but in Debian/Ubuntu systems the option `--system-site-packages` of virtialenv does not add the actual dist-packages path for pypy (`/usr/local/lib/pypy2.7/dist-packages`). As a quick and dirty solution,  I've opted for adding the path explicitly in `run_tests()` to be able to import nose, but it is not a solid solution. So more work is needed. I'll be quite busy the next days and I'm not sure if I will be able to work on this, so I'm posting it hoping than someone can take a look, test it in other systems and also work on it.  This script assumes that there is a directory named `.nx_pr_tests` in your home directory and that the nose package and other libraries are installed system wide in each python environment. It does not import anything from the user defined `$PYTHONPATH`. 
issue
Refactor flow package.#TITLE_END#This refactoring follows from the discussion at #1096 and #1095. It is fully backwards compatible. The main changes are: 1. Rename max_flow and min_cut to maximum_flow and minimum_cut to be consistent with NetworkX naming convention. The short names can still be used (eg nx.max_flow(...)) but are not in the documentation. 2. maximum_flow and minimum_cut accept a new parameter flow_func for the function that will perform the computation. A default_flow_func is set, which is preflow_push_value, based on the benckmarks by @ysitu at #1099. 3. The legacy implementation of ford_fulkerson and firends is separated in its own file and refactored for it to follow the new conventions in the function names, the implementation is the same. 4. Added a new function that return the residual network for each algorithm implementation. Also added docstrings explaining the conventions used in the residual network. 5. Refactored tests. Now it is easy to add a new maximum flow algorithm for testing. They take maybe a bit too long, but now we test all flow related functions. 6. The functions <algo_name>_{value|flow|residual} are not imported to NetworkX base namespace, it is necessary now to explicitly import them from networkx.algorithms.flow, the exception is ford_fulkerson_flow for backwards compatibility. 7. The maximum flow related functions imported in networkx namespace are: maximum_flow, minimum_cut, ford_fulkerson, preflow_push, shortest_augmenting_path. For convenience max_flow and min_cut are also imported. And for backwards compatibility: ford_fulkerson_flow and ford_fulkerson_flow_and_auxiliary. 
issue
Add approximation for average clustering coefficient#TITLE_END#This pull request follows from the discussion and code in #868 . I've changed the name of the function to match the conventions used in `cluster.py`, did cosmetic changes to the code, and added docstrings and tests. @fredzilla, could you take a look at this?  I'm still not sure if it would be better to add this function to `cluster.py` instead of adding it to the `approximation` module. Maybe it would be better to have all clustering related functions together. I have implemented approximations related to node connectivity and I'm also wondering if they should live in the approximation module or with the other connectivity stuff.    
issue
Fix bug in checking minimum version for requests library in test_pr.py#TITLE_END#The script `tools/test_pr.py` depends on the requests library >= 0.10.0. When checking for it, only minor version is considered, thus the script fails with requests >= 1.0.0. 
issue
Travis failures in python 3.2 related to scipy#TITLE_END#I've found this while working on #1126 (I'm not using scipy or numpy there), the tests for python 3.2 fail for tests related to scipy with `RuntimeError: dictionary changed size during iteration`:  https://travis-ci.org/networkx/networkx/jobs/24348012#L2166  I see that we also found this problem in #1134 , however the changes there do use scipy. I'm not sure why I'm seeing this while testing code unrelated to scipy. Another reason for doing #1129 I guess. 
issue
Travis CI failing when trying to install coverage#TITLE_END#I'm seeing Travis CI errors caused by failing to install coverage. It is failing on different version of python each time. Maybe is a problem on their side?  https://travis-ci.org/networkx/networkx/jobs/24113381 
issue
Add a parameter in connectivity functions to select a max_flow function#TITLE_END#Modify all node and edge connectivity functions to accept an additional parameter for defining the implementation in the computation of local maximum flow. With the recent addition of the `preflow_push`  algorithm  by @ysitu, we now have two maximum flow implementations.  I was going to set the new `preflow_push` as the default function, but it seems that in connectivity algorithms it is actually slower than @loicseguin implementation of `ford_fulkerson` algorithm. I've put together a small benchmark for maximum flow implementations for connectivity at:  https://gist.github.com/jtorrents/9992146  You need the changes in this PR to run the benchmark. In my machine (linux 64-bit) the results using python 2.7 are:  ``` jtorrents@saturn:~/projects/benchmark$ python2.7 benchmark_flow.py Name: fast_gnp_random_graph(600,0.02) Type: Graph Number of nodes: 600 Number of edges: 3633 Average degree:  12.1100 Mean and std of computing the maximum flow among 100 pairs of nodes   Kind of capacity: unit     preflow_push:   0.022 (0.0033) seconds     ford_fulkerson: 0.012 (0.0021) seconds   Kind of capacity: random ints     preflow_push:   0.024 (0.0039) seconds     ford_fulkerson: 0.013 (0.0024) seconds Time computing node connectivity     preflow_push:   33.42 seconds     ford_fulkerson: 9.68 seconds Time computing edge connectivity     preflow_push:   3.54 seconds     ford_fulkerson: 1.69 seconds  Name: fast_gnp_random_graph(200,0.2) Type: Graph Number of nodes: 200 Number of edges: 3962 Average degree:  39.6200 Mean and std of computing the maximum flow among 100 pairs of nodes   Kind of capacity: unit     preflow_push:   0.020 (0.0031) seconds     ford_fulkerson: 0.012 (0.0021) seconds   Kind of capacity: random ints     preflow_push:   0.021 (0.0033) seconds     ford_fulkerson: 0.013 (0.0022) seconds Time computing node connectivity     preflow_push:   20.70 seconds     ford_fulkerson: 7.68 seconds Time computing edge connectivity     preflow_push:   0.53 seconds     ford_fulkerson: 0.29 seconds  Name: Power law configuration model Type: Graph Number of nodes: 970 Number of edges: 2413 Average degree:   4.9753 Mean and std of computing the maximum flow among 100 pairs of nodes   Kind of capacity: unit     preflow_push:   0.021 (0.0054) seconds     ford_fulkerson: 0.008 (0.0024) seconds   Kind of capacity: random ints     preflow_push:   0.021 (0.0073) seconds     ford_fulkerson: 0.008 (0.0019) seconds Time computing node connectivity     preflow_push:   49.52 seconds     ford_fulkerson: 12.74 seconds Time computing edge connectivity     preflow_push:   13.25 seconds     ford_fulkerson: 5.05 seconds ```  As you can see, `preflow_push` is quite slower than `ford_fulkerson` for the kind of computations needed for connectivity algorithms (ie multiple max flow computations on arbitrary pairs of non-connected nodes). However, @ysitu in #1082 showed in a different kind of benchmark that `preflow_push` outperformed `ford_fulkerson`. Not sure why is that. Any ideas?   Note that the difference in time is bigger in `node_connectivity` computations. Maybe the structure of the auxiliary digraph used in connectivity algorithms has something to do with `ford_fulkerson` outperforming `preflow_push`?  I did not modify analogously the cut functions because the residual graph returned by `preflow_push` needs preprocessing (remove all edges that exhausted their capacity), thus increasing even more the time required for cuts using the `preflow_push` algorithm. However if someone thinks it would be useful, I can do the necessary changes. 
issue
Use networkx.testing functions for network.readwrite tests.#TITLE_END#Apply the same fix than #1085 to `networkx.readwrite`, for dealing with tests errors reported in #975. This only fixes the reported failures but we might need to search for more instances where we do not use `networkx.testing` for comparing nodes, edges and graphs. 
issue
Use networkx.testing functions for testing bipartite projections#TITLE_END#I think that this fixes the problems with bipartite projection tests in python 3.4 reported in #975. But I cannot test it at the moment. At my system (linux 64bit) it works with 2.7 and 3.3 (using `PYTHONHASHSEED=4` as @Arfrever reported).    In this tests all nodes and edges are sortable, and by using `assert_nodes_equal` and  `assert_edges_equal` from `networkx.testing` I think that we correctly deal with the hash randomization issue. 
issue
Dominating set documentation and more tests#TITLE_END#I've added docstrings and a couple more tests to the functions `dominating_set` and `is_dominating_set`. These functions were added as part of the connectivity module (see #742) but after #916 now live in a separate module. 
issue
Add new module for flow based connectivity and cut algorithms#TITLE_END#With the addition of the function "ford_fulkerson_flow_and_auxiliary" it is now possible to easily access the residual/auxiliary graph resulting of the computation of a maximum flow.  This new module implements flow based connectivity and cut algorithms for both directed and undirected graphs. This pull requests contains also tests and documentation.  I also have a few more nice flow based cut algorithms in the pipeline. They depend on the functions in this new module and are quite more complex, so to keep it simple I would first add the basic functions contained in this pull request. I'll add the more complex ones when/if this is merged. 
issue
Add Robins and Alexander bipartite clustering#TITLE_END#This pull request adds Robins and Alexander (2004) bipartite clustering coefficient computation. It is defined as four times the number of four cycles divided by the number of three paths. This is a commonly used measure in social network analysis literature. I've added tests and also tested the results against the fantastic statnet R suite.   I renamed the current `clustering` function to `latapy_clustering`  but added `clustering = latapy_clustering` for backwards compatibility. There are other interesting bipartite clustering measures, I plan to implement Opsahl (2012) in the future.   Robins, G. and M. Alexander (2004). Small worlds among interlocking directors: Network structure and distance in bipartite graphs. Computational & Mathematical Organization Theory 10(1), 69-94.  Opsahl, T (2012). Triadic closure in two-mode networks: Redefining the global and local clustering coefficients. Social Networks 34 http://arxiv.org/abs/1006.0887 
issue
More flexible generic weighted projection. Addresses #732#TITLE_END#As pointed out in #732, the generic weighted bipartite projection function would be a lot more flexible if its  `weight_function`  could accept two nodes instead/in addition of their two neighbourhoods. As a starting point, this pull requests implements the "instead" approach and thus is backwards incompatible. I think that this way the code is clearer and it is easier to write custom `weight_function`, but I'm not sure of the impact of the change in users code.  Also, updated docs with examples and a new test. 
issue
fix load_betweeness bug#TITLE_END#When computing `load_betweenness` for one node in a digraph that is not strongly connected, NetworkX throws an error if the node of interest is not in the dictionary returned by the `_node_betweenness` helper function. Only nodes reachable from source are in that dictionary.  This pull request fixes the bug and adds a test that computes load betweenness for each node of a non strongly connected digraph. I think that this bug is the same reported in #755. 
issue
python3 syntax error in _decorator.py#TITLE_END#While working on the script for testing pull requests I noticed a python 3 syntax error in line 153 `networkx/external/decorator/_decorator.py`. It is strange because the error is not raised during the tests (thus they pass) but during the setup of the virtualenv.  To reproduce the error run the test_pr.py script from #752 with python3.2 installed. Notice that despite the error tests are actually run with python3.2  ```   File "/home/jtorrents/.nx_pr_tests/venv-python3.2/lib/python3.2/site-packages/networkx/external/decorator/_decorator.py", line 153     exec code in evaldict             ^ SyntaxError: invalid syntax ``` 
issue
Fix name of ford_fulkerson_flow_and_auxiliary in the docs#TITLE_END#I forgot to change the name in the last commit of #11 
issue
Allow access to the residual network in ford fulkerson algorithm#TITLE_END#The first commit of this pull request, following the discussion on ticket 710 [1], reorganizes flow/maxflow.py in order to facilitate access to the residual network. But keeps the new function with the actual algorithm as a "private" function. The second commit makes that function "public" and imports it to the global nx namespace.  I think that, given the multiple uses of the residual network (for instance, to compute node and edge cuts), we should give the user easy access to it.  [1]  https://networkx.lanl.gov/trac/ticket/710 
comment
I took a quick look and have some comments. First, about some general conventions. There is no need for an `exact` package, all algorithms in NetworkX are, except for the ones in the `approximation` package. Each module (ie `.py` file) should have the corresponding test file with a set of tests using the `nose` package. See the tests for other modules in NetworkX for examples. The tests should test only, on the one hand, for the correctness of the algorithm you implemented, and on the other hand, the interface of the function (eg what happens if the input is not correct, are the exceptions raised when expected, etc ...).  Regarding the implementation of the algorithm, I did not check the paper that you reference but I think that the code is not doing what you intended. Adding some tests will help looking into this. A problem that I see is that you are generating nested lists using the `*` operator and then updating what you think are different inner lists. The thing is that they are the same! See for example:  ``` python In [29]: ADJ = [[]]*4  In [30]: ADJ Out[30]: [[], [], [], []]  In [31]: ADJ[0].append('OMG!')   In [32]: ADJ Out[32]: [['OMG!'], ['OMG!'], ['OMG!'], ['OMG!']] ```  Take a look at the `clique.py` module to see how to use NetworkX interface and also the functionality that provides. You should explain in the pull request how the algorithm you are implementing relates that what is already implemented in NetworkX. Also, a short explanation of the algorithm in your own words will help the readers of your code.  Finally, for getting a sorted list of nodes by degree, you can use something like:  ``` python sorted_nodes = sorted(G, key=G.degree) ```  You are copying the graph and removing the nodes one by one. This makes tings go slow, especially for large graphs. Not sure if this is what you intended. 
comment
@feifeipeng I see. I read the paper and checked your implementation, I see now that it gives correct results. However the implementation it's not idiomatic in python, it's confusing because of the use of the huge nested lists with the maximum possible levels of depth that only hold place-holders most of the time (try to put some prints in the while loop to see how `ADJ`,'start' and `last` look like at each iteration to see what I mean). I see that this is because you translated the Fortran code provided by the authors to python (including variable names).  Here is a quick attempt to use more sane data structures but still being very close to the authors implementation. Note that I'm sorting the nodes by core number and degree, which I think is very close (maybe the same?) to the ordering of nodes as described in the paper (removing each time the node with minimum degree). Anyway, according to the authors, sorting nodes is not necessary for the correctness of the algorithm but it helps in high density graphs.  ``` python from collections import defaultdict def maximum_clique_set(G):     # Order nodes by core number and degree     core_number = nx.core_number(G)     nodes = sorted(G, key=lambda n: (core_number.get(n), G.degree(n)))      # Data structures     best_clique = set()     levels = defaultdict(list)     start = {}     last = {}      # Initial values     depth = 1     start[depth] = 0     last[depth] = len(nodes)     levels[depth] = nodes      while True:         start[depth] += 1         if depth == 1 and depth + last[depth] - start[depth] <= len(best_clique):             break          if depth + last[depth] - start[depth] > len(best_clique):             level = depth             depth += 1             start[depth] = 0             last[depth] = 0             #print depth, level, start, last, dict(levels), best_clique             for i in levels[level]:                 if levels[level][start[level]-1] in G[i]:                     levels[depth].append(i)                     last[depth] += 1              if last[depth] == 0:                 levels[depth] = []                 depth -= 1                 if depth > len(best_clique):                     best_clique = {levels[i][start[i]-1] for i in range(1, depth+1)}         else:             levels[depth] = []             depth -= 1      return best_clique ```  This is way faster (about one order of magnitude) than the fortran-like approach mostly because we avoid copying and removing nodes nodes one by one when initially sorting nodes. The thing is, the more pythonic function above is another order of magnitude slower than just:  ``` python def plain_max_clique(G):     return max(list(nx.find_cliques(G)), key=len) ```  Which computes all cliques in the graph and picks one of the largest ones. I think that you should read through the very nice `networkx.algorithms.clique` module to see how it does that. Here are some test in my machine to illustrate:  ``` python In [49]: %paste import networkx as nx from networkx.utils import powerlaw_sequence, create_degree_sequence  def build_power_law(n):     deg_seq = create_degree_sequence(n, powerlaw_sequence, 100)     G = nx.Graph(nx.configuration_model(deg_seq))     G.remove_edges_from(G.selfloop_edges())     G = max(nx.connected_component_subgraphs(G), key=len)     G.name = 'Power law configuration model: {0}'.format(n)     return G  def plain_max_clique(G):     return max(list(nx.find_cliques(G)), key=len) ## -- End pasted text --  In [50]: G = build_power_law(5000)  In [51]: print(nx.info(G)) Name: Power law configuration model: 5000 Type: Graph Number of nodes: 4928 Number of edges: 15760 Average degree:   6.3961  In [52]: %timeit plain_max_clique(G) 10 loops, best of 3: 59.3 ms per loop  In [53]: %timeit maximum_clique_set(G) 1 loops, best of 3: 8.96 s per loop  In [54]: G = nx.gnp_random_graph(100, 0.5)  In [55]: %timeit plain_max_clique(G) 10 loops, best of 3: 106 ms per loop  In [56]: %timeit maximum_clique_set(G) 1 loops, best of 3: 3.85 s per loop ```  I think that we should look to other algorithms for the maximum clique set problem to find something that will be faster that using what we already have in `networkx.algorithms.clique`. This paper seems very useful but I only took a quick look.  http://arxiv.org/abs/1207.4616  Also, please note @jfinkels and @ysitu comments. 
comment
@feifeipeng , great! Could you please make a separate pull request for the exact algorithm for minimum dominating set? We can leave this one open to follow up with the maximum clique work/investigation. As @jfinkels says, the `minimum_dominating_set` function should be in `networkx.algorithms.dominating`. The function currently there is mostly to get a relatively small dominating set fast to solve edge connectivity problems. The exact algorithm for a minimum one is a nice addition, even if not practical for not so large graphs, and very related to what is already there.  By the way, there is some weird indentation thing with your code. In mi editor locally I see it as in github diff (in `Files changed`), sometimes it seems that an indentation level is two spaces, sometimes they seem of 8 or 4 spaces, which makes difficult to read the code (seems to run fine though). It's a good practice to set your text editor to use 4 spaces as a tab for indentation for python code. 
comment
Hi @hcars,  Please look at the documentation of the k_components functions:  https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.connectivity.kcomponents.k_components.html  This function accepts a positional argument, the graph G, and a keyword argument, flow_func, that has to be a function to perform the underlying flow computations. Such as the functions available in the flow module at networkx.algorithms.flow. You are passing a float as a second argument (why?), which is used as a flow_func. The error informs you that this argument has to be a callable.  As an illustration, here is an example on how you are supposed to use  the flow_func argument:  ```python In [1]: import networkx as nx  In [2]: from networkx.algorithms.flow import boykov_kolmogorov  In [3]: G = nx.karate_club_graph()  In [4]: kcoms = nx.k_components(G) # Uses edmonds_karp as a defalult flow_func  In [5]: kcoms_bk = nx.k_components(G, flow_func=boykov_kolmogorov)  In [6]: kcoms[4] == kcoms_bk[4] Out[6]: True  In [7]: kcoms[4] Out[7]: [{0, 1, 2, 3, 7, 8, 13, 30, 32, 33}] ```
comment
Ok, I see the confusion. The functions from the approximation module are not imported in the top level namespace by default. You have to explicitly import them:  ```python In [8]: from networkx.algorithms import approximation as apxa  In [9]: kcoms_approx = apxa.k_components(G, 0.95)  In [10]: kcoms_approx[4] Out[10]: [{0, 1, 2, 3, 7, 8, 13, 30, 32, 33}] ``` 
comment
@OrkoHunter, Aric is proposing to _remove_ `nx_pylab` from NetworkX and put it in a separate repository at github, upload it as a separate package at pypi, ... And @ysitu is suggesting to use the (yet to be defined and implemented) new add-ons system for integrating this new (yet to be created) `networkx-matplotlib` project into NetworkX. Thus, this is a big change that will require planing and coordination with other (more or less concurrent) development efforts, not a good candidate for a quick PR. I suggest you close #1408. This could be a cool GSOC project, which as @pabloem points out, it could also try to address some shortcomings of the current `nx_pylab`, such as arrows in directed graphs, .... 
comment
That would be a great addition to NetworkX. The tricky thing might be to do a linear time implementation. I attach here a dense paper on the subject for reference.  [triconnected_components.pdf](https://github.com/networkx/networkx/files/621161/triconnected_components.pdf) 
comment
Hi @JamesClough , I'm not sure if @jfinkels is working on this. A PR adding this feature will be very welcome. I'll help you in any way I can to get it merged. Thanks!
comment
Very nice indeed! Thanks for working on this @andre-dietrich . I did some quick speed tests with sparse graphs with right skewed degree distributions (a pretty usual use case), and it seems that the proposed implementation is much slower than `nx.all_simple_paths`. See:   ``` python In [135]: %paste import networkx as nx from networkx.utils import powerlaw_sequence, create_degree_sequence  def pick_nodes(G):     u = next(G.nodes_iter())     v = next(nx.non_neighbors(G, u))     return u, v  def build_power_law(n):     deg_seq = create_degree_sequence(n, powerlaw_sequence, 100)     G = nx.Graph(nx.configuration_model(deg_seq))     G.remove_edges_from(G.selfloop_edges())     G = max(nx.biconnected_component_subgraphs(G), key=len)     G.name = 'Biconnected component of power law configuration model: {0}'.format(n)     return G ## -- End pasted text --  In [136]: G = build_power_law(40)  In [137]: print(nx.info(G)) Name: Biconnected component of power law configuration model: 40 Type: Graph Number of nodes: 22 Number of edges: 44 Average degree:   4.0000  In [138]: u, v = pick_nodes(G)  In [139]: timeit paths = list(nx.all_simple_paths(G, u, v)) 10 loops, best of 3: 27.4 ms per loop  In [140]: timeit paths = list(nx.shortest_simple_paths(G, u, v)) 1 loops, best of 3: 2.66 s per loop  In [141]: timeit paths = list(nx.bidirectional.all_simple_paths(G, u, v)) 1 loops, best of 3: 30.6 s per loop ```  @OrkoHunter note that in your speed tests your are not actually computing the paths, you are only creating the generator.  @andre-dietrich  note that we also have a recently added `nx.shortest_simple_paths` which is based on Yen's algorithm for computing k shortest simple paths.    
comment
@andre-dietrich sorry for being slow on this. The more I look to your implementation the more I like it. However I'm not sure I fully understand why the implementation that we already have for `all_simple_paths` is so much faster without `cutoff` than the proposed here. I think it's because, without cutoff, your implementation is doing `len(G)-1` iterations in the for loop, even if no more paths exist. It would be great to be able to break the for loop when no more paths exists, but I don't see an easy way to do this.  Using a `cutoff` your implementation is significantly faster that we already have, and it's very nice that the paths are yield according to their length. However in the case of `nx.shortest_simple_paths`, it also handles weighted paths, so not all functionality is covered. I think that the best approach is to have single entry points in the public API, and then select the best implementation depending on what the user is asking. So I think that it would be better to substitute or complement the existing functions than to add a new `bidirectional` module for them. We could, for instance, use your implementation of `all_simple_paths` if a `cutoff` is passed as argument; the same for `shortest_simple_paths` if `weight` is `None`, ...   I'd like to hear more opinions on this (and yours of course). Given than now we are working towards the 2.0 version, we should think about the API and break backwards compatibility if necessary. For instance, I think that `all_simple_paths` should raise `NetworkXNoPath` instead of nothing if no path exists (this is what most path related functions do).  In terms of coding, I think it's better to have both directed and undirected implemented in the same function. Also variable names should be improved. Maybe something like this would do (not tested, sorry):  ``` python def _all_simple_paths(G, source, target, cutoff):     directed = G.is_directed()      if cutoff < 1:         return      tree = [{(source,)}, {(target,)}]      for i in range(cutoff):         temp = set()         tree_forward = tree[i % 2]         tree_backward = tree[(i + 1) % 2]          if i % 2:             if directed:                 neighbors = G.successors_iter             else:                 neighbors = G.neighbors_iter         else:             if directed:                 neighbors = G.predecessors_iter             else:                 neighbors = G.neighbors_iter          leaves = {x[-1] for x in tree_forward}  # only to reduce some effort         for path in tree_backward:             for s in neighbors(path[-1]):                 if s not in path and s in leaves:                     for _path in tree_forward:                         if s == _path[-1]:                             if not set(_path).intersection(path):                                 if i % 2:                                     yield list(path) + list(reversed(_path))                                 else:                                     yield list(_path) + list(reversed(path))                     temp.add(path + (s,))          tree[(i + 1) % 2] = temp ```  [Edit]: improved a bit the code. 
comment
@andre-dietrich sorry again for being so slow on this. I'm preparing a review and some benchmarks for this functions. I hope to post them later today or tomorrow. 
comment
By the way, can you post the link to the benchmarks on your website? 
comment
@rrssl thanks for reporting this bug, and thanks @wuhaochen for looking at this closely.  It could be that the implementation does not follow the algorithm described in the paper closely enough. So far we have only encountered corner cases (like cycles and path graphs) in which the implementation fails to report the correct results. I think it would help to have a non trivial case in which we miss a cut.  The reasoning behind special casing cycles was that all cuts are easily computed without using the fancy algorithm, and is quite faster. The same could be said for path graphs but this might be a clue that the implementation is not good enough.  Regarding why antichains map to closed sets, the author of the algorithm assumes this is so, and cites as authoritative sources:   J. C. Picard, M. Queyranne, "On the structure of all minimum cuts in a network and applications," Mathematical Programming Study 13,1980, pp. 8-16.  M. O. Ball, J. S. Provan, "Calculating bounds on reachability and connectedness in stochastic netÂ­ works," Networks, vol. 13, 1983, pp. 253-278.   When implementing this, a while ago, I found the extended report [kavensky_kcutsets.pdf](https://github.com/networkx/networkx/files/2138305/kavensky_kcutsets.pdf) very useful, as it's more detailed than the paper.  In any case, implementing a function for Closure, would be a great addition to NetworkX and might help with this algorithm. I don't have much time right now to look at this closely, so if you can try to figure out what is the problem with the implementation, that would be great. If not, I'll try to work on it in the coming weeks.     
comment
Hi @misingnoglic. The algorithm implemented in `nx.network_simplex` is not guaranteed to work if edge weights are floating point numbers, and `nx.max_flow_min_cost` has the same limitations as it's using it. However only the docs for the former say so. As a workaround you can convert your weights to integers. In your example, multiplying cost by 100 will do it:  ``` python In [2]: %paste G = nx.DiGraph() G.add_edges_from([ ('1in', '1out', {'cost': 0, 'capacity': 1}), ('2out', '4in', {'cost': -948, 'capacity': 1}), ('2out', 'destination', {'cost': -1090, 'capacity': 1}), ('2out', '3in', {'cost': -1031, 'capacity': 1}), ('destination', 'sink', {'cost': 0, 'capacity': 1}), ('2in', '2out', {'cost': 0, 'capacity': 1}), ('source', '2in', {'cost': 0, 'capacity': 1}), ('source', '4in', {'cost': 0, 'capacity': 1}), ('source', '1in', {'cost': 0, 'capacity': 1}), ('source', '3in', {'cost': 0, 'capacity': 1}), ('4in', '4out', {'cost': 0, 'capacity': 1}), ('1out', '2in', {'cost': -1031, 'capacity': 1}), ('1out', '4in', {'cost': -1031, 'capacity': 1}), ('1out', 'destination', {'cost':-1090, 'capacity': 1}), ('1out', '3in', {'cost': -948, 'capacity': 1}), ('4out', 'destination', {'cost': -1090, 'capacity': 1}), ('3in', '3out', {'cost': 0, 'capacity': 1}), ('3out', '4in', {'cost': -1031, 'capacity': 1}), ('3out', 'destination', {'cost': -1090, 'capacity': 1}) ]) ## -- End pasted text --  In [3]: %time nx.max_flow_min_cost(G, 'source', 'sink', weight='cost') CPU times: user 1.23 ms, sys: 0 ns, total: 1.23 ms Wall time: 3.12 ms Out[3]:  {'1in': {'1out': 1},  '1out': {'2in': 1, '3in': 0, '4in': 0, 'destination': 0},  '2in': {'2out': 1},  '2out': {'3in': 1, '4in': 0, 'destination': 0},  '3in': {'3out': 1},  '3out': {'4in': 1, 'destination': 0},  '4in': {'4out': 1},  '4out': {'destination': 1},  'destination': {'sink': 1},  'sink': {},  'source': {'1in': 1, '2in': 0, '3in': 0, '4in': 0}} ```  I'll put together a PR (hopefully this weekend) updating the docs (also for `nx.network_simplex`) because they don't mention the conversion to integers workaround.  PS: It's not nice to cross post questions in different sites at the same time. 
comment
Hi @ooii   Thanks for the report. I cannot reproduce the error with the last release of NetworkX (2.1):  ```python In [29]: import networkx as nx In [30]: G = nx.read_gexf('graph.gexf') In [31]: flow_value, flow_dict = nx.maximum_flow(G, "Sink_0", "Aircraft3_37") In [32]: flow_value Out[32]: 77266.346 In [33]: flow_value == nx.maximum_flow_value(G, "Sink_0", "Aircraft3_37") Out[33]: True In [34]: flow_value, flow_dict = nx.maximum_flow(G, "Sink_0", "Aircraft4_40") In [35]: flow_value Out[35]: 84470.023 In [36]: flow_value == nx.maximum_flow_value(G, "Sink_0", "Aircraft4_40") Out[36]: True In [37]: flow_value, flow_dict = nx.maximum_flow(G, "Sink_0", "Aircraft2_32") In [38]: flow_value == nx.maximum_flow_value(G, "Sink_0", "Aircraft2_32") Out[38]: True ```  Can you upgrade NetworkX to 2.1? Version 1.11 is quite old (was released 2 years ago).
comment
Andrey,  This implementation looks good to me, and the tests that you wrote are nice. The only thing to do for the unweighted case is to update the docstrings of `shortest_path` and friends. This will be very helpful for implementing White and Newman's approximation for node independent paths.   Regarding the weighted case, do you think that it is worth to also support restrictions? We could use the same approach, but I'm not sure that it is as useful as in the unweighted case because you always can manipulate the weights to impose restrictions.   @hagberg wrote the current implementation of `bidirectional_shortest_path` (in my opinion, one of the nicer pieces of code of NetworkX), what do you think about this Aric? 
comment
**Test results for commit 0d7f141 merged into master (192d2e7)** Platform: linux2 - python2.6: OK (SKIP=57) Ran 1527 tests (libraries not available: pyyaml, pydot, numpy, matplotlib, ogr, yaml, scipy, pyparsing, pygraphviz) - python2.7: OK (SKIP=2) Ran 1709 tests (libraries not available: ogr) - python3.2: OK (SKIP=10) Ran 1680 tests (libraries not available: pyparsing, ogr, pygraphviz, matplotlib, pydot) 
comment
2012/8/31 aparamon notifications@github.com  > 2012/8/31 Jordi Torrents notifications@github.com >  > > Andrey, > >  > > This implementation looks good to me, and the tests that you wrote are nice. The only thing to do for the unweighted case is to update the docstrings of shortest_path and friends. This will be very helpful for implementing White and Newman's approximation for node independent paths. > >  > > Regarding the weighted case, do you think that it is worth to also support restrictions? We could use the same approach, but I'm not sure that it is as useful as in the unweighted case because you always can manipulate the weights to impose restrictions. >  > Thanks. >  > I think support for the weighted case should be also implemented, and > I think it's not very hard to do. It would be very useful because > currently you cannot get "there is no pass" exception via playing with > weights. And you might not want to play with weights because it's side > effect (frozen graphs etc). >  > Also I think that all _shortest_path_ procedures should support > restrictions. However, the amount of interface changes looks > frightening to do at once. But I don't really see any practical > alternative to passing the options. Copying graph and modifying it > according to restrictions before passing to shortest_path appears to > be just too damn slow :-(  Good points. I agree. We should support restrictions for all shortest paths functions. Regarding the interface changes, they are backwards compatible, so we do not break working code. And, as you said, the implementation of restrictions doesn't slow down the case without restrictions. I'm +1 for implementing this.  > When everyone agrees on the interface I will also push my > implementation of yen_simple_paths. I only need to come up with the > better name probably :-)  That's great. Thank you very much for your work on this! 
comment
Andrey,  The implementation of Yan's looks good to my (on a first sight and without reading the paper nor testing it), it will be a great addition to NetworkX. But we have to use one pull request for each feature. We just migrated our development workflow from trac to github last july, so we are still adjusting (i'm pretty new to git myself) and still have to write proper documentation on the development workflow for NetworkX. We are trying to follow the same approach used in other scipy python projects. I think that the best documentation available is from numpy:  http://docs.scipy.org/doc/numpy/dev/gitwash/development_setup.html http://docs.scipy.org/doc/numpy/dev/gitwash/development_workflow.html  The main idea is: one new branch for each feature, send a pull request from that branch against NetworkX master, and keep the master branch of your fork up to date fetching from NetworkX master (using the alias `upstream`).  Also, notice that with the merge of Chris' enhancement of shortest paths (#744) you changes from commit     0d7f141 do not merge clearly with master anymore. At this point maybe the easiest option is to close this pull request, and then make two new ones following numpy development workflow. What do you think?   Sorry for the trouble. 
comment
2012/9/4, aparamon notifications@github.com:  > 2012/9/4 Jordi Torrents notifications@github.com >  > > The main idea is: one new branch for each feature, send a pull request > > from that branch against NetworkX master, and keep the master branch of > > your fork up to date fetching from NetworkX master (using the alias > > upstream). > >  > > Also, notice that with the merge of Chris' enhancement of shortest paths > > (#744) you changes from commit 0d7f141 do not merge clearly with master > > anymore. At this point maybe the easiest option is to close this pull > > request, and then make two new ones following numpy development workflow. > > What do you think? > >  > > Sorry for the trouble. >  > I see the following message in the pull request thread: > Test results for commit 0d7f141 merged into master (192d2e7) > I thought it means "your change was committed upstream". Apparently it > is not. What does it mean?  That comment was made by a script (adapted from @ipython) for testing github pull requests (see #752 for the code and details). It tests that the pull request clearly merges with the current master, and that the test suite passes in different python environments. But it does not push the merged changes into master. I see now that the message posted at github can be confusing, I'll change that. As we keep saying, we are still adjusting to github workflow.  > I'll try to study git a little, but it would take time. I absolutely > don't mind if in the meantime Someone(TM) properly rearranges my > commits so that my changes end up in the master branch. (Basically, > I've submitted my patches. You are now free to make use of them to > enhance your great library.)  Thank you very much for your code. I'll work on arranging/completing your patches in order to be able to merge them with NetworkX master in the following weeks. I'll leave this pull request open until I manage to create new ones. If in the meantime Someone(TM) ;) is interested in working on this, please leave a comment here so we do not duplicate efforts.  Salut! 
comment
Hi Chris,  2012/9/9 chebee7i notifications@github.com  > Jordi, I can take a look at preparing these patches today. It looks like it should be simple, so I should finish it.  Great! yes, please feel free to work on this.  > Also, the script that tests github pull requests did not seem to run each time a pull request was updated. So while helpful for the initial pull request, we'd need additional feedback anytime the request is updated or else we risk a false sense of security.  The script has to be run manually. You are right that it may give a false sense of security. I guess that the ideal solution would be an automated script. While we don't have that, we could adopt the convention of only posting the result of the script if there is some failure (`test_pr.py` will post a trace of the nose output in a gist). Thus we avoid the false sense of security by only posting its result to signal to the author of the pull request that there is a problem that needs fixing.  Salut! 
comment
2012/9/9 chebee7i notifications@github.com  > It seems there is no way to make pull requests dependent on other pull requests. Does anyone have ideas on how this should be managed, or how other projects have done it? In this situation, the pull request for K-shortest paths is based off the restricted shortest paths pull request.  I think that there is no way to link pull requests like that. A possible way could be to clone Andrey's NetworkX fork and make patches (`git format-patch`) of his changes. Then create a new branches on your fork (one for each change), apply the patches and fix/complete as necessary, and then make new pull requests. I'm still adapting to git and github, so perhaps there is a better (less arduous) way.  > Also, what has been decided about implementing this for the weighted case and other similar shortest path functions? Couldn't almost every NetworkX algorithm be adapted to look over a subset of the graph's nodes or edges? I suppose it is a matter of what people are likely to use.  Andrey suggested to support restrictions for all shortest paths functions using the same approach. I think that this is the ideal solution, but there are a lot of functions (with their docstrings and tests) to update. Maybe we can do this in several steps.  Dan suggested to create a subclass that filters nodes and edges to implement this and make it more general. Andrey suggested that this may not be better than modifying the graph inplace before calling `shortest paths`. Maybe we could explore further the subclass option but I like Andrey's implementation because it is simple and fast. 
comment
I think that the rationale behind `shortest paths` and `shortest path length` is using less memory in the latter.   You are right that shortest path functions are difficult to extend. I think it would be very useful if we can refactor them to make extending them simpler. 
comment
I did some speed tests comparing the context class and the filter function implementations of shortest paths with restrictions using White and Newman approximation algorithm for node independent paths as benchmark. The context class implementation is more than 2 orders of magnitude slower and this figure increases in denser graphs. You can find the benchmark code at: https://gist.github.com/4586874  ``` In [227]: run test_filter.py Testing filters with a G_np random graph of order 1001 and size 25289     Context class: found 45 node independent paths in 0.3062 seconds     Filter function: found 45 node independent paths in 0.0062 seconds Testing filters with a G_np random graph of order 1001 and size 49389     Context class: found 97 node independent paths in 0.0058 seconds     Filter function: found 97 node independent paths in 0.0039 seconds Testing filters with a G_np random graph of order 1001 and size 75234     Context class: found 134 node independent paths in 7.2447 seconds     Filter function: found 134 node independent paths in 0.0158 seconds Testing filters with a G_np random graph of order 1001 and size 99677     Context class: found 199 node independent paths in 19.6490 seconds     Filter function: found 199 node independent paths in 0.0295 seconds ``` 
comment
Yes, that's much faster. It looks good to me. But I was wondering if it would be simpler to just add new method(s) to the `Graph`, `DiGraph` and `MultiGraph` classes (such as `filtered_neighbors(self, n, ignore_nodes=None, ignore_edges=None)`. I'm not sure that this is a good idea. What do you think? 
comment
Ok I agree; it's better to keep those classes simple. Monkey patching with a context manager is fast and simple enough: +1 from me for implementing this. 
comment
I did not notice that there was a PR. Thanks for this @francis-liberty. I think that the proposed changes are quite invasive for the shortest path code. The context manager should be transparent to the code that runs in it. In other words, it has to support NetworkX API, for this it must allow accessing the filtered neighbors via `G[u]`. At some point I also tried to implement this but got stuck with a problem when monkey-patching the `__getitem__` method. With the implementation pasted below `G[u]` gives me unfiltered neighbors  but `G.__getitem__(u)` yields the filtered neighborhood (as expected).   This feature keeps popping up regularly (see @ysitu comment in #1096 for the last instance). It would be great if someone could step up and put together a PR for this. The following example shows the problem that I've found:  ``` ipython In [1]: import networkx as nx In [2]: from filter import filter In [3]: G = nx.path_graph(4) In [4]: with filter(G, nodes=[1]):    ...:     print(G[0])          ...:     print(G.__getitem__(0))    ...:      {1: {}} {} In [5]: print G.__getitem__(0) {1: {}} ```  ``` python import networkx as nx from contextlib import contextmanager import types  @contextmanager def filter(G, nodes=None, edges=None):     directed = G.is_directed()     if nodes is None:         nodes = set()     if edges is None:         edges = set()     edges = set(G.edges(n for n in nodes if n in G)) | set(edges)      if directed:         def __getitem__(G, n):             try:                 return dict((s, d) for s, d in G.adj[n].items() if (n, s)                                  not in edges)             except KeyError:                 raise nx.NetworkXError("The node %s is not in the graph."%(n,))     else:         def __getitem__(G, n):             try:                 return dict((s, d) for s, d in G.adj[n].items() if (n, s)                                 not in edges and (s, n) not in edges)             except KeyError:                 raise nx.NetworkXError("The node %s is not in the graph."%(n,))      def neighbors_iter(G,n):         try:             return iter(s for s in G.adj[n] if (n, s) not in edges                             and (s, n) not in edges)         except KeyError:             raise nx.NetworkXError("The node %s is not in the graph."%(n,))      def successors_iter(G,n):         try:             return iter(s for s in G.adj[n] if (n, s) not in edges)         except KeyError:             raise nx.NetworkXError("The node %s is not in the digraph."%(n,))      def predecessors_iter(G,n):         try:             return iter(p for p in G.pred[n] if (p, n) not in edges)         except KeyError:             raise nx.NetworkXError("The node %s is not in the digraph."%(n,))     try:         gi = G.__getitem__         G.__getitem__ = types.MethodType(__getitem__,G)         if directed:             si = G.successors_iter             G.successors_iter = types.MethodType(successors_iter,G)             pi = G.predecessors_iter             G.predecessors_iter = types.MethodType(predecessors_iter,G)         else:             ni = G.neighbors_iter             G.neighbors_iter = types.MethodType(neighbors_iter,G)         yield G     finally:         G.__getitem__ = gi         if directed:             G.successors_iter = si             G.predecessors_iter = pi         else:             G.neighbors_iter = ni ``` 
comment
@ysitu thanks for looking at this. Given that information I think that we should discard the the monkey patching context manager approach. The `GraphView` approach sounds good to me. 
comment
> Creating a view that has Gview[n] return an iterator while the graph itself returns a dict is certainly possible too.  Maybe we can use this approach for start playing with how it might look like a NetworkX API that returns iterators for all Graph methods. Also we will be doing steps for solving the problem of being able to filter efficiently nodes and edges, a feature that will be useful in a wide range of situations.  
comment
I like it! Good job @gcetusic! Did you build it with actual NetworkX + matplotlib code or is it a drawing? 
comment
@gcetusic I was only curious. I think is OK if the logo is not actually generated via nx + matplotlib (but would be cool indeed!). 
comment
The git submodule approach (and hosting the repo under networkx github account) sounds good to me. The idea of a namespace with optional addons or wrappers to specific tools has been around for a long time, and I think it would be very useful. However I'm not sure how much will increase the complexity of installing/developing/maintaining NetworkX. I think this is important. Maybe we can experiment with this approach with the METIS wrappers. We've discussed the `addons` or `nxtools` namespace approach several times in different places and context, so it will be useful beyond METIS.  By the way @ysitu, I see that you have already been working on this: https://github.com/ysitu/networkx/tree/metis . On a first sight, it seems that changes in METIS source code are necessary (I don't know how important are they). Do you think that we could pass those changes upstream? Having a wrapper would be very useful, but maintaining a fork could be painful. 
comment
Good points @ysitu. I also think that, having some "optional components" under the NetworkX umbrella, will also make these components easier to maintain and to contribute to (we already have a workflow in place, and a small team of people that will look at things if they do not work). In general, I think it will decrease the likelihood that those optional packages become unmaintained. A lot of packages out there depend on only one or very few maintainers. This also means that we should be careful on what we consider "official optional component", we have to be sure that we (or someone we trust) will be able to maintain it in the future.  Regarding the more technical aspects, I cannot comment much because I'm not competent in low level languages/details (my background is in the Social Sciences, and I can only program in python and R). But having a solid and standardized way of being able to replace some NetworkX functions for alternative implementations, sounds very useful and a powerful mechanism of customizing NetworkX.  
comment
@Wind4Greg Thanks for looking at this. In a first read of your implementation, I see that you are removing and adding nodes and then searching for shortest paths. We tested this approach in #762 (see this [comment](https://github.com/networkx/networkx/pull/762#issuecomment-12503042), the context manager in the benchmarks is an implementation of the removing and adding nodes/edges approach) and it's quite slower than the filter approach, as in @aparamon nice implementation at #762.  We didn't merge it in the end because the discussion got diverted towards the more general problem of how to efficiently filter nodes and edges in a graph. And whether or not should we change the interface to all shortest paths functions to accept containers for nodes and edges to exclude. We ended up leaving it unfinished, this happens too frequently but time and energy are limited. Sorry for that Andrey!  Anyway, as @chebee7i says, we should revisit the Graph View discussion for 2.0. It would be great to have a GraphView class that could do the filtering (and many more things!). But I think that we should separate it from Yen's algorithm implementation. We can add it now using a private modification of shortest paths functions `bidirectional_shortest_path` and `bidirectional_dijkstra` that accept containers for nodes and edges to exclude during the shortest path search (as in @aparamon implementation). This is what I did in #1405 (although only for nodes), which implements White and Newman algorithm for node independent paths.  If/when we implement the GraphView class, we can rewrite the functions that use private modifications of standard SP algorithms to use it instead. What do you guys think? I'm quite busy, so if someone can step up and put together a PR, that would be great. If not I'll try to do it in the following days. 
comment
@Wind4Greg yes, I think we should discuss about an API for k-shortest paths for NetworkX. I'm not sure about what should we do if there are less than k paths between source and target. I'd prefer not to raise an exception (unless there is no path) and just return the ones we found.  I'm preparing a PR with my suggestion above (ie generalize the filer approach of Andrey's implementation in #762 to the weighted case, and use custom private modifications of SP algorithms that accept containers for nodes and edges to exclude). If we go this way, then the k-shortest path function is almost an one liner:  ``` python from itertools import islice def k_shortest_paths(G, source, target, k, weight=None):         return list(islice(nx.shortest_simple_paths(G, source, target, weight=weight), k)) ```  I think this one liner is worth including as a function, but maybe is better if we move this discussion to the PR that I'm about to push.  @aparamon A good reason to keep `all_simple_paths` is its speed and scalability (although with simple paths you get to astronomical number of paths pretty quickly). I did some quick tests, and for small graphs is one order of magnitude faster than `shortest_simple_paths`, for slightly bigger graphs (tens of nodes and density > 0.3)  `all_simple_paths` finished in seconds (reporting millions of paths) and I just ctrl-C `shortest_simple_paths` after running for several minutes in the same graphs.  However I think `shortest_simple_paths` is useful in itself and we should include it, either because the user wants the paths sorted by length, or because she only wants to get the k shortest paths. In this latter case, as long as k is small, it will be quite fast.   I'll push the PR with my changes now, even if it still needs work, so we can move the discussion there. I'll also add some benchmarks (against `all_simple_paths` and @Wind4Greg implementation). 
comment
Hello @DinosC , thanks for the report. Your example works with the last development version of NetworkX: ```python In [20]: data = {'1_id':[3483, 3482, 3482, 3480], 'v_id_1':[739, 717, 717, 1058], 'v_id_2':[2232,2196, 2196, 2250], 'v':[38000, 38000, 22000, 22000], 'f':[50, 50, 50, 50]}  In [21]: links = pd.DataFrame(data)  In [22]: links Out[22]:     1_id   f      v  v_id_1  v_id_2 0  3483  50  38000     739    2232 1  3482  50  38000     717    2196 2  3482  50  22000     717    2196 3  3480  50  22000    1058    2250  In [23]: G=nx.from_pandas_dataframe(links, 'v_id_1', 'v_id_2', edge_attr=['v','f'], create_using=nx.MultiGraph())  In [24]: list(G.nodes()) Out[24]: [1058, 739, 2196, 2232, 2250, 717]  In [25]: list(G.edges(data=True)) Out[25]:  [(1058, 2250, {'f': 50, 'v': 22000}),  (739, 2232, {'f': 50, 'v': 38000}),  (2196, 717, {'f': 50, 'v': 38000}),  (2196, 717, {'f': 50, 'v': 22000})]  In [26]: print(nx.__version__) 2.0.dev_20161206165920 ```  If you can I'd recommend that you install the development version of NetworkX from github.
comment
Thanks for fixing this!
comment
@mcognetta, see my inline comments for why the doctest fail.  PS: I also like the single use @DonQuixoteDeLaMancha account for windmill graphs. I guess that now we know who is behind that account ;)
comment
This is implemented (and already merged) at #1405 . 
comment
I think it's better to drop some of the 2.0-tagged issues and expedite the 2.0 release. Looking at the issues and pull requests currently tagged 2.0, I think most of them are not actually show stoppers and can be safely postponed. We should figure out the travis problems with python 3.6 in #2413 as I think it's essential to be fully compatible with 3.6 for the release.  However the big thing that still needs to be done (both if we release 2.0 or do a 1.99 pre release) is writing up all API changes and new features in the documentation.
comment
Nice! didn't notice that we already had a draft for 2.0 API changes.
comment
I also think that having views instead of iterators is worth waiting several weeks for releasing 2.0. We shouldn't break backwards compatibility twice in the base classes API in two consecutive releases.
comment
I think that the best approach is to use `None` as the default value for the `weight` parameter everywhere. Thus if the user wants to compute an unweighted version of the algorithm, she doesn't have to use the optional `weight` parameter even if the input graph has an edge attribute named weight. I think it would be confusing that the same call would yield different results depending on having or not an edge attribute named weight. I think that this is an instance of 'Explicit is better than implicit'. On the other hand, for the shortest paths functions, I think that defaulting to Dijkstra is not a good idea, as it's slower than BFS.
comment
Hi @fangpin, until now you had to compute all `n(n-1)/2` minimum s-t cuts. But In PR #2425 I added the Gomory-Hu tree algorithm which computes a tree that represents all minimum s-t cuts of an undirected graph performing only `n-1` minimum cut computations. In the docstrings of that function I also explain how to compute the set of edges that separate s from t in the input graph.
comment
See #2416 for a workaround. It seems that the problem is that conda for python 3.6 installs the latest version of gdal (2.1.3) which is not compatible with the package libgdal-dev provided by Travis version of Ubuntu. By installing gdal after creating the conda environment that build for 3.6 is not killed and gdal is installed successfully for all other python versions. 
comment
Hi @ericmjl, the API design is a result of the fact that we don't have a proper bipartite graph class in NetworkX, the convention (as explained in the module level docstrings of the bipartite package) is to use a node attribute to keep track of which node set each node belongs to.  As the Notes section of the docstring of `degree_centrality` explains, this measure is the degree of each node divided by the maximum degree possible for each node. In the unipartite case this is `n-1` (where `n` is the number of nodes in the graph) but for the bipartite case the maximum degree of a node in a bipartite node set is the number of nodes in the opposite node set.  Thus we need to know to which node set each node belongs to in order to properly compute centrality metrics (and others) in bipartite graphs. This can be computed using `bipartite.sets`, but the trouble comes in the case of disconnected graphs because more than one valid solution exists. To deal with this ambiguity we require the user to specify the nodes in one of the bipartite node sets as an argument for most bipartite functions to avoid returning wrong results depending on the concrete solution found in `bipartite.sets` (see #2375 for recent work on this).   Do you think that we should update the docstrings for bipartite centrality functions to make this more clear?
comment
@ericmjl The main problem that I see with the approach you suggest is that we would be adding more constraints on the graphs that bipartite functions can successfully process. And thus I think we would be adding more work on the user side in order to use bipartite functions. Note that the use of a node attribute named `bipartite` with values 0 for one node set and 1 for the other is now only a suggestion. We use this convention for bipartite generators, but that's it. The user if free to follow this convention or to organize their data as they see fit.  With the API you suggest it would be a requirement to use a node attribute with this exact name, and we would also to enforce the use of concrete values for the attribute to distinguish node sets. I think it's easier for users to create a container with all nodes in on bipartite node set (which is a line of code) than to follow a convention that might not suit their data. I work a lot with bipartite graphs, and sometimes I didn't generate them but I have to analyze them, sometimes the node set to which each node belong is part of the node name/id, sometimes is a node attribute other than bipartite, etc ... In all these cases, I only need a line of code to gather all nodes in one node set in a container and use all bipartite functions. If we enforce the use of a node attribute with defined values I'd have to do more work in order to use bipartite functions.   Regarding making the docs clearer, what would you suggest? Rereading the docstings I feel that they are clear, but of course I'm not in the best position to see how docs could be improved as I wrote those functions and I've been using them for a long time. 
comment
@ericmjl In general we try to minimize the sanity checks for inputs. For instance, hardly any of the functions in the bipartite package checks that the input graph is actually bipartite. There is a cost in terms of running time in doing sanity checks, and it's impossible to cover all corner cases. In the end the "garbage in -> garbage out" rule always applies. So we assume that users are careful preparing the inputs for functions.  Regarding the documentation, I'll prepare a PR trying to improve the explanations of the APIs in the bipartite package. Thanks for looking at this carefully and for your comments, it's very useful. This is the best way to improve usability.  
comment
Regarding the k-components tests, the problem is with the labels of the nodes of test graphs, which are quite complex and involve several uses of `nx.convert_node_labels_to_integers` and `nx.disjoint_union` in the process of building them. The tests that fail hardcode the sets of nodes that form 3-components and 4-components. With the new dictionary implementation these nodes have a different labels but the components found by the algorithms have the correct node connectivity value.  I'll prepare a PR against your branch shortly.
comment
Aric, I made a pull request against your branch with test fixes for k-components tests and node connectivity approximation doctests. See:  https://github.com/hagberg/networkx/pull/5
comment
Good points @jfinkels , I follow your suggestions for k-components tests in a new pull request against Aric's branch: https://github.com/hagberg/networkx/pull/6  Regarding turning off doctests by default, I think one of the nice features of NetworkX is it's up to date documentation. We risk missing outdated doctests if we do not test them by default. Given that there are only 12 failing doctests (and there are hundreds in total) would using `# doctest: +SKIP` be an option?  
comment
I agree, adding `+SKIP` to some doctests is not optimal, testing doctests for only some python versions is not much better IMO but I guess is the price to pay to support a wide variety of Python versions. I'm not sure how to handle that in Travis-ci but I guess it should be possible.
comment
Another downside of only testing docstrings for some python versions in our Travis-ci instance is that if some distributor (e.g debian) ships NetworkX for a python version for which we do not test docstrings, the tests will fail for them.
comment
**Test results for commit 96c0bd1 merged into master** Platform: linux2 - python2.6: OK - python2.7: OK - python3.2: Failed, log at https://gist.github.com/3285149  Not available for testing:  
comment
**Test results for commit 92709b4 merged into master** Platform: linux2 - python2.6: OK - python2.7: OK - python3.2: OK 
comment
Alejandro,  Thanks for the quick response. I'm adapting an IPython script for testing NetworkX pull requests, while testing yours noticed the error and wanted to test that the gist part works. It's very neat and will be very useful.  I'm preparing a pull request with the adapted script.  2012/8/7 Alejandro Weinstein notifications@github.com  > @jtorrents https://github.com/jtorrents I removed the print statement > that caused the python 3.2 failure in 92709b4https://github.com/networkx/networkx/commit/92709b4(the > print shouldn't be there in the first place). >  > â€” > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/741#issuecomment-7557509. 
comment
Yes I think there is a bug in `_is_connected_by_alternating_path`, concretely in the inner function `_alternating_dfs` that checks if unmatched nodes are linked to other nodes by alternating paths. In this concrete example the unmatched node of the left node set is 2, which is linked by alternating paths to nodes 0 and 3, but the current implementation fails to report them and thus the vertex cover reported (`{0, 1}`) is incorrect.  It's not obvious to me why the inner function `_alternating_dfs` fails in this example. A possible solution could be to reimplement this inner function iteratively instead of recursively, like this:  ```python     def _alternating_dfs(u, along_matched=True):         if along_matched:             edges = cycle([matched_edges, unmatched_edges])         else:             edges = cycle([unmatched_edges, matched_edges])         visited = set()         stack = [(u, iter(G[u]), next(edges))]         while stack:             parent, children, valid_edges = stack[-1]             try:                 child = next(children)                 if child not in visited:                     if (parent, child) in valid_edges or (child, parent) in valid_edges:                         if child in targets:                             return True                         visited.add(child)                         stack.append((child, iter(G[child]), next(edges)))             except StopIteration:                 stack.pop()         return False ```   I've checked this example with this approach and it works as it reports a correct vertex cover (`{1, 3}`). I can prepare a pull request if this approach looks good.  I'll also merge #2375 now as it has already been reviewed and would generate conflicts with a PR for fixing this bug.  
comment
Done! See #2386.
comment
I think that the problem is that when the input graph is disconnected (as is @enj example), then two correct coloring are equally correct (see https://networkx.github.io/documentation/development/reference/algorithms.bipartite.html for more details), and `bipartite.sets` finds one or the other depending on the iteration order of the nodes in the graph (which depends on python version, and in python 3 is randomized).  In the bipartite package, many functions require a container of nodes with all nodes in one node set as argument to deal with this ambiguity. And the recommended practice is to use a node attribute named `bipartite` to keep track of which node set a node belongs.  Maybe the bipartite matching functions should also require one of the two node sets as argument. That will also allow us to skip a `bipartite.sets` call which will improve the running time for relatively large graphs. 
comment
Hello @Kdezaphi, thanks for the report. This problem is fixed in the development version of NetworkX:  ```python In [1]: import networkx as nx In [2]: nx.__version__ Out[2]: '2.0.dev_20161124154931' In [3]: G = nx.Graph() In [4]: G.add_edge(0, 1) In [5]: pos = nx.spring_layout(G, pos = {0:(0.5,0.5)}, fixed = [0]) In [6]: pos Out[6]: {0: array([ 0.5,  0.5]), 1: array([ 0.28741035,  0.21750108])} ```  Note that in the development version (2.0) `G.nodes()` returns a generator not a list, so you can't do `G.nodes()[0]`. If you can I'd recommend to use the last development version from github.
comment
Indeed this is a left over of the early migration to Github (circa 2012). That code was adapted from ipython but it's now obsolete (actually has been obsolete and unused for quite some time). I've created a pull request that removes the whole thing.   
comment
According to numpy documentation we should use `np.array`:  https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html#array-or-matrix-which-should-i-use  I think that `np.matrix` is mainly legacy code. 
comment
This was fixed in #1333  
comment
This was fixed in #1333  
comment
Indeed this is a bug. Thanks for the report, I'll look into it. 
comment
Good point @joelmiller . At a first sight it seemed a bug, but I think you are right that this is a corner case for weakly connected directed graphs. The materials on which we based the implementation were only concerned with undirected graphs (see the docstrings for the references). Generalizing it to directed graphs seemed straightforward but there are always hairy corner cases.  So we have to decide between two options: either consider all weakly connected digraphs as having edge connectivity 0, and thus no edge cut should exist (close to the current not documented behaviour and I'd say the correct one theoretically; we'd only need to change the test for weakly connected to strongly connected as @joelmiller points out). Or, when a weakly connected digraph is passed to `minimum_{edge|node}_cut` return a edge|node cut that when removed disconnects the digraph (ie makes it not weakly connected). This may not be theoretically accurate but might be useful in practice (and it's easy to implement).  I'm all for `practicality beats purity` but we should make sure that it's actually useful in practice. @litaotju can you elaborate a bit on your use case for this feature? In any case we should improve the documentation explaining the behaviour in these corner cases. 
comment
This is fixed in #1925 which is already merged 
comment
There is no collision because the functions in the approximation package are not imported in the main namespace. In order to use the approximation functions you have to explicitly import them:  ``` python In [1]: import networkx as nx  In [2]: from networkx.algorithms import approximation as apxa  In [3]: G = nx.karate_club_graph()  In [4]: nx.average_clustering(G) Out[4]: 0.5706384782076823  In [5]: apxa.average_clustering(G) Out[5]: 0.582 ``` 
comment
For other functions in the approximation package there is an example of usage in the docstrings, but not for `average_clustering` and maybe others. We should fix that. 
comment
Sorry for the late reply, and thanks for looking at this! Indeed this is a bug. I think I've fixed it in the PR referenced above. Instead of trying to handle these corner cases in the implementation of Kanevsky's algorithm, the PR handles them separately. See #1976 for details. With that PR merged:  ``` python In [6]: G = nx.cycle_graph(4)  In [7]: list(nx.all_node_cuts(G)) Out[7]: [{0, 2}, {1, 3}]  In [8]: list(nx.all_node_cuts(G)) Out[8]: [{0, 2}, {1, 3}]  In [9]: G = nx.cycle_graph(6)  In [10]: list(nx.all_node_cuts(G)) Out[10]: [{0, 2}, {0, 3}, {0, 4}, {1, 3}, {1, 4}, {1, 5}, {2, 4}, {2, 5}, {3, 5}]  In [11]: G = nx.complete_graph(5)  In [12]: list(nx.all_node_cuts(G)) Out[12]: [{0, 1, 2, 3}, {0, 1, 2, 4}, {0, 1, 3, 4}, {0, 2, 3, 4}, {1, 2, 3, 4}]  In [13]: G = nx.complete_graph(6)  In [14]: list(nx.all_node_cuts(G)) Out[14]:  [{0, 1, 2, 3, 4},  {0, 1, 2, 3, 5},  {0, 1, 2, 4, 5},  {0, 1, 3, 4, 5},  {0, 2, 3, 4, 5},  {1, 2, 3, 4, 5}] ``` 
comment
Sorry for the late reply. Indeed the documentation for `minimum_st_edge_cut` is confusing. The part that @duhmojo cites is a left over from the last refactor of connectivity. I fixed it in #1977  Note that no functions in `networkx.algorithms.connectivity.cuts.py` consider edge weights, they compute cuts in terms of number of nodes or edges. I hope is clear now in the documentation. Regarding the more general `minimum_cut`, note that the documentation suggests a nice way to compute the edges between the two node sets in the partition that it returns:  ``` python cut_value, partition = nx.minimum_cut(G, 'x', 'y') reachable, non_reachable = partition cutset = set() for u, nbrs in ((n, G[n]) for n in reachable):     cutset.update((u, v) for v in nbrs if v in non_reachable) ``` 
comment
Also, there are some name collisions (eg `node_connectivity`), the tests pass because the exact version is imported after the approximation. I agree we should document this. Maybe as a docstring at the top of `networkx/algorithms/approximation/__init__.py`? so it will show up here:  https://networkx.github.io/documentation/development/reference/algorithms.approximation.html 
comment
Very nice to see so much interest and cool ideas for projects! I'm very busy for the next months and will not be able to mentor, but I'll try to help as much as possibe.  
comment
I'm still catching up with all the discussion on 2.0 API, but I think it might be useful to put together a document that mirrors more or less Python's [PEPs](https://www.python.org/dev/peps/) . If we have a document like this it will be a lot easier to ask people that do not follow closely NetworkX development for feedback about API design. I'm thinking that would be very useful for us to have feedback from people such as Raymond Hettinger, David Eppstein, and others. It might be a lot easier for them to give feedback if we have everything explained in a single document.  Also, it is useful to look at previous discussions about a python Graph API, see:  https://wiki.python.org/moin/PythonGraphApi  I find especially useful this thread of python-dev (referenced in the above wiki page):  http://groups-beta.google.com/group/comp.lang.python/browse_thread/thread/cbca60cb36be39ed/313113af1aa077af 
comment
I think it's a good idea to make a release with the changes that we have now, as Dan says, we already have a lot of changes and new features.  @MridulS , I think that removing the `*_iter` functions should wait until 2.0 as it's an important change of the API, and can potentially break user code.  
comment
I did not think much about this, but I'd prefer to be very explicit about add-ons functionality. This will force users to think in advance how to use the addons in their code. We could require to import the addon and use that namespace to call add-on functions. Something like:  ``` python import networkx as nx from networkx.external.addons import lemon  G = build_a_graph_with_appropiate_attrs() result_nx = nx.network_simplex(G, demand='demand', capacity='capacity', weight='weight') result_lemon = lemon.network_simplex(G, demand='demand', capacity='capacity', weight='weight') assert result_nx == result_lemon ```  This will reduce the 'magic' for the user and the development effort on NetworkX side. I think the one of the main points of having addons is to allow us to use NetworkX API for interacting with other software (as much as possible, eg an addon could accept arguments that will be ignored if not applicable in some functions, ...) and also return results following NetworkX conventions. I even think that, for the users, the standarization of the results returned will be the most useful feature. I, as a user, would really love to be able to get a nested pair of dictionaries with the node connectivity between all pairs of nodes really fast :) 
comment
@SanketDG, sorry to hear that. You are very welcomed to contribute anyway. 
comment
I agree that we should go ahead and enable `networkx.addons` for 1.10. Maybe we could say in the release notes that this is an experimental feature and might be subject to change in the future.  
comment
@hagberg in #1660 I cherry-picked the two commits that fixed the sporadic test failures on Travis for k-components approximation, and tagged the branch `v1.10` as release (by setting `dev=False` in `release.py`).   I think that before releasing we still need to decide how to approach the `addons` feature in v1.10. Is that right @ysitu @OrkoHunter ? I didn't follow closely the discussion on that. 
comment
@OrkoHunter , sounds good. Then I think we are ready to release 1.10 once #1660 is merged into the `v1.10` branch. 
comment
@MridulS @MisterSheik it seems that the test that is failing this time is a different one. It's also based on a random graph (a shell graph in this case). Even though we are not able to reproduce it locally, it seems to fail sporadically on Travis. Because the approximation algorithm does not guarantee an exact solution, I think it's better to remove the tests based on random graphs for this approximation algorithm. I did that in #1658    
comment
@MisterSheik I just restarted the failing build and now passes (these unreproducible and sporadic failures are really annoying). I think it's not necessary for you to rebase. The failures are addressed in #1658 and are not related to your work in this PR. 
comment
@MisterSheik git can be intimidating. Don't worry about rebasing and squashing commits now. Just keep adding changes to your branch until you address all @hagberg comments, and you are happy with the result. I'll keep an eye in this PR, if any failure related to #1658 pops up again I'll deal with that by restarting the build.  When the work on this PR is finished, then you can squash all the commits to keep the history clean. You can take a look at the documentation on git development workflow. There we have a good explanation of what is a rebase and how to do it in the context of NetworkX development:  http://networkx.readthedocs.org/en/latest/developer/gitwash/development_workflow.html 
comment
@MisterSheik I see that you did some experiments. It's painful but it's the best way to learn git ;) Don't worry, PRs are cheap. I would recommend to just start a new PR with the last version of your work on topological sort. 
comment
Hmmm, I cannot reproduce this locally. I restarted the failing build and now it passes. When I was working on the `k_components` this specific test (`test_configuration`) failed at least once in Travis (I was never able to reproduce in my machine, and at Travis passes most of the time).  Any idea on how to debug this further?  Maybe the best approach is to just remove this test.  
comment
Not before, but I just did. 
comment
I'm testing with different values of `PYTHONHASHSEED` to make sure that this error is not due to iteration order. No failures so far. I'm testing it using this approach:  ``` bash for i in {100000..1000000..50000}; do export PYTHONHASHSEED=$i; echo PYTHONHASHSEED=$i; PYTHONHASHSEED=$i nosetests3 -v networkx/algorithms/approximation/tests/test_kcomponents.py; done ```  I'm trying different ranges, but the number of possible hash seeds is huge. 
comment
Fixed in #1639 and #1658  
comment
@OrkoHunter I just created the repository. Let me know if something needs adjusting.  
comment
@ammhatre look at https://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.components.strongly_connected.strongly_connected_components.html 
comment
Great! I like this new implementation better.   Regarding your comment about parallelizing `node_redundancy`, I think we should explore this more. There are several places where we deal sequentially with embarrassingly parallel problems. Also more sophisticated problems can be solved using parallelization, for instance @bjedwards made a nice example for [parallel betweenness](https://github.com/networkx/networkx/blob/master/examples/advanced/parallel_betweenness.py).  
comment
Yes, it seems that experimenting with parallel approaches will likely require use of external libraries, or a non trivial custom solution (probably not pure python). This could be a good candidate for an add-on. Regarding external libraries, this seems interesting :  https://pythonhosted.org/joblib/parallel.html  But I have very little idea of parallelization technology and didn't look at it closely. In any case, it's a very good idea to put TODOs for easy targets. 
comment
@jfinkels very nice implementation. I have a comment on the interface. I'm not sure that is a good idea to have a `create_using` parameter (which, by the way, is a pretty ugly interface in general). If an user uses as argument a directed graph, the directions of the edges between blocks will depend on the number of blocks and on the iteration order of `itertools.combinations`.   Also the comment just above the combinations loop says that we can use combinations because we consider the multipartite graph undirected. I think this is a good approach, it's just a matter to remove the `create_using` parameter and return always an undirected graph. What do you think?  Finally, could you rebase and solve the trivial conflict with `api_2.0.rst`? 
comment
Yes, we discussed removing it altogether some time ago. If I remember correctly the discussion, the alternative in most cases could simply be a boolean parameter `directed` as in `nx.gnp_random_graph`.  The changes look good to me. Let's see if someone else wants to chime in about removing the `create_using` parameter. If not I'll merge it in the following days. 
comment
@theosotr this is very nice! Could you add the `johnson` function to the documentation at `doc/source/reference/algorithms.shortest_paths.rst`? Also, could you add a note at `doc/source/reference/api_2.0.rst` about this new feature (we'll rename this file to `api_1.10.rst`). I'm changing the milestone to 1.10 because it seems that this is ready to merge. @jfinkels @harrymvr do you agree? 
comment
Also, I would add in each docstring of the bipartite generator functions a sentence in the `Notes` section that the function is not imported to the main namespace and that the user has to import the bipartite package explicitly to use them. 
comment
I would write something like:  `This function is not imported in the main namespace. To use it you have to explicitly import the bipartite package.`  This sentence is not necessary for `complete_bipartite_graph`. 
comment
@SanketDG , yes for the moment add it under new freatures. I'd like to do more changes to the bipartite package shortly (refactor projection and maybe add a new clustering algorithm) then we can put all changes in the bipartite package together. 
comment
And do not rebase while we are still making changes. You can do it at the end, but I think it's not necessary. 
comment
Do not worry, this is the best way to learn (even if painfull). You shoud take a look at the development workflow documentation:  http://networkx.github.io/documentation/development/developer/gitwash/development_workflow.html 
comment
I think that after the changes outlined above we are ready to merge this. 
comment
Good job @SanketDG ! 
comment
Thanks for the report. I think that #1379 fixes it.  
comment
Wow. I missed this. It's really great @hagberg ! I think that the only thing that should be changed is the name of the node attribute that indicates the node set. It's `part` and should be `bipartite` for consistency. 
comment
Looks good to me. I think is ready to merge. 
comment
Looks good. The old approach is unmaintainable. I've send you a PR adding a link to my github page in the credits list. I think it would be good to list separately developers and contributors in the credits. The former would be the ones who are at the github NetworkX organization. Which we should probably update. I think is ok to keep people if they are not active at the moment, but we should try to add more people: it's an easy way to make people feel part of the project and contribute more ;) 
comment
You also have to update the documentation at `doc/source/reference/algorithms.dag.rst`. 
comment
I still think more test would be good. I'm not very familiar with this, so I cannot think of a canonical example that we could use as a test. Let's see if someone else has comments or ideas on examples to test. 
comment
@MridulS , looks good to me. If @ysitu gives the OK we can merge this. You should also update `api_2.0.rst` with the additions.  When this is merged I'll make a new PR to use the exposed functions in `all_node_cuts` and to add it to `api_2.0.rst`. 
comment
Hmmm, the error in `pypy` is unrelated to this, but related to `all_node_cuts`. I'll investigate this. 
comment
@MridulS , can you rebase and solve the conflicts in `api_2.0.rst`?  Regarding `pypy3` error in `test_kcutsets.test_configuration`, I cannot reproduce it neither in my local machine nor in other travis jobs (see for instance https://travis-ci.org/networkx/networkx/jobs/54897818 ). Let's see if it pops up again in other travis jobs. 
comment
@MridulS, note that `antichains` tests are still fragile. The output order of antichains (and the order of nodes in each antichain) can change if the topological sort of the transitive clousure is not unique (ie. there is no Hamiltonian path in `TC`).  You could use a helper function to check  for correctness, something like this (untested):  ``` python def _check_antichains(solution, result):     solution = [set(a) for a in solution]     for antichain in result:         assert_true(set(antichain) in solution) ``` 
comment
@MridulS NetworkX nodes only have to be hashable, there is no guarantee that nodes can be sorted. 
comment
Looks good @MridulS. Good job! I think we can merge when Travis finishes (coveralls is too sensitive).  
comment
@MridulS , sorry not enough time to look close al those, but it seems they are covered. You have to be patient and give people time to look at things. We are all very busy and everybody contributes acording to their time and energy availability.  Regarding pep8 and formating continuation lines, I like what this guy says:  https://www.youtube.com/watch?v=x-kB2o8sd5c 
comment
Indeed `ford_fulkerson` is still in 1.9.1. In any case, note that this same algorithm is now implemented at the function [edmonds_karp](https://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.flow.edmonds_karp.html). It's part of the new NetworkX flow API and the output will be a bit different, but the new API is much more powerful (eg you can use different flow algorithms by passign a function as an argument to `nx.maximum_flow` and friends). And for some problems (especially repeated computatcions over different pairs of nodes on the same graph), the new implementation is much, much faster. 
comment
Hi @JBPressac, the function `bipartite.overlap_weighted_projected_graph` returns a unipartite (or one-mode) graph from an input bipartite graph. Bipartite projection consists in creating a graph where each pair of nodes of one node set is linked with an edge if they share one or more common neighbors in the original bipartite graph. The jaccard coefficient here is a way to assign weights to the projected edges (number of nodes in the intersection of their neighborhood in the original bipartite graph divided by the number of nodes in the union). Thus, the diagonal of the adjacency matrix of the projected graph (which refers to self-loops) should not be in play. 
comment
@JBPressac, I'm a sociologist myself, but I think that the naming "projection" or "projected graph" and variants have ended up being the most common way to refer to one mode projections of bipartite graphs, even in the field of sociology. Co-affiliation naming comes from the tradition of analyzing the duality of persons and groups, which is one of the earliest (and coolest) uses of bipartite networks, but they are more general and can be used to model a wide range of things. Thus I think that we should keep the more general (and more common) names. 
comment
That would be great. But maybe something less ambitious would also be useful. Most of the performance tests that we do are based on synthetic ad-hoc graphs, it would be great to have a collection of real world interesting networks (from different fields) to be able to test performance of at least some algorithms. Putting together the networks and the glue code is the easy part I think, but we would need a machine to run the performance tests: some (most?) of them would take hours. 
comment
fixing not properly closed issues from trac migration 
comment
Great work @ysitu! This is very, very nice. I've tagged this as 1.9 because it seems that it's ready for merging, but I didn't look at it closely. Any more comments? 
comment
Nice! I think that the code for generic shortest paths and kosaraju SCCs is clearer using the `reversed` context manager. +1 for merging this. 
comment
I'm not familiar with this algorithm, but this all looks great @ysitu! I think it is not necessary to expose the residual network in this case if the needs of the algorithm make it difficult to make it interchangeable. I guess that the best course of action is merge this first (let's wait a bit for if other people has comments), and then rebase #1102. Do you agree @ysitu? 
comment
@ysitu, I agree we can handle the two PR separately.   @chebee7i yes, I think so. 
comment
> should we get in the habit of updating the release notes in major pull requests? It might cause slightly more conflicts when merging, but they are easily resolved.  Yes, I think it's a good idea. For this release, I'll try to put together a draft for the changes in flow and connectivity packages shortly (I'll post them here), so we can discuss them and make sure that we do not miss anything (there have been a lot of changes, quite a few backwards incompatible). 
comment
Here goes a first draft for 1.9 release notes and api changes for the flow package:  ## Flow package  Complete rewrite of the flow package and new interface to flow algorithms, with backwards incompatible changes. If you had code that was using any of the flow related functions, it will not work unmodified in 1.9. But, trust us, it is worth it. The main changes are: 1. We added two new maximum flow algorithms (`preflow_push` and `shortest_augmenting_path`) and rewrote the Edmonds-Karp algorithm in `flow_fulkerson` which is now at `edmonds_karp`. The legacy Edmonds-Karp algorithm implementation in `ford_fulkerson` is still available but will be removed in the next release. All these functions output a residual network after computing the maximum flow. See `maximum_flow` documentation for the details on the conventions that NetworkX uses for defining a residual network. 2. We removed the old `max_flow` and `min_cut` functions. The main interface to flow algorithms are now the functions `maximum_flow`, `maximum_flow_value` and `minimum_cut` and `minimum_cut_value`, which have new parameters: `flow_func` for defining the algorithm that will do the actual computation (it accepts a function as argument that implements a maximum flow algorithm), `cutoff` for defining a maximum value after which the algorithm stops, and `residual` that accepts as argument a residual network to be reused in maximum flow computations. 3. See `ford_fulkerson` documentation for details on how to obtain the same output than in 1.8.1 using the new NetworkX interface to flow algorithms.  What do you think @ysitu? 
comment
@chebee7i, I think we can add a samll guide based on what is on the legacy `ford_fulkerson` docstrings. It could be something like this:  ``` python import networkx as nx G = nx.icosahedral_graph() nx.set_edge_attributes(G, 'capacity', dict(((u, v), 1) for u, v in G.edges_iter())) ```  In NetworkX 1.8.1:  ``` python flow_value = nx.max_flow(G, 0, 6) cut_value = nx.min_cut(G, 0, 6) flow_value == flow_cut flow_value, flow_dict = nx.ford_fulkerson(G, 0, 6) ```  In NetworkX 1.9:  ``` python flow_value = nx.maximum_flow_value(G, 0, 6) cut_value = nx.minimum_cut_value(G, 0, 6) flow_value == flow_cut # Legacy flow_value, flow_dict = nx.maximum_flow(G, 0, 6, flow_func=nx.ford_fulkerson) # Recommended algorithms: flow_value, flow_dict = nx.maximum_flow(G, 0, 6, flow_func=nx.preflow_push) # This is the default and the same than: flow_value, flow_dict = nx.maximum_flow(G, 0, 6) # other algorithms flow_value, flow_dict = nx.maximum_flow(G, 0, 6, flow_func=nx.shortest_augmenting_path) flow_value, flow_dict = nx.maximum_flow(G, 0, 6, flow_func=nx.edmonds_karp) ``` 
comment
@ysitu, I think that the discussion was at a point of the refactoring of the flow package were we had a lot of redundant functions `{value|flow|residual}` for each algorithm. And after that we did not discuss the issue again. Now the flow functions are imported to the base namespace, but we can change that. I think that most users will only need to use the interface functions (`maximum_flow`, `minimum_cut` and friends), but it is handy to have the flow functions easily available. Do you think it is worth to remove them from the base namespace? 
comment
Here goes a more complete draft for the 1.9 release notes and api changes for the flow package. It already assumes that flow functions are not imported to the base namespace. It's maybe too long, but there are a lot of things to explain:  ## Flow package  Complete rewrite of the flow package and new interface to flow algorithms, with backwards incompatible changes. If you had code that was using any of the flow related functions, it will not work unmodified in 1.9. But, trust us, it is worth it. The main changes are: 1. We added two new maximum flow algorithms (`preflow_push` and `shortest_augmenting_path`) and rewrote the Edmonds-Karp algorithm in `flow_fulkerson` which is now at `edmonds_karp`. @ysitu contributed the very nice implementations of all new maximum flow algorithms [@ysitu do you want your full/real name here? I think you can be proud of your work]. The legacy Edmonds-Karp algorithm implementation in `ford_fulkerson` is still available but will be removed in the next release.  2. All maximum flow algorithm implementations (including the legacy `ford_fulkerson`) output now a residual network (ie a NetworkX DiGraph) after computing the maximum flow. See `maximum_flow` documentation for the details on the conventions that NetworkX uses for defining a residual network. 3. We removed the old `max_flow` and `min_cut` functions. The main interface to flow algorithms are now the functions `maximum_flow`, `maximum_flow_value` and `minimum_cut` and `minimum_cut_value`, which have new parameters that define NetworkX interface to flow algorithms: `flow_func` for defining the algorithm that will do the actual computation (it accepts a function as argument that implements a maximum flow algorithm), `cutoff` for defining a maximum value after which the algorithm stops, `value_only` for stopping the computation as soon as we have the value of the flow, and `residual` that accepts as argument a residual network to be reused in maximum flow computations. 4. All algorithms accept arguments for these parameters, but not all of them can actually act according to them. For instance, `preflow_push` algorithm can stop after the `preflow` phase if we only need the value of the flow, but both `edmonds_karp` and `shortest_augmenting_path` will need to finish for obtaining the flow value. Thus, parameters not applicable to one algorithm will be accepted but ignored. 5. The new function `minimum_cut` returns the cut value and the actual node partition that defines the minimum cut. The function `minimum_cut_value` returns only the value of the cut, which is what the removed `min_cut` function used to return before 1.9. 6. The functions that implement flow algorithms (ie `preflow_push`, `edmonds_karp`, `shortest_augmenting_path`, and `ford_fulkerson`) are not imported to the base NetworkX namespace. You have to explicitly import them from the flow package:  ``` python from networkx.algorithms.flow import (ford_fulkerson, preflow_push,          edmonds_karp, shortest_augmenting_path) ``` 1. Also added a capacity scaling minimum cost flow algorithm: `capacity_scaling`. It supports `MultiDiGraphs` and disconnected networks.  2. Small examples illustrating how to obtain the same output than in NetworkX 1.8.1 using the new interface to flow algorithms introduced in 1.9:  ``` python import networkx as nx G = nx.icosahedral_graph() nx.set_edge_attributes(G, 'capacity', 1) ```  In NetworkX 1.8.1:  ``` python flow_value = nx.max_flow(G, 0, 6) cut_value = nx.min_cut(G, 0, 6) flow_value == flow_cut flow_value, flow_dict = nx.ford_fulkerson(G, 0, 6) ```  In NetworkX 1.9:  ``` python from networkx.algorithms.flow import (ford_fulkerson, preflow_push,          edmonds_karp, shortest_augmenting_path) flow_value = nx.maximum_flow_value(G, 0, 6) cut_value = nx.minimum_cut_value(G, 0, 6) flow_value == flow_cut # Legacy: this returns the exact same output than `ford_fulkerson` in 1.8.1 flow_value, flow_dict = nx.maximum_flow(G, 0, 6, flow_func=ford_fulkerson) # We strongly recommended to use the new algorithms: flow_value, flow_dict = nx.maximum_flow(G, 0, 6) # If no flow_func is passed as argument, the default flow_func (preflow-push) is used.  # Therefore this is the same than: flow_value, flow_dict = nx.maximum_flow(G, 0, 6, flow_func=preflow_push) # You can also use alternative maximum flow algorithms: flow_value, flow_dict = nx.maximum_flow(G, 0, 6, flow_func=shortest_augmenting_path) flow_value, flow_dict = nx.maximum_flow(G, 0, 6, flow_func=edmonds_karp) ``` 
comment
I just started to look into removing them from the base namespace, the simplest approach leads to `module object is not callable` errors because the module names and the function names are the same. Is there another workaround/proper fix than changing the names of the files in the flow package? 
comment
@chebee7i, sorry I was not clear. As @ysitu points out, the error `module object is not callable` was in the doctests related to import statements such as `from networkx.algorithms.flow import preflow_push`: they were picking the module instead of the function and thus doctest failed. In #1173 I've removed the flow functions from the base namespace, and renamed files in the flow package so we obey the module-name-must-be-unique rule and get rid of the doctest failures. 
comment
Yes, seems ready to me.  2014-06-18 18:19 GMT+02:00 chebee7i notifications@github.com:  > Yeah seems good to go! >  > ##  >  > Reply to this email directly or view it on GitHub > https://github.com/networkx/networkx/issues/1162#issuecomment-46458244. 
comment
This is brilliant @ysitu! I'll look at it more closely and benchmark your very nice implementation of the two-phase method in connectivity and cut functions (I might not have time until the weekend). In general, I think that this is the correct approach to provide an interface to flow algorithms, but as discussed in #1096 we still have to figure out all the details. However I think this can be merged as soon as we figure out the refactoring error. It will be easier to play with different interfaces to `max_flow` and friends with this in master. Tanks for this PR, and for your work @ysitu, in a matter of days you've contributed a ton of high quality code. Keep up the good work!  Regarding the error that you get refactoring, I think that you need to `form utils import *` instead of the full `from networkx.algorithms.flow.utils import *`.  Salut! 
comment
Mmmm, you are right. I said that without actually testing it. I just tested It and it is indeed quite strange, maybe @chebee7i or @dschult can take a look and help us figure out what is going on here. 
comment
Ok, I think I've got it. The problem seems to be with naming the new file `utils.py`, if you change its name to `flow_utils.py` or whatever, it works (I've also had to add `__all__` to `flow_utils.py`). I didn't know that we could have this kind of name collision. 
comment
Thanks for looking at this Chris. @ysitu solution looks good to me. I think this is ready for merging. Chris, do you agree? 
comment
@ysitu, I've made a pull request against your branch with a draft for the release notes of the connectivity package. It's maybe too verbose, feel free to edit as you see fit. 
comment
Hi ysitu,  This is great! Thank you very much for your work. The Travis failures on your PR are because you did not add the actual `*.gpickle.bz2` files into the test folder.   It is very handy that you refactored your code to allow retrieving the residual network for minimum cut and friends. Given the benchmarks that you posted, I think we should use this implementation as the default in the `connectivity` module. I'll look into that when this PR is merged.  Salut! 
comment
Ysitu,  You are right, the files are included. I've only checked the Travis log and saw a "File not found error". You also have to edit `setup.py` to include the bz2 files:  ``` diff --git a/setup.py b/setup.py index 610026e..4254a53 100644 --- a/setup.py +++ b/setup.py @@ -95,7 +95,7 @@ package_data     = {      'networkx.algorithms.community': ['tests/*.py'],      'networkx.algorithms.components': ['tests/*.py'],      'networkx.algorithms.connectivity': ['tests/*.py'], -    'networkx.algorithms.flow': ['tests/*.py'], +    'networkx.algorithms.flow': ['tests/*.py', 'tests/*.bz2'],      'networkx.algorithms.traversal': ['tests/*.py'],      'networkx.algorithms.isomorphism': ['tests/*.py','tests/*.*99'],      'networkx.algorithms.link_analysis': ['tests/*.py'], ``` 
comment
Ysitu,  This is one of the best PR that we had at NetworkX, thank you again for this. Regarding the documentation, I've built it in my system and found a small issue; there are 3 typos in each docstring that prevent correct rendering. I'm not sure how to properly do a pull request on your pull request, so I add the patch below. The changes are minimal, it will probably be faster if you apply them by hand.  Salut!  ``` diff --git a/networkx/algorithms/flow/preflow_push.py b/networkx/algorithms/flow/preflow_push.py index 85513d4..8bc15a4 100644 --- a/networkx/algorithms/flow/preflow_push.py +++ b/networkx/algorithms/flow/preflow_push.py @@ -397,7 +397,7 @@ def preflow_push(G, s, t, capacity='capacity', global_relabel_freq=1):      """Find a maximum single-commodity flow using the highest-label      preflow-push algorithm.  -    This algorithm has a running time of `O(n^2 m^(1/2))` for `n` nodes and +    This algorithm has a running time of `O(n^2 m^{(1/2)})` for `n` nodes and      `m` edges.   @@ -414,13 +414,13 @@ def preflow_push(G, s, t, capacity='capacity', global_relabel_freq=1):      t : node          Sink node for the flow.  -    capacity: string +    capacity : string          Edges of the graph G are expected to have an attribute capacity          that indicates how much flow the edge can support. If this          attribute is not present, the edge is considered to have          infinite capacity. Default value: 'capacity'.  -    global_relabel_freq: integer, float +    global_relabel_freq : integer, float          Relative frequency of applying the global relabeling heuristic to speed          up the algorithm. If it is None, the heuristic is disabled. Default          value: 1. @@ -470,7 +470,7 @@ def preflow_push_value(G, s, t, capacity='capacity', global_relabel_freq=1):      """Find a maximum single-commodity flow using the highest-label preflow-      push algorithm.  -    This algorithm has a running time of `O(n^2 m^(1/2))` for `n` nodes and +    This algorithm has a running time of `O(n^2 m^{(1/2)})` for `n` nodes and      `m` edges.   @@ -487,13 +487,13 @@ def preflow_push_value(G, s, t, capacity='capacity', global_relabel_freq=1):      t : node          Sink node for the flow.  -    capacity: string +    capacity : string          Edges of the graph G are expected to have an attribute capacity          that indicates how much flow the edge can support. If this          attribute is not present, the edge is considered to have          infinite capacity. Default value: 'capacity'.  -    global_relabel_freq: integer, float +    global_relabel_freq : integer, float          Relative frequency of applying the global relabeling heuristic to speed          up the algorithm. If it is None, the heuristic is disabled. Default          value: 1. @@ -539,7 +539,7 @@ def preflow_push_flow(G, s, t, capacity='capacity', global_relabel_freq=1):      """Find a maximum single-commodity flow using the highest-label preflow-      push algorithm.  -    This algorithm has a running time of `O(n^2 m^(1/2))` for `n` nodes and +    This algorithm has a running time of `O(n^2 m^{(1/2)})` for `n` nodes and      `m` edges.   @@ -556,13 +556,13 @@ def preflow_push_flow(G, s, t, capacity='capacity', global_relabel_freq=1):      t : node          Sink node for the flow.  -    capacity: string +    capacity : string          Edges of the graph G are expected to have an attribute capacity          that indicates how much flow the edge can support. If this          attribute is not present, the edge is considered to have          infinite capacity. Default value: 'capacity'.  -    global_relabel_freq: integer, float +    global_relabel_freq : integer, float          Relative frequency of applying the global relabeling heuristic to speed          up the algorithm. If it is None, the heuristic is disabled. Default          value: 1. ``` 
comment
Great! Now the doctrings render correctly. I think this is ready to merge. 
comment
Yes, very good idea! Also LGTM. 
comment
Looks good to me. The test failures are unrelated, so I'm merging it. 
comment
Wow! this is really beautiful! I'll look at it more closely, but in a first sight this all looks amazing (the tests, the code, the comments, the documentation). Thank you very much for this PR @ysitu, you rock!   By the way, speaking of speed, by working on #1095 I've found out that `preflow_push` algorithm is slower than the `ford_fulkerson` for connectivity algorithms. If you have time, take a look at it.  Salut! 
comment
I've look at it more closely, and I have a couple of comments. Regarding what is returned, maybe the edge cutset is more useful than the node partition? The former is smaller and this can make a difference in big graphs. Also I'd prefer functions that return only one thing, but I'm not sure if it is useful enough to have two separate functions for the cut set (or the partitions) and the value.   Also you need to add `stoer_wagner` function to the connectivity documentation. You can do something like [1](which renames `cut functions` to `flow based cut functions`).  Thank you very much for your work. This PR is really beautiful.  Salut! [1]  ``` jtorrents@saturn:~/projects/dev/networkx$ git diff doc/source/reference/algorithms.connectivity.rst diff --git a/doc/source/reference/algorithms.connectivity.rst b/doc/source/reference/algorithms.connectiv index 89108f1..9fa5790 100644 --- a/doc/source/reference/algorithms.connectivity.rst +++ b/doc/source/reference/algorithms.connectivity.rst @@ -5,8 +5,8 @@ Connectivity  .. automodule:: networkx.algorithms.connectivity   -Connectivity functions ----------------------- +Flow based Connectivity +-----------------------  .. automodule:: networkx.algorithms.connectivity.connectivity  .. autosummary::     :toctree: generated/ @@ -18,8 +18,8 @@ Connectivity functions     edge_connectivity     all_pairs_node_connectivity_matrix  -Cut functions -------------- +Flow based Cuts +---------------  .. automodule:: networkx.algorithms.connectivity.cuts  .. autosummary::     :toctree: generated/ @@ -29,3 +29,11 @@ Cut functions     minimum_st_edge_cut     minimum_edge_cut  +Stoer Wagner cut +---------------- +.. automodule:: networkx.algorithms.connectivity.stoer_wagner +.. autosummary:: +   :toctree: generated/ + +   stoer_wagner + ``` 
comment
> I find it better to return a node partition than than an edge set because: 1) that is how cuts are defined; 2) it is much easier to find the cutset from a node partition than the other way round; 3) the output has predictable size (== len(G)).  Well, I'm not quite convinced by 1 (because you can properly define a cut in terms of the cutset) and 3 (I don't see a problem with results not having predictable size), but 2 is clearly true. And since this is only a small and lateral issue, I think we can merge this as is if @hagberg agrees. Great work @ysitu ! 
comment
I think this is ready for merging (do you agree @ysitu?). Any more comments? 
comment
This is a first try on how to refactor the interface to flow algorithms for playing nice with the flow based connectivity and cut algorithms. The changes are: 1. Each maximum flow implementation is now in its own file in the flow directory. Each implementation provides at least 3 functions: `name_impl_value`, `name_impl_flow` and `name_impl_residual`. They return the value of the maximum flow, the flow dict, and the residual network. For backwards compatibility ther is a function `name_impl` that returns a tuple with the flow value and the flow dict. 2. The functions max_flow and min_cut accept now a parameter for the maximum flow value function to use, with a global default configuration. This is the same approach taken in #1095 for connectivity and cuts functions. 3. The only removed function is `ford_fulkerson_flow_and_auxiliary`.   I think that having access to the residual network can be useful in general and not only for internal use, at least for a fraction of NetworkX users. Maybe we should set up the flow package as the bipartite package, in which only a few functions are imported in the global namespace and if the user wants to access to all its functionality they have to explicitly import it.  What we import in the global namespace could be the sophisticated max_flow function that you suggest.   What do you think of this approach @ysitu ? I'd like to know the opinion of more people on this, especially @loicseguin and @hagberg  
comment
> It is necessary to unify the output formats of the functions. I think that there are established conventions for the flow value and the flow dictionary. For residual networks, ford_fulkerson and preflow_push use very different things, and ysitu/networkx@e938f80 follows the latter.  I agree that unifying formats of the functions is necessary. And by the way, your implementation of shortest augmenting paths looks great! I can't wait to use it in the connectivity algorithms!  > There needs to be only one function for each algorithm <algo_name>_impl which returns a residual network. max_flow, min_cut, etc. act as adapters that convert the residual network into a flow value, a flow dictionary or both. The precise protocol needs work because some algorithms such as preflow_push have turnable knobs. Some of such options have well-defined default values for specific scenarios, e.g., for min_cut, compute_flow should be False for preflow_push_impl.  I think we can deal with this by using the helper functions `<algo_name>_value` and `<algo_name>_residual`. For instance, in the commit above, `max_flow` and `min_cut` (and all `*_connectivity` functions) accept a function that only returns the value of the flow, in the case of `preflow_push_value` it already takes care of calling `proflow_push_impl` with the adequate parameters.  Same with the residual. For the cut functions to work, it is needed to remove all edges that exhausted their capacity (ie `d['capacity'] - d['flow'] == 0`). In the commit above, I've added this post-processing step to the `preflow_push_residual` function. With this approach we can relax the constrains on the actual implementation of algorithms in `<algo_name>_impl` and still have a coherent interface via the `*_value`, `*_flow` and `*_residual` functions.  > Why is there concern about backward compatibility of the *_impl functions now that preflow_push_impl is the only such function and is not in any release?  Well my concern about backward compatibility was in the changes that I did to `ford_fulkerson` in the commit above, in which I've followed the structure of your implementation and changed, among other things, the name of the function `ford_fulkerson_flow_and_auxiliary` to `ford_fulkerson_impl`.  > What are NetworkX's policies on backward compatibility and breaking changes?  This is difficult to say for me, maybe @hagberg can chime in, but I guess that the approach is always maintain backwards compatibility, unless the benefits of the changes clearly exceed the costs of breaking user code. Thus the bar is quite high. Really disruptive changes have to wait until nx 2.0 
comment
Regarding your proposal of a convention for residual networks. I like it and think that is better than the residual network used in `ford_fulkerson` (with its associated dictionary of infinite capacity flows) . But I don't think that it's necessary to refactor `ford_fulkerson` and firends to obey this convention. I'm not sure if you were actually arguing that. 
comment
> @jtorrents I asked about the policy on backward compatibility because you mentioned removing ford_fulkerson_flow_and_auxiliary, which was surprising given that it is public and documented. I do prefer that it be gone, but I agree that ford_fulkerson and friends are better left untouched until the 2.0 release because there are probably people relying on their current behaviors.  You are right, the commit jtorrents/networkx@f357efb renames `ford_fulkerson_flow_and_auxiliary`  to `ford_fulkerson_impl`, and thus is not backwards compatible. I have to admit that I'm especially motivated to get rid of that function because I feel guilty for having added it as a public function, and with that horrible name. The logic behind that bad decision was that users might find useful to have access to the residual network. In fact I still think that we should have some way for users to access the residual network through the public interface.  Anyway, as @hagberg says, we have to try to maintain backwards compatibility as much as possible. And I think that we can do that here, without scarifying the clarity and consistency of the conventions introduced by @ysitu, by assigning the old awful name to the actual function:   ``` ford_fulkerson_flow_and_auxiliary = ford_fulkerson_impl ```  I think that the big challenge here is to design a more sophisticated `max_flow` and `min_cut` functions, that while being backward compatible can also take advantage of the new flow algorithms. I'd also like to explore the possibility of importing only a small subset of the flow package to nx global namespace, and if the user wants to use all its functionality, then they will have to explicitly import it. This, of course, cannot be backwards compatible if done right, but we can start to take steps in this direction.  I'll try to put together a proper PR for this during the weekend (#1095 is too messy) and jtorrents/networkx@f357efb was mostly to be able to play with the new flow algorithms in connectivity and cut functions.  
comment
> If the user is to be allowed access to the residual network, then I suggest that the saturated edges not be removed because they also carry conveniently accessible information such as capacity (and cost if the model is extended to cover minimum cost flow as well). For internal use, the postprocessing step can be inserted before passing it off to connectivity algorithms, or perhaps better still, predicated views of graphs can be implemented to mask out such edges (cf. #762).  Yes you are right, if we allow public access to the residual network we should not mess with it for internal use. All post processing required for the connectivity and cuts should handled in `connectivity`. It would be really great to have the ability to "filter" or "restrict" nodes and edges in Graphs, and it would solve in a very elegant way the need of post processing the residual network for connectivity functions. However we do not have an implementation.  I tried to implement the monkey-patch context manager approach some time ago but I didn't succeed. I don't have to code of my attempt in front of me right now (I can try to search it and post it if it's useful), but if I recall it correctly, I found a problem when monkey-patching the `__getitem__` method of a Graph (for allowing access to neighbors using `G[u]`). I think that the problem was that `G[u]` gave me the unfiltered list of nodes but `G.__getitem__(u)` was giving me the filtered list.  It would be really great if you have time and energy to take a closer look at this. It would be really useful in many algorithms and a great addition to NetworkX. 
comment
> I meant my comments on backward compatibility to be a little more liberal especially related to newer code contributions. I don't think we should worry too much about changing interfaces that are obviously not what we want, confusing, or have some other issue.   Yes, I think this is a sensible approach. I'll try to work on this this weekend and put together a PR so we can discuss about something more concrete. By the way, @ysitu if you feel like working on the refactoring of flow interfaces (eg making a PR), please do so. Given that most of the code in the flow package is, as of right now, from you I think you have a lot to say on how their interfaces should look like. 
comment
Yes, I think that the new convention that you introduced for flow dicts is better. I  prefer this output:  ``` In [34]: G = nx.Graph() In [35]: G.add_path([1, 2, 3, 4], capacity=1) In [36]: nx.preflow_push(G, 1, 4) Out[36]: (1, {1: {2: 1}, 2: {1: 0, 3: 1}, 3: {2: 0, 4: 1}, 4: {3: 0}}) ```  Regarding the deprecation of `ford_fulkerson` and friends, I agree. Especially seeing that you already started working in an alternative implementation of the Edmonds-Karp algorithm (I like very much that it uses a bidirectional search for the shortest path). However, I think we should keep `ford_fulkerson` and friends at least until 2.0. I'd like to make some speed benchmarks to make sure that the new implementation is indeed faster, In #1102 I've adapted the tests to manage both the legacy and the new conventions for flow_dicts. We will have to also work on the documentation for explaining this. 
comment
With #1122 merged we can close this. 
comment
I'm closing this manually, it didn't work automatically in f0eb950f3d029c21af874ac6beaa9975c1124b25  
comment
I think that this pull request fixes the problems in `networkx/bipartite/tests/test_project.py`. I think it is reasonable to mostly use sortable nodes for our tests, so we can use the functions from `networkx.testing` to comfortably compare nodes and edges. 
comment
@Arfrever , thank you for keeping an eye on this. I also could reproduce the errors following your instructions. That was very useful. Given the precedents, I guess that some more of this kind of bug may pop up in the future. We'll squash them as they come.  Salut! 
comment
Hi Fred,  It think that an approximation for average clustering coefficient would be a nice addition to NetworkX. However this pull request needs quite a bit of work. A few comments: 1. First and foremost, you need to write some tests. No new function goes in without a nice set of tests. In this case, being an approximation, it is a bit harder to write good tests. The tests should also cover some corner cases to make sure that the function always gives meaningful results. For instance, what should happen if `numTrials > G.order()`. 2. Try to follow the python coding style defined at http://www.python.org/dev/peps/pep-0008  3. Variable names shouldn't use camelCase name styles; it also might be worth thinking of better and shorter names for variables. For instance `nodeConnections` could simply be `nbrs`. 4. You should not call `.keys()` for a list of neighbors, dictionaries are just fine because the default in python is iterating over its keys, and sample will also pick keys if you pass a dictionary instead of a list as the first parameter. Avoiding the conversion to a list we gain speed and lower memory requirements. 5. For testing if two nodes are neighbors you should use `u in G[v]` for the same reasons. Checking membership in a dictionary is (almost?) as fast as checking it in a set.   It will help you a lot if you read some code from NetworkX in order to get an idea of the conventions that we use. Check also how tests are organized.  Remember that if you push more commits to this branch, this pull request will be updated automatically.   Thanks for your work. Salut! 
comment
Hi Fred,  Sorry for the delay, I'm quite busy these days. I think that this pull request still needs work. Some comments: 1. You addressed the possibility that `numTrials < 0` but I was worried about the possibility that `numTrials > G.order()` . What should happen if an user asks for 1000 trials in a graph with 100 nodes? I'm not sure that is necessary to raise an error in this case, but we should think about this possibility. 2. According to pep-8 the maximum length of a line of code has to be 79 characters (if possible 72 for docstrings and comments). You have very long lines in your code. Also the comments in the code are too verbose for my taste and some variable names are too long (what about `triangles` instead of `triangles_found` ).  3. The documentation for the function is split between the module and the function docstring. It should be all in the function docstring. 4. Maybe the name of the function should be `approx_average_clustering` in order to be consistent with the naming conventions in `cluster.py`. I'm also wondering if it would be better to add this function to `cluster.py` instead of adding it to the `approximation` module. I think it would be better to have all clustering related function together. 5. The tests that you wrote are not well suited for inclusion because they print information that has to be interpreted by a human, you also are testing the speed of the approximation. This is interesting information, but the tests should be fully automated and should test only for correctness. Take a look at the tests for other clustering functions at `networkx/networkx/algorithms/tests/test_cluster.py`  6. Use the `nose.tools` functions for your tests. You do not have to return anything from test functions or methods, use only the `assert_*` functions of `nose.tools`. 7. Being an approximation is hard to test for correctness in the general case, but we can test for corner cases with well defined results. For instance, testing that complete graphs have average clustering of 1.0, that empty graphs have 0.0, that bipartite or grid or cubic graphs have also 0.0 average clustering, etc ...  8. In one of the tests that you wrote you are testing that the approximation has an error of less than 5%, does the paper assert that this is the margin of error that we should expect? Sorry didn't look at the paper closely.    Thanks for you work, and sorry again for the delay.  Jordi 
comment
Fred,  Sorry for the delay. I've created a new pull request based on your code at #934 , could you take a look at it? Thanks.  Salut! 
comment
Thanks for looking at it Fred. I'm closing this PR, we can follow the discussion in #934 . 
comment
I think that NetworkX implements correctly Freeman's definition of closeness centrality; it is defined as the inverse of `farness` which is the sum of its distances to all other nodes. As far as I can tell, this is also the approach implemented in other network analysis software packages, such as Igraph and Ucinet.   Regarding the problem of computing closeness centrality on disconnected graphs, Tore Opsahl proposes a different approach from what we currently have in NetworkX:  http://toreopsahl.com/2010/03/20/closeness-centrality-in-networks-with-disconnected-components/  But I'm not sure that it is better because it doesn't normalize by component size. 
comment
Thomas,  Sorry I looked at your issue too quickly and misunderstood your question. You are right, we report the closeness value for each node so higher value means higher centrality, and according to Freeman's definition it should be the other way around.  I think that it is more useful as we do it now (I've checked Igraph and they use the same approach). So I've updated the documentation to make this clear. Do you think that the clarification in the docstring is good enough? 
comment
I've added the formula for closeness centrality, a reference to Freeman's paper, and further clarified the docstring. 
comment
Ups, sorry I just merged the pull request #760 without checking this issue. I didn't see the new comment from @wschlauch so I reopen the issue. However, I think that that #760 fixes this issue because the problem was that when computing load_betweenness for one node in a digraph that was not strongly connected, NetworkX throwed an error if the node of interest was not in the dictionary returned by the _node_betweenness helper function. Only nodes reachable from source are in that dictionary.   The implementation consists in computing the shortest of each node in the graph to all other nodes and then computing which fraction go through the node of interest. 
comment
**Test results for commit cab2ce8 merged into master** Platform: linux2 - python2.6: OK - python2.7: OK - python3.2: OK  Not available for testing:  
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
fixing not properly closed issues from trac migration 
comment
I'm closing this issue because pull request #11 is now merged 
