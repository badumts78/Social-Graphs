issue
Add a from_node_dataframe function#TITLE_END#Leaving this issue here for newcomers who might want to contribute.  --------  @dschult to address the final point:  > What about having a function from_node_dataframe?  I like the idea! I think it could be left for another newcomer to contribute, since I think the pattern is similar:  ```python def from_node_dataframe(df):     G = nx.Graph()     for n, d in df.iterrows():         G.add_node(n, **d)     return G ```  Probably could be made more efficient, more flexible with custom funcs too, but I think it's worth fleshing out in a different PR, maybe with a design sketch via discussion on the issue tracker! I'll raise this up there.  _Originally posted by @ericmjl in https://github.com/networkx/networkx/pull/4217#issuecomment-700636540_  --------  For newcomers who would like to make a contribution, this could be a good one! You would have to flesh out the function a bit more, make it look like the sibling functions in `convert_matrix.py`, and figure out good test cases. 
issue
Structured labelled arrays#TITLE_END#This PR adds three functions allow a user to generate _labelled_ adjacency tensors and _labelled_ node feature matrices from a graph. 
issue
Remove numpy matrix from directed_combinatorial_laplacian_matrix#TITLE_END#`directed_combinatorial_laplacian_matrix` needs to be changed. The code block below is a possible implementation.  ```python     from scipy.sparse import spdiags, linalg      P = _transition_matrix(G, nodelist=nodelist, weight=weight,                            walk_type=walk_type, alpha=alpha)      n, m = P.shape      evals, evecs = linalg.eigs(P.T, k=1)     v = evecs.flatten().real     p = v / v.sum()     Phi = spdiags(p, [0], n, n)      return Phi - (Phi @ P + P.T @ Phi) / 2.0 ```  Underneath the hood, `_transition_matrix` is called, so that needs to be fixed too.  Possible implementation:  ```python     import scipy as sp     from scipy.sparse import identity, spdiags     if walk_type is None:         if nx.is_strongly_connected(G):             if nx.is_aperiodic(G):                 walk_type = "random"             else:                 walk_type = "lazy"         else:             walk_type = "pagerank"      M = nx.to_scipy_sparse_matrix(G, nodelist=nodelist, weight=weight,                                   dtype=float)     n, m = M.shape     if walk_type in ["random", "lazy"]:         DI = spdiags(1.0 / sp.array(M.sum(axis=1).flat), [0], n, n)         if walk_type == "random":             P = DI @ M     ####### watch out this line         else:             I = identity(n)             P = (I + DI @ M) / 2.0     ####### watch out this line      elif walk_type == "pagerank":         if not (0 < alpha < 1):             raise nx.NetworkXError('alpha must be between 0 and 1')         # this is using a dense representation        ### M = M.todense()   ######## delete this line ########         # add constant to dangling nodes' row         dangling = sp.where(M.sum(axis=1) == 0)         for d in dangling[0]:             M[d] = 1.0 / n         # normalize         M = M / M.sum(axis=1)         P = alpha @ M + (1 - alpha) / n     else:         raise nx.NetworkXError("walk_type must be random, lazy, or pagerank")      return P ```  For the lines that we need to watch out for, here's a few examples we tried today:  ``` In [46]: np.array(s1.todense()) * np.array(s2.todense())                                                                                                      Out[46]:  array([[1., 0., 0.],        [0., 1., 0.],        [0., 0., 1.]])  In [47]: np.array(s1.todense()) @ np.array(s2.todense())                                                                                                      Out[47]:  array([[1., 1., 0.],        [0., 1., 1.],        [1., 1., 1.]])  In [48]: (s1 * s2).todense()                                                                                                                                  Out[48]:  matrix([[1., 1., 0.],         [0., 1., 1.],         [1., 1., 1.]]) ```
issue
Heads-Up: cuGraph!#TITLE_END#I'm very excited with this development from NVIDIA: cuGraph, which is one of their RAPIDSAI projects!  In particular, I have been nudging them to "just copy the NetworkX API", because it's so beginner-friendly and would really help with adoption if I could just `import cugraph as nx`, and be done with the rest of my code.  Tagging here @seunghwak and @dschult, in case you two haven't (e-)met each other just yet.
issue
Bug report: Issue with node colours#TITLE_END#I am facing an issue with node colours. When I draw a graph without node colours, everything is fine:  ``` nx.draw_kamada_kawai(hg.ctgraph.graph, with_labels=True, ax=ax) ```  ![image](https://user-images.githubusercontent.com/2631566/38712296-ab48118a-3e98-11e8-9fa8-852b716cc83e.png)  However, when I draw the same graph with node colours, parts of the graph get clipped off.  ``` node_colors = [d['helix_id'] for n, d in hg.ctgraph.graph.nodes(data=True)] nx.draw_kamada_kawai(hg.ctgraph.graph, with_labels=True, ax=ax, node_color=node_colors) ```  ![image](https://user-images.githubusercontent.com/2631566/38712342-f0b67df6-3e98-11e8-8be2-f5489ee2c40f.png)  (Some nodes are intentionally not coloured.)  Is there a reason why this is happening?
issue
Feature Request: Graph from Pandas DataFrame#TITLE_END#I hope I'm posting in the right place!   Pandas DataFrames contain one very crucial difference to numpy matrices - the index labels and column labels. Could there be a way to parse DFs into a graph, automatically creating the nodes from the index & column labels, and edges with weights based on the matrix values? 
issue
Counter-intuitive API design for degree centrality in bipartite module#TITLE_END#I have noticed something kind of counter-intuitive with the bipartite module.  In the [degree centrality docs](https://networkx.github.io/documentation/development/_modules/networkx/algorithms/bipartite/centrality.html#degree_centrality), the degree centrality computation requires passing in a list of nodes from one partition, but returns the degree centrality distribution over all nodes. Just wondering, why is the API design as such? Perhaps I'm lacking some historical background here.  I think it might be more intuitive if it were one of the following API designs, perhaps? Here are my thoughts:  - Pass in a container of nodes, and return only degree centrality scores for that container of nodes. - Do not require passing in a container of nodes, and return degree centrality scores for all nodes in both partitions. - Make container of nodes an optional keyword, and return DC scores for those nodes only if a container of nodes is passed in, otherwise return all DC scores.  What do others think? As I mentioned earlier, I might be lacking some historical perspective here, so please pardon me if I misunderstood the original API design.
issue
Help with pickled graph sizes#TITLE_END#Hi everybody! I'm having an issue with saving my graph objects to disk, in which the graph objects are taking up large amounts of disk space regardless of whether or not I have node or edge metadata attached to them. I'd like to know if there's a way in which I can reduce the graph file size on disk. (For those who are interested in the research question behind all of this, I am trying to do deep learning using convolutional neural networks operating on graph-structured data.)  This is what I've done. In order to model protein structures as graphs, I have sub-classed NetworkX's `DiGraph` object into a `ProteinInteractionNetwork` object, which can be found [here](https://github.com/ericmjl/protein-interaction-network/). (Code may be a few commits behind what I have locally; haven't pushed yet.) I use that code to construct my graphs from Protein Data Bank files that contain mutant versions of one protein. Each node has a bunch of metadata, and a `numpy` "features" array. Each edge also has a `numpy` "features" array. All "features" arrays are computed based on node and edge metadata.   If I pickle this graph to disk, it's size is 20MB-ish. Not a big deal individually, except that I have `3401 * 3` models that I'm creating, and taken together, this can easily take up ~200GB of space. To see if removing metadata helps with saving disk space, on my graph, I set the metadata to be an empty set, by running the following code:  ``` # remove metadata for n, d in p.nodes(data=True):     p.node[n] = dict() #     for k, v in d.items(): #         del p.node[n][k]  for u1, u2, d in p.edges(data=True): #     for k, v in d.items(): #         del p.edge[u1][u2][k]     p.edge[u1][u2] = dict()  nx.write_gpickle(p, 'no_metadata.pkl') ```  The file size of `no_metadata.pkl` is still ~20 MB.  I'm not quite sure why this is the case. Is there a different disk storage format that I can use instead of pickling, that can handle `set`s and (optionally) `numpy` arrays? (Optional, because I can always opt to compute on the fly the `numpy` array based on node metadata.) 
issue
Python 2 and Python 3 reading pickles#TITLE_END#In a Python 2 environment, I have a pickled network saved using `nx.write_gpickle()`.  If I want to open the same file in a Python 3.4 environment, I get a `UnicodeError`: `UnicodeDecodeError: 'ascii' codec can't decode byte 0xd0 in position 0: ordinal not in range(128)`.  How might I be able to specify encoding when opening a Py2 pickle? 
comment
@NeilGirdhar I think that would be a great suite of PRs! As long as @dschult agrees, I wouldn't mind helping you review them if you're open to kickstarting the type annotations. (Btw, doesn't have to be the entire library in one PR, one or a handful of files at a time would probably be good.)
comment
This would be a neat PR. I can see using Dask as a generalized backend for parallel computations.
comment
I'm glad you found the error, @chuckuntulis!   As for where to put the error, I have a suspicion this might be a deeper-rooted thing in the stack. Also not a user of PySimpleGUI, so I can't speak to what's going on, but in the dependency chain I can see so far, it's networkx's drawing --depends on--> matplotlib --depends on --> other stuff. If the problem is deeper-rooted, then error messages in NetworkX is only a bandage; the the earliest place in the stack where this interaction happens should be where the error message gets stuck.
comment
Indeed, probably not.  That said, I think it’s probably better to provide sane defaults. My proposal is to default to returning sparse matrices, because memory usage, and allow some time to see if users instead want an kwarg in the API that allows returning of dense matrices instead:  ```python def adjacency_matrix_func(G, *args, **kwargs):     # stuff     mat = calculation_that_gives_sparse_matrix     if return_dense_matrix=True:         mat = sparse_matrix.to_dense()     return mat ```
comment
@rossbar to give this one a bit of momentum kick, once you have the list out, I think this can be one of those map-reduce-able newcomer tasks that we do on a bigger sprint. I think I can leverage this to put together some first-time contributor training material that we use in a live sprint.
comment
@willpeppo, I just remembered, a bunch of us on the NetworkX team are going to be doing an "open office hours"-style thing where we can hack on this. Date is this Friday!   If you're open to it, we should connect during the sprint hours! Let me know on LinkedIn if you can or cannot make it, and if you can, I'll let you know what the virtual meeting room link is that way.
comment
I just had a chance to catch up with @rossbar and @vdshk today about the issue.  @dschult I kinda think that there _might_ be initial confusion with the behavior of the function with undirected graphs. This arises from a mismatch between an intuitive understanding of the terms vs. a rigorous understanding of the behavior of the functions.   One might be able to make the mathematical case that tracing ancestors of a node requires recursively tracing all in-edges that bring one to that node, and that all undirected graphs can be re-interpreted as bidirectional edge directed graphs, and hence in this case the behavior is well-defined... and hence it will yield the equivalent of connected components. However, knowing that undirected graphs can be reinterpreted as a special case of directed graphs might not be common knowledge.  What I think is a reasonable prior on common knowledge is that a user of ancestors and descendants will expect this to work with directed graphs. Being permissive with the function's use on undirected graphs may be surprising for the average user.  Having typed out the logic that tries to explain why the function should work with undirected graphs, I think there are the following ways to make the function less confusing:  1. Include a concise version of the aforementioned explanation in the docstring so that the behavior with undirected graphs is unsurprising and well-explained, and move the function out of DAGs. 2. Restrict the function such that it only works with DAGs, i.e. not just DiGraphs but acyclic ones, such that calling the function on anything non-DiGraph and non-acyclic will yield an error. (This would be a breaking change, so we'd do a warning first, as @rossbar suggested.)  In our chat with @vdshk today, we brought up the phenomena that NetworkX has gradually become the _de facto_ library for graph analysis especially amongst Pythonistas. I think we've got a good opportunity to educate here! A function might be well-defined but still confusing behaviorally. In this case, I think we could do more to provide clarity.  @dschult, what do you think would be the more appropriate course of action?
comment
@NeilGirdhar yes, that's probably mostly it, I think. `to_numpy_matrix` underneath the hood already calls `to_numpy_array`. @dschult would this entail a deprecation warning first, before finally removing it? I'm only asking because it might break users' code without warning.   An alternative that might still be user-friendly might be to add the warning, but indicate in the warning that `to_numpy_matrix` now returns a `numpy.ndarray` instead of a `numpy.matrix`.   Just some thoughts, trying to think about this from the perspective of keeping the changes user-friendly, informative, and not suddenly/silently breaking somebody's code base.
comment
@dschult this apparently could be a good first issue, given how routine this one is. It mirrors the first issue that I worked on for matplotlib - removing all the pylab examples in the documentation (which really was just a bunch of changing imports). This looks like one that a newcomer could tackle (with some guidance, just as I had from the matplotlib devs).  I'll take a stab at a deprecation statement for to_numpy_matrix, but I'd like to hold off on the full removal of to_numpy_matrix from the codebase - it's a win-win opportunity to bring someone new on board!
comment
@CrazyPython @NareshPeshwe though PEP8 convention dictates lowercase for variable names, NetworkX (and network science) convention has been to call graphs by a single capitalized variable name. I will defer to @hagberg on what would be the best way forward for this specific little thing, though. 
comment
@jarrodmillman this is definitely a nice example to include! In terms of principles of viz, I would opt to prioritize the node position first rather than use a spring-like layout, but since the purpose is not scientific inference, I guess anything goes here :)
comment
Hi @umitkaanusta! It looks like something that you have implemented multiple times over. Do you have a function sketch here? If you do, we'd be happy to work with you to get the function inserted into the library in the right place!
comment
Hi @tanjia123456! Looking at your code:  ```python '''with open("HLN-12-1vertices.txt") as node_list: for eachline in node_list: G.add_node()''' with open("HLN-12-1edges.txt") as edge_list:     for eachline in edge_list:         G.add_edge()  # <-- error on this line         G.add_node()  # <-- error on this line ```  You have an error on the two lines I highlighted above. From observing your code, it seems to me there's a more fundamental Python problem that you might be having, as `G.add_node()` an `G.add_edge()` have to have arguments passed into them, in this case, the node that you want to add (which would come from parsing `eachline` in the "right", where "right" is specific to how your data are represented in a text file).  Can I check, have you followed the [official tutorial](https://networkx.github.io/documentation/stable/tutorial.html) to learn how to use NetworkX? If not, I'd highly recommend walking through it at least once. If you want an extended tutorial, I have one that is in the form of a YouTube recording (4 hours long) that you can use. (You can find it [here](https://ericmjl.github.io/teaching/network-analysis-made-simple/).)
comment
Thanks @joelmiller! Would you be kind enough to point us to a source please? I must admit I have never heard of the Jefferson High network, neither have I heard of Chains of Affection.
comment
Skimming the PDF, it appears to me that there's no computer-parseable source data for the graph. Do you have a link for that?
comment
Closing this due to inactivity. @joelmiller, if you have access to a computer-parsable format, please feel free to re-open!
comment
Closing this issue. I think @MridulS has resolved it sufficiently. Please feel free to re-open if this is not the case.  @GCPBigData, we appreciate your posting here, though I think it would also be helpful to take advantage of being able to format your posting properly so that it's easier for the rest of us to read. Please do consider that in the future.
comment
@TomekTrzeciak, I’m not an active core dev, but I’m nonetheless curious to hear what your thoughts are on what modules would constitute the “core” and what modules would constitute the “peripheral”. Would you be kind enough to elaborate?
comment
@ekohilas check out the docs: https://networkx.github.io/documentation/stable/reference/classes/graph.html  A number of class methods exist to add nodes and edges from a Python iterable.
comment
I see, your question is clearer now. It appears that this is not implemented in NetworkX, unless my cursory search of the docs was incomplete.  If you figure it out, though, I'd encourage you to contribute it to the library. Good software gets built that way, and NetworkX (and other contributions) are full of similar contributions that the rest of us have benefited from! A name for a function that does what you're trying to do might be `from_lattice()` or `from_grid()`.
comment
@dschult do consider `nxviz` on your radar as well! I have some pretty nice plots available there too. https://github.com/ericmjl/nxviz
comment
With shortest path, and many more, it’s possible to use NetworkX, though you’re limited at that point by computational complexity rather than memory. I’d encourage you to try it out on a subset of nodes or a smaller graph first if you’re feeling unsure. 
comment
@joelmiller that'd be an awesome addition! And if you add it to NetworkX, I'll create a consistent viz API on nxviz by wrapping your implementation. (And yes, I'm still developing nxviz to create a declarative, seaborn-like API for graphs!)
comment
Hi @huyptruong, chiming in because I've recently discovered the same deprecated API. I think you should use `from_pandas_edgelist` if your dataframe is an edge-list formatted thing, or `from_pandas_adjacency` if your dataframe is instead an adjacency matrix.
comment
@arunwise thank you for your PR! I really enjoyed reading through the code and seeing what you've put in. I have not yet gone through a detailed code review yet, but I will be blocking out an hour this Saturday to do so. Mostly I'll just be checking through the tests to make sure they are correct, which I trust they are, just need to have a second pair of eyes.  @dschult please let me know if there's something else I should look out for apart from correctness of the algorithm.
comment
Some notes for myself.  From @dschult's checklist:  - proper documentation:     - [ ] add the module to the rst file     - [x] use one line short summary at top of docstring followed by blank line     - [x] list parameters and return values - [x] tests that use random numbers should specify a seed and not repeatedly test many random inputs in these tests (though that code could remain as commented code). - [x] The API should be simple and where relevant match current API. - [x] The algorithm should not be already implemented somewhere else in NetworkX and if there is overlap, it should be determined whether to refactor common parts or leave them separate (and duplicate). Duplicate code is not always bad, but often it is better to avoid it. 
comment
Wonderful, @arunwise! Thank you too for your PR :smile:.  Some final reminders - once you're done, don't forget to delete your fork and re-fork the repo for any new PR. I noticed that you had PR-ed in from `arunwise/master`, which isn't recommended practice, but in any case, I didn't want to stop the PR from moving forward, so the solution around potentially future messy git logs is to delete and re-fork.   @dschult, if you're ok with the PR, please let me know, I'd love to hit the big green button! Also proposing merging by squashing, so that the commit history is clean (I do this with pyjanitor), unless you have other preferences.
comment
@arunwise sorry to bother you about this, but I hadn't heard back from @dschult and was about to just hit the merge PR button when I saw the conflict.  It appears I can't resolve the conflicts, as I might not have the necessary permissions.  Could you pull in the latest networkx master into your branch, resolve the conflict in `CONTRIBUTORS.rst` (keep both names), and push up the changes? I will go ahead and hit the merge button as soon as everything is green.
comment
@arunwise and @dschult, I took the liberty of merging! Thank you @arunwise for all the hard work!
comment
I am going to close this issue, as I think cuGraph is the right answer. The API developments are in the right direction, though not yet at parity with NetworkX.
comment
@tambulkar I'm not sure NetworkX will provide what you are looking for, but I think @dschult and @hagberg might know better.  That said, it looks like you're possibly reimplementing Dask - have you perhaps looked into it as a possible parallelization engine for your broader problem? (I'm of course writing as someone who is completely ignorant of your problem space, so please forgive me if you see my comment as mere blabbering!)
comment
Woohoo! I think this is a pretty rad idea :smile:! Yes, happy to review the PR, please don’t forget to tag me as a reviewer, and don’t forget docstrings and tests!
comment
Awesomeness :smile: I’m on a road trip right now, so if I can, I’ll try to squeeze in some time for a more thorough review. Meanwhile, from a cursory glance, things look good to me. Final approval will have to be on @jarrodmillman and @dschult, naturally.
comment
@bakerwho I'm super in favour of this! Though I do also want to hear your thoughts: what about making it part of the `nx.___` API as a function, rather than as a class method?   As far as I have seen, there's an (admittedly fuzzy) distinction between what goes on as a class method and what becomes a networkx function, but I sort of see it as "elementary graph properties" become class methods, while "less elementary graph properties" (e.g. triadic closures) would be functions.  It would look like:  - `nx.triadic_closures(G)` - `nx.focal_closures(G)` - `nx.stable_triads(G)`  We could then dispense away with the `get`ters, which is less Pythonic than simply naming a function by the logical thing it's supposed to return.  What do you think?
comment
I think we should pull in @dschult into the discussion - he would have a much better idea than I would! :smile:
comment
Ahhh the turkey!   @bakerwho those sound like good ideas! I'm going to defer to @dschult, and maybe also pull in @MridulS and @jarrodmillman on the thread to see if they have other thoughts.  In terms of "actual work", I would recommend submitting a PR that isn't too large so that it's easier to review. A suggestion you I can offer (that you're free to reject) is to first submit `nx.triadic_closures(G)` and its implementation, docs, and tests. Happy to help with the review, as long as you tag me in there :smile:.
comment
--@dschult sorry to bother you about this, but I'm finding it hard to debug why the CI builds have been failing. Do you have any insight here?--  @bakerwho here's a paste of the currently failing test that I found digging through the CI logs. I realized how hard it is to scroll to it on the CI system, thought this might make it a bit easier on you  ```python _____________________________ test_triads_by_type ______________________________ 1358 1359    def test_triads_by_type(): 1360        """Tests the all_triplets function.""" 1361        G = nx.DiGraph() 1362        G.add_edges_from(['01', '02', '03', '04', '05', '12', '16', '51', '56', 1363                          '65']) 1364        all_triads = nx.all_triads(G) 1365        expected = {} 1366        for triad in all_triads: 1367            name = nx.triad_type(triad) 1368>           expected[name].append(triad) 1369E           KeyError: '030T' 1370 1371algorithms/tests/test_triads.py:96: KeyError 1372 ```
comment
@bakerwho I think I have an idea for this one:  > Any order-3 DiGraph will necessarily be one of the 16 triad types. The only way this fails is if the DiGraph does not have 3 nodes (ie it is not a triad). Currently, I'm using an assert statement in triad_type() to ensure that this is the case. How do you suggest I do it instead? If I'm raising an exception, which one should I raise (I'm guessing ValueError?).  Yes, instead of an assert statement in `triad_type()`, I would use an if/raise check, briefly:  ```python if not some_condition:     raise SomeError() ```  It's better than asserts, because asserts can be disabled in Python at the command line (though I haven't run into any real-world scenario yet). In any case, it's idiomatic to use the if/raise pattern.  Perhaps raising one of the errors defined [here](https://github.com/networkx/networkx/blob/aeeeef92f2229b0cc8ebbbbfcfc87128f546d55f/networkx/exception.py) might be good? One proposal I have here is `NetworkXAlgorithmError`, what do you think?  @dschult I just looked through the details of the `triad_type()` function once more, and I think it's correct as well. At least, in `test_triads.py`, all 16 possible cases. It might not be a fruitful avenue to test further, except maybe for checking that an appropriate error is raised if an invalid graph is entered? I can think of at least the following:  - Graph that doesn't have 3 nodes. - Graph with self-loops.  We're getting there :tada:!
comment
@dschult - just marked approval on the PR. I think it's comprehensively tested enough, and the docs are definitely well-written and clear.  @bakerwho if we get further questions or issues raised about triadic analysis, can we tag you in them?
comment
@dschult I’ve given my approval on the PR. It’s thoroughly done, looks like it can withstand the evolution of the nx API long-term, and is well-tested. 
comment
@dschult I just checked, looks like the relevant parts of the PR are still present and complete. @Radcliffe can you confirm?
comment
@luanamarinho FYI I edited your code block just so that it's formatted correctly. Hope you don't mind :smile:.
comment
Going to merge!
comment
Thanks @nandahkrishna!
comment
@Moradnejad thank you for the contribution! Yes, could you please add a regression test for this graph?
comment
@dschult I'm not quite familiar with this part of the code base, so this is a learning opportunity for me as well. Is a NetworkXError supposed to be raised somewhere here?
comment
Ok, thanks a lot, @dschult! Just to double-check, I restarted the build job to see if it'll consistently error the same way. I'll double-check according to your instructions.
comment
@dschult, confirming that the aforementioned error is the only error in the test suite. Merging in.
comment
Thank you, @Moradnejad, for the contribution! I am also sure that other real-world graphs are welcome too!
comment
@dschult I can confirm that this renders properly.   To get there: "Files changed -> View file".  Screenshot attached. ![screen shot 2019-01-09 at 2 57 19 pm](https://user-images.githubusercontent.com/2631566/50924973-014ee480-141f-11e9-8dcd-a1db66801dd8.png) 
comment
Because this change is minimal and renders properly, I am feeling comfortable doing the merge. Hope you don't mind, @dschult :smile:.
comment
@dschult also looks small enough that I'll just merge it.
comment
Looking at the traceback, this looks like a Spark issue, and not a NetworkX issue, because the final Python call in the traceback comes from `py4j` and not NetworkX.   Have you raised this issue with the pyspark team?
comment
@NeilGirdhar I updated your comment so that we can take advantage of special keywords the GitHub uses to link PRs to issues (and auto-close them when done). Hope you don't mind! :smile:
comment
@NeilGirdhar let's do this: I'll add the warning in `nx.to_numpy_matrix`, remove the `asmatrix` call, and let's see how the tests pass on that. I'll work on a separate branch from this PR.
comment
@dschult I would like to merge this PR, I think it's reasonable, and all tests pass anyways. Would you please let me know if there's any objections to it?
comment
By "relative distance of edges", do you mean plotting the distribution of lengths?   If so, then to start with, you are getting node attributes rather than edge attributes, so I would first make sure you have:  ```python length = nx.get_edge_attributes(G, 'length') ```  Then, plot the cumulative distribution as follows:  ```python import numpy as np import matplotlib.pyplot as plt  def ecdf(data):     x, y = np.sort(data), np.arange(1, len(data)+1) / len(data)     return x, y  fig = plt.figure() ax = fig.add_subplot(111) x, y = ecdf(length) ax.scatter(x, y) ```  I recommend the cumulative distribution for the reasons listed in [my blog post](https://ericmjl.github.io/blog/2018/7/14/ecdfs/).  On the other hand, if you are trying to show the edge weights on a drawing of the graph, then that is different:  ```python nx.draw(G, pos=nx.spring_layout(G)) nx.draw_networkx_edge_labels(G, pos=nx.spring_layout(G), edge_labels=length) plt.show() ```
comment
@GoingMyWay according to what you've described, the bottom part of my first response is exactly what you'll need then -- try it out! I'll close this issue, feel free to reopen if it still isn't what you need.
comment
@GoingMyWay thanks for looping back.  Just a small point to clarify, the following code block doesn't give you a distribution of lengths:  ```python In [8]: G = nx.Graph()    ...: G.add_edge(0, 1, length=2)    ...: G.add_edge(1, 2, length=9)    ...: G.add_edge(0, 2, length=15)    ...: pos = nx.spring_layout(G)    ...: length = nx.get_edge_attributes(G, 'length')    ...:    ...: nx.draw(G, pos=nx.spring_layout(G))    ...: nx.draw_networkx_edge_labels(G, pos=nx.spring_layout(G), edge_labels=length)    ...:    ...: plt.show() ```  ![figure_1](https://user-images.githubusercontent.com/2631566/43083937-a02bb0fa-8e65-11e8-9ba1-caef641bd21a.png)  And to illustrate what happens when we try to respect the weights using a Kamada-Kawaii (force-directed) layout:  ```python In [9]: G = nx.Graph()    ...: G.add_edge(0, 1, weight=2)    ...: G.add_edge(1, 2, weight=9)    ...: G.add_edge(0, 2, weight=15)    ...: length = nx.get_edge_attributes(G, 'weight')    ...:    ...: nx.draw(G, pos=nx.kamada_kawai_layout(G))    ...: nx.draw_networkx_edge_labels(G, pos=nx.kamada_kawai_layout(G), edge_labels=l    ...: ength)    ...: plt.show() ```  ![figure_1](https://user-images.githubusercontent.com/2631566/43084006-ce684abe-8e65-11e8-9b19-57262e839254.png)  As you can see, it is impossible in a 3-node system to accurately represent all edge weights simultaneously using the built-in force-directed layout. Even if I change one of the weights, the layout fundamentally remains the same:  ![figure_1](https://user-images.githubusercontent.com/2631566/43084115-095437f0-8e66-11e8-8509-ec3db59b71b4.png)  As such, I agree with @dschult. If you want to draw the map of America with networkx, you have to pre-compute the node positions with longitude and latitude. More generally, my personal sense is that to rationally view a graph (beyond a random/hairball layout), it is important to first prioritize the position of nodes, instead of using edge weights as a surrogate for positioning the nodes. As @dschult mentioned, this ends up requiring more than 2 dimensions to have everything computed correctly.   If you're looking to do basic geographic graph viz, I am developing a declarative interface for geographic graph viz in [nxviz](https://github.com/ericmjl/nxviz/). There is a branch, [geoplot](https://github.com/ericmjl/nxviz/tree/geoplot), which you can install from GitHub (the interface for matplotlib is complete; the altair implemention is not done). Once I have finished the altair implementation, I will cut a new release to PyPi and conda-forge. An example, from the Chicago divvy dataset, is shown below, with code:  ```python g = GeoPlot(G_new,             node_lat='latitude',             node_lon='longitude',             node_color='dpcapacity',             node_size=0.005) ```  An example of the data stored on each node:  ``` (5,   {'name': 'State St & Harrison St',   'latitude': 41.87395806,   'longitude': -87.62773949,   'dpcapacity': 19,   'landmark': 30,   'online date': Timestamp('2013-06-28 00:00:00')  } ) ```  ![figure_1](https://user-images.githubusercontent.com/2631566/43083542-c509a6e4-8e64-11e8-8905-b0998967e01d.png)  I intend for nxviz to be a seaborn-like viz library to "get a first-pass feel" for your graph data. If you need more complicated geospatial analysis tools, that would be out of scope for nxviz. 
comment
My guess here is that `all_simple_paths`'s complexity increases super-linearly with the number of nodes and edges in the graph. Inherent here is a fundamental problem with the implemented algorithm, and perhaps even the problem definition itself -- finding all simple paths might not necessarily have an efficient solution.   For your actual problem, do you need to find the number of simple paths between two nodes? If we were to reframe the problem as "the number of paths between two nodes of length `k`", if I am calculating this correctly, raising the adjacency matrix to the power `k` gives you the number of paths that exist between two nodes of length `k`, and this is an extremely efficient operation to perform.
comment
@Ananthan-Rajasekharan thanks for posting! I'm not 100% sure of the source myself, but I have dug out the source code that you can inspect: https://github.com/networkx/networkx/blob/master/networkx/algorithms/link_analysis/pagerank_alg.py
comment
@zjpjack I noticed you're using system python on a Mac. If it's possible, do consider giving the Anaconda Python distribution a shot - at that point, NetworkX will be a simple `condo install networkx` away.
comment
@jhpratt - just out of curiosity, is the large graph that you're trying to show amenable to `nxviz`? (It's a package I'm creating for visualizing large graphs, built on top of `matplotlib`, and implements Circos plots with Hive Plots in the works right now.)   Unfortunately I don't have an answer for you re: `graphviz`. 
comment
@jhpratt the API is designed to be declarative (and hence stable) going forth; it's also been used in a Network Analysis course I've taught on DataCamp, which means I've got to keep it stable for them too :smile:.  Please give it a shot and let me know what you think - to the best of my knowledge, the only thing you'll have to be worried about is plots taking some time to render, so I'd start with a subsample of edges in your graph and work up from there. Happy to discuss any issues on `nxviz`'s GH repo.
comment
@jtorrents thanks for the PR! Yes, I think the new documentation makes it clearer as to why the API is designed as such.   I happen to be making a DataCamp course on network analysis, and for the sections on bipartite graphs, I can add in this information there too!
comment
@edouardklein when you re-read the edgelist, do you instantiate it as a Directed graph? I'm assuming you do, but just wanted to do a sanity check.
