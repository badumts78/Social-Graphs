issue
Louvain community Detection#TITLE_END#Implements Louvain community detection on (positive) weighted graphs 
issue
sparse matrix todense changes diag#TITLE_END#I was adding in code for community detection, and all my tests started failing.  Found this behaviour when roundtripping from_numpy_matrix, and adj_matrix:  In [63]: jnk = np.eye(10) array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])  In [64]: graph = networkx.from_numpy_matrix(jnk)  In [65]: mat = networkx.adjacency_matrix(graph).todense()  In [66]: mat Out[66]:  matrix([[ 2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.]])  In [67]: graph = networkx.from_numpy_matrix(mat)  In [68]: mat2 = networkx.adjacency_matrix(graph).todense()  In [69]: mat2 Out[69]:  matrix([[ 4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  4.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  4.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.]])  Thoughts? 
comment
I have recently started working on [Brainx](https://github.com/fperez/brainx). We have implemented two community finding algorithms (Simulated Annealing, Newmans Spectral Partitioning from 2006 PNAS paper).   This code has implemented a `GraphPartition` class, and returns the partition `GraphPartition.index` as a dict of sets, keys are just ints 0->number_of_partitions. I have been doing quite a bit of refactoring on this code recently, and one of the main issues is moving away from a dict, to a list of sets.  This is primarily to benefit the algorithms, but this lack of structure is in some ways more intuitive. Having a `frozenset` as a final result is appealing, but agree it would be nice to have some nice functions for converting.  I was planning on submitting a pull request after I finish (somehwhat substantial) refactoring of the code, but wanted to chime in on this discussion in case our experience is useful.  I have been meaning to look at @taynaud  python-louvain implementation, to gain inspiration for the `GraphPartition` class, and (thanks to this discussion) also plan to look at @haberg Community class;  this discussion is timely.  Just wanted to add my 2c   
comment
@dschult solution +1 the main reason for adding the undirected_self_loop_factor=2.0 is for clarity... I will often move back and forth between a graph representation and a matrix representation when working with an algorithm, it is helpful to know if I should take the factor of 2 into mind, or if it is handled by networkx....I just dont want self-loops manipulated in an unexpected way, so clarity helps  (@hagberg comment on principle of least astonishment).  In some cases I will coerce a binary incidence matrix into a weighted adjacency matrix, so will be good to clearly see how adjacency_matrix(), to_numpy_matrix(), and to_scipy_matrix().todense()  behave.  On Tue, Mar 18, 2014 at 9:31 AM, Aric Hagberg notifications@github.comwrote:  > @dschult https://github.com/dschult, yes let's do that. And also the > same for to_scipy_sparse_matrix(). The sparse representation is the default > now. >  > I'm indifferent about the keyword factor of 2 approach. Anyone have a use > case for using a 1 there instead of a 2? >  > ##  >  > Reply to this email directly or view it on GitHubhttps://github.com/networkx/networkx/pull/1078#issuecomment-37954550 > .  ##   Cindee Madison Jagust Lab UC Berkeley cindee@berkeley.edu 
