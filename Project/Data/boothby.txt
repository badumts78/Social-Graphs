issue
modified max_weight_matching to be non-recursive#TITLE_END#I ran into the recursion limit using `max_weight_matching` on a large graph.  This algorithm has some rather large recursive subroutines, so I have not exercised my typical performance-oriented rigor for fear of breaking things and making it completely unreadable.  Instead, I used a trampoline pattern for its simplicity and at-a-glance correctness.  ```python def original_f(*args):     ...     original_f(*r_args)     ...  def new_f(*args):     def _recurse(*args):         ...         yield r_args         ...     stack = [_recurse(*args)]     while stack:         top = stack[-1]         for r_args in top:             stack.append(_recurse(*r_args))             break         else:             stack.pop() ```
issue
add simple cycle enumerator for undirected class#TITLE_END#The need to enumerate simple cycles in a graph has come up frequently in my life and it's a common question that people have.  There are two ways of accomplishing this:  1. enumerate all connected sums of cycles in a cycle basis 2. exhaustive search  The first is probably the most efficient way to get all cycles, but filtering down to certain cycle lengths would not be efficient at all.  The code here is reasonably fast given the problem it is solving, but leaves some performance improvements on the table.  The biggest one is that I should probably iterate over 2-edge-connected components to avoid brute-forcing large trees.  This is a work in progress; I have not yet added tests.  And, of course, I'm open to discussion on how best to handle this -- I was unsure whether or not to dispatch this algorithm with `all_simple_cycles` in the directed case.  Also, there is a question of credit:  this was the obvious brute-force algorithm when I naively sat down with the problem, and I've found a few occurrences of this or related algorithms in literature and code snippets online; [a 2006 paper claims its novelty](https://ieeexplore.ieee.org/document/1602189) but that paper does not appear to handle equivalence by reversal.  The only point of novelty that I see is that other algorithms (including an ancient implementation of mine) assume sortability of node labels, which is not strictly necessary.  I've included a flag to enumerate chordless cycles, which is effectively a free addition to the algorithm -- again, there are several variations of this in the literature / internet.
issue
De-Regression: eagerly evaluate not_implemented_for in decorated generators#TITLE_END#See issue #5520 for details.  ~Work in progress, needs doc updates.~  ~Also, there's an issue where `open_file` and `not_implemented_for` are like oil and water -- specifically `not_implemented_for` is like oil and should go on top.  This isn't currently an issue in the codebase but should be addressed before it causes an obnoxious bug (see #5520 for discussion)~
issue
Remove decorator dependency#TITLE_END#Pursuant to removing the dependency on `decorator`, I've made a general decorating class which applies functions to specified arguments.  See discussion in #4732.
issue
Performance & display regressions in matplotlib plotting#TITLE_END#I was using the current networkx repo at work yesterday, and did a little bit of plotting.  Below is some code that highlights two significant issues.  The first is that `draw_networkx_edges` has incurred a significant performance regression -- what takes less than a second in networkx 2.5 has exploded to almost a minute now.  The second is that the edges don't touch the nodes.  This plot is admittedly noisy, but the current version is completely illegible.  ### Steps to Reproduce Requires the `dwave_networkx` package to generate the graph I'm plotting  ```python import dwave_networkx as dnx, networkx as nx, time, random from matplotlib import pyplot as plt def draw_pegasus(p):     pos = dnx.drawing.pegasus_layout(p, crosses=True)     nx.draw_networkx_edges(p, pos, arrows=False)     nx.draw_networkx_nodes(p, pos, node_size=20, linewidths=0)  p = dnx.pegasus_graph(16)  plt.figure(figsize=(20, 20))  draw_pegasus(p)  plt.savefig('yield.png')  plt.close()  ```   ### Environment <!--- Please provide details about your local environment --> Python version: 3.8 NetworkX version: 2.5 vs current main branch  ### Additional context I suspect that this was introduced in #4360; and that a solution to the performance regression may be to use `LineCollection` when `arrows=False`.  Networkx 2.5 ![25](https://user-images.githubusercontent.com/569654/118373888-f320ae00-b56d-11eb-9cd5-da0205ef60c2.png)  networkx/main ![26pre](https://user-images.githubusercontent.com/569654/118373894-f6b43500-b56d-11eb-9aa0-caaaeb804dd4.png)  
issue
implemented faster sweep algorithm for kernighan_lin_bisection#TITLE_END#Discussion in #3330 
issue
Correct handling of zero-weight edges in all_shortest_paths#TITLE_END#Discussion in #3685.
issue
Add weight functions to bidirectional_dijkstra and astar#TITLE_END#See discussion in #3754 and #3774.  Knock-on benefits:  1. Uncovered/fixed an inconsistency in Bellman-Ford (easy 1-line fix) 2. Added support for multigraphs in `astar`
issue
Make connected_components safe to component set mutation#TITLE_END#Discussion in #3823
issue
UnionFind's union doesn't accurately track set sizes#TITLE_END#Conversation in #3567
issue
option for partially-periodic lattices (networkx #3586)#TITLE_END#I'm not super happy with how I wrote those docstrings; suggestions are welcome.
issue
fix initializer for kamada_kawai_layout (networkx #3658)#TITLE_END#See discussion at #3658
issue
preflow_push does not make enough levels#TITLE_END#Computing the maximum flow from 0 to 3 in the following graph fails:  ``` D = DiGraph() D.add_path([0,1,3],capacity=1) D.add_path([1,2,3],capacity=1) ```  I've traced the error to the line  ``` levels = [Level() for i in range(2 * n - 1)]  #n is the number of vertices in D ```  which interacts poorly with   ```     heights = reverse_bfs(src)  #maximum height may be n-1     ...         for u in heights:             heights[u] += n #maximum height may be 2n-1     ...     for u, new_height in heights.items():     ....                 levels[new_height].inactive.add(u) #BOOM! ``` 
issue
Makes enough levels in preflow_push#TITLE_END#This change resolves issue #1542 
comment
Would you please provide a graph that exhibits that behavior?
comment
I have managed to extract minimal working examples from proprietary data before.  Does the algorithm exhibit the same behavior on the induced graph of the single cycle you're seeing repeated?  That is, if `C` is the first cycle and you run `simple_cycles(G.subgraph(C))` does it still repeat it?  If that's the case, can you remove the labels from that graph with `nx.relabel.convert_node_labels_to_integers`?  That is typically enough to strip any proprietary information.  Your implementation is incorrect, unfortunately.  In addition to reporting the same cycle multiple times (from different starting points) it fails to list some cycles due to how you're using `visited`.  ```python D = nx.DiGraph([   (0, 1), (0, 2), (0, 3),   (1, 2), (1, 3), (2, 3),   (1, 4), (2, 4), (3, 4),   (4, 0) ]) ```  When I run your code on the graph above, it does not find the cycle `(0, 2, 4)`.  Note that your use of `visited` only allows each node to be visited a single time during the exploration rooted at each given node.  Therefore, only a quadratic number of cycles can be found.  Where this is guaranteed to be fast, it must be incorrect as there may be an exponential number of simple cycles in a graph, for example using the graph `D` below with even `n`, the number of cycles grows like the fibonacci numbers (see https://oeis.org/A005248 for the counting sequence)  ```python D = nx.cycle_graph(range(n), create_using=nx.DiGraph) nx.add_cycle(D, range(0, n, 2)) nx.add_cycle(D, range(1, n, 2)) ```
comment
Thanks for this report.  In the code is an undocumented upper-bound calculated from weights and capacities, which is being violated in your example.  ```python     faux_inf = (         3         * max(             chain(                 [                     sum(c for c in DEAF.edge_capacities if c < inf),                     sum(abs(w) for w in DEAF.edge_weights),                 ],                 (abs(d) for d in DEAF.node_demands),             )         )         or 1     ) ```
comment
Sorry to duplicate your findings; I skipped to poking at code while sipping coffee this morning and forgot to read the bug report :roll_eyes:   For what it's worth, I've minified your example down to a tree (pretty hard to find a negative cycle in a tree, right?)  ```python import networkx as nx import traceback   cap = 1000000000 def create_graph(with_capacity):     G = nx.DiGraph()      c = dict(capacity=cap) if with_capacity else {}      # Add nodes with demands     G.add_node('s0', demand=-89)     G.add_node('s1', demand=-197)     G.add_node('s2', demand=89)     G.add_node('s3', demand=212)     G.add_node('s4', demand=32)     G.add_node('s5', demand=83)     G.add_node('s6', demand=-159)     G.add_node('s7', demand=-68)     G.add_node('s8', demand=116)     G.add_node('s9', demand=120)     G.add_node('s10', demand=-65)     G.add_node('s11', demand=-74)      # Add edges     G.add_edge('s0', 's1', weight=1, **c)     G.add_edge('s1', 's2', weight=1)     G.add_edge('s1', 's3', weight=1)     G.add_edge('s1', 's4', weight=1)     G.add_edge('s10', 's0', weight=1)     G.add_edge('s11', 's9', weight=1)     G.add_edge('s2', 's5', weight=1)     G.add_edge('s5', 's11', weight=1)     G.add_edge('s5', 's8', weight=1)     G.add_edge('s6', 's10', weight=1)     G.add_edge('s7', 's0', weight=1)      return G   # Calculate min-cost flow try:     G1 = create_graph(False)     flow_value, flow_dict = nx.network_simplex(G1)  # fails except Exception:     print(traceback.format_exc())  G2 = create_graph(True) flow_value, flow_dict = nx.network_simplex(G2)  # succeeds print(flow_value, flow_dict) ```
comment
As I'm staring at this, the pseudo-infinity is failing because it seems to assume that no pipe will carry more than any single consumer/producer.  If we insert a `sum`, this bug is apparently* resolved in 3 bytes.  ```python ...                 sum(abs(d) for d in DEAF.node_demands), ... ```  Dan, I'm not sure what to make of that constant either.  \* I say "apparently" because it only resolves this one issue.  I remain suspicious.
comment
and with that observation, we get a (provably?) minimal example  ```python cap = 1000000000 def create_graph(with_capacity):     G = nx.DiGraph()      c = dict(capacity=cap) if with_capacity else {}      # Add nodes with demands     G.add_node('s0', demand=-4)     G.add_node('s1', demand=-4)     G.add_node('ns', demand=0)     G.add_node('nc', demand=0)     G.add_node('c0', demand=4)     G.add_node('c1', demand=4)      # Add edges     G.add_edge('s0', 'ns', weight=1, **c)     G.add_edge('s1', 'ns', weight=1)     G.add_edge('ns', 'nc', weight=1)     G.add_edge('nc', 'c0', weight=1)     G.add_edge('nc', 'c1', weight=1)      return G  ```  Here, we see that `(nc, ns)` carries twice the max-abs-demand from the two suppliers to the two consumers, and exactly half of the sum-abs-demand.  I still don't know what the 3 is doing there, but I could argue that this example tightly achieves the sum-abs-demand bound.  I could argue for a slightly finer bound; `min(sum(d for d in demands if d > 0), -sum(d for d in demands if d < 0))`, and dropping the multiplication by 2, but that's quibbling.
comment
Pawing through the history a bit, the `faux_inf` bound was introduced in #1280 and has remained unchanged except for formatting since that time.  I haven't yet read through the references introduced by @ysitu in detail, but I've skimmed [Kiraly and Kovacs](https://arxiv.org/pdf/1207.6381) and didn't spot a similar bound.  I also skimmed the source of the [Lemon implementation](http://lemon.cs.elte.hu/hg/lemon/file/a278d16bd2d0/lemon/network_simplex.h) that (I think) the K&K paper discusses, and it uses either `inf` or `INT_MAX` for its bound.  That said, I haven't fully grokked either implementation.  It'd be most convenient if @ysitu could tell us where the algorithm is described and where the bound comes from.
comment
Note that we do have an implementation to find minimum-weight cycle bases, if that's still a thing you desire.  https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.cycles.minimum_cycle_basis.html
comment
I've read over this thread and those feeding into it, and the notion of returning `(nodes, edges)` does sound quite nice.  However, when I want to use networkx as a "desk calculator for graphs", I want the very nice `nx.complete_graph`, `nx.path_graph`, etc to work as expected.  I propose that this could be accomplished through a couple decorators, which could be adopted gradually -- a big patch initial decorating all existing generators with `@graph_generator` would immediately support the notation `nx.complete_graph.nodes_and_edges(...)` without impacting existing code, and later generators could use `@node_and_edge_generator`.   I haven't given much thought to this naming; bikeshedding (and other comments) very welcome.  ``` def node_and_edge_generator(f):     @wraps(f)     def graph(*args, **kwargs):         nodes, edges = f(*args, **kwargs)         g = nx.Graph(nodes)         g.update(edges = edges)         return g     graph.nodes_and_edges = f     return graph  def graph_generator(f):     @wraps(f)     def nodes_and_edges(*args, **kwargs):         g = f(*args, **kwargs)         return g.nodes(), g.edges()     f.nodes_and_edges = nodes_and_edges     return f  @node_and_edge_generator def path_graph(n):     """an overly simplified path graph implementation"""     return range(n), list(zip(range(n-1), range(1, n)))  @graph_generator def complete_graph(n):     """an overly simplified complete graph implementation"""     g = nx.Graph(range(n))     g.update(edges = list(itertools.combinations(range(n), 2)))     return g ```  Note: all of the above is quite oversimplified for comprehensibility -- details like `create_using`, etc., are omitted.
comment
> Graph/DiGraph/MultiGraph/MultiDiGraph available `CG = path_graph.CustomGraph(4)` CustomGraph available if name in globals?  Oooh, that *is* magic.  Scary magic.  I was pondering if such a syntax would work, and hadn't thought of `__getattr__`'s visibility into globals.  Fleshing out my original thought a bit more, I intended for decorators to use `create_using` as you mention -- `nx.path_graph(4, create_using = nx.MultiGraph)`.  But let's play!  ```python from functools import partial as _partial, wraps as _wraps import networkx as nx  graph_class_names = {'DiGraph', 'Graph', 'MultiDiGraph', 'MultiGraph', 'OrderedDiGraph', 'OrderedGraph', 'OrderedMultiDiGraph', 'OrderedMultiGraph'}  graph_classes = {name: eval('nx.{}'.format(name)) for name in graph_class_names}  class graph_parts:     def __init__(self, gen):         self._gen = gen         self.__default__ = self.Graph      def __call__(self, *a, **k):         return self.__default__(*a, **k)      def __creator__(self, create_using):         @_wraps(self._gen)         def create(*a, **k):             n, e = self._gen(*a, **k)             g = create_using()             g.update(nodes = n, edges = e)             return g         return create      def __getattr__(self, name):         try:             gclass = eval(name)         except NameError:             if name in graph_classes:                 gclass = graph_classes[name]             else:                 raise         return self.__creator__(gclass)  @graph_parts def path_graph(n):     """basic docstring for a very basic graph"""     return range(n), zip(range(n-1), range(1, n))  if __name__ == "__main__":     print(list(path_graph(5).edges())) # good     print(path_graph.__doc__) # nope! but IPython figures it out somehow     print(list(path_graph.MultiGraph(5).edges())) # good     print(path_graph.MultiGraph.__doc__) # good ```  However things go awry when you use this from another module:  ```python import networkx as nx  if __name__ == "__main__":     class CustomGraph(nx.Graph): pass     p5 = nx.path_graph.CustomGraph(5) # boom, NameError ```  However, I do notice that `Graph` et al do not support `Graph(callable)` in general.  I think that similar shenanigans could be used here: `MultiGraph(path_graph)` could return a function or class; supporting the oddish notation `MultiGraph(path_graph)(5)` but it would probably break a lot of code that depends on knowing the type `nx.Graph` etc.  The signatures for `Graph`, etc., already take keyword arguments, which are passed into the `g.graph` dictionary, so I'm disinclined to overload that behavior with something like `MultiGraph(path_graph, 5)`  But!  Python's system for custom codecs gives me an idea.  If we return to the `path_graph.CustomGraph` issue... there's a solution in sight.  We need only add a system to register new graph classes:  ```python graph_classes = {name: eval('nx.{}'.format(name)) for name in graph_class_names}  def register_graph_class(graphclass):     graph_classes[graphclass.__name__] = graphclass ```  ```python import networkx as nx  if __name__ == "__main__":     class CustomGraph(nx.Graph): pass     nx.register_graph_class(CustomGraph)     p5 = nx.path_graph.CustomGraph(5) # good! ```
comment
That's a nice idea with a nice solution.  My preliminary implementation had a `__creator__` private method, which provides precisely the functionality of your `CustomGraph` method above.  What do you think of this?  ``` python class graph_generator:     ...     def create_using(self, graph_class): # was __creator__         @_wraps(self._gen)         def create(*a, **k):             return self._gen(*a, create_using = graph_class, **k)         return create      def __getattr__(self, name):         if name in graph_classes:             gclass = graph_classes[name]         else:             raise TypeError("Unknown graph class name.  Please use the create_using method of this object.")         return self.create_using(gclass) ```  Then, we support `path_graph.MultiGraph(5)` and `path_graph.create_using(CustomClass)(5)` (is this preferable to `path_graph.create_using(CustomClass, 5)`?  I'm leaning towards 'no').  Note that I've left some details out; namely `return self._gen(*a, create_using = graph_class, **k)` is written with the expectation that `_gen` is wrapped in an appropriate decorator.  I'll have to think about how we could support things like `path_graph.sparse_matrix`, but I've finished my breakfast and it's time for the day job.
comment
See more discussion on what constitutes a path on #6690.  To be consistent, you'd need to update much more than this single function; you'd need to additionally change every algorithm that finds/enumerates paths as well.  And from the perspective of those algorithms, it is extremely convenient for the base case to consist of zero or one nodes.
comment
> Surely, the performance penalty would be negligible and much would be gained in terms of consistency and clarity.  I must disagree.  If a user can implement their own post-processing, it is preferable that they do so, because if we take that choice away, we will harm performance (and correctness!) for users who do not want that post-processing to occur.  Consistency and clarity are indeed important goals.  We must be _internally_ consistent, and our documentation must be consistent with the assumptions and definitions in the code.  But we cannot be consistent with the entire world of graph theory, because the literature contains numerous incompatible definitions.
comment
The debate over path length is as old as time itself -- is it the number of nodes or the number of edges?  Is a path a sequence of edges, or a sequence of nodes with edges between them?  Throughout our library, we have used the latter.   I think that it's important to remain consistent on this, and that strongly implies that we should treat `[0]` as a sequence of nodes starting at `0` and ending at `0` where all consecutive pairs of nodes are joined by an edge -- a path.
comment
More supporting evidence: `len(nx.path_graph(1))` is `1`.
comment
Just a quick comment, in case this ever gets reopened  ```     ...     cycles = list(nx.simple_cycles(graph))     if cycles:         .... ```  For most graphs, this will consume all available memory and die.  Much better to use `nx.is_directed_acyclic_graph()` instead.
comment
FWIW I agree that a local value is preferable to loading the stack down with longer state tuples.  I prefer a slightly different idiom than the one used here:  ```python      while stack:          parent, children = stack[-1]          try:              child = next(children)              ....          except StopIteration:              stack.pop()  # without try/except      while stack:         parent, children = stack[-1]         for child in children:             ...             break         else:             stack.pop() ``` In avoiding exceptions, the `for,break,else` tends to be a little faster.
comment
@Tortar I have implemented my suggestion here: https://github.com/boothby/networkx/tree/tortar-patch-8 -- I would push the changes to your repo but the pre-commit hooks are currently faulty in your branch (not your fault; easily resolved with `git rebase -i main`) -- after rebasing I cannot push to your branch without `--force` which is awfully impolite.  Note that we only need to `break` when we grow the stack -- the `for,break,else` pattern is most effective in `dfs_labeled_edges` where non-tree edges are yielded "for free" within the iterator resulting in fewer touches on the stack:  ```python         while stack:             parent, children = stack[-1]             for child in children:                 if child in visited:                     yield parent, child, "nontree"                 else:                     yield parent, child, "forward"                     visited.add(child)                     if depth_now < depth_limit:                         stack.append((child, iter(G[child])))                         depth_now += 1                         break                     else:                         yield parent, child, "reverse-depth_limit"             else:                 stack.pop()                 depth_now -= 1                 if stack:                     yield stack[-1][0], parent, "reverse" ```
comment
Agreed, I applaud the effort to improve the performance of these functions in specific.  They form the backbone of so many algorithms throughout the library; a little elbow grease here goes a long way!
comment
This looks good to me, @alabarre.  Would you mind adding tests in [`/networkx/algorithms/tests/test_cycles.py`](https://github.com/networkx/networkx/tree/main/networkx/algorithms/tests/test_cycles.py)?
comment
I agree that the weighted version should be a separate PR.  Negative weights, for example, would confound this algorithm.
comment
I presume that simple cycles are assumed.  I hadn't considered the weighted girth problem today, but there is interest in negative weights in the literature.  https://www.sciencedirect.com/science/article/pii/S1570866715001021  I'll re-read this PR today.
comment
Back to you, Dan.  I tightened up the length bound, and reverted a change you made that iterated over all of the edges of G; all in the name of performance.  I also added the trick of deleting nodes as we go, and in so doing caught a bug in handling isolated nodes.  Sorry; your version was so short and sweet.
comment
And for what it's worth, the calls to bfs and dijkstra are throwing away a lot of information and re-computing it.  I've implemented this algorithm "my way" in https://github.com/alabarre/networkx/compare/girth...boothby:networkx:girth-faster but I have not been so presumptuous to push a wholesale rewrite.
comment
That's a good point, Dan!  It introduces a tiny amount of pain because all predecessors are recorded rather than the first; but that's easily remedied by traversing `distance` as it's conveniently in topological order.  This ends up faster than my ad-hoc BFS version.
comment
Shower thought: don't throw extra predecessors away; they can only be involved in even-length cycles!  With a little elbow grease, we can limit ourselves to one or two candidate cycles per root!
comment
performance history -- my benchmark is a single call to `girth(complete_graph(200))`  ``` commit  | runtime              | girth 5f516c6 |   1.6237123683094978 | 3 a9f9343 |    1.589095588773489 | 3 bb815f6 |    1.538610529154539 | 3 e85f26c |   1.6589345261454582 | 3 344f5ea |   1.7083751186728477 | 3 25a4ae7 |   1.6159080415964127 | 3 c2ac2ff |   1.5488385148346424 | 3 282312e |   1.6491199433803558 | 3 722039a |   1.5877377912402153 | 3 b0a63af |   3.8303282000124454 | 3 07e673a |   0.3171605058014393 | 3 54f2921 |   0.3066920265555382 | 3 7b21cc9 |  0.27495066821575165 | 3 40dde43 | 0.060972753912210464 | 3 1af2d48 |  0.04138858616352081 | 3 ```
comment
I removed the copy&delete trick.  Generally, it is only helpful on large graphs with large girth, which do exist, but they're quite rare.  And, best case, it only cuts the runtime in half.
comment
Thanks for your compliments!  I really like how this came out, too :)  I was hoping to use `git-bisect` to run those benchmarks.  I know it's possible, but Ross's merge commit put a bunch of superfluous commits into the history and I didn't want to muss about.  Instead, I wrote a quick & dirty (note the useless use of `cat`) shell script, and manually pasted the diffs of interest into a file named "heads".  ``` for i in `cat heads`; do   git checkout $i &> /dev/null;    echo -n "$i ";   python benchmark.py; done ```  FWIW, I did benchmark on a variety of graphs in a REPL -- cycles, trees, a girth-5 chordal cycle graph (10067).  The presence of many short cycles as the one you suggest makes the search take linear time; a bit faster than the complete graph because it has low average degree.  > Can you explain the new test graph you added?  I was hypothesis testing at some point -- I thought that I could elide the check `length < girth` but I had not quite gotten the framework right yet.  I put in an assert, and the test graph made it scream.  Basically; that test graph revealed a bug that was only present during development but it still passes my threshold of "always test (minimal) observed bugs forever."  > can we process the levels as we create them? That is, look for odd-cycles after each level of the check for even cycles? Does that help cut off the search?  Yes we can, and it would help early cutoff especially in graphs with only odd cycles (which are quite rare, I think).  It made the code look a bit finnicky; I didn't quite get it right and I ran out of time yesterday and pushed what I had.  However, note that the even-cycle search is linear-time and the odd-cycle search is quadratic; so it feels more or less okay to keep them separate.    > ... Toward that goal, would it be better to use the `nx.bfs_layers` generator instead of `nx.predecessor` which completes the BFS up to depth_limit?  :face-palm: I swear to you, I looked for that and did not find it.  Yes.  That does require recomputation of preds, which is a shame, but it should really clean up the code and more or less maintain the performance.  
comment
Okay, I gave this all a good hard stare.  What this algorithm really calls for is a function analogous to `dfs_labeled_edges`.  Here's what I came up with: in our breadth first search, one sees four kinds of edges (tree, forward, back, and level).  It's easy to efficiently traverse the graph and label each edge with the following:  ```python BACK_EDGE = "back" TREE_EDGE = "tree" FORWARD_EDGE = "forward" LEVEL_EDGE = "level" def bfs_labeled_edges(G, sources):     if sources in G:         sources = [sources]      depth = {s: 0 for s in sources}     queue = deque((0, s) for s in sources)     label = [LEVEL_EDGE, FORWARD_EDGE, BACK_EDGE]     while queue:         du, u = queue.popleft()         for v in G[u]:             dv = depth.get(v)             if dv is None:                 depth[v] = dv = du + 1                 queue.append((dv, v))                 yield u, v, du, dv, TREE_EDGE             else:                 yield u, v, du, dv, label[dv-du] ```  this greatly simplifies our girth computation:  ```python def girth(G):     girth = depth_limit = inf     skip_labels = (         nx.algorithms.traversal.breadth_first_search.BACK_EDGE,         nx.algorithms.traversal.breadth_first_search.TREE_EDGE,     )     forward_edge = nx.algorithms.traversal.breadth_first_search.FORWARD_EDGE     level_edge = nx.algorithms.traversal.breadth_first_search.LEVEL_EDGE     for n in G:         for u, v, du, dv, label in nx.bfs_labeled_edges(G, n):             if du > depth_limit:                 break             if label in skip_labels:                 pass             else:                 length = du + dv + 1                 if length < girth:                     depth_limit = du - (label is level_edge)                     girth = length      return girth ```  This gets my K200 benchmark down to ~25ms.  Should I commit it?  I'm mostly seeking commentary on `bfs_labeled_edges` here.  The name is probably weird because I'm yielding depth information along with the edges.  A more "pure" version might omit that info from the generator and recompute it (quick enough).  A version seen as impure could live in `cycles.py`.  ```python ...      for n in G         depth = {n:0}         for u, v, label in nx.bfs_labeled_edges(G, n):             if label is TREE_EDGE:                 depth[v] = depth[u] + 1 ...
comment
It occurs to me that we could do `girth(G, parity = None/'even'/'odd')` by optionally stuffing an extra label into `skip_labels` and thinking carefully about the stopping criterion.
comment
As I drifted off to sleep last night, the directed case occurred to me.  There, back-jumps can be arbitrarily long so my triplet is certainly invalid.  I think you're assuming that each edge should occur exactly once, and concluding that back-edges won't happen in the undirected case.  And that's certainly achievable, but currently my code emits each undirected edge exactly twice.  Perhaps this? ```python def _undirected_bfs_labeled_edges(G, sources):     if sources in G:         sources = [sources]      depth = {s: 0 for s in sources}     visited = set()     queue = deque((0, s) for s in sources)     while queue:         du, u = queue.popleft()         visited.add(u)         for v in G[u]:             dv = depth.get(v)             if dv is None:                 depth[v] = dv = du + 1                 queue.append((dv, v))                 yield u, v, TREE_EDGE             else:                 if du == dv:                     # could use either "in" or "not in" -- this emits self-loops                     if v in visited:                         yield u, v, LEVEL_EDGE                 elif du < dv:                     yield u, v, FORWARD_EDGE  def _directed_bfs_labeled_edges(G, sources):     if sources in G:         sources = [sources]      depth = {s: 0 for s in sources}     queue = deque((0, s) for s in sources)     while queue:         du, u = queue.popleft()         for v in G[u]:             dv = depth.get(v)             if dv is None:                 depth[v] = dv = du + 1                 queue.append((dv, v))                 yield u, v, TREE_EDGE             elif du < dv:                 yield u, v, FORWARD_EDGE             elif du > dv:                 yield u, v, BACK_EDGE             else:                 yield u, v, LEVEL_EDGE ``` 
comment
Oh, drat.  I was hasty in adding the option for odd&even girth.  Thanks, Petersen Graph, for yet again showing us the way.
comment
> The use of visited should apply in the same way to both types of graphs -- so that self.loops are handled in both cases  Self-loops are reported in both directed and undirected right now.  The directed case does not need to track `visited` because edges are only traversed in a single direction.  I'm reluctant to use the extra memory when not needed.  In the undirected case, every edge is traversed twice, so we need to use `visited` or another construct to filter edges from occurring twice (or, produce "reverse" edges -- which is also fine, but then self-loops would be reported only once).  By using the `visited` filter, there are two choices  1. mark `u` as visited before or after iterating over `G[u]` 2. check if `v in visited` or `v not in visited`  Toggling either choice changes whether or not loops are reported in the undirected case.  
comment
new benchmark ``` head    K200   C1000  CC1019 5f516c6 1.7005 3.5432 0.4079  a9f9343 1.6351 3.5444 0.4078  bb815f6 1.7013 3.5563 0.4143  e85f26c 1.6755 3.4549 0.3939  344f5ea 1.6772 3.4542 0.4120  25a4ae7 1.6477 3.4241 0.3932  c2ac2ff 1.6864 3.5351 0.4074  282312e 1.7071 3.5349 0.4011  722039a 1.6862 3.4973 0.3965  b0a63af 3.8599 2.8787 0.7263  07e673a 0.2681 1.1041 0.0331  54f2921 0.2803 1.1071 0.0333  7b21cc9 0.2475 0.5321 0.0141  40dde43 0.0340 0.5402 0.0107  1af2d48 0.0132 1.3616 0.0120  db297d4 0.0132 0.7919 0.0073  2a60dd8 0.0135 0.8059 0.0076  2b2516c 0.0134 0.7952 0.0077  c789b37 0.0131 0.7806 0.0072  0f44580 0.0136 0.8134 0.0078  0d99c88 0.0138 0.8153 0.0077  c101cf2 0.0109 0.7663 0.0069  bcae004 0.0104 0.7665 0.0080 0e426c8 0.0093 0.4212 0.0063  ```  ```python from networkx import girth, complete_graph, cycle_graph, chordal_cycle_graph, Graph from time import perf_counter  test_graphs = complete_graph(200), cycle_graph(1000), Graph([(u, v) for u, v, _ in chordal_cycle_graph(1019).edges if u != v]) g = girth for G in test_graphs:     t0, *girths, t1 = (         perf_counter(),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G), g(G),         perf_counter()     )     print(f"{(t1-t0)/len(girths):.4f}", end=' ') print() ```
comment
I just noticed that this ticket is (now) related to #6340 and #6359 -- my approach is slightly different; it's worth looking at them holistically.
comment
@dschult or @rossbar please take a (hopefully) final look.  I should really be done picking at this.
comment
For what it's worth, you can get a nice drawing with the following:  ```python G = nx.grid_2d_graph(2, 3) pos = dict(zip(G, G)) nx.draw(G, pos=pos) ```  The graph is not changing between the two pictures; only their drawing shows unnecessary edge crossings.  This behavior is expected.
comment
On naming: the function `_compute_knotty_centrality` does what I would expect a function named `knotty_centrality` would do.  The function `knotty_centrality`, according to the reference, finds the "connective core" or "knotty center" of a graph; the subset of nodes with maximum knotty centrality.  I think that it would make sense to expose a function named `knotty_centrality` to compute the knotty centrality measure for a given subgraph, as well as a function `knotty_center` to find a knotty center.
comment
The `inspect` module has an official `isclass` method for this purpose.  If we peek at the source of `inspect`, we see:  ```python def isclass(object):     """..."""     return isinstance(object, type) ```  which makes a lot of sense.  The existing code is a special case of this, which happens to pass for our graph classes.  There are some... creative solutions in the dregs of this SO answer: https://stackoverflow.com/questions/395735/how-to-check-whether-a-variable-is-a-class-or-not
comment
Gabriel Kissin contacted me privately and mentioned the R packages tsna (Temporal Social Network Analysis) and ndtv (Network Dynamic Temporal Visualization)
comment
I encountered this just yesterday to some frustration; I think it would be good to raise a warning when the parameter is provided but not used.
comment
It certainly looks inefficient, but looks aren't everything.  As @mturnansky notes, the call to `edge_stack.index` is linear time in the size of the entire graph, while the for-loop is merely linear in the size of the component.  To kill both birds with one stone, one could track the index in a dict  ```python         edge_stack = []         edge_stack_index = {} ...                         if components:                             edge_stack_index[parent, child] = len(edge_stack)                             edge_stack.append((parent, child)) ...                             ind = edge_stack_index[grandparent, parent]                             yield edge_stack[ind:]                             edge_stack = edge_stack[:ind] ... ```
comment
That does make sense, and I agree with your analysis (and I agree that my proposal is rather ugly).  To do bulk in-place truncation, one can use ```python del edge_stack[ind:] ```  in place of  ```python edge_stack = edge_stack[:ind] ```
comment
@mturnansky thank you for entertaining our suggestions!  I haven't actually done my own implementation, I just eyeballed yours and made suggestions.  Please commit your latest version of the code to your branch, and it will update the PR.  After that, I'll review the code and from the sounds of things, I'll almost certainly approve it without changes.  We usually like two reviews per ticket, but this one is pretty simple, so I'd expect it to be merged soon.
comment
This looks pretty good, @GuyAglionby, I'll go over it in detail in the next few days.  Thanks for your contribution!
comment
Are you sure this increases efficiency?  A microbenchmark tells me otherwise. ``` In [1]: def original(parents, path):     ...:     for node in path:     ...:         parents[node] = None     ...: def improved(parents, path):     ...:     for node in path[:-1]:     ...:         parents[node] = None     ...: path = list(range(10))     ...: parents = {}     ...: print("original:")     ...: %timeit original(parents, path)     ...: print("improved:")     ...: %timeit improved(parents, path)     ...: print()                                                                                           original: 623 ns ± 18 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) improved: 701 ns ± 31.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) ```  It should be a tiny bit faster to call `path.pop` or avoid appending the root in the first place
comment
Hey Lucas,  Thanks for your contributions in this space, I'm happy to see graph polynomials in NetworkX and I'll try to review it this week.
comment
Oh, I forgot to mention one thing.  Is it actually good to simplify the polynomial?  It doesn't look like `sympy` does a terribly good job factoring the polynomials -- it peels off an `x` from the complete graph whose polynomial is `prod(x-i for i in range(n))`, so I'm wondering if that's worth the effort.
comment
This is not intentional.  I'll look for a fix this week.
comment
Well, it isn't entirely intentional, anyway.  The problem is here:  ```python         if inspect.isgeneratorfunction(f):              def func(*args, __wrapper=None, **kwargs):                 yield from argmap._lazy_compile(__wrapper)(*args, **kwargs)          else:              def func(*args, __wrapper=None, **kwargs):                 return argmap._lazy_compile(__wrapper)(*args, **kwargs) ```  That is, if we're decorating a generator, we make a generator.  Therefore, the prequel to the eventual function call is not evaluated until the generator is pumped for the first time.  My first inclination was to add a parameter to `argmap` so that `not_implemented_for` could clobber the generator:   ```python         if inspect.isgeneratorfunction(f) or self._clobber_generator:              def func(*args, __wrapper=None, **kwargs):                 yield from argmap._lazy_compile(__wrapper)(*args, **kwargs)          else:              def func(*args, __wrapper=None, **kwargs):                 return argmap._lazy_compile(__wrapper)(*args, **kwargs) ```  However, that would make the behavior position-dependent because of the flattening behavior -- in the code below, `foo` would delay its exception and `bar` would raise it immediately.  ```python @open_file('bla') @not_implemented_for(digraph) def foo(...):    yield ...  @not_implemented_for(digraph) @open_file('bla') def bar(...):     yield ... ```  I'm tempted to remove the generator-dependent behavior altogether.  I think I got the idea from `decorator`, but I don't know why or if it's important.  And upon revisiting the code, I suspect that `yield from` is actually slower than `return`.  I'll time that and report back.
comment
``` Python 3.8.10 (default, Mar 15 2022, 12:22:08)  Type 'copyright', 'credits' or 'license' for more information IPython 7.13.0 -- An enhanced Interactive Python. Type '?' for help.  In [1]: def foo():     ...:     return baz()     ...: def bar():     ...:     yield from baz()     ...: def baz():     ...:     yield 0; yield 1; yield 2; yield 3; yield 4; yield 5; yield 6; yield 7; yield 8; yield 9     ...:                                                                                                                                       In [2]: %timeit [*foo()]                                                                                                                     317 ns ± 0.967 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  In [3]: %timeit [*bar()]                                                                                                                     507 ns ± 9.25 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) ```  My *extremely* scientific benchmarking confirms my suspicions.  We can get both (miniscule) runtime and (trifling) compile-time performance boost by removing the generator-dependent behavior.  The only reason I can imagine wanting to maintain this code is if `argmap` was doing something super expensive and/or we wanted perfect laziness.  I can't think of anything that we'd need that for in the library... @rossbar @jarrodmillman are you aware of any?
comment
Alternatively we could stop flattening across `not_implemented_for`, moving the `self._clobber_generator` check out of `__call__` and into `self.assemble`.  There is some position-dependence, which seems okay; the only issue I see is if somebody expects `open_file` to happen lazily but opens all the files in a large array, they could exceed the file descriptor limit.  To which, I think the answer is to do it lazily and don't make an array?
comment
> My vote would be for the simplest (IYO) solution!  Yeah, that's my preference too; I'm mostly documenting my thought process (with invitation to participate) before I bring the hatchet down.
comment
Aha, there is some necessary complexity here.  The following unit test fails:  ```python     def test_contextmanager_iterator(self):         container = []          def contextmanager(x):             nonlocal container             return x, lambda: container.append(x)          @argmap(contextmanager, 0, 1, 2, try_finally=True)         def foo(x, y, z):             yield from (x, y, z)          q = foo("a", "b", "c")         assert next(q) == "a"         assert container == [] # AssertionError: assert ['c', 'b', 'a'] == []         assert next(q) == "b"         assert container == []         assert next(q) == "c"         assert container == []         with pytest.raises(StopIteration):             next(q)          # context exits are called in reverse         assert container == ["c", "b", "a"] ```  The problem being that `finally` blocks are executed after returning the generator, and not after exhausting the generator.  In terms of the actual decorators we use:  ```python @open_file('logfile') def algorithm_with_file_logging(G, logfile):     for v in G:         print("processing node", v, file=logfile)         yield v ... <ipython-input-19-164e90d9b1ad> in <module> ----> 1 for x in algorithm_with_file_logging(nx.Graph([(1, 2)]), 'foo'): print(x)  <ipython-input-18-788928c53307> in algorithm_with_file_logging(G, logfile)       2 def algorithm_with_logging(G, logfile):       3     for v in G: ----> 4         print("processing node", v, file=logfile)       5         yield v       6   ValueError: I/O operation on closed file ```  This has a fairly fundamental consequence: we can't eagerly raise an exception for a bad file (without some extreme convolutions, I'm sure).  I'll sleep on this.
comment
*sigh* it is not as simple as I would have liked.  Code generation for  ```python @not_implemented_for('digraph') def foo(x):     yield x ``` was previously equivalent to ```python def foo(x):     if x.is_directed():         raise NetworkXError     yield x ``` and is now ```python def foo(x):     def intern_foo(x):         yield x     if x.is_directed():         raise NetworkXError     return intern_foo(x) ``` 
comment
By the definition of a vertex cover, every node with a self-loop must belong to every vertex cover.  The most obvious solution would be to run the existing algorithm on the subgraph of nodes without self-loops.  I'm not sure what approximation ratio that algorithm would achieve, but I suspect that it would be the same or better, and not worse.
comment
> If your idea is to run the existing algorithm and then augment with the nodes having self-loops, I don't think that will work.  That will not work indeed.  What I suggested was to remove the nodes with self-loops, run the algorithm on the remaining graph, and then add the nodes with self-loops to the result.  In your example, we'd remove nodes 1 and 3, get an empty vertex cover on the remaining subgraph since it has no edges, and return {1, 3}.  (though, in a good implementation, we wouldn't actually modify the input graph -- as I said, work on a subgraph)  I hadn't read #5104, but it does look like that should do the job. (@rossbar fyi you tagged the wrong issue)
comment
Hi @arav-agarwal2, it looks like your file contains quite a few lines with `#` characters.  By default, those are treated as comments in `read_edgelist`.  This can be fixed by setting the `comments` parameter to a string that doesn't occur in the file, or to be a little sneaky,`'\n'`:  ``` G = nx.readwrite.edgelist.read_edgelist(filename, create_using=nx.DiGraph(), comments='\n') ```
comment
Hi, you're using a weighted graph, and the weighted shortest-path distance.  The distance from 0 to 1 is reported correctly as the weight of the edge (0, 1).  If you want the unweighted distance, please use `nx.single_source_shortest_path`.
comment
I've reviewed our decorators, and unless I missed something, they all take the form `f(x, y, z) -> f(x, g(y), z)` -- that is, we apply various functions to parameters before calling the decorated function.  In my head, I'm calling this process "chutneying", but in my code, I'm calling it "argmap"ping.  I don't see a good way to do this without inspecting the decorated function.  However!  By rolling our own, we can get away with using `inspect` once per base function (marginally speeding things up when multiple decorators are used -- I could brag about O(1) vs O(n), but I don't see us using a nest of decorators more than 2-3 deep).  I'm also calling `inspect.signature` instead of the more-expensive (and less-informative) `inspect.fullargspec`, so that should marginally improve matters.  With this approach, we can see a slight improvement in import time over the status quo.  The other consideration is call-time.  By collapsing the decorators, I'm able to generate wrapper code with a fixed call depth, which should reduce the call-time overhead of multiply-decorated functions.  I don't think I can avoid O(n) calls to `compile` without doing some serious work to make the decorator lazy, but again, 2-3 deep is a pretty reasonable expected bound.  Consider the below -- the three `foo` functions are equivalent.  ```python def original_foo(x, y):     return x, y def add_one(x):     x = x+1  @argmap(add_one, 'y', 0) #quoted name / index syntax -- we'll call add_one on x and y before calling foo @argmap(x=sum) #unquoted name syntax -- we'll use x = sum(x) before calling foo def foo(x, y):     return x, y  @argmap(add_one, 'y', 0) def foo(x, y):     x = sum(x)     return original_foo(x, y)  def foo(x, y):     x = sum(x)     x = add_one(x)     y = add_one(y)     return original_foo(x, y) ```  I'll put a draft PR up in a few.
comment
Phooey.  I worked my way up the file, and the `open_file` decorator is a sticky wicket because it's both an `argmap` and a `contextmanager`.  I feel that this should be approachable, but haven't got the trick yet.
comment
In the current patch, I've simply re-implemented `py_random_state` as one-liner (not counting documentation, omitted here for brevity):  ```python def py_random_state(random_state_index):     return argmap(create_py_random_state, random_state_index) ```  My goal thus far is to not intrude on existing decorations.  Last night, I got pretty much everything to work except for `open_file`.  The only tedium was in testing; my decorator is more fussy about generators, so some tests need a call to `next` to surface an exception.  And for what it's worth, as long as decorators are very well behaved, the mixing of argument-indices and keyword arguments isn't terribly painful.  The difficulty with `open_file` isn't merely that it needs to be a context manager... worse, it's a maybe-context-manager -- some inputs require cleanup, others don't.  I don't hate how this looks on the interface end of things, but the code generation is getting ugly.  ```python def open_file(arg):     def _file_opener(path_or_file):         ...         if close_after_use:             file.close_after_use = True         return file              @argmap.try_finally # <--- this defers the call until a finally block     def _file_closer(file):         if hasattr(file, 'close_after_use'):             file.close()     def decorate(func):         func = argmap(_file_opener, arg)(func)         return argmap(_file_closer, arg)(func)     return decorate     ```
comment
I've read through the updated decorator source, and they are saving at import time, but I dislike what they do at runtime.  Specifically, the call to `inspect.Signature.bind()` looks pretty heavyweight.  https://github.com/python/cpython/blob/823fbf4e0eb66cbef0eacb7e8dbfb5dc8ea83b40/Lib/inspect.py#L3028-L3164  I've updated my implementation to be lazy -- it makes a wrapper function which compiles the decorated function upon its first call, and then intrusively replaces the `__code__` object of the wrapper so the decorated function will be fast on all subsequent calls.
comment
I don't mind nuking it here, but should we do a proper deprecation 'til 3.0?
comment
I did some runtime tests last night -- there's a fast path for `@open_file` when the filename is None -- and found that my lazy-compiled approach roughly breaks even with decorator 5.0.7 in 5 calls.  I started work on a simpler approach, which doesn't depend on inspect but requires direct access to the wrapped function's `__code__` object (which doesn't exist for all callables, but seems to work out in our codebase).  If that works out, I might post a second PR with it so we can debate merits.
comment
Bad news all around.  The simpler approach is a touch too simple, and can't handle functions with defaults.  Also, I switched my test to use nodes_or_number, and the break-even point looks to be around 15 calls -- though, this is still acceptable in my mind, as the first-call overhead is around 0.35ms on my not-speedy laptop.
comment
I made some progress on the defaults last night, at some cost to the simplicity of the approach, and it's looking promising in terms of performance -- it won't/can't be as fast as the compiled approach in #4739, but it's faster than decorator-5.0.7 for both import time and call time.
comment
In short, the approach in #4739 is to precisely duplicate the function signature, and simply copy over the `__defaults__` and `__kwdefaults__` -- this sacrifices some flexibility (like, we can't decorate builtins) to avoid calling `bind`.  The first-time call is quite slow, as it calls `inspect.signature` and later compiles code from text.    Subsequent calls are _fast_.  The newer approach (sorry, it's not in a state to PR yet) interrogates the `__code__` object and collects enough information to emulate a bare-bones `bind` with, I think, the same loss of flexibility in #4739.  The goal with this new approach is to make import time, and subsequent calls, all faster than 5.0.7, and also make first-time call faster than #4739, without the complexity of assembling and compiling function wrappers.  But, it's getting complicated and I might only be convincing myself that #4739 is okay
comment
Thanks, @alexchandel, after some back and forth, we're going to remove the decorator dependency for the 2.6 release.  Sorry for the inconvenience.
comment
> What use case do you have with fixed nodes not in the graph?  This came up for me a few months ago, when I was trying to draw a graph (of an absurdly large data structure) progressively.  It's convenient to store positions for a set of nodes, and use those positions to guide the spring-layout of the next 'level' of nodes.  In the end, I want a single dict that contains all the final positions.  It isn't difficult to do those separately and merge them as necessary, or filter the big dictionary for each call, but it's more convenient to use a big dict and not filter it.
comment
I've started looking around the `networkx.algorithms.flow` module and the first thing I noticed is:  ``` def _build_residual_network(G, demand, capacity, weight):     ... ```  Presumably, the `demand` and `weight` functions should receive the same treatment.  However, I'm unsure of how to proceed given a multigraph.  In `shortest_paths`, the situation is simple -- we take the min of edge-weights.  I haven't read enough of the internals to have a good idea if/how these parameters generalize, but at least for `capacity`, the code in `_build_residual_network` preserves the multi-edges; so a `capacity` function outputting a single number doesn't really make sense to me.  Thoughts?
comment
Hi, and thanks for submitting your first issue!  The Steiner Tree problem is NP-Hard, and the approximation algorithm is only guaranteed to be within a factor of an optimal solution.  From the docs,  > This algorithm produces a tree whose weight is within a (2 - (2 / t)) factor of the weight of the optimal Steiner tree where *t* is number of terminal nodes.  So while it is a somewhat unsatisfactory answer, the result you're getting is within that threshold.  The solution isn't wrong, per se, but sub-optimal.  As an experiment, you can double the edge weights among the nodes (0, 1, 2, 3).  This is a sufficient penalty to coax the algorithm into producing an optimal solution.
comment
Can you provide a definition of `to_networkx`?  On Tue, Apr 27, 2021 at 12:28 AM Mridul Seth ***@***.***> wrote:  > Could you provide a bit more details about the crash? Does it output any > error? > > — > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/networkx/networkx/issues/4762#issuecomment-827381518>, > or unsubscribe > <https://github.com/notifications/unsubscribe-auth/AAELCNSO25KG2UZR3UPZOTTTKZRTFANCNFSM43UCD4LA> > . > 
comment
Awesome!  Life has been getting in the way, but I'll set aside some time to read this tonight.
comment
>  I hate "to ask" Kelly to create a PR that may not ultimately be accepted, but it may be the best way forward if they are still willing to work on it knowing the NXEP is still a draft (and hasn't been accepted yet).  No worries there.  It's mostly written in bits and pieces throughout the conversation, it's just a matter of patching it together and writing a few sample generators.  With regards to backwards-compatibility, I think that we can possibly get away without any breakage there. The proposal mentions moving generators into the `nx.builders` namespace.  Perhaps we can maintain the `nx.generators` namespace with an automated wrapper?  This is a little half-baked so disagreement is more than welcome.
comment
I hacked something together recently -- it's nowhere up to snuff, but it might be a start for a quick matplotlib implementation:  ``` import networkx as nx from matplotlib import pyplot as plt def draw3d(G, pos):     ax = plt.gca(projection='3d')     X, Y, Z = zip(*pos.values())     ls = [[pos[u], pos[v]] for u,v in G.edges()]     lc = Line3DCollection(ls, linewidths=0.5, colors='b')     ax.add_collection(lc)     ax.plot(X, Y, Z, '.r', markersize=3)  g = nx.GridGraph([3,3,3]) draw3d(g, pos) plt.show() ```
comment
@zcollins0 are you still planning to work on this?  I recently became annoyed with how slow the K-L implementation is, and I can take this over if you're busy.
comment
I implemented the column and diagonal scanning algorithms, as well as an even simpler sweep which alternates between sides to move single nodes to/from.  This "best node first" strategy appears to be common practice.  I tried a bunch of examples, dense and sparse, and didn't observe a significant difference in the distribution of cuts produced by these methods.  The best-first algorithm has the simplest code by far, and ran the fastest overall.  As per @dschult 's comment, I used pre-existing data structures.  I did limited testing of both the PairingHeap and BinaryHeap (with a handful of dense and sparse graphs), and the BinaryHeap won out by about 10%.
comment
I traced this issue, and the root cause is that `all_shortest_paths` defaults to using Dijkstra's algorithm, which is only correct for positive weights.  Bellman-Ford does not have this limitation, but the runtime is worse.  I see two options here:  1) default to Bellman-Ford and suffer the runtime hit, or 2) check if all weights are positive to determine which algorithm to dispatch.
comment
Okay, I stand corrected.  I have a modification to Dijkstra's algorithm that handles zero-weight edges.  This does kick the problem down the road a little.  Minifying @nycnighthawk's example, I add the test:  ```     def test_all_shortest_paths_zero_weight_edge(self):         g = nx.Graph()         nx.add_path(g, [0, 1, 3])         nx.add_path(g, [0, 1, 2, 3])         g.edges[1, 2]['weight'] = 0         paths03 = list(nx.all_shortest_paths(g, 0, 3, weight='weight'))         paths30 = list(nx.all_shortest_paths(g, 3, 0, weight='weight'))         assert sorted(paths03) == sorted(p[::-1] for p in paths30) ```  Inside `all_shortest_paths`, we compute predecessors,      >>> nx.dijkstra_predecessor_and_distance(g, 0, 3, weight='weight')     {0: [], 1: [0], 3: [1, 2], 2: [1]}     >>> nx.dijkstra_predecessor_and_distance(g, 3, 0, weight='weight')         {3: [], 1: [3], 2: [3, 1], 0: [1]}  If we modify Dijkstra's to handle zero-weight edges correctly, we have      >>> nx.dijkstra_predecessor_and_distance(g, 0, 3, weight='weight')     {0: [], 1: [0, 2], 3: [1, 2], 2: [1]}     >>> nx.dijkstra_predecessor_and_distance(g, 3, 0, weight='weight')         {3: [], 1: [3, 2], 2: [3, 1], 0: [1]}  This looks fine, but results in "paths" of indefinite length.  Thus, I'm adding cycle-detection in the `all_shortest_paths` method.  However, I'm a little concerned about other consumers of `nx.algorithms.shortest_path._dijkstra_multisource` falling into this trap.  Thoughts?
comment
With regard to (1), I believe that the fix I posted should be correct in all cases of ties -- I'm not actually checking for zero-weight edges; I'm only checking for ties.  With regard to (2), we should first agree on the definition of a path.  The definition that I work with is that paths are not allowed to repeat nodes [1].  Under that definition, there are a finite number of paths from `s` to `t`; regardless of edge weights.  If nodes are allowed to repeat, then we'd be computing shortest _walks_, which admit infinite solutions in the presence of nonpositive edges, and I agree that the only feasible solution is to document the issue.  Under this definition of path, I believe my fix to be correct.  Edit: the following probably belongs on a new ticket  I've been looking a bit deeper into other shortest-path algorithms, and this issue isn't unique to Dijkstra.  I haven't done a comprehensive reading of the literature, but in what I'm reading; it seems to be standard to (a) prove that an algorithm works on digraphs in the presence of nonpositive edges, and (b) recommend treating undirected graphs as digraphs.  The issue that keeps cropping up is that 2-cycles do not exist in simple graphs, but they trap algorithms meant for digraphs.  So, that "standard advice" becomes a problem and the appropriate treatment of zero-weight edges is to contract the edges... but I'm yet unsure of how to properly handle negative-weight edges in undirected graphs.  I'll post some examples tonight of what's breaking down and what I've been able to fix.  Edit 2: oh, duh, handling negative undirected weights properly would solve the longest path problem (NP complete)...  [1] https://en.wikipedia.org/wiki/Path_(graph_theory)#Walk,_trail,_path
comment
As I suspected, there's more than one path-tracing implementation -- the one in `_bellman_ford` has the same flaw for undirected graphs.  ``` >>> G = nx.Graph()                                                                                                                    >>> G.add_weighted_edges_from([(0,1,0), (1,2,0), (2,3,1), (3,0,1)])                                                                   >>> nx.bellman_ford_path(G, 1, 0)  [infinite memory consumption loop] ```
comment
Yeah, this is a real bug in Bellman-Ford -- it can get tripped up on zero-cycles that don't contain zero-weight edges too:  ``` >>> D = nx.DiGraph() >>> D.add_weighted_edges_from([(0,1,1), (1,2,1), (2,3,1), (3,1,-2)])   >>> nx.bellman_ford_path(D, 1, 3) [hangs, gobbling memory] ```  It seems to me that we can't efficiently enumerate "all shortest paths" unless we actually mean all shortest simple paths -- if a node ever gets repeated, that indicates that paths of unbounded length will be generated.  Giving up on efficiency, we could store all prefixes of a given length, and `yield` the ones that find terminate on targets (this seems absurd).
comment
Yes, negative-weight cycles are handled correctly in theory and implementation -- the issue only crops up when tracing the paths back to a source.  I'm not sure why we'd want to report zero-weight cycles though.  Are you suggesting we raise an exception in Bellman-Ford?  IMHO the most permissive solution would be to also tweak Bellman-Ford to construct simple paths.  I've just pushed that to my current branch; factoring out the path-tracing algorithm from `all_shortest_paths` to an internal `_all_shortest_simple_paths(G, sources, target, pred)`.  Let me know what you think.
comment
I haven't dug into the example, but I read over `unionfind` and found a bug in union:  ```     def union(self, *objects):         """Find the sets containing the objects and merge them all."""         roots = [self[x] for x in objects]         # Find the heaviest root according to its weight.         heaviest = max(roots, key=lambda r: self.weights[r])         for r in roots:             if r != heaviest:                 self.weights[heaviest] += self.weights[r]                 self.parents[r] = heaviest ```  If `roots` has a repeated element (other than `r`), then that root's `weight` will get accumulated too much.  This can be fixed by turning the list-comprehension into a set.  Edit: the example won't be impacted by this...
comment
It looks like the spiral method suggested by @Radcliffe is quite biased towards the poles, and I share concerns about higher dimensions.  To add another candidate to the mix, there's a fast method to pick uniformly random points on a hypersphere due to Muller; 1959: pick a set of *d*-dimensional points with each coordinate normally distributed, and divide by their 2-norm to obtain unit vectors.      import numpy as np     import matplotlib.pyplot as plt     from mpl_toolkits.mplot3d import Axes3D      def hypersphere_points(n, dim):         points = np.random.normal(size=(dim, n))         return points / np.linalg.norm(points, axis=0)      def plot_points(n):         fig = plt.figure()         ax = fig.add_subplot(111, projection='3d')         P = hypersphere_points(n, 3)         ax.scatter(P[0,:], P[1,:], P[2,:], marker='o')         plt.show()      plot_points(200)  ![image](https://user-images.githubusercontent.com/569654/72377800-0810d700-36c5-11ea-9c77-7c90701be0ca.png)  see: http://mathworld.wolfram.com/HyperspherePointPicking.html
comment
@milcent that's a great question.  I did a little study to compare the uniform initialization as @stefanhannie suggested, to the hypersphere initialization that I contemplate above.  https://gist.github.com/boothby/007bfbfc632e9328b0260f1437aac7cb  My interpretation of the results is both initialization methods result in equally good layouts, so it's best to go with the simplest one.
