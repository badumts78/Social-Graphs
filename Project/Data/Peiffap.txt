issue
maint: perf improvements for writing edge- and adjlists#TITLE_END#Uses `str.join` instead of adding line breaks manually, allowing a single file write. Also adds benchmarks for this change and the change to `generate_adjlist` in #8146 (5400927 being the last commit _before_ that change got merged).  <details> <summary> asv continuous --bench listCompleteGraph 5400927 maint/edgelist </summary>  ``` | Change   | Before [54009272] <jj/keep/35f6b59dc342b28e93cd93769199988bbcc90f44~1>   | After [a2efd001] <jj/keep/a2efd0015e82073914c71d20f024eaf8424c39e9>   |   Ratio | Benchmark (Parameter)                                                                                   | |----------|--------------------------------------------------------------------------|-----------------------------------------------------------------------|---------|---------------------------------------------------------------------------------------------------------| | -        | 12.9±0.3ms                                                               | 11.4±0.3ms                                                            |    0.88 | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('Graph', 200, ('weight',))      | | -        | 3.91±0.1ms                                                               | 3.42±0.09ms                                                           |    0.88 | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('MultiGraph', 100, ('weight',)) | | -        | 6.54±0.2ms                                                               | 5.73±0.2ms                                                            |    0.88 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('Graph', 100, True)                       | | -        | 7.31±0.2ms                                                               | 6.42±0.08ms                                                           |    0.88 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiGraph', 100, True)                  | | -        | 13.7±0.2ms                                                               | 11.9±0.2ms                                                            |    0.87 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiDiGraph', 100, ('weight',))         | | -        | 53.6±1ms                                                                 | 46.0±1ms                                                              |    0.86 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiDiGraph', 200, True)                | | -        | 29.4±0.4ms                                                               | 24.8±0.3ms                                                            |    0.84 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiGraph', 200, True)                  | | -        | 49.0±0.5ms                                                               | 40.6±0.6ms                                                            |    0.83 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('DiGraph', 200, ('weight',))              | | -        | 48.2±0.4ms                                                               | 40.1±1ms                                                              |    0.83 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('DiGraph', 200, True)                     | | -        | 56.1±0.3ms                                                               | 46.6±0.5ms                                                            |    0.83 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiDiGraph', 200, ('weight',))         | | -        | 3.33±0.2ms                                                               | 2.74±0.09ms                                                           |    0.82 | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('Graph', 100, True)             | | -        | 3.63±0.2ms                                                               | 2.96±0.03ms                                                           |    0.82 | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('MultiGraph', 100, True)        | | -        | 29.6±0.4ms                                                               | 24.4±0.3ms                                                            |    0.82 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiGraph', 200, ('weight',))           | | -        | 12.7±0.9ms                                                               | 10.2±0.2ms                                                            |    0.81 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('DiGraph', 100, ('weight',))              | | -        | 27.5±0.4ms                                                               | 22.3±0.3ms                                                            |    0.81 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('Graph', 200, ('weight',))                | | -        | 27.1±1ms                                                                 | 21.9±0.4ms                                                            |    0.81 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('Graph', 200, True)                       | | -        | 14.6±0.3ms                                                               | 11.8±0.4ms                                                            |    0.81 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiDiGraph', 100, True)                | | -        | 33.8±0.3ms                                                               | 27.4±0.1ms                                                            |    0.81 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiDiGraph', 200, False)               | | -        | 7.64±0.09ms                                                              | 6.19±0.05ms                                                           |    0.81 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiGraph', 100, ('weight',))           | | -        | 4.87±0.09ms                                                              | 3.93±0.1ms                                                            |    0.81 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiGraph', 100, False)                 | | -        | 19.0±0.3ms                                                               | 15.3±0.1ms                                                            |    0.81 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiGraph', 200, False)                 | | -        | 12.3±0.3ms                                                               | 9.78±0.3ms                                                            |    0.8  | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('Graph', 200, True)             | | -        | 2.52±0.1ms                                                               | 2.02±0.04ms                                                           |    0.8  | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('MultiGraph', 100, False)       | | -        | 6.91±0.3ms                                                               | 5.53±0.2ms                                                            |    0.8  | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('Graph', 100, ('weight',))                | | -        | 3.82±0.4ms                                                               | 3.04±0.1ms                                                            |    0.8  | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('Graph', 100, False)                      | | -        | 15.4±0.2ms                                                               | 12.2±0.2ms                                                            |    0.79 | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('MultiGraph', 200, ('weight',)) | | -        | 8.99±0.5ms                                                               | 7.14±0.09ms                                                           |    0.79 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('MultiDiGraph', 100, False)               | | -        | 3.74±0.1ms                                                               | 2.94±0.08ms                                                           |    0.78 | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('Graph', 100, ('weight',))      | | -        | 7.80±0.2ms                                                               | 6.09±0.4ms                                                            |    0.78 | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('Graph', 200, False)            | | -        | 9.53±0.4ms                                                               | 7.41±0.3ms                                                            |    0.78 | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('MultiGraph', 200, False)       | | -        | 4.06±0.1ms                                                               | 3.13±0.2ms                                                            |    0.77 | benchmark_readwrite.AdjlistCompleteGraph.time_write_multiline_adjlist('MultiGraph', 100)                | | -        | 14.4±0.7ms                                                               | 11.0±0.2ms                                                            |    0.77 | benchmark_readwrite.EdgelistCompleteGraph.time_bipartite_write_edgelist('MultiGraph', 200, True)        | | -        | 26.4±0.6ms                                                               | 20.3±0.3ms                                                            |    0.77 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('DiGraph', 200, False)                    | | -        | 14.3±0.4ms                                                               | 11.0±0.2ms                                                            |    0.77 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('Graph', 200, False)                      | | -        | 28.2±0.8ms                                                               | 21.1±0.3ms                                                            |    0.75 | benchmark_readwrite.AdjlistCompleteGraph.time_write_multiline_adjlist('MultiDiGraph', 200)              | | -        | 3.37±0.1ms                                                               | 2.48±0.08ms                                                           |    0.74 | benchmark_readwrite.AdjlistCompleteGraph.time_write_multiline_adjlist('Graph', 100)                     | | -        | 7.24±0.3ms                                                               | 5.37±0.08ms                                                           |    0.74 | benchmark_readwrite.AdjlistCompleteGraph.time_write_multiline_adjlist('MultiDiGraph', 100)              | | -        | 15.9±0.3ms                                                               | 11.7±0.2ms                                                            |    0.74 | benchmark_readwrite.AdjlistCompleteGraph.time_write_multiline_adjlist('MultiGraph', 200)                | | -        | 6.77±0.2ms                                                               | 5.01±0.2ms                                                            |    0.74 | benchmark_readwrite.EdgelistCompleteGraph.time_write_edgelist('DiGraph', 100, False)                    | | -        | 13.0±0.3ms                                                               | 9.13±0.6ms                                                            |    0.7  | benchmark_readwrite.AdjlistCompleteGraph.time_write_multiline_adjlist('Graph', 200)                     | | -        | 5.57±0.08ms                                                              | 3.64±0.08ms                                                           |    0.65 | benchmark_readwrite.AdjlistCompleteGraph.time_write_multiline_adjlist('DiGraph', 100)                   | | -        | 1.82±0.1ms                                                               | 1.14±0.07ms                                                           |    0.62 | benchmark_readwrite.AdjlistCompleteGraph.time_write_adjlist('Graph', 100)                               | | -        | 22.1±1ms                                                                 | 13.2±0.1ms                                                            |    0.6  | benchmark_readwrite.AdjlistCompleteGraph.time_write_multiline_adjlist('DiGraph', 200)                   | | -        | 1.48±0.1ms                                                               | 854±20μs                                                              |    0.58 | benchmark_readwrite.AdjlistCompleteGraph.time_generate_adjlist('Graph', 100)                            | | -        | 2.73±0.07ms                                                              | 1.55±0.05ms                                                           |    0.57 | benchmark_readwrite.AdjlistCompleteGraph.time_write_adjlist('DiGraph', 100)                             | | -        | 6.61±0.2ms                                                               | 3.76±0.1ms                                                            |    0.57 | benchmark_readwrite.AdjlistCompleteGraph.time_write_adjlist('Graph', 200)                               | | -        | 2.26±0.07ms                                                              | 1.25±0.03ms                                                           |    0.55 | benchmark_readwrite.AdjlistCompleteGraph.time_generate_adjlist('DiGraph', 100)                          | | -        | 6.19±0.2ms                                                               | 3.35±0.04ms                                                           |    0.54 | benchmark_readwrite.AdjlistCompleteGraph.time_generate_adjlist('Graph', 200)                            | | -        | 9.62±0.3ms                                                               | 5.08±0.06ms                                                           |    0.53 | benchmark_readwrite.AdjlistCompleteGraph.time_generate_adjlist('DiGraph', 200)                          | | -        | 10.4±0.3ms                                                               | 5.59±0.2ms                                                            |    0.53 | benchmark_readwrite.AdjlistCompleteGraph.time_write_adjlist('DiGraph', 200)                             | | -        | 2.36±0.1ms                                                               | 1.22±0.03ms                                                           |    0.52 | benchmark_readwrite.AdjlistCompleteGraph.time_write_adjlist('MultiGraph', 100)                          | | -        | 8.68±0.2ms                                                               | 4.44±0.4ms                                                            |    0.51 | benchmark_readwrite.AdjlistCompleteGraph.time_write_adjlist('MultiGraph', 200)                          | | -        | 3.54±0.04ms                                                              | 1.79±0.09ms                                                           |    0.5  | benchmark_readwrite.AdjlistCompleteGraph.time_write_adjlist('MultiDiGraph', 100)                        | | -        | 2.02±0.06ms                                                              | 949±10μs                                                              |    0.47 | benchmark_readwrite.AdjlistCompleteGraph.time_generate_adjlist('MultiGraph', 100)                       | | -        | 8.07±0.4ms                                                               | 3.75±0.02ms                                                           |    0.46 | benchmark_readwrite.AdjlistCompleteGraph.time_generate_adjlist('MultiGraph', 200)                       | | -        | 14.2±0.3ms                                                               | 6.57±0.2ms                                                            |    0.46 | benchmark_readwrite.AdjlistCompleteGraph.time_write_adjlist('MultiDiGraph', 200)                        | | -        | 3.24±0.1ms                                                               | 1.44±0.03ms                                                           |    0.44 | benchmark_readwrite.AdjlistCompleteGraph.time_generate_adjlist('MultiDiGraph', 100)                     | | -        | 13.6±0.5ms                                                               | 5.91±0.1ms                                                            |    0.44 | benchmark_readwrite.AdjlistCompleteGraph.time_generate_adjlist('MultiDiGraph', 200)                     |  SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY. PERFORMANCE INCREASED. ``` </details>
issue
Make dominance functions consistent with definitions#TITLE_END#Fixes #6710. Fixes #6713.  This enforces consistency with definitions: * The entry node does not have an immediate dominator - this follows from the definition that `d` immediately dominates `n` if `d` strictly dominates `n` + another condition, but `d` cannot strictly dominate itself; I've added a reference to corroborate this. * The dominance frontier should handle loops on the entry node properly. The second linked issue (#6713) has a detailed example of what the current behavior is vs. what applying the definition leads to. ~It also synergizes well with the first change.~  Edit: after discussing, it made more sense to exclude the start node from the returned dictionary in `immediate_dominators`.
issue
BUG: allow graphs with nonstandard node labels in FISTA#TITLE_END#Closes #8271. Supersedes #8328.  This PR fixes the erroneous assumption that graphs for the FISTA algorithm always have node labels `0` through `n - 1`. It also adds a regression test for two such cases.
issue
BUG: `nx.approximation.densest_subgraph` with FISTA method errors for graphs with nonstandard node labels#TITLE_END#### Current Behavior  When using the FISTA algorithm for the approximate densest subgraph problem, graphs with nonstandard node labels (i.e. anything other than 0, ..., n) raise an `IndexError` in the underlying `numpy` code.  ### Expected Behavior  These inputs are valid graphs and should not raise, which they don't when `method="greedy++"` is used instead.  ### Steps to Reproduce This example uses integer node labels, but they are not consecutive. ```pycon >>> G = nx.complete_graph([0, 1, 3]) >>> nx.approximation.densest_subgraph(G, method="fista") Traceback (most recent call last):   ... IndexError: index 3 is out of bounds for axis 0 with size 3 ```  This example uses consecutive integer node labels that don't start at 0. ```pycon >>> G = nx.complete_graph([1, 2, 3]) >>> nx.approximation.densest_subgraph(G, method="fista") Traceback (most recent call last):   ... IndexError: index 3 is out of bounds for axis 0 with size 3 ```  This example uses non-integer node labels: ```pycon >>> G = nx.complete_graph("abc") >>> nx.approximation.densest_subgraph(G, method="fista") Traceback (most recent call last):   ... IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices ```  For reference, these graphs are correctly handled by the other algorith, Greedy++: ```pycon >>> G = nx.complete_graph([0, 1, 3]) >>> nx.approximation.densest_subgraph(G, method="greedy++") (1.0, {0, 1, 3}) >>> G = nx.complete_graph([1, 2, 3]) >>> nx.approximation.densest_subgraph(G, method="greedy++") (1.0, {1, 2, 3}) >>> G = nx.complete_graph("abc") >>> nx.approximation.densest_subgraph(G, method="greedy++") (1.0, {'b', 'a', 'c'}) ```  ### Environment  <!--- Please provide details about your local environment -->  Python version: Python 3.13.7 (main, Aug 14 2025, 11:12:11) [Clang 17.0.0 (clang-1700.0.13.3)] on darwin NetworkX version: latest development branch as of 0bad061e  ### Additional context  I believe this can be fixed quite easily: ```diff diff --git a/networkx/algorithms/approximation/density.py b/networkx/algorithms/approximation/density.py index 858b81ed9..b722df22c 100644 --- a/networkx/algorithms/approximation/density.py +++ b/networkx/algorithms/approximation/density.py @@ -92,8 +92,8 @@ def _fractional_peeling(G, b, x, node_to_idx, edge_to_idx):      remaining_nodes = set(G.nodes)       # Initialize heap with b values -    for idx in remaining_nodes: -        heap.insert(idx, b[idx]) +    for idx, node in enumerate(G): +        heap.insert(node, b[idx])       num_edges = G.number_of_edges() ```  I have a minor refactoring PR in the queue at #8270. Once that is merged, I can make a PR to with this fix, to avoid merge conflicts. Or I can include it in that PR since it's a tiny change, whatever you prefer! :) 
issue
DOC: rework betweenness centrality docstrings#TITLE_END#Fix #8263.  This PR addresses some of the points in the linked issue. Currently, I've only included an example for `betweenness_centrality`. If that one seems reasonable enough, I'll adapt it for the other 3 functions.
issue
DOC: update docs for betweenness centrality functions#TITLE_END#The docstrings of the betweenness centrality functions (`(edge_)betweenness_centrality(_subset)`) have grown to be quite outdated and incomplete/unclear, especially since #7908 and #8256 recently got merged. They could use some TLC, too!  In particular, the following points should be clarified: - The docstring poorly addresses the effect of setting `k` (it only affects the number of $s$ nodes, not $t$). - There is currently no mention of how the `k` and `endpoints` parameters affect normalization. - The description for the `normalized` parameter incorrectly distinguishes between directed and undirected graphs. - In general, an example is worth a thousand words. :)  Additionally, some minor nits: - The math expressions are sometimes poorly formatted (no curly braces around sets, missing use of backticks or math-mode in some places, ...). - Inconsistent spacing and indentation. - It might be clearer to avoid duplicating information about both `betweenness_centrality` and `betweenness_centrality_subset` in both docstrings.
issue
MAINT: use `assert np.allclose` instead of `np.testing.assert_almost_equal`#TITLE_END#Inspired by https://github.com/scipy/scipy/pull/23592. ~NumPy recommends that `assert_allclose` be used instead of [`assert_almost_equal`](https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_almost_equal.html).~ [Ross suggested](https://github.com/networkx/networkx/pull/8278#pullrequestreview-3241858655) using `assert np.allclose` instead. NetworkX has no uses of `assert_array_almost_equal` or `assert_approx_equal`.  From the linked SciPy PR: > For `assert_almost_equal` [...], the `decimal` parameter becomes `atol=1.5e-(decimal), rtol=0` in `assert_allclose()`.  ~The default value is `decimal=7`, which has thus been replaced with `atol=1.5e-7, rtol=0`. As `assert_allclose` was introduced in NumPy 2.0.0, it should be good for the NetworkX 3.6 release according to [SPEC 0](https://scientific-python.org/specs/spec-0000/#support-window).~  
issue
CI: remove doctests from `coverage` job#TITLE_END#The coverage job should probably not be running doctests: 1. if doctests are adding coverage, then we should flag the missing lines for additional, non-doctest coverage 2. if they aren't, we might as well exclude them.  Based on https://github.com/networkx/networkx/pull/8273#discussion_r2353936390.
issue
TST: test `topo_sort` skips visited nodes in `goldberg_radzik`#TITLE_END#Closes #8223.  The branch https://github.com/networkx/networkx/blob/e8a8c1944440bd8cf216381366df40a8210a0743/networkx/algorithms/shortest_paths/weighted.py#L2079-L2081 was not reliably being entered because iterating over `relabeled` is not deterministic.  This PR adds a test that guarantees the branch would have been entered/the set difference is doing something.
issue
CI/TST: remove `runslow` tests from `coverage` workflow#TITLE_END#The `coverage` job is currently the longest CI job we have, and if #8178 gets merged in its current state, that will only get worse. We should try to bring down its runtime.  ---  I think the highest-impact approach to "fixing" the coverage job is to get the `runslow` tests out of there. I suspect that at least some of the tests marked `slow` have no impact on coverage at all. So I'd propose something like:  1. Produce a coverage report now, as-is-, with the slow tests included  2. Produce one without the slow tests  3. diff the two coverage reports to identify the subset of functions where coverage is reduced when the slow tests are removed  4. For this subset of tests, see if we can't develop *new* unit tests that cover the missing lines with a more manageable resource load  5. Add these tests to get coverage back to where it was before removing the `slow` tests from the coverage job  6. Move the `slow` tests to a different, dedicated job in CI.  _Originally posted by @rossbar in https://github.com/networkx/networkx/issues/8178#issuecomment-3206934468_  ---  The following files need to have their non-slow coverage improved (taken from https://github.com/networkx/networkx/issues/8223#issuecomment-3249510785).  - [x] `networkx/algorithms/community/asyn_fluid.py`: #8224 - [x] `networkx/algorithms/connectivity/kcomponents.py`: #8239 - [x] `networkx/algorithms/connectivity/kcutsets.py`: #8230 - [x] `networkx/algorithms/isomorphism/isomorphvf2.py`: #8251 - [x] `networkx/drawing/nx_pylab.py`: #8232 - [x] `networkx/generators/cographs.py`: #8228 - [x] `networkx/generators/degree_seq.py`: #8226 - [x] `networkx/generators/directed.py`: #8231 - [x] `networkx/generators/internet_as_graphs.py`: #8225 - [x] `networkx/generators/random_graphs.py`: #8252 - [x] `networkx/utils/misc.py`: #8233 - [ ] `networkx/algorithms/shortest_paths/weighted.py`: #8279  ---  Once the above all get checked off, the `coverage` job should be updated to remove the `runslow` flag, and a new CI job should be added created somewhere in CI to keep track of the slow tests without running coverage on them (step 6). - [x] update `coverage` job and migrate `slow` tests: #8273
issue
TST: add coverage for some branches in `internet_as_graphs.py`#TITLE_END#Towards #8223.  This PR adds coverage for several hard-to-hit branches in `internet_as_graphs.py`. The current difference in coverage stems from the test for `random_internet_as_graph` in `test_all_random_functions.py`, which was hitting the unsuccessful branch in `add_m_peering_link`.
issue
DOC: fix wrong reference in `leiden` docs#TITLE_END#https://www.nature.com/articles/s41598-019-41695-z shows this is the right name :)
issue
CI/MAINT: update coverage badge with CircleCI coverage status#TITLE_END#The current coverage badge in `README.md` still refers to the `codecov` coverage report. It should be updated/removed now that the `coverage` job runs on CircleCI.
issue
DOC/MAINT: Use `itertools.pairwise` in `pairwise` and add docstring#TITLE_END#The current version of `nx.utils.pairwise` on main still uses an outdated recipe from before an optimized version of `itertools.pairwise` was added to the standard library in 3.10. This PR defaults to using that implementation, except when `cyclic == True` in which case the old behavior is maintained. ~Existing uses of `nx.utils.pairwise` have been replaced where possible.~  Edit: This PR also adds a docstring to the function.
issue
TST: add `random_k_out_graph` to tests to hit `try except` path#TITLE_END#Towards #8223.  The current non-`slow` tests only test the pure Python and `numpy` implementations, not the `random_k_out_graph` function itself (which is tested in the `slow` `test_all_random_functions`): https://github.com/networkx/networkx/blob/6fad2d34112d5d860ef9f3dd997936c77d454b09/networkx/generators/directed.py#L512-L515 This test ensures parity in coverage. It also adds a seed to the "no self-loops" test because, which shouldn't matter for coverage but having deterministic tests seems like a reasonable thing to do in general.  This PR only makes change to (somewhat artificially) improve coverage. A related, more important question that would be good to tackle in a follow-up: there are multiple places in the code base where this "try to use `numpy`, and if not available fall-back to Python" pattern is used. This is usually not a problem because the CI pipeline has jobs that run with and jobs that run without `numpy` available. The coverage job only runs with `numpy` enabled. What would be the best way to make sure we can reach full coverage in these situations?  EDIT ----  This PR also ensures `numpy` scalars are unwrapped.
issue
TST: ensure determinism in `nx_pylab` drawing tests#TITLE_END#Towards #8223.  This PR is an attempt to find out more about why the `draw` method of `CurvedArrowText` sometimes gets hit, and sometimes doesn't. https://github.com/networkx/networkx/blob/6fad2d34112d5d860ef9f3dd997936c77d454b09/networkx/drawing/nx_pylab.py#L248-L254 In #8223 for example, running only the non-`slow` tests covered these lines, whereas running all tests didn't (as such, I guess this doesn't strictly belong in #8223, but it'd be good to fix this anyways).  I can confirm this is _not_ an issue with `pytest-cov`; adding an `assert False` in the `draw` method sometimes fails, and sometimes doesn't. Perhaps even more surprisingly, it's not immediately obvious to me where the randomness is coming from, here. `draw_planar` is one of the functions for which we hit the failing `assert` I added most often, yet it has no obvious source of randomness.  5c2eb6d adds an explicit `seed` value to some random tests; this had no effect on coverage but is a reasonable change regardless. a8a72e7 parametrized the `test_draw` test, to make it easier to debug which functions were hitting the code path. faf4ae3 just added a missing function to the `function` parameter.  cc @rossbar (you somewhat recently worked on `draw_bipartite`, which also sometimes hits this code path) and @dg-pb (you wrote the original `CurvedArrowText`).
issue
TST: add coverage for `isomorphvf2`#TITLE_END#Towards #8223.  This adds random seeds to some functions in `test_isomorphvf2` and adds some new tests from a random search of the input space to improve test coverage for `DiGraphMatcher`.
issue
BUG/MAINT: fix edge betweenness centrality scaling when `k<N` and merge all b.c. rescale helper functions#TITLE_END#This PR refactors the rescaling code in `betweenness.py` and `betweenness_subset.py` to use the common `_rescale` function. Previously, these were separate functions for each situation (subset or not, node or edge).  Only one of the four cases (`_rescale` in `betweenness.py`, the version we keep) uses the more complicated logic with `k`, `endpoints`, and `sampled_nodes`. By reworking the helper function to no longer use a `k` parameter (instead relying on `sampled_nodes` being either `None`, or having length `k`), and giving default values to the other parameters, we end up being able to reuse the same code each time (modulo an off-by one change for edge betweenness centralities).  It also fixes an incorrect reference for the edge betweenness centralities.  One thing that stood out to me while working on this is that the (removed in this PR) function `_rescale_e` in `betweenness.py` had a `k` parameter, but never set it to anything other than `None` (and it didn't have a `sampled_nodes` parameter at all). None of the tests seem to catch this no matter what I set it to, but it'd be good to make sure this is supposed to be ignored!
issue
TST/MAINT: add non-slow coverage for random generators#TITLE_END#Towards #8223.   This PR adds some tests to improve the non-`slow` coverage of random number generation.
issue
TST: Add a nonslow test for `all_node_cuts` with shortest augmenting path flow function#TITLE_END#Towards #8223.  Initially, I hoped to fix the coverage difference between `slow` and non-`slow` tests by removing the `slow` marker from the alternative flow function test, but `preflow_push` still takes a long time and so I couldn't just remove the marker. I decided to leave the change (9927206) because IMO it's slightly cleaner this way, probably easier to debug if it fails at some point, and if in the future we decide to mark individual flow functions as `slow` using fixtures then this will be slightly easier to update. Note that recomputing the node connectivity takes negligible time compared to `all_node_cuts`.  ca6f248 adds only the `shortest_augmenting_path` flow function case, for one graph. This ensures we hit this branch https://github.com/networkx/networkx/blob/6fad2d34112d5d860ef9f3dd997936c77d454b09/networkx/algorithms/connectivity/kcutsets.py#L118-L119 which was previously only being hit in the `slow` test.  ---  These tests have a decent amount of redundant code. Might be worth trying to clean some of that up in follow-ups!
issue
ENH: add `directed` kwarg to `edges_equal`#TITLE_END#Fixes #8137.  This PR adds a `directed` kwarg to `edges_equal` to indicate edgelists should be treated as coming from directed graphs.
issue
maint: use `nx.circulant_graph` to generate Harary graphs#TITLE_END#This PR does a few related things: - Batch edge addition in `nx.circulant_graph`. - Clean up docstrings for the Harary graph generators. - Use `nx.circulant_graph` in the Harary graph generators + batch edge additions where possible.
issue
`edges_equal` implicitly assumes edges are undirected#TITLE_END#While working on #8077, I realised `edges_equal` currently duplicates edges when building the attribute dictionaries for each edge. https://github.com/networkx/networkx/blob/9f8e7e298828ced14200e6bdabcd6d02f5965cec/networkx/utils/misc.py#L540-L546  This means that directed graphs that only differ in the direction of their edges are handled incorrectly: ```python-console >>> import networkx as nx >>> G = nx.DiGraph([(0, 1)]) >>> H = nx.DiGraph([(1, 0)]) >>> nx.utils.edges_equal(G.edges, H.edges) True ```  This behavior is not necessarily intuitive—nothing about the function name or even description mentions the directedness of edges. This leaves the following options: 1. Always assume edges are directed, but that would be backward incompatible. 2. Add an argument specifying whether edges should be considered as directed or not, although that brings up the issue of what to do if only one of the edge lists given is directed (add a flag for each edge list?) 3. Explicitly clarify that edge directedness is ignored in the docstring, as is done with the order of the edges. This is the simplest solution by far, but it does mean there's a gap in functionality (tools for comparing edges of directed graphs) that I think is reasonable to fill. 
issue
Use CircleCI for coverage workflow#TITLE_END#Fix #8157.  Trying out running the coverage job on CircleCI. This is an alternative to #8177 to fix #8157.
issue
TST: add non-`slow` coverage for random graph generators#TITLE_END#Towards #8223.  This PR adds two tests to ensure random graph generation is equally well-tested with and without `slow` tests.
issue
MAINT: clean up tests for `steiner_tree`#TITLE_END#Follow-up to #8052.  This PR has the remaining nits by Ross and myself for the linked PR: - leave off specialized assertion messages; - make `method` a fixture that can be reused everywhere; - parametrize `test_weighted` on `weight` and `expected_edges`.
issue
perf: add additional benchmark for `is_reachable`#TITLE_END#It would be nice to split up the benchmark `is_reachable` benchmarks into two functions—one where the nodes are known to be reachable, and one where they aren't reachable! You can compute SCCs to enforce this condition I believe. That should better showcase the usefulness and importance of short-circuiting—or, if the short-circuits are already being hit, whether the speed-up is equally impressive without short-circuiting. In this case, I believe both cases will be significantly faster than before, but it doesn't hurt to make sure!  _Originally posted by @Peiffap in https://github.com/networkx/networkx/pull/8112#discussion_r2210245206_             
issue
MAINT/TST: clean up tests for `degree_seq`#TITLE_END#Follow-up from #8226.  This PR cleans up and consolidates some of the tests in `test_degree_seq.py`, including a suggestion from @dschult that didn't get taken into account. It also fixes a logic error introduced in the linked PR that assumed `expected_degree_graph` will return a graph with exactly matching degree sequence, which is not always the case.
issue
TST: test `max_iter` in `asyn_fluidc`#TITLE_END#Towards #8223.  This PR improves the logic for handling the maximum number of iterations in `asyn_fluidc` and adds a test to verify `max_iter` is taken into account as expected.
issue
TST: improve coverage for `generators/deg_seq.py`#TITLE_END#Towards #8223.  625d99c adds a small test that hits the `False` branch of https://github.com/networkx/networkx/blob/6fad2d34112d5d860ef9f3dd997936c77d454b09/networkx/generators/degree_seq.py#L434-L436  849dcae adds a test to hit the "too many nodes" branch: https://github.com/networkx/networkx/blob/6fad2d34112d5d860ef9f3dd997936c77d454b09/networkx/generators/degree_seq.py#L686-L688 I obtained the degree sequence for this test by inspecting the test in `test_all_random_functions.py` that passes through this branch with `runslow` (the one for `nx.random_powerlaw_tree`). But... that's not a valid degree sequence for a tree... I'll raise a separate issue to address including a more complete "tree check", but I'm worried it won't be as trivial of a change as might seem, specifically because of other functions that depend on `degree_sequence_tree`.
issue
BUG: add check for isolated nodes in `degree_sequence_tree`#TITLE_END#Fixes #8227.  This PR adds the check suggested in the linked issue for zero-degree nodes, which cannot appear in a valid degree sequence.  To do so, a utility function `is_valid_tree_degree_sequence` has been added, which returns whether a degree sequence follows two basic rules: the number of nodes must be equal to one more than the number of edges, and there cannot be any negative degrees in the degree sequence unless the degree sequence represents the trivial graph. It also gives a string representing which of the two checks was failed, to simplify error reporting downstream; I wasn't sure whether returning the string was worth doing; perhaps downstream functions using this utility function should just raise with a generic `"invalid degree sequence for tree"` message.  This change affects `random_powerlaw_tree_sequence` in that it now becomes (significantly?) less likely to yield a "valid" sequence in the given number of `tries`.
issue
BUG: `degree_sequence_tree` accepts degree sequences with isolated nodes#TITLE_END#<!-- If you have a general question about NetworkX, please use the discussions tab to create a new discussion -->  <!--- Provide a general summary of the issue in the Title above -->  ### Current Behavior  <!--- Tell us what happens instead of the expected behavior --> Calling `nx.degree_sequence_tree` with an obviously invalid degree sequence for a tree (with isolated nodes, i.e. degree zero) does not raise an error, and instead reports a nonsensical answer.  ### Expected Behavior  <!--- Tell us what should happen --> `nx.degree_sequence_tree` should either raise for all degree sequences for which we can easily determine no tree exists, or not check its input at all.  For this specific case, detect offending degree sequences can be detected by raising if `not all(d > 0 for d in deg_sequence)`.  ### Steps to Reproduce  <!--- Provide a minimal example that reproduces the bug --> Minimal example from #8226.  ```python-console >>> import networkx as nx >>> deg_seq = [1, 16, 1, 4, 0, 0, 1, 1, 0, 1, 2, 0, 1, 0, 1, 5, 1, 2, 1, 0] >>> G = nx.degree_sequence_tree(deg_seq) >>> print(G.nodes) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25] >>> print(G.edges) [(1, 2), (2, 3), (3, 4), (3, 7), (3, 8), (4, 5), (4, 9), (4, 10), (4, 11), (5, 6), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 18), (5, 19), (5, 20), (5, 21), (5, 22), (5, 23), (5, 24), (5, 25)] ``` The obvious problem here is that `G` has length 25, whereas `deg_seq` only has length 20.  ### Environment  <!--- Please provide details about your local environment -->  Python version: 3.13.7 NetworkX version: latest development version ### Additional context  <!--- Add any other context about the problem here, screenshots, etc. --> At least one other function (`nx.random_powerlaw_tree`) depends on this behavior in some way. Simply detecting offending sequences and raising if one is found breaks this, but a workaround might be possible.
issue
MAINT/TST: increase non-`slow` coverage in `k_components`#TITLE_END#Towards #8223.  This PR adds some minor perf/readability improvements in `k_components` to bring down the runtime of the `slow` tests (~5% improvement), and adds some smaller, non-`slow` tests to hit the branches that were previously only being hit by the `slow` tests.
issue
DOC: Add docstring for `number_of_cliques`#TITLE_END#While working on #8214, I saw this function doesn't have a proper docstring despite being part of the public API.
issue
DOC: add docstring for `degree_sequence_tree`#TITLE_END#Noticed while working on #8235 that `degree_sequence_tree` does not have a proper docstring. This PR adds one (with changes included to reflect the changes from #8235, so these should go in the same release).
issue
TST: add seed for `random_cograph` test#TITLE_END#Towards #8223.  This test would sometimes fail to hit one of these branches. https://github.com/networkx/networkx/blob/main/networkx/generators/cographs.py#L63-L66  Adding a fixed seed of 42 resolves the spurious coverage misses.
issue
feat(api): update non-tree check in `_tree_center`  and move to `tree` subpackage#TITLE_END#Fixes #8098.  This PR moves the `_tree_center` function to the `tree` subpackage in a new `tree.distance_measures` module, allowing users to write `nx.tree.center(G)`.  Other than changing the location and making this function more visible, there are several other changes: - Clean-ups to the docstring (backticks, grammar) - The error type for nontree inputs was changed from `nx.NetworkXError` to `nx.NotATree`---since this was previously not exposed, and the call in `nx.center` only happens after verifying the graph is a tree, this shouldn't be backward incompatible. - The check for nontree input is slightly more robust now by also checking if there are candidate centers left (are there still cases we don't catch?). - Additional tests.  This will also make an eventual migration of the tree centroid/barycenter function (after #8089 gets merged) easier.  Cc @amcandio who made the initial improvement, @rossbar who suggested moving to the `tree` subpackage.   <details> <summary> EDIT: Comprehensive diff </summary>  Dan asked for a comprehensive overview of what changed. Here's the output of `git diff --no-index old.py new.py` where `old.py` and `new.py` are two files with the old version and new version respectively of the tree center function.  ```diff diff --git a/old.py b/new.py index ff75b8c5..3acb8087 100644 --- a/old.py +++ b/new.py @@ -1,13 +1,13 @@  @not_implemented_for("directed") -def _tree_center(G): +def center(G):      """Returns the center of an undirected tree graph.      The center of a tree consists of nodes that minimize the maximum eccentricity.      That is, these nodes minimize the maximum distance to all other nodes.      This implementation currently only works for unweighted edges.      If the input graph is not a tree, results are not guaranteed to be correct and while -    some non-trees will raise a ``NetworkXError`` not all non-trees will be discovered. +    some non-trees will raise a `nx.NotATree` exception; not all non-trees will be discovered.      Thus, this function should not be used if caller is unsure whether the input graph -    is a tree. Use ``networkx.is_tree(G)`` to check. +    is a tree. Use ``nx.is_tree(G)`` to check.      Parameters      ----------      G : NetworkX graph @@ -18,27 +18,27 @@ def _tree_center(G):          A list of nodes in the center of the tree. This could be one or two nodes.      Raises      ------ -    NetworkXError -        If algorithm detects input graph is not a tree. There is no guarantee +    NotATree +        If the algorithm detects input graph is not a tree. There is no guarantee          this error will always raise if a non-tree is passed.      Notes      -----      This algorithm iteratively removes leaves (nodes with degree 1) from the tree until      there are only 1 or 2 nodes left. The remaining nodes form the center of the tree. -    This algorithm's time complexity is O(N) where N is the number of nodes in the tree. +    This algorithm's time complexity is ``O(N)`` where ``N`` is the number of nodes in the tree.      Examples      --------      >>> G = nx.Graph([(1, 2), (1, 3), (2, 4), (2, 5)]) -    >>> _tree_center(G) +    >>> nx.tree.center(G)      [1, 2] -    >>> G = nx.Graph([(1, 2), (2, 3), (3, 4), (4, 5)]) -    >>> _tree_center(G) -    [3] +    >>> G = nx.path_graph(5) +    >>> nx.tree.center(G) +    [2]      """      center_candidates_degree = dict(G.degree)      leaves = {node for node, degree in center_candidates_degree.items() if degree == 1}   -    # It's better to fail than an infinite loop, so check leaves to ensure progress +    # It's better to fail than an infinite loop, so check leaves to ensure progress.      while len(center_candidates_degree) > 2 and leaves:          new_leaves = set()          for leaf in leaves: @@ -47,14 +47,14 @@ def _tree_center(G):                  if neighbor not in center_candidates_degree:                      continue                  center_candidates_degree[neighbor] -= 1 -                if center_candidates_degree[neighbor] == 1: +                if (cddn := center_candidates_degree[neighbor]) == 1:                      new_leaves.add(neighbor) +                elif cddn == 0 and len(center_candidates_degree) != 1: +                    raise nx.NotATree("input graph is not a tree")          leaves = new_leaves   -    if not leaves and len(center_candidates_degree) >= 2: -        # We detected graph is not a tree. This check does not cover all cases. -        # For example, it does not cover the case where we have two islands (A-B) and (B-C) -        # where we might eliminate (B-C) leaves and return [A, B] as centers. -        raise nx.NetworkXError("Input graph is not a tree") +    if (n := len(center_candidates_degree)) not in {1, 2} or n == 2 and not leaves: +        # We detected graph is not a tree. This check does not necessarily cover all cases? +        raise nx.NotATree("input graph is not a tree")        return list(center_candidates_degree) ``` </details>
issue
Move `_tree_center` to a separate module#TITLE_END#> What I would propose is that the definition gets moved to the `networkx/algorithms/tree` package, perhaps in a new, private module `tree/_distance_measures.py`[^1] with the name exported *only* to the `tree` namespace. Therefore users could in principle call the function like `nx.tree.center`. I'd definitely be in favor of keeping the condition in `distance_measures.center` as well as you've done here (L756 would just change from `return _tree_center(G)` to `return nx.tree.center(G)`)  [^1]: I suspect that `center` is not the only distance measure for which a more efficient algorithm exists specifically for trees!  ---  This came up again in #8089 but was _originally posted by @rossbar in https://github.com/networkx/networkx/pull/7946#pullrequestreview-2728050181_             
issue
BUG: Raise on directed graphs in `nx.find_cliques_recursive`#TITLE_END#Fixes #8209.   This PR adds the `not_implemented_for("directed")` decorator to `nx.find_cliques_recursive`, similarly to `nx.find_cliques`. It also adds a test (for both functions) that directed graphs now raise `nx.NetworkXNotImplemented`.
issue
`nx.find_cliques_recursive`: nonsensical output for some directed graphs#TITLE_END#<!-- If you have a general question about NetworkX, please use the discussions tab to create a new discussion -->  <!--- Provide a general summary of the issue in the Title above -->  ### Current Behavior  <!--- Tell us what happens instead of the expected behavior --> When computing the cliques in a directed graph with `find_cliques_recursive`, the results can sometimes be nonsensical: ```python-console >>> import networkx as nx >>> G = nx.path_graph(4, create_using=nx.DiGraph) >>> list(nx.find_cliques_recursive(G)) [[0, 1], [2, 3], [3]] ```  ### Expected Behavior  <!--- Tell us what should happen --> It probably makes the most sense to exclude directed graphs (as is mentioned in the comment above the function definition): https://github.com/networkx/networkx/blob/cbc117da86247dc5c15ced7d48f378cd5ab10d35/networkx/algorithms/clique.py#L300-L302  This is also what is the other clique functions do: ```python-console >>> list(nx.find_cliques(G)) Traceback (most recent call last):   File "<python-input-3>", line 1, in <module>     list(nx.find_cliques(G))          ~~~~~~~~~~~~~~~^^^   File "/Users/gillespeiffer/Documents/foss/networkx/networkx/utils/decorators.py", line 784, in func     return argmap._lazy_compile(__wrapper)(*args, **kwargs)            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^   File "<class 'networkx.utils.decorators.argmap'> compilation 25", line 3, in argmap_find_cliques_21     import gzip          ^^^^^^   File "/Users/gillespeiffer/Documents/foss/networkx/networkx/utils/decorators.py", line 87, in _not_implemented_for     raise nx.NetworkXNotImplemented(errmsg) networkx.exception.NetworkXNotImplemented: not implemented for directed type >>> >>> list(nx.enumerate_all_cliques(G)) Traceback (most recent call last):   File "<python-input-4>", line 1, in <module>     list(nx.enumerate_all_cliques(G))          ~~~~~~~~~~~~~~~~~~~~~~~~^^^   File "/Users/gillespeiffer/Documents/foss/networkx/networkx/utils/decorators.py", line 784, in func     return argmap._lazy_compile(__wrapper)(*args, **kwargs)            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^   File "<class 'networkx.utils.decorators.argmap'> compilation 30", line 3, in argmap_enumerate_all_cliques_26     import gzip          ^^^^^^   File "/Users/gillespeiffer/Documents/foss/networkx/networkx/utils/decorators.py", line 87, in _not_implemented_for     raise nx.NetworkXNotImplemented(errmsg) networkx.exception.NetworkXNotImplemented: not implemented for directed type ```  Another possibility is to search for sets of nodes such that there are bidirectional edges between each pair of them, but that would not be consistent with the other clique functions as mentioned above.  ### Steps to Reproduce  <!--- Provide a minimal example that reproduces the bug --> See above.  ### Environment  <!--- Please provide details about your local environment -->  Python version: 3.13.7 NetworkX version: 3.5.1rc0.dev0  ### Additional context  <!--- Add any other context about the problem here, screenshots, etc. --> 
issue
MAINT: remove `try except` for `tomllib` in `generate_requirements`#TITLE_END#With the minimum supported Python version being 3.11, this is no longer necessary.
issue
MAINT: use `matrix_power` from `scipy.sparse` in `number_of_walks`#TITLE_END#Probably could have done this as part of #8080 but I didn't know about it then.  `sp.sparse.linalg.matrix_power` is [in Scipy since 1.12.0](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.matrix_power.html), which will be the [lowest supported version](https://scientific-python.org/specs/spec-0000/) in NetworkX 3.6.
issue
Avoid re-exploring nodes in Kosaraju's SCC algorithm#TITLE_END#Fixes #4181.  EDIT ---- This PR speeds up the Kosaraju SCC computation by 1. Avoiding re-exploring nodes multiple times in Kosaraju's SCC algorithm. 2. Short-circuiting when all nodes in the graph have been seen, similarly to what was done in #7971. 3. Some minor docstring improvements, most notably adding an entry for the `source` argument and simplifying the examples.  --- <details> <summary>Outdated description</summary> This PR does three things:  1. Makes more intelligent use of the `seen` set in `kosaraju_strongly_connected_components` to avoid revisiting nodes that have already been visited—I'm 99% sure this is what the OP in #4181 was referring to. 4. Uses `G._adj` in the inner loops of `strongly_connected_components` to achieve a reasonably significant speed-up. 5. Adds benchmarks for both algorithms. I'm not sure if these will run automatically, or if they need to be triggered (or if I even did this right). I'm not sure how many graphs or how many different densities are reasonable to explore here, so I've kept it fairly minimal for now.  ---  It's always messy for me to try to run `asv` benchmarks locally, so I've included some ugly, manual testing code to get an idea for the speed-up. `fast_kscc` and `tarjan_scc` are the updated versions of the Kosaraju and Tarjan algorithms: ```python import networkx as nx import time   def fast_kscc(G, source=None):     post = list(nx.dfs_postorder_nodes(G.reverse(copy=False), source=source))      seen = set()     while post:         r = post.pop()         if r in seen:             continue         new = {r}         seen.add(r)         stack = [r]         while stack:             v = stack.pop()             for w in G._adj[v]:                 if w not in seen:                     new.add(w)                     seen.add(w)                     stack.append(w)         yield new   def tarjan_scc(G):     preorder = {}     lowlink = {}     scc_found = set()     scc_queue = []     i = 0  # Preorder counter     for source in G:         if source not in scc_found:             queue = [source]             while queue:                 v = queue[-1]                 if v not in preorder:                     i = i + 1                     preorder[v] = i                 done = True                 for w in G._adj[v]:                     if w not in preorder:                         queue.append(w)                         done = False                         break                 if done:                     lowlink[v] = preorder[v]                     for w in G._adj[v]:                         if w not in scc_found:                             if preorder[w] > preorder[v]:                                 lowlink[v] = min([lowlink[v], lowlink[w]])                             else:                                 lowlink[v] = min([lowlink[v], preorder[w]])                     queue.pop()                     if lowlink[v] == preorder[v]:                         scc = {v}                         while scc_queue and preorder[scc_queue[-1]] > preorder[v]:                             k = scc_queue.pop()                             scc.add(k)                         scc_found.update(scc)                         yield scc                     else:                         scc_queue.append(v)   for n in [10, 100, 1000, 10000]:     for p in [0.1, 0.01, 0.001, 0.0001]:         G = nx.fast_gnp_random_graph(n, p, directed=True, seed=42)         print(f"\nn={n}, p={p}")                  # Time fast_kscc         start = time.time()         kscc_result = list(fast_kscc(G))         kscc_time = time.time() - start         print(f"fast_kscc time: {kscc_time:.4f} seconds")          # Time nx.kosaraju_strongly_connected_components         start = time.time()         nx_result = list(nx.kosaraju_strongly_connected_components(G))         nx_time = time.time() - start         print(f"networkx kosaraju time: {nx_time:.4f} seconds")          # Time tarjan_scc         start = time.time()         tarjan_result = list(tarjan_scc(G))         tarjan_time = time.time() - start         print(f"tarjan scc time: {tarjan_time:.4f} seconds")          # Time nx.strongly_connected_components         start = time.time()         scc_result = list(nx.strongly_connected_components(G))         scc_time = time.time() - start         print(f"networkx scc time: {scc_time:.4f} seconds")          # Check that the results match (as sets of frozensets)         assert set(map(frozenset, kscc_result)) == set(map(frozenset, nx_result)) == set(map(frozenset, scc_result)) == set(map(frozenset, tarjan_result)), "Results do not match!" ```  Running this on my machine gives the following speed-ups: ```console ❯ python test_kosaraju.py  n=10, p=0.1 fast_kscc time: 0.0010 seconds networkx kosaraju time: 0.0013 seconds tarjan scc time: 0.0000 seconds networkx scc time: 0.0010 seconds  n=10, p=0.01 fast_kscc time: 0.0001 seconds networkx kosaraju time: 0.0002 seconds tarjan scc time: 0.0000 seconds networkx scc time: 0.0001 seconds  n=10, p=0.001 fast_kscc time: 0.0001 seconds networkx kosaraju time: 0.0002 seconds tarjan scc time: 0.0000 seconds networkx scc time: 0.0001 seconds  n=10, p=0.0001 fast_kscc time: 0.0001 seconds networkx kosaraju time: 0.0001 seconds tarjan scc time: 0.0000 seconds networkx scc time: 0.0000 seconds  n=100, p=0.1 fast_kscc time: 0.0006 seconds networkx kosaraju time: 0.0007 seconds tarjan scc time: 0.0005 seconds networkx scc time: 0.0005 seconds  n=100, p=0.01 fast_kscc time: 0.0002 seconds networkx kosaraju time: 0.0009 seconds tarjan scc time: 0.0001 seconds networkx scc time: 0.0002 seconds  n=100, p=0.001 fast_kscc time: 0.0002 seconds networkx kosaraju time: 0.0005 seconds tarjan scc time: 0.0001 seconds networkx scc time: 0.0002 seconds  n=100, p=0.0001 fast_kscc time: 0.0003 seconds networkx kosaraju time: 0.0006 seconds tarjan scc time: 0.0001 seconds networkx scc time: 0.0002 seconds  n=1000, p=0.1 fast_kscc time: 0.0188 seconds networkx kosaraju time: 0.0250 seconds tarjan scc time: 0.0455 seconds networkx scc time: 0.0414 seconds  n=1000, p=0.01 fast_kscc time: 0.0036 seconds networkx kosaraju time: 0.0040 seconds tarjan scc time: 0.0054 seconds networkx scc time: 0.0070 seconds  n=1000, p=0.001 fast_kscc time: 0.0013 seconds networkx kosaraju time: 0.0111 seconds tarjan scc time: 0.0019 seconds networkx scc time: 0.0022 seconds  n=1000, p=0.0001 fast_kscc time: 0.0014 seconds networkx kosaraju time: 0.0054 seconds tarjan scc time: 0.0009 seconds networkx scc time: 0.0029 seconds  n=10000, p=0.1 fast_kscc time: 3.3322 seconds networkx kosaraju time: 6.8289 seconds tarjan scc time: 4.6371 seconds networkx scc time: 4.8860 seconds  n=10000, p=0.01 fast_kscc time: 0.4190 seconds networkx kosaraju time: 0.3396 seconds tarjan scc time: 0.4548 seconds networkx scc time: 0.4941 seconds  n=10000, p=0.001 fast_kscc time: 0.0505 seconds networkx kosaraju time: 0.0676 seconds tarjan scc time: 0.0696 seconds networkx scc time: 0.0988 seconds  n=10000, p=0.0001 fast_kscc time: 0.0184 seconds networkx kosaraju time: 0.2674 seconds tarjan scc time: 0.0215 seconds networkx scc time: 0.0304 seconds ``` </details>
issue
chore: make benchmarking and release requirements extras in `pyproject.toml`#TITLE_END#A small QoL improvement that allows me to do          $ uv pip install -e . -r pyproject.toml --all-extras  without having to manually add the benchmarking requirements. The changes in CI can be backed out of easily if they break things. 
issue
enh: short-circuit in `is_regular` for directed graphs#TITLE_END#This PR adds a small performance improvement, allowing `is_regular` to avoid checking all in-/outdegrees when a directed graph has nodes with different out-/indegrees. I've also slightly tweaked the docstring and added/refactored a few tests to check everything works as it should.
issue
feat: add directed star graph#TITLE_END#While working on #8148, directed star graphs would have been useful to easily created directed graphs with many "leaf nodes" in their DFS tree. This PR implements adds that possibility and adds a test for it.
issue
refactor: simplify `k_factor`#TITLE_END#This PR simplifies the `k_factor` function by removing the largely redundant classes for gadgets. I've also cleaned up the docstring a little bit ~and made the test suite more comprehensive~.  Since this function returns a graph, should it also be renamed to include the `_graph` suffix?
issue
test: clean up `k_factor` tests#TITLE_END#This pulls out the changes to the test suite from #8139. CC @rossbar.
issue
feat(drawing): add missing connection styles in `draw_networkx_edge_labels` and `display`#TITLE_END#Fixes #8106.  This contains the suggested fix by @dg-pb in #8106 to allow `"arc"` and `"angle"` in `draw_networkx_edge_labels`. I'm not sure the `AssertionError` for invalid `pos` is sufficiently clear. I've also augmented the test introduced in #7739 to check all styles.  On the (slightly modified) example provided in the linked issue, we now get: ```python import matplotlib.pyplot as plt import networkx as nx  g = nx.DiGraph() g.add_edge(1, 2)  pos = nx.spring_layout(g)  nx.draw_networkx_nodes(g, pos) nx.draw_networkx_labels(g, pos)  edge = (1, 2)  styles = ['arc', 'angle']  for cs in styles:     nx.draw_networkx_edges(g, pos, edgelist = [edge], connectionstyle = cs)     nx.draw_networkx_edge_labels(g, pos, edge_labels = {edge: 'edge'}, connectionstyle = cs)  plt.show() ``` ![cs](https://github.com/user-attachments/assets/40004f30-43ae-4be6-b0e3-27a8e8a432f6)   
issue
chore: bump codecov action to v5#TITLE_END#This should fix the failing codecov coverage uploads in CI. It will only work after a maintainer adds `secrets.CODECOV_TOKEN` to the GitHub repository secrets. This shouldn't take more than 3 minutes todo; there's a short but detailed walkthrough when you navigate to (presumably?) https://app.codecov.io/github/networkx/networkx/new.
issue
refactor: improve `generate_adjlist`#TITLE_END#This is a reasonably simple PR for `generate_adjlist` that makes it more readable and efficient.  A more complete changelist is the following: - Add examples of how directed graphs and multigraphs are handled. - Avoid string concatenation and manual delimiter handling by building lists and joining them with `.join()`. - Add tests for `generate_adjlist`, which was previously untested.  `write_adjlist`, which does have several tests already, internally uses `generate_adjlist`. Those tests are unchanged and still pass, which might make verifying correctness of the new implementation simpler.
issue
Performance improvement and tests for `edges_equal`#TITLE_END#A little maintenance PR while reading over the functions in the `utils/` folder.  <details> <summary>Mini benchmark.</summary>  ```python import time from itertools import zip_longest import networkx as nx  trials = 10 n = 1000  def take_best(arr):     sarr = sorted(arr)[:5]     return sum(sarr) / len(sarr)  g1, g2 = nx.complete_graph(n), nx.complete_graph(n) #g2.remove_edge(n-1, n-2)  nx_times = [0] * trials for i in range(trials):     start = time.time()     a = nx_ee(g1.edges(), g2.edges())  # the current function     nx_times[i] = time.time() - start  print(take_best(nx_times))  it_times = [0] * trials for i in range(trials):     start = time.time()     b = new_ee(g1.edges(), g2.edges())  # the new function     it_times[i] = time.time() - start  assert a == b print(take_best(it_times)) ```  </details>  Benchmarking these changes shows something like a 40-50% time reduction for the complete graph in the timing code.  `K_1000` vs `K_1000`: ```zsh 1.8564390659332275  # old 0.8450418949127197  # new ``` `K_1000` vs `K_1000` with one edge removed: ```zsh 1.3363954544067382  # old 0.7565835952758789  # new ```  When comparing the complete graph to a much smaller graph, short-circuiting on the length gives an even more significant improvement:  `K_1000` vs `K_10`: ```zsh 0.5531469345092773  # old 7.357597351074219e-05  # new ```
issue
SCC benchmarks and use of `G._adj` in Tarjan algorithm#TITLE_END#This PR is a minified version of #8056 that removes the changes to Kosaraju to simplify review of both, i.e. it:  - Uses `G._adj` in `strongly_connected_components`. - Adds benchmarks for the SCC functions. These would be good to have to complement the discussion in #8056.
issue
Replace `maybe_regular_expander` with `maybe_regular_expander_graph`#TITLE_END#This adds the `_graph` suffix to the function name. I hope I got all the places the old function was used!  _Follow-up to a comment by @dschult in https://github.com/networkx/networkx/pull/8048#pullrequestreview-2847815247_
issue
Maintenance for broadcasting.py#TITLE_END#Some code maintenance for broadcasting.py. The commit messages give a good overview of what each change does, but for clarity, here's the list again.  - Clean up the docstring for both functions - Speed up the shortest path calculation by doing all sources at once - Ensure the proper errors get raised, and clarify this in the docstring. - Remove an import from within networkx - Parametrize the existing tests which were just wrapping a `for` loop.
issue
Add input validation to `non_randomness()` and clarify its behavior#TITLE_END#Fixes #4115.  This PR does three related things:  1. It adds input validation for `k`, the number of communities, in `non_randomness()`. In particular, it raises a `ValueError` when `k` doesn't satisfy `1 <= k < n`. Tests for this scenario are also added. 2. It checks that the value of `p` computed can be interpreted as a probability, i.e. `0 < p < 1`. This avoids the mysterious failure in #4115 and [this SO question](https://stackoverflow.com/questions/71593402/calculating-non-randomness-with-networkx). The test from the issue was added to check this. 3. It improves (imo) the docstring, adding an example of this weird behavior for certain values of `k` and all-round making it more informative.
issue
Failing round-tripping test for empty graphs#TITLE_END#The `test_round_trip_empty_graph` test in `nx_agraph` is failing in CI and locally. https://github.com/networkx/networkx/blob/f32dd409623285e0b67d0c07033b533414eb2bba/networkx/drawing/tests/test_agraph.py#L173-L184  Interestingly, running the test in the REPL doesn't give any issues. ```python-console ❯ python Python 3.13.5 (main, Jun 11 2025, 15:36:57) [Clang 17.0.0 (clang-1700.0.13.3)] on darwin Type "help", "copyright", "credits" or "license" for more information. >>> import networkx as nx >>> from networkx.utils import graphs_equal >>> def test_round_trip_empty_graph(): ...         G = nx.Graph() ...         A = nx.nx_agraph.to_agraph(G) ...         H = nx.nx_agraph.from_agraph(A) ...         # assert graphs_equal(G, H) ...         AA = nx.nx_agraph.to_agraph(H) ...         HH = nx.nx_agraph.from_agraph(AA) ...         assert graphs_equal(H, HH) ...         G.graph["graph"] = {} ...         G.graph["node"] = {} ...         G.graph["edge"] = {} ...         assert graphs_equal(G, HH) ... >>> test_round_trip_empty_graph() >>> ```  Possibly related to the graphviz 13.0.x releases from this month.
issue
Use `scipy.sparse` array versions where applicable#TITLE_END#This gets rid of some TODOs by using `sp.sparse.diags_array` and `sp.sparse.eye_array` instead of wrapping calls in `sp.sparse.csr_array`. 
issue
Take 2: add test for (and fix) the pydot long node name problem#TITLE_END#Fixes #7648.  This leverages the fact that (apparently?) the graph's nodes end up in order at the end of the pydot graph object. I added the test from the issue to validate it works.  If we really don't want to fix this, let's at least add an `xfail` test for the long node name situation so we're made aware if the issue ever gets fixed upstream.
issue
Fix typo in extra name#TITLE_END#With #8101 getting closed, this pulls out the change to the extra name in the doc deploy action.  This fixes the following warning/error hidden in the logs. > WARNING: networkx 3.5 does not provide the extra 'tests'
issue
Migrate CI workflows from `pip` to `uv`#TITLE_END#I don't really expect this to get merged, but it's a fun exercise in understanding how the CI works and it does save several minutes off the total time in CI so I'm putting it here in case it garners interest.  This migrates the CI workflow from using `pip` to using `uv lock`/`uv sync`. The main benefit is an improvement in performance. Some of the workflows didn't get tested yet, I'm still thinking about the best way to  do so.  ## Changes  - Added `uv.lock` for reproducible, deterministic installs in CI - Updated all GitHub Actions workflows to use `uv sync` with virtual environment activation and caching—except for release.yml, which I didn't want to mess with (and which doesn't run that often anyways; but it could be added) - Updated CircleCI configuration to use `uv` as well - Added dependency groups for benchmarking and release tooling to `pyproject.toml`—these now get auto-generated as well. - Configured `uv` in `pyproject.toml` - Fixed a `tests` -> `test` typo in deploy-docs.yml  ## Developer Experience  This change maintains flexibility for contributors:  - Developers can continue using `pip` locally if preferred - The lockfile is primarily used for CI reproducibility - All workflows now activate virtual environments to simplify command execution
issue
Replace `random_lobster` with `random_lobster_graph`#TITLE_END#This renames the `random_lobster` generator to use the `_graph` suffix: `random_lobster_graph`.
issue
Remove structuralholes.py from `needs_(num|sci)py`#TITLE_END#While reading the PR that recently updated structuralholes.py to understand #8087, I saw it added that file to the `needs_numpy` and `needs_scipy` lists in conftest.py. This excludes them from being tested when scipy (or numpy) is not available (strictly, this isn't the case, as the `nodes` argument is also considered in the `if`—but afaik there's no good reason to exclude these from the tests). However, the functions with a scipy implementation explicitly wrap `import scipy` in a `try ... except` using scipy where possible. As a result, the vanilla implementation does not get tested properly: when running the `default` tests (with scipy), the scipy implementation gets tested, and when running `default-without-scipy`, the file gets skipped.
issue
Workaround for long node names in `pydot_layout`#TITLE_END#Fixes #7648.  This has a very hack-y solution for the problem in #7648. This will hopefully be resolved when pydot/pydot#420 gets merged in, but in the meantime, this should work.  - Adds the test case from the issue. - Instead of trying to predict it, this "fix" lets pydot (or graphviz, I suppose) tell us how it will mangle long node names.
issue
Fix typo in `min_edge_cover` docstring#TITLE_END#Came across this while hunting down test failures in #8074.
issue
Fix behavior for iterable `sources` argument in `bfs_layers`#TITLE_END#This addresses the issue reported in #8009. It makes sense to me to ensure the function works correctly for generators as well; especially since the fix is as simple as this. I also thought it was more reasonable to gracefully handle duplicate nodes in the `sources` argument; at every depth other than 0, nodes never appear multiple times (because of the `visited` set), and I see no convincing reason to have the sources be handled any differently.  List of changes: * Avoid silent failure when `sources` is a generator * Only handle duplicate nodes once * Add test cases where `sources` is a generator and list with duplicate elements * Minor clarifications in docstring  Fixes gh-8009.
issue
Add note about cycles in `maximum_flow()`#TITLE_END#Fixes #6740.  This adds the clarification in https://github.com/networkx/networkx/issues/6740#issuecomment-2726729072 (approved by the issue author) that seemingly never got added to the docstring. I'm not sure how important or valuable the change actually is, but let's get the issue closed anyways. 
issue
Fix edge case in ISMAGS symmetry detection#TITLE_END#Fixes #4915.  Paraphrasing the original issue (#4915): there is an error in the ISMAGS implementation. For some specific cases it trips an `AssertionError`. This happens for graphs of this shape, where all nodes and edges are equivalent. ``` 5 - 4 \     / 12 - 13        0 - 3 9 - 8 /     \ 16 - 17 ``` During symmetry detection/analysis, nodes 0 and 3 are coupled and no longer equivalent. At that point, {4, 8} and {12, 16} are no longer equivalent, and neither are {5, 9} and {13, 17}. Coupling 4 then refining results in 5 and 9 getting their own partitions, but not 13 and 17. This results in tripping the following `assert`: https://github.com/networkx/networkx/blob/679191810fc962e282b606622c90355f2e6f58ad/networkx/algorithms/isomorphism/ismags.py#L1090-L1092  The good news is that this case cannot result in new symmetries, so it can be safely changed to an `if not cond: return [], cosets`.  This fix is a straight port of the one in https://github.com/marrink-lab/vermouth-martinize/pull/338/commits/d5be20bb17992eaef3b34484d2b3dd7097e1c51c, with [permission from the original author](https://github.com/networkx/networkx/issues/4915#issuecomment-2894255592). It replaces the assertion by the aforementioned `if` block and adds the minimal test case above (which fails on [networkx/main](https://github.com/networkx/networkx/tree/main)) to the test suite.   
issue
Fix intermittent test failures in expander graph generator tests#TITLE_END#Fixes gh-8046.  This adds an explicit seed to the `maybe_regular_expander` and `random_regular_expander_graph` (is there a reason for the inconsistency in appending `_graph` to generator names?) tests which avoids stochastic failures when the maximum number of iterations gets exceeded.  Since this is a simple fix and the linked issue has a discussion surrounding _how_ it should be fixed, this can also be left as a good first issue imo. 
issue
Enforce correct graph types for graph matchers#TITLE_END#Fixes #7964.  Check that both graphs have the correct directedness in `(Multi-)(Di-)GraphMatcher` initialization.  @dschult, you mentioned switching to a function-oriented interface in the linked issue. Is this something that has been discussed previously/is there some kind of consensus on what that should look like? I feel the graph matching subsection of the codebase could use a revamp (there are still untouched lines of code from 2007 in there! :) ). I'd give it a shot, but it would entail some significant, opinionated API changes, and I'm curious what people think about that.
issue
Update copyright license years#TITLE_END#Changes license formatting to match the 3-clause BSD pattern. Updates copyright license years for 2025. 
issue
Add pre-commit.ci autoupdate#TITLE_END#<!-- Please use pre-commit to lint your code. For more details check out step 1 and 4 of https://networkx.org/documentation/latest/developer/contribute.html --> This automatically sends PRs with updates to the pre-commit hook versions; the three scheduling options are "weekly", "monthly", or "quarterly". More info [here](https://pre-commit.ci/).  This was the last remaining change of #7488 after #7547 got merged.
issue
`rooted_product()` should raise `NodeNotFound` when `root` is not in `H`#TITLE_END#`rooted_product()` currently raises a `NetworkXError` when `root` is not in `H`, but I believe `NodeNotFound` makes more sense ([ref.](https://networkx.org/documentation/stable/_modules/networkx/exception.html#NodeNotFound)).  I suggest waiting with this until #7484 gets merged/closed, otherwise it will introduce conflicts. 
issue
Only allow connected graphs in `eigenvector_centrality_numpy`#TITLE_END#<!-- Please use pre-commit to lint your code. For more details check out step 1 and 4 of https://networkx.org/documentation/latest/developer/contribute.html --> Fixes #6888.  As discussed in #6888, the SciPy solver does not handle disconnected graphs well, leading to inconsistent results. The proposed solution was to add a check that `G` is connected, as long as that didn't make the algorithm much slower (for reference, the execution time was usually around 1-2% longer with the check).  This PR:   - Changes `eigenvector_centrality_numpy` to only accept connected graphs (strongly connected for directed graphs)   - Updates the docstring to clearly mention the reason for this choice (i.e. the inconsistency, and its explanation by @dschult)   - Adds two tests to verify that the function properly raises when given a disconnected graph as input   - Reorganises the arguments in the docstring to match the arguments of the function   - Has other very minor changes (improvements?) to the docstring.  
issue
Add `--doctest-modules` to base test workflow#TITLE_END#<!-- Please use pre-commit to lint your code. For more details check out step 1 and 4 of https://networkx.org/documentation/latest/developer/contribute.html -->  I came across this while reviewing (ref. https://github.com/networkx/networkx/pull/7443#issuecomment-2156054489). I'm fairly confident it's intentional, but just in case I thought I'd make a quick PR to make sure (if it is intentional, feel free to close!). For reference, going through the git blame shows that this change happened in #5339.
issue
Prettify `README.rst`#TITLE_END#- Add hyperlinks - Use code blocks for shell commands - Clean up blue mark between badges - Add some PyPI badges  <!-- Please use pre-commit to lint your code. For more details check out step 1 and 4 of https://networkx.org/documentation/latest/developer/contribute.html --> 
issue
Remove deprecated ruff linter settings and fix string formatting warnings#TITLE_END#Prior to this PR, running `% ruff check .` from the project root gave ```zsh % ruff check . warning: The top-level linter settings are deprecated in favour of their counterparts in the `lint` section. Please update the following options in `pyproject.toml`:   - 'select' -> 'lint.select'   - 'per-file-ignores' -> 'lint.per-file-ignores' warning: `SIM111` has been remapped to `SIM110`. examples/algorithms/plot_dedensification.py:54:11: UP031 Use format specifiers instead of percent format examples/algorithms/plot_dedensification.py:67:11: UP031 Use format specifiers instead of percent format examples/algorithms/plot_snap.py:57:5: UP032 [*] Use f-string instead of `format` call examples/algorithms/plot_snap.py:83:5: UP032 [*] Use f-string instead of `format` call networkx/algorithms/centrality/group.py:411:21: UP031 Use format specifiers instead of percent format networkx/algorithms/tests/test_summarization.py:258:21: UP031 Use format specifiers instead of percent format Found 6 errors. [*] 2 fixable with the `--fix` option (4 hidden fixes can be enabled with the `--unsafe-fixes` option). ```  After the PR, this becomes ```zsh % ruff check . All checks passed! ``` 
issue
Clarify generation number in `dorogovtsev_goltsev_mendes_graph()`#TITLE_END#Clarifies the meaning of the generation number in DGM graphs, as discussed in #7472.  Also contains additional tests based on average clustering and average shortest path length in the original paper, as well as minor changes to wording.  Fixes #7472. 
issue
Update NetworkX reference links in doc index#TITLE_END#<!-- Please use pre-commit to lint your code. For more details check out step 1 and 4 of https://networkx.org/documentation/latest/developer/contribute.html --> This updates some broken reference links. Closes #7479. 
issue
Add `polynomials.py` to `needs_numpy`#TITLE_END#<!-- Please use pre-commit to lint your code. For more details check out step 1 and 4 of https://networkx.org/documentation/latest/developer/contribute.html --> This adds `polynomials.py` to the `needs_scipy` list ~~and fixes a test failure in `test_lazy_imports.py` when scipy is available but not numpy~~. Related to #7388 and #7403.  Edit: rather than add to `needs_scipy`, use numpy and add to `needs_numpy`. 
comment
This is related to/potentially closes #7986. @Schefflera-Arboricola you had opinions/questions about this, have those been addressed?
comment
I'm working through the example provided here.  We want to find the dominance frontiers of `A`, `B`, and `C`.  To find the dominance frontier of `A`, according to the [definition](https://en.wikipedia.org/wiki/Dominator_(graph_theory)) on Wikipedia, we want to find all nodes `n` such that `A` dominates an immediate predecessor of `n`, but `A` does not strictly dominate `n`. Let's start by excluding the nodes `n` that `A` strictly dominates: `A` dominates all nodes in the graph as it is the entry node and all nodes are reachable from it. It strictly dominates all of those except for itself. This only leaves `A` as candidate `n`. The only immediate predecessor of `A` is `B`, which `A` does dominate, hence the dominance frontier of `A` is `{A}`.  The dominance frontier of `B`: `B` strictly dominates `C`, so that leaves only `A` and `B` as candidates for `n`. `A` has one immediate predecessor (`B`), which `B` does dominate, so `A` is in the dominance frontier of `B`, and `B` has one immediate predecessor (`A`), which `B` does not dominate. The dominance frontier of `B` is thus `{A}`.  The dominance frontier of `C` is much more straightforward: `C` only dominates itself, but is not an immediate predecessor to any nodes. Its dominance frontier is empty.  In code, this means I would expect ```python >>> G = nx.DiGraph([(1, 2), (2, 3), (2, 1)]) >>> nx.dominance_frontiers(G, 1) {1: {1}, 2: {1}, 3: set()} ```  This agrees with the expected behavior at the top of the issue, which is sweet! I think this warrants a change to the tests (and implementation, of course).  ---  @dschult, the comment you found (thanks!) mentions immediate dominators, but nothing in the definition of the dominance frontier mentions immediate dominators—the implementation of `nx.dominance_frontiers` shows that the computation can use them, but I believe the results one arrives at going "from first principles" (i.e. applying the definitions) are more trustworthy. I have a hunch there was some confusion either with the unfortunate use of "immediate predecessors" in the definition, or with the notion of a dominator tree which is in the next bullet of the Wikipedia article (and does mention immediate dominators)...  As for the discussion on whether the "back-edge to entry point" case is sensical, I did find a mention of ["loop headers"](https://en.wikipedia.org/wiki/Control-flow_graph#Loop_management), which does explicitly address when the entry point (of a loop) has a back-edge.  ---  I've gone ahead and make these changes on a local branch. I'll make a PR for that where we can further discuss whether everyone can agree on a definition.
comment
The docstring updates look good and make sense to me. It looks like `edge_load_centrality` uses `cutoff=False` as the default, which obviously gets impacted by your change to `predecessor` (and explains why CI does _not_ like the PR). I think it makes sense to change functions using `predecessor` to default to `None` instead of `False` from a semantic point of view (although the [default value of `None` and handling of the `break`](https://github.com/networkx/networkx/commit/22c6dc6) precedes (pun intended) [the latest commits affecting `edge_load_centrality` using `False`](https://github.com/networkx/networkx/commit/8a62ac9) by at least 6 years). Note that this would be a breaking change in some scenarios (as evidenced by the test failures), requiring a deprecation.
comment
I'm not sure I'll be able to go over this in depth until next week-end (Oct. 11-12) :(. I haven't forgotten about this PR though. When is 3.6 planned to be released?
comment
You can also add a "closes #8302" keyword to the PR body to automatically link to (and close) the related issue (which I think is appropriate now that the `metric_closure` function has been deprecated in #8304).
comment
Yeah, CircleCI doesn't provide an easy way to generate coverage badges without using 3rd party services...
comment
As the original issue author, I've done some digging to find whether it's realistic for a tournament graph to have multiple strongly connected components, and as such, whether an additional benchmark makes sense. While it's relatively easy to come up with edge cases where this can happen (e.g. generate a tournament graph, add a node `s` with edges from all nodes in the graph to `s`, and then query `is_reachable(G, s, <any node in G>)`), the probability of a tournament graph having multiple SCCs becomes vanishingly small as `n` grows; it scales with $O(2^{-n})$, roughly. [[1](https://www.cambridge.org/core/journals/canadian-mathematical-bulletin/article/almost-all-tournaments-are-irreducible/16E883BBD708196FFE489A661BA58091), [2](https://oeis.org/A054946)].  In light of that, I agree it makes sense to close this PR and the linked issue. :) Thanks @Yasserelhaddar for taking a look, and Dan and Ross for the thoughtful reviews!
comment
I did some timing too, comparing the following: 1. `sort`, which sorts the entire array then slices the top `D` and reverses. 2. `heap`, which is the current implementation. 3. `quickselect`, which finds the `D` largest elements, sorts, and reverses.  This is what I got for `D = 100`—it agrees with Dan's comment, and shows that quickselect is even faster. ![topd_timing](https://github.com/user-attachments/assets/33d074e9-f419-487e-a423-6d5c73812ecd) <details> <summary>Timing code</summary>  ```python import numpy as np import heapq import timeit import matplotlib.pyplot as plt   def method_sort(arr, D):     return np.sort(arr)[-D:][::-1]   def method_heap(arr, D):     h = []     for x in arr:         if len(h) < D:             heapq.heappush(h, x)         else:             heapq.heappushpop(h, x)     return sorted(h, reverse=True)   def method_quickselect(arr, D):     part = np.partition(arr, -D)[-D:]     return np.sort(part)[::-1]   def benchmark(N=100_000, D=100, repeats=5):     arr = np.random.random(N)     res = []     times = {}     for name, method in [         ("sort", method_sort),         ("heap", method_heap),         ("quickselect", method_quickselect),     ]:         res.append(method(arr, D))         t = timeit.timeit(lambda: method(arr, D), number=repeats)         times[name] = t / repeats     assert np.allclose(res[0], res[1]) and np.allclose(res[0], res[2])     return times   def run_plot():     sizes = [10_000, 50_000, 100_000, 500_000, 1_000_000, 5_000_000, 10_000_000]     D = 100     results = {m: [] for m in ["sort", "heap", "quickselect"]}      for N in sizes:         times = benchmark(N, D)         for method, t in times.items():             results[method].append(t)      for method, times in results.items():         plt.plot(sizes, times, label=method)      plt.xlabel("Array Size (N)")     plt.ylabel("Time (s)")     plt.title(f"Top {D} Largest Elements (Decreasing Order)")     plt.legend()     plt.grid(True)     plt.tight_layout()     plt.show() ``` if __name__ == "__main__":     run_plot()  </details>  Since the quickselect implementation is reasonably significantly faster (especially at the multi-million node level they mention in the paper)— and I expect it to be faster for smaller values of `D` still—and not more complicated than sorting and slicing, my vote goes to doing that.  ---  Then I set out to locally test all of that so I could just use the suggestion feature on here. (In the process, I discovered you can shift-select to get a much larger suggestion, which would've come in handy in previous PRs.) Only to find out that 5 years ago, you already came to this same conclusion for `panther_similarity`: https://github.com/networkx/networkx/blob/d1c41f886add2ab86c1ce4c1d788d3c286721351/networkx/algorithms/similarity.py#L1629-L1633 That made me stop changing things locally, since I'd just be repeating what's already higher up. This does make me think that perhaps the similarity (pun intended) between the `panther_*similarity` functions is larger than just the preparation of paths—but that doesn't need to be done in this PR.  ---  Tl;dr: let's use quickselect, like `panther_similarity` already does.  Some minor other notes: we can wait until we have a the top-`D` before multiplying by `inv_sample_size`, which I imagine would be marginally faster. And there's probably something to be said for taking care to avoid converting between lists/sets/numpy arrays more than strictly necessary.
comment
Let me start off by saying I agree with the you that we shouldn't optimize things before we've identified them as bottlenecks. With that being said, we're in the innermost loop of the code, computing similarity between each node and _all other nodes_—and Michael claims that the algorithm is regularly used for hundreds of thousands or even millions of nodes. I'd be very surprised if that's not a significant part of the runtime... But it's not impossible of course. I'm not sure how much time the `KDTree` creation and querying will take.  Perhaps I should phrase it as a question instead: why do you say the code we have (with quicksort) will never run for tens of thousands of nodes?  It's my understanding that if we just sort and slice, we're sorting an array with `N` (the number of nodes, i.e. up to at least 1000000) entries, then slicing the top `D`, with `D` being ~100. The heap implementation that's currently being reviewed goes through the `N` entries, keeping the top `D` and sorting only those. If that's what you're referring to, are you saying we should disregard your [original comment](https://github.com/networkx/networkx/pull/4400#issuecomment-2933025276) on the heap being slower than sorting and slicing?  Either way, the only satisfactory answer here is going to involve some more benchmarking, even if just to see whether collecting the top `D` entries is a bottleneck.
comment
What do you call the numpy implementation? What do the timings look like for `panther_similarity`? Do those match the timings in the paper?
comment
Ah, I didn't realise the initial "sort" was already basically quickselect/introselect! I thought you were sorting the full list before selecting the top `D` entries. Disregard my previous comments!
comment
I think Dan just means you should go through the docstrings for the functions related to the center, and add ```sphinx :func:`~networkx.algorithms.tree.distance_measures.tree_centroid` ``` to the "See Also" section where appropriate. At least, if my link is correct...  The same thing we do in a few other places where functions in different parts of the code base are related; for example: https://github.com/networkx/networkx/blob/d1c41f886add2ab86c1ce4c1d788d3c286721351/networkx/algorithms/centrality/eigenvector.py#L96-L100 
comment
I'm not sure what the issue is with the dispatch tests.  I did have a think about the equivalence between tree centroid and tree barycenter (in the unweighted case) while at the gym, though, and I have a sketch of a proof that they are, in fact, equivalent. I'm not a mathematician and it's late here, so bear with me.  Proof sketch ------------ Let $c$ be a centroid of a tree $T$ with $N$ nodes. Because $c$ is a centroid, we know that $T \setminus \\{c\\}$ minimizes the size of its largest connected component. In fact, we know that this component's size is at most $N/2$.  To see this, just consider that if the largest component is larger than $N/2$, say it has size $N/2 + 1$, moving one step towards that component (node $n$) would reduce its size by one (to now $N/2$), while increasing the size of the second largest component by at most one (since it now includes $c$)—but this second largest component could have size at most $N - (N/2 + 1) - 1 + 1 = N/2 - 1$, where we have subtracted the sizes of the largest component and of $n$ which is now removed, and added back $c$ to the component. This means the largest component of $T \setminus \\{n\\}$ is $N/2$, which contradicts $c$ being a centroid.  Back to the proof. The largest component of $T \setminus \\{c\\}$ has size at most $N/2$. Let us now consider the sum of distances from $c$ to any node $v$ in the tree, which we'll write $S(c)$. Moving one step towards any component (say, towards node $u$) will decrease the distance to all nodes in that component by 1, while increasing the distance to all nodes outside of that component by 1 as well. Let $s$ be the size of the largest component. We thus have $S(u) = S(c) - s + (N - s) = S(c) + N - 2s$. But we know $s \le N/2$, and thus we have that $S(c) \le S(u)$ is minimal. This proves that each centroid is also a barycenter, since it minimizes the distance sum.  Finally, for completeness, we have to prove that the centroids are the only barycenters. When there are two centroids, this is true, since there can be at most two barycenters (the reference you provide, 2.1.55 in West's book, shows that the barycenter of a tree is a single vertex or an edge, i.e. two adjacent nodes). Similarly, when there is only one barycenter, it must equal the centroid. So we can simplify our analysis to a hypothetical single centroid, double barycenter case.  Since we know $c$ is a barycenter, the only possible other options for barycenters, according to the reference, are the nodes adjacent to $c$, which we are assume are _not_ centroids. Let $u$ be any of these nodes. 2.1.56 tells us that a unique centroid is such that for all edges $e$, the component of $T$ with edge $e$ removed has size at least $\lceil N/2 \rceil$, and that this is the only node for which this is the case. In particular, let $e = (c, u)$. When moving from $c$ to $u$, the distance to all nodes in the component containing $c$ increases by 1, and the distance to at most all other nodes (the $\lfloor N/2 \rfloor - 1$ remaining nodes) decreases by 1. So $S(u) \ge S(c) + \lceil N/2 \rceil - (\lfloor N/2 \rfloor -1) > S(c)$. $u$ cannot be a barycenter, and this holds for any possible candidate second barycenters.  When there is one centroid, it must be the only barycenter as well. When there are two, they are both barycenters. There cannot be more than two centroids or barycenters. This completes the proof!  ---  With that said, I stand by my suggestion that your implementation should be leveraged similarly to what was done in #7946. We can then (in a separate PR) move both of the faster tree implementations to a separate module to make them more discoverable and nicer to use for trees. I'm curious what everyone else thinks about that.
comment
I might be missing something, but aren't we saying the same thing?  And yes, this is a special case for unweighted trees - but it can be used to speed up the barycenter computation in that case and I think we should do that, too (the other PR I've linked was similar in that it only concerned a specific subset of inputs, so there is precedent).
comment
I'm not sure I understand this part of your proof: >In the former case, it is sufficient for $N_A > N_B$ to make the weight of $b$ worse.  The former case is when the weight at $a$ is $N_B$. I'm understanding this to mean "the largest component after removing $a$ is has $N_B$ nodes". How do you get $N_A > N_B$ from that? I feel like it only tells us $N_B \ge N_A - 1$, but perhaps I'm misunderstanding your terminology.  --- >Having said all of this I don't think I oppose the proposed use, but hope that this can be done in a follow-on PR instead of added to this one.  I don't think the code changes need to be done here. We can take care of that in a separate PR—in fact, I agree it's better to keep this one more minimal. It probably makes sense to note in the docstring that the centroid(s) is/are also the barycenter(s) in unweighted trees—this can replace the part comparing it to the center. 
comment
I think you might be confusing `G.neighbors(n)` and `nx.all_neighbors(G, n)`. This is what it looks like for me: ```python >>> G = nx.Graph([(0, 1), (1, 2)]) >>> DG = nx.DiGraph([(0, 1), (1, 2)]) >>> list(G.neighbors(1)) [0, 2] >>> list(DG.neighbors(1)) [2] >>> list(nx.all_neighbors(G, 1)) [0, 2] >>> list(nx.all_neighbors(DG, 1)) [0, 2] ```
comment
A PR with doc improvements would be nice!
comment
If I'm understanding the discussion in #8190 correctly, we've determined returning `n - 1` nodes is actually the expected behavior for complete graphs when working with the definition in the paper linked to in the docstring.  To check if there are any remaining issues with the `minimum_node_cut` implementation, your `hypothesis` test could maybe use something like ```python def test_random_connected_graph(G, flow_func):     node_cut = nx.minimum_node_cut(G, flow_func=flow_func)     H = G.copy()     H.remove_nodes_from(node_cut)     assert (len(H) == 1 and H.number_of_edges() == 0) or not nx.is_connected(H)  # Trivial graph or disconnected. ```  What to return in the "complete graph with self-loops on all nodes" case is still not fully clear to me, although I agree with Dan that it might not be worth bothering to special-case or document it either way.
comment
This issue can be closed as it was fixed in #8097.
comment
I'm still in favor of not raising for complete graphs, but if we want to go with the definition Dan is referring to, we should be mindful that the trivial graph is not the null graph but the graph with one node and no edges... Which matches the current behavior of the function if I understand correctly. It does leave the question of how to handle self-loops on the remaining node, e.g. what should we return on a graph where each node has edges to all other nodes and a self-loop.
comment
Can you add the timing script used to generate the output in the PR description?
comment
I was reviewing this before you made the latest changes—let's make sure my suggestions don't undo some of the other changes you've made!
comment
I believe this issue can be closed now that #8153 has been merged! :)
comment
I can take a look at adding it, if nobody else is doing so already.
comment
@akshitasure12 When you say Python code, is that just by replacing `two_neighborhood` function with Dan's explicit suggestion? Or did you also add his second optimization?
comment
The test failure is my bad; I didn't realize you were using the built-in Python `random` module and thought it was the NumPy equivalent. Since this is the only place NumPy is used, let's go back to what it was before.
comment
Both the docstring and the Wikipedia article mention DAGs in the context of periodicity, so I'm not sure we can just disregard all graphs that aren't strongly connected...
comment
Yes, my comment was not particularly clear! I meant to say that the docstring and Wikipedia article mention that DAGs are not aperiodic. It seems to me like everywhere the two are mentioned together, someone is citing either the Wikipedia page, or the NetworkX docstring, and everyone seems to just accept it without questioning.  I have a hard time deciding whether definition (2) makes more sense than (3) or the other way around, which tells me the simplest approach (raising an error for graphs that aren't strongly connected) is probably the easiest to justify. I'd also be in favor of adding examples where we show how the alternative definitions can be implemented.  Note that given these discussions, I believe we ought to move the function to a different home in the codebase - it would make very little sense to keep a function in dag.py that... doesn't work for DAGs. :)  EDIT: I haven't had time to read about it in detail, but this seems like it might be of interest, if someone wants to dig deeper into it: https://en.wikipedia.org/wiki/Aperiodic_finite-state_automaton.
comment
There was some interesting discussion in this PR! @fei0319 are you still interested in working on some of these changes?
comment
This was an issue with pydot and has been addressed upstream in pydot/pydot#487.
comment
Should this be closed now that #8096 has been merged?
comment
I think no workflow currently runs the image tests, right? If there's a good reason for that (or rather, if the old reason is still valid), let's go with this over #8094. If not, let's add the mpl tests to CI _and_ move them to a separate file like you've done here (i.e. I'm +1 on pulling the image comparison tests out into a separate file if they're different from the other `nx_pylab` tests).  This probably warrants some changes to the places where everything fits in the requirements—but I also understand if you don't want to fix something that isn't "broken." 
comment
Do you have a reference for the value being 1? We have two separate ways of computing the constraint value (one using `scipy`, one without `scipy`) and they both give the same result.  Going off the formula in the [Wikipedia article](https://en.wikipedia.org/wiki/Structural_holes#Constraint), I get the following: $c_1 = c_{12} + c_{13} = 2 c_{12}$ because of symmetry—and for the same reason, $c_1 = c_2 = c_3$. So we're interested in $c_{12}$. According to the formula, $c_{ij} = (p_{ij} + \sum_q p_{iq} p_{qj})^2, i \neq q \neq j$. We also have (again, symmetry) that all $p_{ij} = \frac12, i \neq j$, since the network is unweighted and energy is evenly distributed between the node's two neighbors. So that gives $c_{12} = (p_{12} + p_{13} p_{32})^2 = (\frac12 + \frac12 \frac12)^2 = \frac9{16}$, and hence $c_1 = 2 c_{12} = \frac98 = 1.125$.  I'm not very familiar with the concept though, so happy to be shown where I'm wrong, if I am!
comment
Thanks for double-checking!
comment
I don't feel particularly strongly about this, but doesn't it make more sense to add these as "blockers" to PRs? I think in most cases these are the kinds of small issues with a PR that should be easy to "debug," and should be fixable quickly (either by the original PR author or by the maintainer merging it in) without placing a significant additional burden on either—at least certainly not more than having to occasionally go through all of them like you did in this PR.  Also, if you're operating under the assumption that these build warnings should all get fixed eventually anyways, it makes more sense to me in terms of git blame to have the sphinx updates grouped with the actual change to documentation or code, rather than to have intermittent "make sphinx happy" commits.
comment
@pckroon Sorry for reviving such an old thread... Any updates on this issue? I can (with your permission) port the fix you linked to if you don't have the time to work on this!
comment
I feel this issue was (imo) adequately addressed by #6009—is there a reason this issue is still kept open?
comment
In #6212, a comment was added to the docstring of the `pagerank` function to clarify the ambiguity, but this issue wasn't closed. Is there any particular reason for that? Is additional clarification necessary?
comment
What would be the best way to do this? Some options:  - increase the maximum number of iterations - set a fixed seed that works - wrap the graph creation in a `try ... except` until it succeeds - use `pytest.xfail()` if the graph creation fails
comment
Thanks for reporting this. I'm not particularly familiar with the graph6 format, but I wanted to provide context by pointing out that the documentation for `nx.from_graph6_bytes` explicitly mentions the input bytes _should not_ include a trailing newline.  https://github.com/networkx/networkx/blob/d7132daa8588f653eacac7a5bae1ee85a183fa43/networkx/readwrite/graph6.py#L65-L71  I don't know the underlying reasoning for this choice, but it seems to have been intentional (#2299 introduced this, but didn't add any explanation as to why). Perhaps @jfinkels still remembers why.
comment
I realize my question wasn't very clear - I originally wanted to ask about the module name not having an underscore, and added "(test_)" to indicate that the underscore was also missing for the test file.
comment
I played around with timing this a little; I suppose now that we're here, we might as well use `G._adj` instead of `G`: ```pycon % python Python 3.12.4 (main, Jun  7 2024, 04:37:10) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin Type "help", "copyright", "credits" or "license" for more information. >>> import networkx as nx >>> import timeit >>> G = nx.erdos_renyi_graph(100, 0.5) >>> v = 0  # arbitrary, to make things look prettier >>> timeit.timeit(lambda: len(G[v]) == 0 or (set(G[v]) == {v}))  # v1 2.5413331049494445 >>> timeit.timeit(lambda: len(G._adj[v]) == 0 or (set(G._adj[v]) == {v}))  # v1 with ._adj 1.1885209559695795 >>> timeit.timeit(lambda: all(u == v for u in G[v]))  # v2 0.8064324429724365 >>> timeit.timeit(lambda: all(u == v for u in G._adj[v]))  # v2 with ._adj 0.42397298908326775 ```  The set version is slightly faster when average degree is low. ```pycon >>> G = nx.erdos_renyi_graph(100, 0.02) >>> timeit.timeit(lambda: all(u == v for u in G._adj[v]))  # v2 with ._adj 0.395926904049702 >>> timeit.timeit(lambda: len(G._adj[v]) == 0 or (set(G._adj[v]) == {v}))  # v1 with ._adj 0.12690257106442004 ```  It scales extremely poorly though. ```pycon >>> G = nx.erdos_renyi_graph(1000, 0.5) >>> timeit.timeit(lambda: not G[v] or len(set(G._adj[v])) == 1 and v in G[v])  # v1 with ._adj 10.684730764012784 >>> timeit.timeit(lambda: all(u == v for u in G._adj[v]))  # v2 with ._adj 0.41895601199939847 ```
comment
Thanks for taking a look at this! It'd be nice to get this over the line eventually, but that's a monumental amount of (boring) work if it needs to be done manually.  Is there some way that you know of to see how many violations each rule found? I can try to see if there's a way to implement autofixes for some of the more common ones to save us some of the trouble.
comment
I believe this was fixed in #7614 and can be closed now.
comment
I'm unable to reproduce this issue on 3.3 with Python 3.12.3.  ```pycon Python 3.12.3 (main, Apr  9 2024, 16:54:45) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin Type "help", "copyright", "credits" or "license" for more information. >>> import networkx as nx >>> import numpy as np >>> g_arr = np.array([[0, 1, 0, 1, 0, 0, 0, 0, 0], ...                 [1, 0, 1, 0, 1, 0, 0, 0, 0], ...                 [0, 1, 0, 0, 0, 1, 0, 0, 0], ...                 [1, 0, 0, 0, 1, 0, 1, 0, 0], ...                 [0, 1, 0, 1, 0, 1, 0, 1, 0], ...                 [0, 0, 1, 0, 1, 0, 0, 0, 1], ...                 [0, 0, 0, 1, 0, 0, 0, 1, 0], ...                 [0, 0, 0, 0, 1, 0, 1, 0, 1], ...                 [0, 0, 0, 0, 0, 1, 0, 1, 0]], dtype=float) >>> G = nx.from_numpy_array(g_arr) >>> M = nx.minimum_spanning_tree(G, algorithm='kruskal') >>> m_arr = nx.adjacency_matrix(M) >>> print(m_arr)   (0, 1)	1.0   (0, 3)	1.0   (1, 0)	1.0   (1, 2)	1.0   (1, 4)	1.0   (2, 1)	1.0   (2, 5)	1.0   (3, 0)	1.0   (3, 6)	1.0   (4, 1)	1.0   (4, 7)	1.0   (5, 2)	1.0   (5, 8)	1.0   (6, 3)	1.0   (7, 4)	1.0   (8, 5)	1.0 >>> print(M.edges()) [(0, 1), (0, 3), (1, 2), (1, 4), (2, 5), (3, 6), (4, 7), (5, 8)] ```
comment
This is probably better suited to be a [discussion](https://github.com/networkx/networkx/discussions). It could also use some additional information and clarification. What do you mean by Gen Z's social behavior? Do you have a specific data set in mind? Perhaps we will be better able to help you if you can figure out what the answers to those questions are.
comment
Now that #7454 has been merged, I believe this issue can be closed.
comment
Erik's suggestion was fine, I just didn't think it through/test it locally when I implemented it! Thanks for taking a look at this!
comment
Maybe slightly ironic to ask this on a PR trying to uniformize across repos, but is there a reason we don't have any of the `pydocstyles` (`D`) rules?
comment
I agree with Erik's suggestions too, but I don't think autoformatting will...
comment
This is pretty strange... I'm on macOS 12.7.5 for reference, and I get the test failures. Here's what I got when trying the commands you gave.  <details> <summary> Output </summary>  ```sh networkx on  main [!?] via  v3.12.4 ❯ python -m venv nx-devdeps  networkx on  main [!?] via  v3.12.4 ❯ source nx-devdeps/bin/activate  networkx on  main [!?] via  v3.12.4 (nx-devdeps) ❯ pip install --upgrade pip Requirement already satisfied: pip in ./nx-devdeps/lib/python3.12/site-packages (24.0) Collecting pip   Using cached pip-24.1.1-py3-none-any.whl.metadata (3.6 kB) Using cached pip-24.1.1-py3-none-any.whl (1.8 MB) Installing collected packages: pip   Attempting uninstall: pip     Found existing installation: pip 24.0     Uninstalling pip-24.0:       Successfully uninstalled pip-24.0 Successfully installed pip-24.1.1  networkx on  main [!?] via  v3.12.4 (nx-devdeps) took 3s ❯ pip install -U --pre --extra-index-url https://pypi.org/simple -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple numpy scipy matplotlib  Looking in indexes: https://pypi.anaconda.org/scientific-python-nightly-wheels/simple, https://pypi.org/simple Collecting numpy   Downloading https://pypi.anaconda.org/scientific-python-nightly-wheels/simple/numpy/2.1.0.dev0/numpy-2.1.0.dev0-cp312-cp312-macosx_10_9_x86_64.whl (20.9 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.9/20.9 MB 4.5 MB/s eta 0:00:00 WARNING: Cache entry deserialization failed, entry ignored Collecting scipy   Downloading https://pypi.anaconda.org/scientific-python-nightly-wheels/simple/scipy/1.15.0.dev0/scipy-1.15.0.dev0-cp312-cp312-macosx_10_13_x86_64.whl (39.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.1/39.1 MB 7.2 MB/s eta 0:00:00 WARNING: Cache entry deserialization failed, entry ignored Collecting matplotlib   Downloading https://pypi.anaconda.org/scientific-python-nightly-wheels/simple/matplotlib/3.10.0.dev344%2Bg651e9109e9/matplotlib-3.10.0.dev344%2Bg651e9109e9-cp312-cp312-macosx_10_12_x86_64.whl (7.9 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 8.5 MB/s eta 0:00:00 WARNING: Cache entry deserialization failed, entry ignored Collecting contourpy>=1.0.1 (from matplotlib)   Downloading https://pypi.anaconda.org/scientific-python-nightly-wheels/simple/contourpy/1.3.0.dev1/contourpy-1.3.0.dev1-cp312-cp312-macosx_10_9_x86_64.whl (267 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 267.8/267.8 kB 7.3 MB/s eta 0:00:00 Collecting cycler>=0.10 (from matplotlib)   Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB) Collecting fonttools>=4.22.0 (from matplotlib)   Using cached fonttools-4.53.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (162 kB) Collecting kiwisolver>=1.3.1 (from matplotlib)   Using cached kiwisolver-1.4.5-cp312-cp312-macosx_10_9_x86_64.whl.metadata (6.4 kB) Collecting packaging>=20.0 (from matplotlib)   Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB) Collecting pillow>=8 (from matplotlib)   Using cached pillow-10.4.0-cp312-cp312-macosx_10_10_x86_64.whl.metadata (9.2 kB) Collecting pyparsing>=2.3.1 (from matplotlib)   Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB) Collecting python-dateutil>=2.7 (from matplotlib)   Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB) Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib)   Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB) Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB) Using cached fonttools-4.53.0-cp312-cp312-macosx_10_9_universal2.whl (2.8 MB) Using cached kiwisolver-1.4.5-cp312-cp312-macosx_10_9_x86_64.whl (67 kB) Using cached packaging-24.1-py3-none-any.whl (53 kB) Using cached pillow-10.4.0-cp312-cp312-macosx_10_10_x86_64.whl (3.5 MB) Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB) Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB) Using cached six-1.16.0-py2.py3-none-any.whl (11 kB) Installing collected packages: six, pyparsing, pillow, packaging, numpy, kiwisolver, fonttools, cycler, scipy, python-dateutil, contourpy, matplotlib Successfully installed contourpy-1.3.0.dev1 cycler-0.12.1 fonttools-4.53.0 kiwisolver-1.4.5 matplotlib-3.10.0.dev344+g651e9109e9 numpy-2.1.0.dev0 packaging-24.1 pillow-10.4.0 pyparsing-3.1.2 python-dateutil-2.9.0.post0 scipy-1.15.0.dev0 six-1.16.0  networkx on  main [!?] via  v3.12.4 (nx-devdeps) took 45s ❯ pip install -r requirements/test.txt Collecting pytest>=7.2 (from -r requirements/test.txt (line 3))   Using cached pytest-8.2.2-py3-none-any.whl.metadata (7.6 kB) Collecting pytest-cov>=4.0 (from -r requirements/test.txt (line 4))   Using cached pytest_cov-5.0.0-py3-none-any.whl.metadata (27 kB) Collecting iniconfig (from pytest>=7.2->-r requirements/test.txt (line 3))   Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB) Requirement already satisfied: packaging in ./nx-devdeps/lib/python3.12/site-packages (from pytest>=7.2->-r requirements/test.txt (line 3)) (24.1) Collecting pluggy<2.0,>=1.5 (from pytest>=7.2->-r requirements/test.txt (line 3))   Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB) Collecting coverage>=5.2.1 (from coverage[toml]>=5.2.1->pytest-cov>=4.0->-r requirements/test.txt (line 4))   Using cached coverage-7.5.4-cp312-cp312-macosx_10_9_x86_64.whl.metadata (8.2 kB) Using cached pytest-8.2.2-py3-none-any.whl (339 kB) Using cached pytest_cov-5.0.0-py3-none-any.whl (21 kB) Using cached coverage-7.5.4-cp312-cp312-macosx_10_9_x86_64.whl (205 kB) Using cached pluggy-1.5.0-py3-none-any.whl (20 kB) Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB) Installing collected packages: pluggy, iniconfig, coverage, pytest, pytest-cov Successfully installed coverage-7.5.4 iniconfig-2.0.0 pluggy-1.5.0 pytest-8.2.2 pytest-cov-5.0.0  networkx on  main [!?] via  v3.12.4 (nx-devdeps) took 3s ❯ pip install -e . Obtaining file:///Users/admin/Documents/foss/networkx   Installing build dependencies ... done   Checking if build backend supports build_editable ... done   Getting requirements to build editable ... done   Preparing editable metadata (pyproject.toml) ... done Building wheels for collected packages: networkx   Building editable for networkx (pyproject.toml) ... done   Created wheel for networkx: filename=networkx-3.4rc0.dev0-0.editable-py3-none-any.whl size=5995 sha256=0af4a87bfb331a19366080e761136fe6dedf4a892e8961783371d031749bd274   Stored in directory: /private/var/folders/yr/38162y1s2qx3lvcqgh1k9_p00000gn/T/pip-ephem-wheel-cache-0qrngqpk/wheels/9b/d5/ab/ee13bb19f70a858ef59738b9a81521cc40e52d8757750c3f6a Successfully built networkx Installing collected packages: networkx Successfully installed networkx-3.4rc0.dev0  networkx on  main [!?] via  v3.12.4 (nx-devdeps) took 9s ❯ pip list Package         Version                   Editable project location --------------- ------------------------- ------------------------------------ contourpy       1.3.0.dev1 coverage        7.5.4 cycler          0.12.1 fonttools       4.53.0 iniconfig       2.0.0 kiwisolver      1.4.5 matplotlib      3.10.0.dev344+g651e9109e9 networkx        3.4rc0.dev0               /Users/admin/Documents/foss/networkx numpy           2.1.0.dev0 packaging       24.1 pillow          10.4.0 pip             24.1.1 pluggy          1.5.0 pyparsing       3.1.2 pytest          8.2.2 pytest-cov      5.0.0 python-dateutil 2.9.0.post0 scipy           1.15.0.dev0 six             1.16.0  networkx on  main [!?] via  v3.12.4 (nx-devdeps) ❯ pytest networkx/algorithms/centrality/tests/test_current_flow_betweenness_centrality.py =================================================== test session starts ==================================================== platform darwin -- Python 3.12.4, pytest-8.2.2, pluggy-1.5.0 rootdir: /Users/admin/Documents/foss/networkx configfile: pyproject.toml plugins: cov-5.0.0 collected 20 items  networkx/algorithms/centrality/tests/test_current_flow_betweenness_centrality.py FFFFFFFFFFFFFFFFF... ```  </details>  It'll be hard to see without terminal highlighting, but the nightly installs all printed `WARNING: Cache entry deserialization failed, entry ignored`. I don't think it's related to our problem here, but I thought I'd point it out. @rossbar perhaps it's a Python version thing? Do you happen to have a way of trying on 3.12.4?
comment
I see your point @rossbar; the original implementation clearly wasn't an oversight; it just assigned a different meaning to the generation parameter. From a quick Google search, it appears that [Wolfram MathWorld](https://mathworld.wolfram.com/Dorogovtsev-Goltsev-MendesGraph.html) uses the same parameter as the current implementation, i.e. starting at `n = 0`. GitHub also finds about 4.1k matches for "dorogovtsev_goltsev_mendes_graph," which makes me hesitant to push a breaking change.  I agree that the documentation should still explicitly mention this difference (and that the formulas for number of edges and nodes should be corrected, obviously). If this sounds good to everyone, I'll update #7473 accordingly.
comment
To add on to that, you can run `pre-commit install` which will automatically run the relevant pre-commit hooks whenever you try to `git commit`.
comment
Could you apply the same indent rules for the citations in the test files? Thanks.
comment
The test file references still have one space too many.
comment
@epogrebnyak the links you need are:  - For the conference proceedings: http://conference.scipy.org.s3-website-us-east-1.amazonaws.com/proceedings/scipy2008/index.html - For the NetworkX paper: http://conference.scipy.org.s3-website-us-east-1.amazonaws.com/proceedings/scipy2008/paper_2/ - For the PDF: http://conference.scipy.org.s3-website-us-east-1.amazonaws.com/proceedings/scipy2008/paper_2/full_text.pdf - For the BibTex entry: http://conference.scipy.org.s3-website-us-east-1.amazonaws.com/proceedings/scipy2008/paper_2/reference.bib  Let me know if you want to update these yourself or if you want me to do it.
