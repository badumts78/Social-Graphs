{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/suneman/socialgraphs2025/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow those simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Tuesday November 4th, 2025 at 23:55. Hand in your IPython notebook file (with extension `.ipynb`) via DTU Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercises below, I describe the exercises in a general way. Drawing in the right parts of the exercises is part of the assignment. (That way we're helping you get a little bit more ready for the Final Project, where you have to decide what information to include in your report and analysis). \n",
    "\n",
    "# Part 1: Analyze the network\n",
    "\n",
    "The questions in this part are based on Lecture 5.\n",
    "\n",
    "* Present an analysis/description of the network of bands/artists using tools from Lecture 5. Imagine that you have been tasked with presenting the important facts about the network to an audience who knows about network science, but doesn't know about this particular network.\n",
    "   - It's OK to also use basic concepts like degree distributions (even though they're from week 4) in your analysis. That way you can make the analysis a standalone, coherent thing.\n",
    "   - I would like you to include concepts like centrality and assortativity in your analysis.\n",
    "   - Use a network backbone in your analysis.\n",
    "   - In addition to standard distribution plots (e.g. degree distributions, etc), your analysis should also include at least one network visualization (but it doesn't have to display the entire network, you can also visualize a network backbone).\n",
    "   - **Note**: As I write above, an important part of the exercise consists is *selecting the right elements of the lecture* to create a meaningful analysis. So don't solve this part by going exhaustive and just calculating everything you can think of in one massive analysis. Try to focus on using what you've learned to characterize the network. \n",
    "\n",
    "# Part 2: Genres and communities and plotting \n",
    "\n",
    "The questions below are based on Lecture 7, part 2.\n",
    "\n",
    "* Write about genres and modularity.\n",
    "* Detect the communities, discuss the value of modularity in comparison to the genres.\n",
    "* Calculate the matrix $D$ and discuss your findings.\n",
    "* Plot the communities and comment on your results.\n",
    "\n",
    "# Part 3: TF-IDF to understand genres and communities \n",
    "\n",
    "The questions below  are based on Lecture 7, part 2, 4, 5, 6 (and a little bit on part 3).\n",
    "\n",
    "* Explain the concept of TF-IDF in your own words and how it can help you understand the genres and communities.\n",
    "* Calculate and visualize TF-IDF for the genres and communities.\n",
    "* Use the matrix $D$ (Lecture 7, part 2) to dicusss the difference between the word-clouds between genres and communities.\n",
    "\n",
    "# Part 4: Sentiment of the artists and communities\n",
    "\n",
    "The questions below are based on Lecture 8\n",
    "\n",
    "* Calculate the sentiment of the band/artist pages (it is OK to work with the sub-network of artists-with-genre) and describe your findings using stats and visualization, inspired by the first exercise of week 8.\n",
    "* Discuss the sentiment of the communities. Do the findings using TF-IDF during Lecture 7 help you understand your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import statistics\n",
    "from scipy.stats import pearsonr\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "files = os.listdir(\"Musicians\")\n",
    "\n",
    "for file in files:\n",
    "    filepath = \"Musicians/\" + file\n",
    "    with open(filepath, \"r\") as f:\n",
    "        G.add_node(file.replace(\".txt\", ''), length_of_content = len(f.read().split()))\n",
    "\n",
    "for file in files:\n",
    "    filepath = \"Musicians/\" + file\n",
    "    with open(filepath, \"r\") as f:\n",
    "        filecontent = f.read()\n",
    "\n",
    "        links = re.findall(r'\\[\\[([^|\\]#]+)(?:#[^\\]]*)?(?:\\|([^\\]]+))?\\]\\]', filecontent)\n",
    "\n",
    "        for link, _ in links:\n",
    "            link = link.replace(\"\\\\\", \"\")\n",
    "            link = link.replace('/', ' ')\n",
    "            if link in list(G.nodes):\n",
    "                G.add_edge(file.replace(\".txt\", ''), link.replace('/', ' '))\n",
    "\n",
    "noConnections = [node for node in G.nodes if (G.out_degree(node) == 0 and G.in_degree(node) == 0)]\n",
    "\n",
    "G.remove_nodes_from(noConnections)\n",
    "\n",
    "largest_wcc = max(nx.weakly_connected_components(G), key=len)\n",
    "\n",
    "GL = G.subgraph(largest_wcc).copy()\n",
    "\n",
    "G_und = GL.to_undirected() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_genre(genre_text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Cleans, lowercases, and normalizes a single genre string.\n",
    "    \"\"\"\n",
    "    # Clean and lowercase\n",
    "    g = genre_text.lower().strip()\n",
    "    \n",
    "    if not g:\n",
    "        return None\n",
    "    \n",
    "    g = g.replace('-', ' ')\n",
    "        \n",
    "    # Normalize \"rock'n'roll\", \"rock & roll\", etc.\n",
    "    # This rule is intentionally broad to catch variations.\n",
    "    if 'rock' in g and ('roll' in g or '’n’' in g or '&' in g or \"'n'\" in g):\n",
    "        return 'rock and roll'\n",
    "    \n",
    "    g = g.replace('&', 'and')\n",
    "    \n",
    "    if 'early' in g:\n",
    "        g = g.replace(' (early)', '')\n",
    "    \n",
    "    if 'later' in g:\n",
    "        g = g.replace(' (later)', '')\n",
    "        \n",
    "        \n",
    "    # Add any other normalization rules here\n",
    "    # For example:\n",
    "    # if g == 'r&b':\n",
    "    #     return 'r&b'\n",
    "        \n",
    "    return g\n",
    "\n",
    "def extract_all_genres(directory_path: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extracts genres for all artists from wikitext files in a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: The path to the folder containing artist .txt files.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping artist names to a list of their genres.\n",
    "    \"\"\"\n",
    "    artist_genres_map: Dict[str, List[str]] = {}\n",
    "    p = Path(directory_path)\n",
    "    \n",
    "    if not p.is_dir():\n",
    "        print(f\"Error: Directory not found at {directory_path}\")\n",
    "        return {}\n",
    "\n",
    "    # Define regex patterns\n",
    "    # 1. Finds the main infobox\n",
    "    infobox_re = re.compile(\n",
    "        r\"\\{\\{Infobox musical artist[\\s\\S]*?^\\}\\}\", \n",
    "        re.MULTILINE | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # 2. Finds the genre line within the infobox\n",
    "    # This version stops when it sees the *next parameter* (e.g. \"| label =\")\n",
    "    # or the end of the infobox, rather than just any line starting with \"|\".\n",
    "    genre_line_re = re.compile(\n",
    "        r\"\\| *genre *= *([\\s\\S]*?)\\n *(?=\\| *[a-z_ ]+ *=|^\\}\\})\", \n",
    "        re.MULTILINE | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # 3-7. Cleaning regex patterns\n",
    "    link_display_re = re.compile(r\"\\[\\[[^|\\]]+\\|([^\\]]+)\\]\\]\") # [[link|display]] -> display\n",
    "    link_re = re.compile(r\"\\[\\[([^\\]]+)\\]\\]\")                 # [[link]] -> link\n",
    "    # NEW: Separator for adjacent links like ]] [[\n",
    "    link_separator_re = re.compile(r\"\\]\\](\\s+)\\[\\[\")\n",
    "    \n",
    "    list_template_re = re.compile(                           # {{hlist|a|b}} -> a|b\n",
    "        r\"\\{\\{(?:hlist|flatlist|unbulleted list)\\s*\\|([\\s\\S]*?)\\}\\}\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    other_template_re = re.compile(r\"\\{\\{[^}]+\\}\\}\")         # {{template}} -> \"\"\n",
    "    comment_re = re.compile(r\"<!--[\\s\\S]*?-->\")              # <!-- comment --> -> \"\"\n",
    "    ref_re = re.compile(r\"<ref[\\s\\S]*?(/>|</ref>)\", re.IGNORECASE) # <ref ... /> -> \"\"\n",
    "    # UPDATED: Add <br> variants to the separator list\n",
    "    separator_re = re.compile(r\"[|\\n*]|<br */?>\", re.IGNORECASE) # |, \\n, *, <br> -> ,\n",
    "\n",
    "    for file_path in p.glob('*.txt'):\n",
    "        artist_name = file_path.stem\n",
    "        wikitext = \"\"\n",
    "        \n",
    "        try:\n",
    "            # Read the raw content\n",
    "            raw_content = file_path.read_text(encoding='utf-8')\n",
    "            \n",
    "            # Try to parse as the 'ast' structure from your example\n",
    "            try:\n",
    "                data = ast.literal_eval(raw_content)\n",
    "                page_id = list(data.keys())[0] # Get the first (only) page ID\n",
    "                wikitext = data[page_id]['revisions'][0]['*']\n",
    "            except (ValueError, SyntaxError, KeyError, IndexError, TypeError):\n",
    "                # Fallback: assume the file is just raw wikitext\n",
    "                wikitext = raw_content\n",
    "                \n",
    "            if not wikitext:\n",
    "                print(f\"No wikitext found for {artist_name}\")\n",
    "                continue\n",
    "\n",
    "            # 1. Find infobox\n",
    "            infobox_match = infobox_re.search(wikitext)\n",
    "            if not infobox_match:\n",
    "                # print(f\"No infobox found for {artist_name}\")\n",
    "                continue\n",
    "            \n",
    "            infobox_text = infobox_match.group(0)\n",
    "\n",
    "            # 2. Find genre line\n",
    "            genre_match = genre_line_re.search(infobox_text)\n",
    "            if not genre_match:\n",
    "                # print(f\"No genre line found for {artist_name}\")\n",
    "                continue\n",
    "                \n",
    "            content = genre_match.group(1)\n",
    "\n",
    "            # --- Start cleaning the extracted genre content ---\n",
    "            # The order of these operations is critical.\n",
    "            \n",
    "            # 0. Replace HTML entities like &nbsp;\n",
    "            content = content.replace('&nbsp;', ' ')\n",
    "            \n",
    "            # 1. Remove HTML comments <!-- ... -->\n",
    "            content = comment_re.sub(\"\", content)\n",
    "            # 2. Remove <ref> tags (which can contain nested templates)\n",
    "            content = ref_re.sub(\"\", content)\n",
    "            \n",
    "            # 3. NEW: Insert a separator (pipe) between adjacent links: ]] [[ -> ]] | [[\n",
    "            # This handles cases like [[hard rock]] [[arena rock]]\n",
    "            content = link_separator_re.sub(r\"]]|\\1[[\", content)\n",
    "            \n",
    "            # 4. Simplify hlist/flatlist templates: {{hlist|a|b|c}} -> a|b|c\n",
    "            content = list_template_re.sub(r\"\\1\", content)\n",
    "            # 5. Remove any other simple templates {{template}} -> \"\"\n",
    "            content = other_template_re.sub(\"\", content)\n",
    "            # 6. Resolve links: [[link|display]] -> display\n",
    "            content = link_display_re.sub(r\"\\1\", content)\n",
    "            # 7. Resolve links: [[link]] -> link\n",
    "            content = link_re.sub(r\"\\1\", content)\n",
    "            # 8. Replace common separators (pipe, newline, asterisk, <br>) with commas\n",
    "            content = separator_re.sub(\",\", content)\n",
    "            \n",
    "            # 9. Final cleanup: strip whitespace and any trailing template braces\n",
    "            content = content.strip().rstrip('}')\n",
    "            # --- Done cleaning ---\n",
    "\n",
    "            # 10. Split by comma and process each potential genre\n",
    "            potential_genres = content.split(',')\n",
    "            genres_list = []\n",
    "            \n",
    "            for g_text in potential_genres:\n",
    "                # Extra cleanup for trailing braces on individual items\n",
    "                g_clean = g_text.strip().rstrip('}')\n",
    "                \n",
    "                # Skip any empty strings or leftover template/HTML bits\n",
    "                # ADDED: Filter out template parameters (like 'italic=no' by checking for '=')\n",
    "                # ADDED: Filter out very short items (like 'de' by checking length > 2)\n",
    "                if (g_clean and\n",
    "                    not g_clean.startswith('<') and\n",
    "                    not g_clean.startswith('{') and\n",
    "                    'class=' not in g_clean and\n",
    "                    '=' not in g_clean and\n",
    "                    len(g_clean) > 2):\n",
    "                    \n",
    "                    normalized = normalize_genre(g_clean)\n",
    "                    if normalized:\n",
    "                        genres_list.append(normalized)\n",
    "\n",
    "            # 11. Add to map if we found any genres\n",
    "            if genres_list:\n",
    "                # Remove duplicates and sort\n",
    "                unique_genres = list(set(genres_list))\n",
    "                artist_genres_map[artist_name] = unique_genres\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {artist_name} ({file_path.name}): {e}\")\n",
    "            \n",
    "    return artist_genres_map\n",
    "\n",
    "\n",
    "genre_dict = extract_all_genres(\"Musicians\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Analyze the network\n",
    "\n",
    "**We want to present the some important facts about the network that results from the links between pages from rock musicians on wikipedia.**\n",
    "\n",
    "**We start of with some basic facts on the network:**\n",
    "\n",
    "**What is the number of nodes?**\n",
    "\n",
    "**What is the number of edges?**\n",
    "\n",
    "**What are the average, median, mode, minimum and maximum value of the in-degree and of the out-degree?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degrees = dict(GL.in_degree())\n",
    "out_degrees = dict(GL.out_degree())\n",
    "\n",
    "nodes = list(GL.nodes())\n",
    "x_out = [out_degrees[n] for n in nodes]\n",
    "y_in  = [in_degrees[n] for n in nodes]\n",
    "\n",
    "def compute_stats(values):\n",
    "    return {\n",
    "        \"average\": sum(values) / len(values),\n",
    "        \"median\": statistics.median(values),\n",
    "        \"mode\": statistics.mode(values),\n",
    "        \"min\": min(values),\n",
    "        \"max\": max(values) ,\n",
    "    }\n",
    "\n",
    "in_stats = compute_stats(y_in)\n",
    "out_stats = compute_stats(x_out)\n",
    "\n",
    "print(f\"The network has {len(GL.nodes)} nodes and {len(GL.edges)} edges.\")\n",
    "print(f\"The average degree of the nodes is {in_stats[\"average\"]:.2f}.\")\n",
    "print(f\"The median of the in-degrees is {in_stats[\"median\"]}, the mode is {in_stats[\"mode\"]}, the minimum is {in_stats[\"min\"]}, and the maximum {in_stats[\"max\"]}.\")\n",
    "print(f\"If we compare this to the out-degrees, we notice that the out-degree distribution seems to be different, since the median is {out_stats[\"median\"]}, the mode is {out_stats[\"mode\"]}, the minimum is {out_stats[\"min\"]}, and the maximum {out_stats[\"max\"]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To investigate this further, we will now plot the in- and out-degree distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_bins = 50\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist([x for x in y_in], bins=number_bins)\n",
    "plt.xlabel(\"In-degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"In-degree Distribution Musicians\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist([x for x in x_out], bins=number_bins)\n",
    "plt.xlabel(\"Out-degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Out-degree Distribution Musicians\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most musicians have an in-degree of 0 or a very small one, and a few musicians have a very high in-degree. The out-degree follows more of a normal distribution. Our theory for why this is the case is that many musicians draw inspiration from a very small group of highly influential musicians. \n",
    "\n",
    "**To investige this further we will now look into the degree distributions of the individual musicians in more detail.**\n",
    "\n",
    "**First we will create a scatter plot, where each point is a musician, and the axes show in- versus out-degree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_out, y_in, alpha=0.6, s=30)\n",
    "\n",
    "print(y_in[nodes.index(\"Guns N' Roses\")])\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    if x_out[i] > 60 or y_in[i] > 60:\n",
    "        plt.text(x_out[i]+0.1, y_in[i]+0.1, str(node), fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Out-degree\")\n",
    "plt.ylabel(\"In-degree\")\n",
    "plt.title(\"Scatter plot of In-degree vs Out-degree\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatter plot shows the in- versus out-degree for each musician. To avoid too much overlapping of names, only the nodes that have an in- or out-degree over 60 have a label.\n",
    "\n",
    "We notice that there is indeed some few musicians with a very high in-degree, who are all well known, which supports our theory from above.\n",
    "\n",
    "We also notice that most musicians have an in- and out-degree both less than 20. However, we can also see not two much due to the many nodes in that area. \n",
    "\n",
    "**So we will create a heatmap in the next step, that zooms in on just the in-degree $[0,20]$ and out-degree $[0,20]$ area of the plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.hist2d(\n",
    "    x_out,\n",
    "    y_in,\n",
    "    bins=[20, 20],\n",
    "    range=[[0, 20], [0, 20]],\n",
    "    cmap='viridis'\n",
    ")\n",
    "\n",
    "plt.colorbar(label=\"Number of musicians\")\n",
    "plt.xlabel(\"Out-degree\")\n",
    "plt.ylabel(\"In-degree\")\n",
    "plt.title(\"Heatmap of In-degree vs Out-degree (zoomed 0-20)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this heatmap we can see again support for our theory. We see that the majority of musicians has lower in-degree then out-degree.\n",
    "\n",
    "**To further support our theory, we will now calculate the pearson correlation between the length of the wikipedia articals and the in-degree, since longer wikipedia articiles could also be a sign of famous characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [G.nodes[n].get('length_of_content', 0) for n in G.nodes()]\n",
    "in_degrees_pearson = [G.in_degree(n) for n in G.nodes()]\n",
    "\n",
    "pearson_in  = pearsonr(lengths, in_degrees_pearson)\n",
    "print(f\"The pearson correlation is {pearson_in.correlation:.4f}, which is a very high correlation, which again supports our theory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Genres and communities and plotting \n",
    "\n",
    "**We extracted the genre of all musicians with help of the infoboxes of Wikipedia. We start investigating the genres my calculating the number of nodes for which we could find a genre, the average number of genres per node, the total number of distinct genres, and a histogram showing artist counts for the top 15 genres.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As suggested in the part 2 of week 5, we used an LLM to extract the genres and report on the statistics, with extensive testing and revisions to ensure everything works as expected\n",
    "\n",
    "def report_genre_stats(artist_genres_map: Dict[str, List[str]]):\n",
    "    \"\"\"\n",
    "    Calculates and prints genre statistics, then displays a bar chart.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Genre Statistics ---\")\n",
    "    \n",
    "    # 1. Number of nodes with genres\n",
    "    num_nodes_with_genres = len(artist_genres_map)\n",
    "    if num_nodes_with_genres == 0:\n",
    "        print(\"No genres found for any artists.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Nodes with genres found: {num_nodes_with_genres}\")\n",
    "\n",
    "    # 2. Average number of genres per node\n",
    "    total_genres_assigned = sum(len(genres) for genres in artist_genres_map.values())\n",
    "    avg_genres = total_genres_assigned / num_nodes_with_genres\n",
    "    print(f\"Average genres per node: {avg_genres:.2f}\")\n",
    "\n",
    "    # 3. Total number of distinct genres\n",
    "    all_genres_list = [genre for genres in artist_genres_map.values() for genre in genres]\n",
    "    distinct_genres = set(all_genres_list)\n",
    "    num_distinct_genres = len(distinct_genres)\n",
    "    print(f\"Total distinct genres:   {num_distinct_genres}\")\n",
    "\n",
    "    # 4. Histogram for top 15 genres\n",
    "    \n",
    "    genre_counts = Counter(all_genres_list)\n",
    "    top_15_genres = genre_counts.most_common(15)\n",
    "    \n",
    "    if not top_15_genres:\n",
    "        print(\"No genre data to display.\")\n",
    "        return\n",
    "\n",
    "    # Unzip the data for plotting\n",
    "    # We reverse the lists [::-1] so the highest bar is at the top\n",
    "    genres = [genre for genre, count in top_15_genres][::-1]\n",
    "    counts = [count for genre, count in top_15_genres][::-1]\n",
    "\n",
    "    # Create horizontal bar plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(genres, counts)\n",
    "    plt.xlabel('Number of Artists')\n",
    "    plt.ylabel('Genre')\n",
    "    plt.title('Top 15 Most Common Genres')\n",
    "    \n",
    "    # Add counts to the end of the bars\n",
    "    for i, v in enumerate(counts):\n",
    "        plt.text(v + 0.5, i, str(v), color='blue', va='center')\n",
    "\n",
    "    plt.tight_layout()  # Adjust plot to prevent label overlap\n",
    "    plt.show()          # Display the plot\n",
    "\n",
    "report_genre_stats(genre_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that almost all of the nodes have at least one genre, which allows us to maybe use it as a good measurement for detecting communities in the network. However, each artist has an average of 3.75 genres, so the question is which one to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_genre_modularity(artist_genres_map: Dict[str, List[str]], which_mode):    \n",
    "    \"\"\"\n",
    "    Calculates the modularity of a network based on genre communities.\n",
    "    \n",
    "    The partition is created by assigning each node to the *first* genre\n",
    "    in its list.\n",
    "    \n",
    "    Args:\n",
    "        G_und: The full undirected NetworkX graph.\n",
    "        artist_genres_map: The dictionary mapping artists to genre lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting modularity calculation...\")\n",
    "    \n",
    "    # 1. Work from the undirected version, keeping only nodes with genre info\n",
    "    nodes_with_genres = set(artist_genres_map.keys())\n",
    "    nodes_in_graph = set(G_und.nodes())\n",
    "    \n",
    "    valid_nodes = nodes_with_genres.intersection(nodes_in_graph)\n",
    "    \n",
    "    if not valid_nodes:\n",
    "        print(\"Error: No nodes from your graph were found in the genre map.\")\n",
    "        return\n",
    "\n",
    "    # Create the filtered subgraph. .copy() is important!\n",
    "    G_filtered = G_und.subgraph(valid_nodes).copy()\n",
    "    print(f\"Filtered graph created: {G_filtered.number_of_nodes()} nodes, {G_filtered.number_of_edges()} edges.\")\n",
    "    \n",
    "    if G_filtered.number_of_edges() == 0:\n",
    "        print(\"Filtered graph has no edges. Modularity is 0.\")\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Create the partition: {node: first_genre}\n",
    "    partition = {}\n",
    "    for node in G_filtered.nodes():\n",
    "        # Check that the node (which we know is in the map) has genres\n",
    "        if artist_genres_map.get(node): # Use .get() for safety\n",
    "            if which_mode == 0:\n",
    "                first_genre = artist_genres_map[node][0]\n",
    "            elif which_mode == 1:\n",
    "                temp = 0\n",
    "                if len(artist_genres_map[node]) > 1:\n",
    "                    temp = 1\n",
    "                first_genre = artist_genres_map[node][temp]\n",
    "            else:\n",
    "                first_genre = artist_genres_map[node][random.randint(0, len(artist_genres_map[node])-1)]\n",
    "            partition[node] = first_genre\n",
    "    \n",
    "    # We must remove nodes from the filtered graph that ended up with no\n",
    "    # genre (e.g., if their list was empty, which shouldn't happen with\n",
    "    # the current extract_all_genres, but it's good practice)\n",
    "    nodes_in_partition = set(partition.keys())\n",
    "    if not nodes_in_partition:\n",
    "        print(\"Error: No nodes could be assigned to a partition.\")\n",
    "        return 0.0\n",
    "        \n",
    "    G_partitioned = G_filtered.subgraph(nodes_in_partition).copy()\n",
    "    \n",
    "    L = G_partitioned.number_of_edges()\n",
    "    \n",
    "    if L == 0:\n",
    "        print(\"Partitioned graph has no edges. Modularity is 0.\")\n",
    "        return 0.0\n",
    "\n",
    "    two_L = 2.0 * L\n",
    "    print(f\"Partitioned graph created: {G_partitioned.number_of_nodes()} nodes, {L} edges.\")\n",
    "    \n",
    "    # 3. Calculate Modularity using Equation 9.12\n",
    "    # M = Σ_c [ (L_c / L) - (k_c / 2L)^2 ]\n",
    "    \n",
    "    print(f\"Calculating modularity from formula (Eq 9.12)...\")\n",
    "\n",
    "    # 3a. Get all unique communities and their nodes\n",
    "    communities = {} # {community_name: [node1, node2, ...]}\n",
    "    for node, comm in partition.items():\n",
    "        if comm not in communities:\n",
    "            communities[comm] = []\n",
    "        communities[comm].append(node)\n",
    "    \n",
    "    print(f\"Found {len(communities)} unique communities (genres).\")\n",
    "    \n",
    "    # 3b. Get L (total edges) and 2L\n",
    "    # L and two_L are already defined above\n",
    "    degrees = dict(G_partitioned.degree())\n",
    "    \n",
    "    # 3c. Sum terms for each community\n",
    "    modularity_sum = 0.0\n",
    "    for comm_name, nodes_in_comm in communities.items():\n",
    "        \n",
    "        # Calculate L_c: number of edges *within* the community\n",
    "        # Create a subgraph view of just the nodes in this community\n",
    "        comm_subgraph = G_partitioned.subgraph(nodes_in_comm)\n",
    "        L_c = comm_subgraph.number_of_edges()\n",
    "        \n",
    "        # Calculate k_c: sum of degrees of all nodes in the community\n",
    "        # (These are degrees from the *full* partitioned graph)\n",
    "        k_c = 0.0\n",
    "        for node in nodes_in_comm:\n",
    "            k_c += degrees[node]\n",
    "            \n",
    "        # Calculate the term for this community\n",
    "        term_1 = L_c / L\n",
    "        term_2 = (k_c / two_L)**2\n",
    "        \n",
    "        modularity_sum += (term_1 - term_2)\n",
    "        \n",
    "    M = modularity_sum\n",
    "        \n",
    "    return M\n",
    "\n",
    "calculate_genre_modularity(genre_dict, 3)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
